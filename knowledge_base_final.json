{
  "https://www.youtube.com/watch?v=00Q0G84kq3M&t=536s": {
    "title": "RAG vs. Fine Tuning",
    "url": "https://www.youtube.com/watch?v=00Q0G84kq3M&t=536s",
    "description": "Get the guide to GAI, learn more → https://ibm.biz/BdKTbF\nLearn more about the technology → https://ibm.biz/BdKTbX\n\nJoin Cedric Clyburn as he explores the differences and use cases of Retrieval Augmented Generation (RAG) and fine-tuning in enhancing large language models. This video covers the strengths, weaknesses, and common applications of both techniques, and provides insights on how to choose between them using machine learning and natural language processing principles\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://www.ibm.com/account/reg/us-en/signup?formid=news-urx-52120\n\n#AI #LargeLanguageModels #FineTuning #RAG #ReinforcementLearning #MachineLearning #NaturalLanguageProcessing",
    "duration": 537,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en let's talk about rag versus fine-tuning let's talk about rag versus fine-tuning let's talk about rag versus fine-tuning now they're both powerful ways to now they're both powerful ways to now they're both powerful ways to enhance the capabilities of large enhance the capabilities of large enhance the capabilities of large language models but today you're going language models but today you're going language models but today you're going to learn about their strengths their use to learn about their strengths their use to learn about their strengths their use cases and how you can choose between cases and how you can choose between cases and how you can choose between them so one of the biggest issues with them so one of the biggest issues with them so one of the biggest issues with dealing with generative AI right now is dealing with generative AI right now is dealing with generative AI right now is one enhancing the models but also to one enhancing the models but also to one enhancing the models but also to dealing with their limitations for dealing with their limitations for dealing with their limitations for example I just recently asked my example I just recently asked my example I just recently asked my favorite llm a simple question who won favorite llm a simple question who won favorite llm a simple question who won the Euro 2024 World Championship and the Euro 2024 World Championship and the Euro 2024 World Championship and while this might seem like a simple while this might seem like a simple while this might seem like a simple query for my model well there's a slight query for my model well there's a slight query for my model well there's a slight issue because the model wasn't trained issue because the model wasn't trained issue because the model wasn't trained on that specific information it can't on that specific information it can't on that specific information it can't give me an accurate or up-to-date answer give me an accurate or up-to-date answer give me an accurate or up-to-date answer at the same time these popular models at the same time these popular models at the same time these popular models are very generalistic and so how do we are very generalistic and so how do we are very generalistic and so how do we think about specializing them for think about specializing them for think about specializing them for specific use cases and adapt them in specific use cases and adapt them in specific use cases and adapt them in Enterprise applications because your Enterprise applications because your Enterprise applications because your data is one of the most important things data is one of the most important things data is one of the most important things that you can work with and in the field that you can work with and in the field that you can work with and in the field of AI using techniques such as rag or of AI using techniques such as rag or of AI using techniques such as rag or fine-tuning will allow you to fine-tuning will allow you to fine-tuning will allow you to supercharge the capabilities that your supercharge the capabilities that your supercharge the capabilities that your application delivers so in the next few application delivers so in the next few application delivers so in the next few minutes we're going to learn about both minutes we're going to learn about both minutes we're going to learn about both of these techniques the differences of these techniques the differences of these techniques the differences between them and where you can start between them and where you can start between them and where you can start seeing and using them in let's get seeing and using them in let's get seeing and using them in let's get started so let's begin with retrieval started so let's begin with retrieval started so let's begin with retrieval augmented generation which is a way to augmented generation which is a way to augmented generation which is a way to increase the capabilities of a model increase the capabilities of a model increase the capabilities of a model through retrieving external and through retrieving external and through retrieving external and up-to-date information augmenting the up-to-date information augmenting the up-to-date information augmenting the original prompt that was given to the original prompt that was given to the original prompt that was given to the model and then generating a response model and then generating a response model and then generating a response back using that context and information back using that context and information back using that context and information and this is really powerful because if and this is really powerful because if and this is really powerful because if we think back about that example of with we think back about that example of with we think back about that example of with the Euro Cup well the model didn't have the Euro Cup well the model didn't have the Euro Cup well the model didn't have the information in context to provide an the information in context to provide an the information in context to provide an answer and this is one of the big answer and this is one of the big answer and this is one of the big limitations of llms but this is limitations of llms but this is limitations of llms but this is mitigated in a way with rag because now mitigated in a way with rag because now mitigated in a way with rag because now instead of having an incorrect or instead of having an incorrect or instead of having an incorrect or possibly um a hallucinated answer we're possibly um a hallucinated answer we're possibly um a hallucinated answer we're able to work with what's known as a able to work with what's known as a able to work with what's known as a corpus of information so this could be corpus of information so this could be corpus of information so this could be data this could be PDFs documents data this could be PDFs documents data this could be PDFs documents spreadsheets things that are relevant to spreadsheets things that are relevant to spreadsheets things that are relevant to our specific organization or knowledge our specific organization or knowledge our specific organization or knowledge that we need to specialize in so when that we need to specialize in so when that we need to specialize in so when the query comes in this time we're the query comes in this time we're the query comes in this time we're working with what's known as a retriever working with what's known as a retriever working with what's known as a retriever that's able to pull the correct doc that's able to pull the correct doc that's able to pull the correct doc doents and Rel relative context to what doents and Rel relative context to what doents and Rel relative context to what the question is and then pass that the question is and then pass that the question is and then pass that knowledge uh as well as the original knowledge uh as well as the original knowledge uh as well as the original prompt to a large language model and prompt to a large language model and prompt to a large language model and with its intuition and pre-trained data with its intuition and pre-trained data with its intuition and pre-trained data it's able to give us a response back it's able to give us a response back it's able to give us a response back based on that contextualized information based on that contextualized information based on that contextualized information uh which is really really powerful uh which is really really powerful uh which is really really powerful because we can start to see that we can because we can start to see that we can because we can start to see that we can get better responses back from a model get better responses back from a model get better responses back from a model with our proprietary and confidential with our proprietary and confidential with our proprietary and confidential information without needing to do any information without needing to do any information without needing to do any retraining on the model uh and this is a retraining on the model uh and this is a retraining on the model uh and this is a great and popular way to enhance the great and popular way to enhance the great and popular way to enhance the capabilities of a model uh without capabilities of a model uh without capabilities of a model uh without having to do any fine-tuning so as the having to do any fine-tuning so as the having to do any fine-tuning so as the name implies what this involves is name implies what this involves is name implies what this involves is taking a large language foundational taking a large language foundational taking a large language foundational model but this time we're going to be model but this time we're going to be model but this time we're going to be specializing it in a certain domain or specializing it in a certain domain or specializing it in a certain domain or area so we're working with labeled and area so we're working with labeled and area so we're working with labeled and targeted data that's going to be targeted data that's going to be targeted data that's going to be provided to the model and and when we do provided to the model and and when we do provided to the model and and when we do some processing we'll have a specialized some processing we'll have a specialized some processing we'll have a specialized model for a specific use case to talk in model for a specific use case to talk in model for a specific use case to talk in a certain style to have a certain tone a certain style to have a certain tone a certain style to have a certain tone that could represent our organization or that could represent our organization or that could represent our organization or company and so then when a model is company and so then when a model is company and so then when a model is queried from um a user or any other type queried from um a user or any other type queried from um a user or any other type of way we'll have a a response that of way we'll have a a response that of way we'll have a a response that gives the correct tone and output or gives the correct tone and output or gives the correct tone and output or specialty in a domain that we'd like to specialty in a domain that we'd like to specialty in a domain that we'd like to receive and this is really important receive and this is really important receive and this is really important because what we're doing is essentially because what we're doing is essentially because what we're doing is essentially baking in this context and intuition baking in this context and intuition baking in this context and intuition into the model um and it's really into the model um and it's really into the model um and it's really important because this is now part of important because this is now part of important because this is now part of the model's weights versus being the model's weights versus being the model's weights versus being supplemented on top with a a technique supplemented on top with a a technique supplemented on top with a a technique like like like rag okay so we understand how both of rag okay so we understand how both of rag okay so we understand how both of these techniques can enhance a model's these techniques can enhance a model's these techniques can enhance a model's accur output and performance but let's accur output and performance but let's accur output and performance but let's take a look at their strengths and take a look at their strengths and take a look at their strengths and weaknesses in some common use cases weaknesses in some common use cases weaknesses in some common use cases because the direction that you go in can because the direction that you go in can because the direction that you go in can greatly affect a model's performance its greatly affect a model's performance its greatly affect a model's performance its accuracy outputs compute cost and much accuracy outputs compute cost and much accuracy outputs compute cost and much much more so let's begin with retrieval much more so let's begin with retrieval much more so let's begin with retrieval augmented generation and something that augmented generation and something that augmented generation and something that I want to point out here is that because I want to point out here is that because I want to point out here is that because we're working with a corpus of we're working with a corpus of we're working with a corpus of information and data this is perfect for information and data this is perfect for information and data this is perfect for dynamic data sources such as databases dynamic data sources such as databases dynamic data sources such as databases uh and other data repositories where we uh and other data repositories where we uh and other data repositories where we want to continuously pull information want to continuously pull information want to continuously pull information and have that up to date for the model and have that up to date for the model and have that up to date for the model to use to use to use understand and at the same time because understand and at the same time because understand and at the same time because we're working with this retriever system we're working with this retriever system we're working with this retriever system and passing in the information as and passing in the information as and passing in the information as context in the prompt well that really context in the prompt well that really context in the prompt well that really helps with hallucinations and providing helps with hallucinations and providing helps with hallucinations and providing the sources for this information is the sources for this information is the sources for this information is really important in systems where we really important in systems where we really important in systems where we need trust and transparency when we're need trust and transparency when we're need trust and transparency when we're using AI so this is fantastic but let's using AI so this is fantastic but let's using AI so this is fantastic but let's also think about this whole system also think about this whole system also think about this whole system because um having this efficient because um having this efficient because um having this efficient retrieval system uh is really important retrieval system uh is really important retrieval system uh is really important in how we select and pick the data that in how we select and pick the data that in how we select and pick the data that we want to provide in that limited we want to provide in that limited we want to provide in that limited context window and so maintaining this context window and so maintaining this context window and so maintaining this is also something that you need to think is also something that you need to think is also something that you need to think about and at the same time what we're about and at the same time what we're about and at the same time what we're doing here in this system is effectively doing here in this system is effectively doing here in this system is effectively supplementing that information on top of supplementing that information on top of supplementing that information on top of the model so we're not essentially the model so we're not essentially the model so we're not essentially enhancing the base model itself we're enhancing the base model itself we're enhancing the base model itself we're just giving it the relative and just giving it the relative and just giving it the relative and contextual information it needs versus contextual information it needs versus contextual information it needs versus fine-tuning is a little bit different fine-tuning is a little bit different fine-tuning is a little bit different because we're actually baking in that because we're actually baking in that because we're actually baking in that context and intuition into the model context and intuition into the model context and intuition into the model well we have greater um influence um in well we have greater um influence um in well we have greater um influence um in essentially how the model behaves and essentially how the model behaves and essentially how the model behaves and reacts in different situations is it an reacts in different situations is it an reacts in different situations is it an insurance adjuster can it summarize insurance adjuster can it summarize insurance adjuster can it summarize documents whatever we want the model to documents whatever we want the model to documents whatever we want the model to do we can essentially use fine tuning in do we can essentially use fine tuning in do we can essentially use fine tuning in order to uh help with that process and order to uh help with that process and order to uh help with that process and at the same time because that is baked at the same time because that is baked at the same time because that is baked into the model's weights itself well into the model's weights itself well into the model's weights itself well that's really great for Speed and that's really great for Speed and that's really great for Speed and inference cost and a variety of other um inference cost and a variety of other um inference cost and a variety of other um factors that come to running models so factors that come to running models so factors that come to running models so for example we can use smaller prompt for example we can use smaller prompt for example we can use smaller prompt context windows in order to get the context windows in order to get the context windows in order to get the responses that we want from the model responses that we want from the model responses that we want from the model and as we begin to special these models and as we begin to special these models and as we begin to special these models they can get smaller and smaller for they can get smaller and smaller for they can get smaller and smaller for specific use case so it's really great specific use case so it's really great specific use case so it's really great for running these specific uh for running these specific uh for running these specific uh specialized models in a variety of use specialized models in a variety of use specialized models in a variety of use cases but at the same time we have the cases but at the same time we have the cases but at the same time we have the same issue of cut off so up until the same issue of cut off so up until the same issue of cut off so up until the point where the model is trained well point where the model is trained well point where the model is trained well after that we have no more additional after that we have no more additional after that we have no more additional information that we can give to the information that we can give to the information that we can give to the model so the same issue that we had with model so the same issue that we had with model so the same issue that we had with the World Cup example so both of these the World Cup example so both of these the World Cup example so both of these have their strengths and weaknesses but have their strengths and weaknesses but have their strengths and weaknesses but let's actually see this in some examples let's actually see this in some examples let's actually see this in some examples and use cases here so when you're and use cases here so when you're and use cases here so when you're thinking about choosing between r and thinking about choosing between r and thinking about choosing between r and fine-tuning it's really important to fine-tuning it's really important to fine-tuning it's really important to consider your AI enabled application consider your AI enabled application consider your AI enabled application priorities and requirements so namely priorities and requirements so namely priorities and requirements so namely this starts off with the data is the this starts off with the data is the this starts off with the data is the data that you're working with slow data that you're working with slow data that you're working with slow moving or is it fast for example if we moving or is it fast for example if we moving or is it fast for example if we need to use uh up-to-date external need to use uh up-to-date external need to use uh up-to-date external information and have that ready information and have that ready information and have that ready contextually every time we use a model contextually every time we use a model contextually every time we use a model then this could be a great use case for then this could be a great use case for then this could be a great use case for rag for example a product documentation rag for example a product documentation rag for example a product documentation chatbot where we can continually update chatbot where we can continually update chatbot where we can continually update the responses with up-to-date the responses with up-to-date the responses with up-to-date information now at the same time let's information now at the same time let's information now at the same time let's think about the industry that you might think about the industry that you might think about the industry that you might be in now fine tuning is really uh be in now fine tuning is really uh be in now fine tuning is really uh powerful for specific industries that powerful for specific industries that powerful for specific industries that have nuances in their writing styles have nuances in their writing styles have nuances in their writing styles terminology vocabulary and so for terminology vocabulary and so for terminology vocabulary and so for example if we have a legal document example if we have a legal document example if we have a legal document summarizer well this could be a perfect summarizer well this could be a perfect summarizer well this could be a perfect use case for fine tuning now let's think use case for fine tuning now let's think use case for fine tuning now let's think about sources this is really important about sources this is really important about sources this is really important right now in having um transparency right now in having um transparency right now in having um transparency behind our models and with rag being behind our models and with rag being behind our models and with rag being able to provide the context and where able to provide the context and where able to provide the context and where the information came from uh is really the information came from uh is really the information came from uh is really really great so this could be a great really great so this could be a great really great so this could be a great use case again for that chatbot for use case again for that chatbot for use case again for that chatbot for retail insurance and a variety of other retail insurance and a variety of other retail insurance and a variety of other uh uh uh specialities where having that uh uh uh specialities where having that uh uh uh specialities where having that source and information in the context of source and information in the context of source and information in the context of the prompt is very important but at the the prompt is very important but at the the prompt is very important but at the same time we may have things such as same time we may have things such as same time we may have things such as past data in our organization that we past data in our organization that we past data in our organization that we can use to train a model so let it be uh can use to train a model so let it be uh can use to train a model so let it be uh accustomed to the data that we're going accustomed to the data that we're going accustomed to the data that we're going to be working with for example again to be working with for example again to be working with for example again that legal summarizer could have past that legal summarizer could have past that legal summarizer could have past data on different legal cases and and data on different legal cases and and data on different legal cases and and documents that we feed it so that it documents that we feed it so that it documents that we feed it so that it understands the situation that's working understands the situation that's working understands the situation that's working in we have better more desirable outputs in we have better more desirable outputs in we have better more desirable outputs so this is cool but I think the best um so this is cool but I think the best um so this is cool but I think the best um situation is a combination of both of situation is a combination of both of situation is a combination of both of these methods so let's say we have a these methods so let's say we have a these methods so let's say we have a financial news reporting service well we financial news reporting service well we financial news reporting service well we could fine-tune it to be uh native to could fine-tune it to be uh native to could fine-tune it to be uh native to the industry of finance and understand the industry of finance and understand the industry of finance and understand all the lingo there uh we could also all the lingo there uh we could also all the lingo there uh we could also give it past data of financial records give it past data of financial records give it past data of financial records and let it understand um how we work in and let it understand um how we work in and let it understand um how we work in that specific industry but also be able that specific industry but also be able that specific industry but also be able to provide the most up-to-date sources to provide the most up-to-date sources to provide the most up-to-date sources for news and data and be able to provide for news and data and be able to provide for news and data and be able to provide that with a level of confidence and that with a level of confidence and that with a level of confidence and transparency and Trust to the end user transparency and Trust to the end user transparency and Trust to the end user who's making that decision and needs to who's making that decision and needs to who's making that decision and needs to know the source and this is really where know the source and this is really where know the source and this is really where a combination of fine-tuning and rag is a combination of fine-tuning and rag is a combination of fine-tuning and rag is so awesome because we can really build so awesome because we can really build so awesome because we can really build amazing applications taking advantage of amazing applications taking advantage of amazing applications taking advantage of both rag as a way to retrieve that both rag as a way to retrieve that both rag as a way to retrieve that information and have it up to date but information and have it up to date but information and have it up to date but fine tuning to specialize our data uh fine tuning to specialize our data uh fine tuning to specialize our data uh but also specialize our model in a but also specialize our model in a but also specialize our model in a certain domain so uh they're both certain domain so uh they're both certain domain so uh they're both wonderful techniques and they have their wonderful techniques and they have their wonderful techniques and they have their strengths but the choice to use one or strengths but the choice to use one or strengths but the choice to use one or combination of both techniques is up to combination of both techniques is up to combination of both techniques is up to you and your specific use case and data you and your specific use case and data you and your specific use case and data so thank you so much for watching uh as so thank you so much for watching uh as so thank you so much for watching uh as always if you have any questions about always if you have any questions about always if you have any questions about fine-tuning rag or all AI related topics fine-tuning rag or all AI related topics fine-tuning rag or all AI related topics let us know in the comment section below let us know in the comment section below let us know in the comment section below don't forget to like the video and don't forget to like the video and don't forget to like the video and subscribe to the channel for more subscribe to the channel for more subscribe to the channel for more content thanks so much for watching",
    "chunks": [
      "Kind: captions Language: en let's talk about rag versus fine-tuning let's talk about rag versus fine-tuning let's talk about rag versus fine-tuning now they're both powerful ways to now they're both powerful ways to now they're both powerful ways to enhance the capabilities of large enhance the capabilities of large enhance the capabilities of large language models but today you're going language models but today you're going language models but today you're going to learn about their strengths",
      "their use to learn about their strengths their use to learn about their strengths their use cases and how you can choose between cases and how you can choose between cases and how you can choose between them so one of the biggest issues with them so one of the biggest issues with them so one of the biggest issues with dealing with generative AI right now is dealing with generative AI right now is dealing with generative AI right now is one enhancing the models but also to one enhancing the models",
      "but also to one enhancing the models but also to dealing with their limitations for dealing with their limitations for dealing with their limitations for example I just recently asked my example I just recently asked my example I just recently asked my favorite llm a simple question who won favorite llm a simple question who won favorite llm a simple question who won the Euro 2024 World Championship and the Euro 2024 World Championship and the Euro 2024 World Championship and while this might",
      "seem like a simple while this might seem like a simple while this might seem like a simple query for my model well there's a slight query for my model well there's a slight query for my model well there's a slight issue because the model wasn't trained issue because the model wasn't trained issue because the model wasn't trained on that specific information it can't on that specific information it can't on that specific information it can't give me an accurate or up-to-date answer give me an",
      "accurate or up-to-date answer give me an accurate or up-to-date answer at the same time these popular models at the same time these popular models at the same time these popular models are very generalistic and so how do we are very generalistic and so how do we are very generalistic and so how do we think about specializing them for think about specializing them for think about specializing them for specific use cases and adapt them in specific use cases and adapt them in specific use cases and",
      "adapt them in Enterprise applications because your Enterprise applications because your Enterprise applications because your data is one of the most important things data is one of the most important things data is one of the most important things that you can work with and in the field that you can work with and in the field that you can work with and in the field of AI using techniques such as rag or of AI using techniques such as rag or of AI using techniques such as rag or fine-tuning will",
      "allow you to fine-tuning will allow you to fine-tuning will allow you to supercharge the capabilities that your supercharge the capabilities that your supercharge the capabilities that your application delivers so in the next few application delivers so in the next few application delivers so in the next few minutes we're going to learn about both minutes we're going to learn about both minutes we're going to learn about both of these techniques the differences of these techniques the differences",
      "of these techniques the differences between them and where you can start between them and where you can start between them and where you can start seeing and using them in let's get seeing and using them in let's get seeing and using them in let's get started so let's begin with retrieval started so let's begin with retrieval started so let's begin with retrieval augmented generation which is a way to augmented generation which is a way to augmented generation which is a way to increase the",
      "capabilities of a model increase the capabilities of a model increase the capabilities of a model through retrieving external and through retrieving external and through retrieving external and up-to-date information augmenting the up-to-date information augmenting the up-to-date information augmenting the original prompt that was given to the original prompt that was given to the original prompt that was given to the model and then generating a response model and then generating a response model",
      "and then generating a response back using that context and information back using that context and information back using that context and information and this is really powerful because if and this is really powerful because if and this is really powerful because if we think back about that example of with we think back about that example of with we think back about that example of with the Euro Cup well the model didn't have the Euro Cup well the model didn't have the Euro Cup well the model",
      "didn't have the information in context to provide an the information in context to provide an the information in context to provide an answer and this is one of the big answer and this is one of the big answer and this is one of the big limitations of llms but this is limitations of llms but this is limitations of llms but this is mitigated in a way with rag because now mitigated in a way with rag because now mitigated in a way with rag because now instead of having an incorrect or instead of",
      "having an incorrect or instead of having an incorrect or possibly um a hallucinated answer we're possibly um a hallucinated answer we're possibly um a hallucinated answer we're able to work with what's known as a able to work with what's known as a able to work with what's known as a corpus of information so this could be corpus of information so this could be corpus of information so this could be data this could be PDFs documents data this could be PDFs documents data this could be PDFs",
      "documents spreadsheets things that are relevant to spreadsheets things that are relevant to spreadsheets things that are relevant to our specific organization or knowledge our specific organization or knowledge our specific organization or knowledge that we need to specialize in so when that we need to specialize in so when that we need to specialize in so when the query comes in this time we're the query comes in this time we're the query comes in this time we're working with what's known as a",
      "retriever working with what's known as a retriever working with what's known as a retriever that's able to pull the correct doc that's able to pull the correct doc that's able to pull the correct doc doents and Rel relative context to what doents and Rel relative context to what doents and Rel relative context to what the question is and then pass that the question is and then pass that the question is and then pass that knowledge uh as well as the original knowledge uh as well as the original",
      "knowledge uh as well as the original prompt to a large language model and prompt to a large language model and prompt to a large language model and with its intuition and pre-trained data with its intuition and pre-trained data with its intuition and pre-trained data it's able to give us a response back it's able to give us a response back it's able to give us a response back based on that contextualized information based on that contextualized information based on that contextualized information",
      "uh which is really really powerful uh which is really really powerful uh which is really really powerful because we can start to see that we can because we can start to see that we can because we can start to see that we can get better responses back from a model get better responses back from a model get better responses back from a model with our proprietary and confidential with our proprietary and confidential with our proprietary and confidential information without needing to do any",
      "information without needing to do any information without needing to do any retraining on the model uh and this is a retraining on the model uh and this is a retraining on the model uh and this is a great and popular way to enhance the great and popular way to enhance the great and popular way to enhance the capabilities of a model uh without capabilities of a model uh without capabilities of a model uh without having to do any fine-tuning so as the having to do any fine-tuning so as the having",
      "to do any fine-tuning so as the name implies what this involves is name implies what this involves is name implies what this involves is taking a large language foundational taking a large language foundational taking a large language foundational model but this time we're going to be model but this time we're going to be model but this time we're going to be specializing it in a certain domain or specializing it in a certain domain or specializing it in a certain domain or area so we're working",
      "with labeled and area so we're working with labeled and area so we're working with labeled and targeted data that's going to be targeted data that's going to be targeted data that's going to be provided to the model and and when we do provided to the model and and when we do provided to the model and and when we do some processing we'll have a specialized some processing we'll have a specialized some processing we'll have a specialized model for a specific use case to talk in model for a specific",
      "use case to talk in model for a specific use case to talk in a certain style to have a certain tone a certain style to have a certain tone a certain style to have a certain tone that could represent our organization or that could represent our organization or that could represent our organization or company and so then when a model is company and so then when a model is company and so then when a model is queried from um a user or any other type queried from um a user or any other type queried",
      "from um a user or any other type of way we'll have a a response that of way we'll have a a response that of way we'll have a a response that gives the correct tone and output or gives the correct tone and output or gives the correct tone and output or specialty in a domain that we'd like to specialty in a domain that we'd like to specialty in a domain that we'd like to receive and this is really important receive and this is really important receive and this is really important because what we're",
      "doing is essentially because what we're doing is essentially because what we're doing is essentially baking in this context and intuition baking in this context and intuition baking in this context and intuition into the model um and it's really into the model um and it's really into the model um and it's really important because this is now part of important because this is now part of important because this is now part of the model's weights versus being the model's weights versus being the",
      "model's weights versus being supplemented on top with a a technique supplemented on top with a a technique supplemented on top with a a technique like like like rag okay so we understand how both of rag okay so we understand how both of rag okay so we understand how both of these techniques can enhance a model's these techniques can enhance a model's these techniques can enhance a model's accur output and performance but let's accur output and performance but let's accur output and performance",
      "but let's take a look at their strengths and take a look at their strengths and take a look at their strengths and weaknesses in some common use cases weaknesses in some common use cases weaknesses in some common use cases because the direction that you go in can because the direction that you go in can because the direction that you go in can greatly affect a model's performance its greatly affect a model's performance its greatly affect a model's performance its accuracy outputs compute cost",
      "and much accuracy outputs compute cost and much accuracy outputs compute cost and much much more so let's begin with retrieval much more so let's begin with retrieval much more so let's begin with retrieval augmented generation and something that augmented generation and something that augmented generation and something that I want to point out here is that because I want to point out here is that because I want to point out here is that because we're working with a corpus of we're working with a",
      "corpus of we're working with a corpus of information and data this is perfect for information and data this is perfect for information and data this is perfect for dynamic data sources such as databases dynamic data sources such as databases dynamic data sources such as databases uh and other data repositories where we uh and other data repositories where we uh and other data repositories where we want to continuously pull information want to continuously pull information want to continuously",
      "pull information and have that up to date for the model and have that up to date for the model and have that up to date for the model to use to use to use understand and at the same time because understand and at the same time because understand and at the same time because we're working with this retriever system we're working with this retriever system we're working with this retriever system and passing in the information as and passing in the information as and passing in the information as",
      "context in the prompt well that really context in the prompt well that really context in the prompt well that really helps with hallucinations and providing helps with hallucinations and providing helps with hallucinations and providing the sources for this information is the sources for this information is the sources for this information is really important in systems where we really important in systems where we really important in systems where we need trust and transparency when we're need",
      "trust and transparency when we're need trust and transparency when we're using AI so this is fantastic but let's using AI so this is fantastic but let's using AI so this is fantastic but let's also think about this whole system also think about this whole system also think about this whole system because um having this efficient because um having this efficient because um having this efficient retrieval system uh is really important retrieval system uh is really important retrieval system uh is",
      "really important in how we select and pick the data that in how we select and pick the data that in how we select and pick the data that we want to provide in that limited we want to provide in that limited we want to provide in that limited context window and so maintaining this context window and so maintaining this context window and so maintaining this is also something that you need to think is also something that you need to think is also something that you need to think about and at the",
      "same time what we're about and at the same time what we're about and at the same time what we're doing here in this system is effectively doing here in this system is effectively doing here in this system is effectively supplementing that information on top of supplementing that information on top of supplementing that information on top of the model so we're not essentially the model so we're not essentially the model so we're not essentially enhancing the base model itself we're enhancing the",
      "base model itself we're enhancing the base model itself we're just giving it the relative and just giving it the relative and just giving it the relative and contextual information it needs versus contextual information it needs versus contextual information it needs versus fine-tuning is a little bit different fine-tuning is a little bit different fine-tuning is a little bit different because we're actually baking in that because we're actually baking in that because we're actually baking in",
      "that context and intuition into the model context and intuition into the model context and intuition into the model well we have greater um influence um in well we have greater um influence um in well we have greater um influence um in essentially how the model behaves and essentially how the model behaves and essentially how the model behaves and reacts in different situations is it an reacts in different situations is it an reacts in different situations is it an insurance adjuster can it",
      "summarize insurance adjuster can it summarize insurance adjuster can it summarize documents whatever we want the model to documents whatever we want the model to documents whatever we want the model to do we can essentially use fine tuning in do we can essentially use fine tuning in do we can essentially use fine tuning in order to uh help with that process and order to uh help with that process and order to uh help with that process and at the same time because that is baked at the same time",
      "because that is baked at the same time because that is baked into the model's weights itself well into the model's weights itself well into the model's weights itself well that's really great for Speed and that's really great for Speed and that's really great for Speed and inference cost and a variety of other um inference cost and a variety of other um inference cost and a variety of other um factors that come to running models so factors that come to running models so factors that come to",
      "running models so for example we can use smaller prompt for example we can use smaller prompt for example we can use smaller prompt context windows in order to get the context windows in order to get the context windows in order to get the responses that we want from the model responses that we want from the model responses that we want from the model and as we begin to special these models and as we begin to special these models and as we begin to special these models they can get smaller and",
      "smaller for they can get smaller and smaller for they can get smaller and smaller for specific use case so it's really great specific use case so it's really great specific use case so it's really great for running these specific uh for running these specific uh for running these specific uh specialized models in a variety of use specialized models in a variety of use specialized models in a variety of use cases but at the same time we have the cases but at the same time we have the cases but at",
      "the same time we have the same issue of cut off so up until the same issue of cut off so up until the same issue of cut off so up until the point where the model is trained well point where the model is trained well point where the model is trained well after that we have no more additional after that we have no more additional after that we have no more additional information that we can give to the information that we can give to the information that we can give to the model so the same issue",
      "that we had with model so the same issue that we had with model so the same issue that we had with the World Cup example so both of these the World Cup example so both of these the World Cup example so both of these have their strengths and weaknesses but have their strengths and weaknesses but have their strengths and weaknesses but let's actually see this in some examples let's actually see this in some examples let's actually see this in some examples and use cases here so when you're and use",
      "cases here so when you're and use cases here so when you're thinking about choosing between r and thinking about choosing between r and thinking about choosing between r and fine-tuning it's really important to fine-tuning it's really important to fine-tuning it's really important to consider your AI enabled application consider your AI enabled application consider your AI enabled application priorities and requirements so namely priorities and requirements so namely priorities and requirements",
      "so namely this starts off with the data is the this starts off with the data is the this starts off with the data is the data that you're working with slow data that you're working with slow data that you're working with slow moving or is it fast for example if we moving or is it fast for example if we moving or is it fast for example if we need to use uh up-to-date external need to use uh up-to-date external need to use uh up-to-date external information and have that ready information and have",
      "that ready information and have that ready contextually every time we use a model contextually every time we use a model contextually every time we use a model then this could be a great use case for then this could be a great use case for then this could be a great use case for rag for example a product documentation rag for example a product documentation rag for example a product documentation chatbot where we can continually update chatbot where we can continually update chatbot where we can",
      "continually update the responses with up-to-date the responses with up-to-date the responses with up-to-date information now at the same time let's information now at the same time let's information now at the same time let's think about the industry that you might think about the industry that you might think about the industry that you might be in now fine tuning is really uh be in now fine tuning is really uh be in now fine tuning is really uh powerful for specific industries that powerful for",
      "specific industries that powerful for specific industries that have nuances in their writing styles have nuances in their writing styles have nuances in their writing styles terminology vocabulary and so for terminology vocabulary and so for terminology vocabulary and so for example if we have a legal document example if we have a legal document example if we have a legal document summarizer well this could be a perfect summarizer well this could be a perfect summarizer well this could be a",
      "perfect use case for fine tuning now let's think use case for fine tuning now let's think use case for fine tuning now let's think about sources this is really important about sources this is really important about sources this is really important right now in having um transparency right now in having um transparency right now in having um transparency behind our models and with rag being behind our models and with rag being behind our models and with rag being able to provide the context and",
      "where able to provide the context and where able to provide the context and where the information came from uh is really the information came from uh is really the information came from uh is really really great so this could be a great really great so this could be a great really great so this could be a great use case again for that chatbot for use case again for that chatbot for use case again for that chatbot for retail insurance and a variety of other retail insurance and a variety of other",
      "retail insurance and a variety of other uh uh uh specialities where having that uh uh uh specialities where having that uh uh uh specialities where having that source and information in the context of source and information in the context of source and information in the context of the prompt is very important but at the the prompt is very important but at the the prompt is very important but at the same time we may have things such as same time we may have things such as same time we may have",
      "things such as past data in our organization that we past data in our organization that we past data in our organization that we can use to train a model so let it be uh can use to train a model so let it be uh can use to train a model so let it be uh accustomed to the data that we're going accustomed to the data that we're going accustomed to the data that we're going to be working with for example again to be working with for example again to be working with for example again that legal",
      "summarizer could have past that legal summarizer could have past that legal summarizer could have past data on different legal cases and and data on different legal cases and and data on different legal cases and and documents that we feed it so that it documents that we feed it so that it documents that we feed it so that it understands the situation that's working understands the situation that's working understands the situation that's working in we have better more desirable outputs in we",
      "have better more desirable outputs in we have better more desirable outputs so this is cool but I think the best um so this is cool but I think the best um so this is cool but I think the best um situation is a combination of both of situation is a combination of both of situation is a combination of both of these methods so let's say we have a these methods so let's say we have a these methods so let's say we have a financial news reporting service well we financial news reporting service well",
      "we financial news reporting service well we could fine-tune it to be uh native to could fine-tune it to be uh native to could fine-tune it to be uh native to the industry of finance and understand the industry of finance and understand the industry of finance and understand all the lingo there uh we could also all the lingo there uh we could also all the lingo there uh we could also give it past data of financial records give it past data of financial records give it past data of financial",
      "records and let it understand um how we work in and let it understand um how we work in and let it understand um how we work in that specific industry but also be able that specific industry but also be able that specific industry but also be able to provide the most up-to-date sources to provide the most up-to-date sources to provide the most up-to-date sources for news and data and be able to provide for news and data and be able to provide for news and data and be able to provide that with a",
      "level of confidence and that with a level of confidence and that with a level of confidence and transparency and Trust to the end user transparency and Trust to the end user transparency and Trust to the end user who's making that decision and needs to who's making that decision and needs to who's making that decision and needs to know the source and this is really where know the source and this is really where know the source and this is really where a combination of fine-tuning and rag is a",
      "combination of fine-tuning and rag is a combination of fine-tuning and rag is so awesome because we can really build so awesome because we can really build so awesome because we can really build amazing applications taking advantage of amazing applications taking advantage of amazing applications taking advantage of both rag as a way to retrieve that both rag as a way to retrieve that both rag as a way to retrieve that information and have it up to date but information and have it up to date but",
      "information and have it up to date but fine tuning to specialize our data uh fine tuning to specialize our data uh fine tuning to specialize our data uh but also specialize our model in a but also specialize our model in a but also specialize our model in a certain domain so uh they're both certain domain so uh they're both certain domain so uh they're both wonderful techniques and they have their wonderful techniques and they have their wonderful techniques and they have their strengths but the",
      "choice to use one or strengths but the choice to use one or strengths but the choice to use one or combination of both techniques is up to combination of both techniques is up to combination of both techniques is up to you and your specific use case and data you and your specific use case and data you and your specific use case and data so thank you so much for watching uh as so thank you so much for watching uh as so thank you so much for watching uh as always if you have any questions about",
      "always if you have any questions about always if you have any questions about fine-tuning rag or all AI related topics fine-tuning rag or all AI related topics fine-tuning rag or all AI related topics let us know in the comment section below let us know in the comment section below let us know in the comment section below don't forget to like the video and don't forget to like the video and don't forget to like the video and subscribe to the channel for more subscribe to the channel for more",
      "subscribe to the channel for more content thanks so much for watching"
    ],
    "chunk_count": 58,
    "content_id": "e07d5e06-b222-4da5-b5bc-286cd42dc4fd",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554798"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=zYGDpG-pTho&t=1s": {
    "title": "RAG vs Fine-Tuning vs Prompt Engineering: Optimizing AI Models",
    "url": "https://www.youtube.com/watch?v=zYGDpG-pTho&t=1s",
    "description": "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdndTs\n\nLearn more about RAG vs. Fine-Tuning vs. Prompt Engineering here → https://ibm.biz/BdndTi\n\nHow do AI chatbots deliver better responses? 🤔 Martin Keen explains RAG 🛠️, fine-tuning 🎯, and prompt engineering ✏️—methods that extend knowledge, refine responses, and build domain expertise. Learn how these strategies optimize large language models and improve AI outputs today! 🚀\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdndTj\n\n#retrievalaugmentedgeneration #finetuning #promptengineering",
    "duration": 790,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Remember how back in the day people Remember how back in the day people Remember how back in the day people would Google themselves? You type your would Google themselves? You type your would Google themselves? You type your name into a search engine and you see name into a search engine and you see name into a search engine and you see what it knows about you? Well, the what it knows about you? Well, the what it knows about you? Well, the modern equivalent of that is to do the modern equivalent of that is to do the modern equivalent of that is to do the same thing with a chatbot. So, when I same thing with a chatbot. So, when I same thing with a chatbot. So, when I ask a large language model, who is ask a large language model, who is ask a large language model, who is Martin Keen? Well, the response varies Martin Keen? Well, the response varies Martin Keen? Well, the response varies greatly depending upon which model I'm greatly depending upon which model I'm greatly depending upon which model I'm asking because different models, they asking because different models, they asking because different models, they have different training data sets. They have different training data sets. They have different training data sets. They have different knowledge cutoff dates. have different knowledge cutoff dates. have different knowledge cutoff dates. So what a given model knows about me? So what a given model knows about me? So what a given model knows about me? Well, it differs greatly. But how could Well, it differs greatly. But how could Well, it differs greatly. But how could we improve the model's answer? Well, we improve the model's answer? Well, we improve the model's answer? Well, there's three ways. So let's start with there's three ways. So let's start with there's three ways. So let's start with a model here. And we're going to see how a model here. And we're going to see how a model here. And we're going to see how we can improve its responses. Well, the we can improve its responses. Well, the we can improve its responses. Well, the first thing it could do is it could go first thing it could do is it could go first thing it could do is it could go out and it could perform a search. a out and it could perform a search. a out and it could perform a search. a search for new data that either wasn't search for new data that either wasn't search for new data that either wasn't in its training data set or it was just in its training data set or it was just in its training data set or it was just data that became available after the data that became available after the data that became available after the model finished training. And then it model finished training. And then it model finished training. And then it could incorporate those results from the could incorporate those results from the could incorporate those results from the search back into its answer. That is search back into its answer. That is search back into its answer. That is called rag or retrieval augmented called rag or retrieval augmented called rag or retrieval augmented generation. That's one method. Or we generation. That's one method. Or we generation. That's one method. Or we could pick a specialized model, a model could pick a specialized model, a model could pick a specialized model, a model that's been trained on, let's say, that's been trained on, let's say, that's been trained on, let's say, transcripts of these videos. That would transcripts of these videos. That would transcripts of these videos. That would be an example of something called fine be an example of something called fine be an example of something called fine tuning. Or we could ask the model a tuning. Or we could ask the model a tuning. Or we could ask the model a query that better specifies what we're query that better specifies what we're query that better specifies what we're looking for. So maybe the LLM already looking for. So maybe the LLM already looking for. So maybe the LLM already knows plenty about the Martin Keen of knows plenty about the Martin Keen of knows plenty about the Martin Keen of the world, but let's tell the model that the world, but let's tell the model that the world, but let's tell the model that we're referring to the Martin Keen who we're referring to the Martin Keen who we're referring to the Martin Keen who works at IBM rather than the Martin Keen works at IBM rather than the Martin Keen works at IBM rather than the Martin Keen that founded Keen Shoes. That is an that founded Keen Shoes. That is an that founded Keen Shoes. That is an example of prompt example of prompt example of prompt engineering. Three ways to get better engineering. Three ways to get better engineering. Three ways to get better outputs out of large language models, outputs out of large language models, outputs out of large language models, each with their pluses and each with their pluses and each with their pluses and minuses. Let's start with rag. So, let's minuses. Let's start with rag. So, let's minuses. Let's start with rag. So, let's break it down. First, there's retrieval. break it down. First, there's retrieval. break it down. First, there's retrieval. So, retrieval of external up-to-ate So, retrieval of external up-to-ate So, retrieval of external up-to-ate information. Then, there's augmentation. information. Then, there's augmentation. information. Then, there's augmentation. That's augmentation of the original That's augmentation of the original That's augmentation of the original prompt with the retrieved information prompt with the retrieved information prompt with the retrieved information added in. And then, finally, there's added in. And then, finally, there's added in. And then, finally, there's generation. That's generation of a generation. That's generation of a generation. That's generation of a response based on all of this enriched response based on all of this enriched response based on all of this enriched context. So, we can think of it like context. So, we can think of it like context. So, we can think of it like this. So we start with a query and the this. So we start with a query and the this. So we start with a query and the query comes in to a large language query comes in to a large language query comes in to a large language model. Now what rag is going to do is model. Now what rag is going to do is model. Now what rag is going to do is it's first going to go searching through it's first going to go searching through it's first going to go searching through a corpus of information. So we have this a corpus of information. So we have this a corpus of information. So we have this corpus here full of some sort of data. corpus here full of some sort of data. corpus here full of some sort of data. Now perhaps that's your organization's Now perhaps that's your organization's Now perhaps that's your organization's documents. So it might be spreadsheets, documents. So it might be spreadsheets, documents. So it might be spreadsheets, PDFs, internal wiks, you know, stuff PDFs, internal wiks, you know, stuff PDFs, internal wiks, you know, stuff like that. But unlike a typical search like that. But unlike a typical search like that. But unlike a typical search engine that just matches keywords, Rag engine that just matches keywords, Rag engine that just matches keywords, Rag converts both your question, the query, converts both your question, the query, converts both your question, the query, and all of the documents into something and all of the documents into something and all of the documents into something called called called vector vector vector embeddings. So these are all converted embeddings. So these are all converted embeddings. So these are all converted into vectors. essentially turning words into vectors. essentially turning words into vectors. essentially turning words and phrases into long lists of numbers and phrases into long lists of numbers and phrases into long lists of numbers that capture their meaning. So when you that capture their meaning. So when you that capture their meaning. So when you ask a query like what was our company's ask a query like what was our company's ask a query like what was our company's revenue growth last quarter? Well, Rag revenue growth last quarter? Well, Rag revenue growth last quarter? Well, Rag will find documents that are will find documents that are will find documents that are mathematically similar in meaning to mathematically similar in meaning to mathematically similar in meaning to your question, even if they don't use your question, even if they don't use your question, even if they don't use the exact same words. So it might find the exact same words. So it might find the exact same words. So it might find documents mentioning fourth quarter documents mentioning fourth quarter documents mentioning fourth quarter performance or quarterly sales. Those performance or quarterly sales. Those performance or quarterly sales. Those don't contain the keyword revenue don't contain the keyword revenue don't contain the keyword revenue growth, but they are semantically growth, but they are semantically growth, but they are semantically similar. Now, once rag finds the similar. Now, once rag finds the similar. Now, once rag finds the relevant information, it adds this relevant information, it adds this relevant information, it adds this information back into your original information back into your original information back into your original query before passing it to the language query before passing it to the language query before passing it to the language model. So instead of the model just kind model. So instead of the model just kind model. So instead of the model just kind of guessing based on its training data, of guessing based on its training data, of guessing based on its training data, it can now generate a response that it can now generate a response that it can now generate a response that incorporates your actual facts and incorporates your actual facts and incorporates your actual facts and figures. So this makes rag particularly figures. So this makes rag particularly figures. So this makes rag particularly valuable when you are looking for valuable when you are looking for valuable when you are looking for information that is up to date and it's information that is up to date and it's information that is up to date and it's also very valuable when you need in to also very valuable when you need in to also very valuable when you need in to add in information that is domain add in information that is domain add in information that is domain specific as specific as specific as well. But there are some costs to this. well. But there are some costs to this. well. But there are some costs to this. Let's go with the red pen. So one cost Let's go with the red pen. So one cost Let's go with the red pen. So one cost that would be the cost of performance that would be the cost of performance that would be the cost of performance for performing all of this because you for performing all of this because you for performing all of this because you have this retrieval step here and that have this retrieval step here and that have this retrieval step here and that adds latency to each query compared to a adds latency to each query compared to a adds latency to each query compared to a simple prompt to a model. There are also simple prompt to a model. There are also simple prompt to a model. There are also costs related to just kind of the the costs related to just kind of the the costs related to just kind of the the processing of this as well. So if we processing of this as well. So if we processing of this as well. So if we think about what we having to do here, think about what we having to do here, think about what we having to do here, we've got documents that need to be we've got documents that need to be we've got documents that need to be vector embeddings and we need to store vector embeddings and we need to store vector embeddings and we need to store these vector embeddings in a database. these vector embeddings in a database. these vector embeddings in a database. All of this adds to processing costs. It All of this adds to processing costs. It All of this adds to processing costs. It adds to infrastructure costs to make adds to infrastructure costs to make adds to infrastructure costs to make this solution work. All right, next up, this solution work. All right, next up, this solution work. All right, next up, fine tuning. So remember how we fine tuning. So remember how we fine tuning. So remember how we discussed getting better answers about discussed getting better answers about discussed getting better answers about me by training a model specifically on me by training a model specifically on me by training a model specifically on let's say my video transcripts? Well, let's say my video transcripts? Well, let's say my video transcripts? Well, that is fine tuning in action. So what that is fine tuning in action. So what that is fine tuning in action. So what we do with fine-tuning is we take a we do with fine-tuning is we take a we do with fine-tuning is we take a model but specifically an existing model model but specifically an existing model model but specifically an existing model and that existing model has broad and that existing model has broad and that existing model has broad knowledge and then we're going to give knowledge and then we're going to give knowledge and then we're going to give it additional specialized training on a it additional specialized training on a it additional specialized training on a focused data set. So this is now focused data set. So this is now focused data set. So this is now specialized to what we want to develop specialized to what we want to develop specialized to what we want to develop particular expertise on. Now during particular expertise on. Now during particular expertise on. Now during fine-tuning, we're updating the model's fine-tuning, we're updating the model's fine-tuning, we're updating the model's internal parameters through additional internal parameters through additional internal parameters through additional training. So the model starts out with training. So the model starts out with training. So the model starts out with some weights here like some weights here like some weights here like this. And those weights were optimized this. And those weights were optimized this. And those weights were optimized during its initial pre-training. And as during its initial pre-training. And as during its initial pre-training. And as we fine-tune, we're making small we fine-tune, we're making small we fine-tune, we're making small adjustments here to the model's weights adjustments here to the model's weights adjustments here to the model's weights using this specialized data set. So this using this specialized data set. So this using this specialized data set. So this is being is being is being incorporated. Now this process typically incorporated. Now this process typically incorporated. Now this process typically uses supervised learning where we uses supervised learning where we uses supervised learning where we provide input output pairs that provide input output pairs that provide input output pairs that demonstrate the kind of responses we demonstrate the kind of responses we demonstrate the kind of responses we want. So for example, if we're want. So for example, if we're want. So for example, if we're fine-tuning for technical support, we fine-tuning for technical support, we fine-tuning for technical support, we might provide thousands of examples of might provide thousands of examples of might provide thousands of examples of customer queries and those would be customer queries and those would be customer queries and those would be paired with correct technical responses. paired with correct technical responses. paired with correct technical responses. The model adjusts its weight through The model adjusts its weight through The model adjusts its weight through back propagation to minimize the back propagation to minimize the back propagation to minimize the difference between its predicted outputs difference between its predicted outputs difference between its predicted outputs and the targeted responses. So we're not and the targeted responses. So we're not and the targeted responses. So we're not just teaching the model new facts here. just teaching the model new facts here. just teaching the model new facts here. We're actually modifying how it We're actually modifying how it We're actually modifying how it processes information. The model is processes information. The model is processes information. The model is learning to recognize domain specific learning to recognize domain specific learning to recognize domain specific patterns. So fine-tuning shows its patterns. So fine-tuning shows its patterns. So fine-tuning shows its strength when you particularly need a strength when you particularly need a strength when you particularly need a model that has very deep model that has very deep model that has very deep domain domain domain expertise. That's what we can really add expertise. That's what we can really add expertise. That's what we can really add in with fine-tuning and also it's much in with fine-tuning and also it's much in with fine-tuning and also it's much faster specifically at inference time. faster specifically at inference time. faster specifically at inference time. So when we are putting the queries in, So when we are putting the queries in, So when we are putting the queries in, it's faster than rag because it doesn't it's faster than rag because it doesn't it's faster than rag because it doesn't need to search through external data. need to search through external data. need to search through external data. And because the knowledge is kind of And because the knowledge is kind of And because the knowledge is kind of baked into the model's weights, you baked into the model's weights, you baked into the model's weights, you don't need to maintain a separate vector don't need to maintain a separate vector don't need to maintain a separate vector database. But there's some downsides as database. But there's some downsides as database. But there's some downsides as well. Well, there's certainly issues well. Well, there's certainly issues well. Well, there's certainly issues here with the training complexity of all here with the training complexity of all here with the training complexity of all of this. You're going to need thousands of this. You're going to need thousands of this. You're going to need thousands of high quality training examples. There of high quality training examples. There of high quality training examples. There also issues with computational cost. The also issues with computational cost. The also issues with computational cost. The computational cost for training this computational cost for training this computational cost for training this model can be substantial and is going to model can be substantial and is going to model can be substantial and is going to require a whole bunch of GPUs. And require a whole bunch of GPUs. And require a whole bunch of GPUs. And there's also challenges related to there's also challenges related to there's also challenges related to maintenance as well because unlike rag maintenance as well because unlike rag maintenance as well because unlike rag where you can easily add new documents where you can easily add new documents where you can easily add new documents to your knowledge base at any point, to your knowledge base at any point, to your knowledge base at any point, updating a fine-tune model requires updating a fine-tune model requires updating a fine-tune model requires another round of training. And then another round of training. And then another round of training. And then perhaps most importantly of all, there perhaps most importantly of all, there perhaps most importantly of all, there is a risk of something called is a risk of something called is a risk of something called catastrophic forgetting. Now that's catastrophic forgetting. Now that's catastrophic forgetting. Now that's where the model loses some of its where the model loses some of its where the model loses some of its general capabilities while it's busy general capabilities while it's busy general capabilities while it's busy learning these specialized ones. So learning these specialized ones. So learning these specialized ones. So finally, let's explore prompt finally, let's explore prompt finally, let's explore prompt engineering. Now specifying Martin Keen engineering. Now specifying Martin Keen engineering. Now specifying Martin Keen who works at IBM versus Martin Keen who who works at IBM versus Martin Keen who who works at IBM versus Martin Keen who founded Keen Shoes. That's prompt founded Keen Shoes. That's prompt founded Keen Shoes. That's prompt engineering. But at its most basic, engineering. But at its most basic, engineering. But at its most basic, prompt engineering goes far beyond prompt engineering goes far beyond prompt engineering goes far beyond simple clarification. So let's think simple clarification. So let's think simple clarification. So let's think about when we input a prompt, the model about when we input a prompt, the model about when we input a prompt, the model receives this prompt and it processes it receives this prompt and it processes it receives this prompt and it processes it through a series of through a series of through a series of layers. And these layers are essentially layers. And these layers are essentially layers. And these layers are essentially attention mechanisms. And each one attention mechanisms. And each one attention mechanisms. And each one focuses on different aspects of your focuses on different aspects of your focuses on different aspects of your prompt text that came in. And by prompt text that came in. And by prompt text that came in. And by including specific elements in your including specific elements in your including specific elements in your prompt, so examples or context or how prompt, so examples or context or how prompt, so examples or context or how you want the format to look, you're you want the format to look, you're you want the format to look, you're directing the model's attention to directing the model's attention to directing the model's attention to relevant patterns it learned during relevant patterns it learned during relevant patterns it learned during training. So for example, telling a training. So for example, telling a training. So for example, telling a model to think about this step by step model to think about this step by step model to think about this step by step that activates patterns it learned from that activates patterns it learned from that activates patterns it learned from training data where methodical reasoning training data where methodical reasoning training data where methodical reasoning led to accurate results. So a led to accurate results. So a led to accurate results. So a wellengineered prompt can transform a wellengineered prompt can transform a wellengineered prompt can transform a model's output without any additional model's output without any additional model's output without any additional training or without data retrieval. So training or without data retrieval. So training or without data retrieval. So take an example of a of a prompt. Let's take an example of a of a prompt. Let's take an example of a of a prompt. Let's say we say is this code secure. It's not say we say is this code secure. It's not say we say is this code secure. It's not a very good prompt. An engineer prompt. a very good prompt. An engineer prompt. a very good prompt. An engineer prompt. It might read a bit more like this. It's It might read a bit more like this. It's It might read a bit more like this. It's much more detailed. Now we haven't much more detailed. Now we haven't much more detailed. Now we haven't changed the model. We haven't added new changed the model. We haven't added new changed the model. We haven't added new data. we've just better activated its data. we've just better activated its data. we've just better activated its existing capabilities. Now, I think the existing capabilities. Now, I think the existing capabilities. Now, I think the benefits to this are pretty obvious. One benefits to this are pretty obvious. One benefits to this are pretty obvious. One is that we don't need to change any of is that we don't need to change any of is that we don't need to change any of our backend infrastructure here because our backend infrastructure here because our backend infrastructure here because there are no infrastructure changes at there are no infrastructure changes at there are no infrastructure changes at all in order to prompt better. It's all all in order to prompt better. It's all all in order to prompt better. It's all on the user. There's also the benefit on the user. There's also the benefit on the user. There's also the benefit that by doing this you get to see that by doing this you get to see that by doing this you get to see immediate immediate immediate responses and immediate results to what responses and immediate results to what responses and immediate results to what you do. We don't have to add in new you do. We don't have to add in new you do. We don't have to add in new training data or any kind of data training data or any kind of data training data or any kind of data processing. But of course there are some processing. But of course there are some processing. But of course there are some limitations to this as well. Prompt limitations to this as well. Prompt limitations to this as well. Prompt engineering is as much an art as it is a engineering is as much an art as it is a engineering is as much an art as it is a science. So there is certainly a good science. So there is certainly a good science. So there is certainly a good amount of trial and error in this sort amount of trial and error in this sort amount of trial and error in this sort of process to find effective prompts. of process to find effective prompts. of process to find effective prompts. And you're also limited in what you can And you're also limited in what you can And you're also limited in what you can do here. You're limited to existing do here. You're limited to existing do here. You're limited to existing knowledge because you're not able to knowledge because you're not able to knowledge because you're not able to actually add anything else in here. No actually add anything else in here. No actually add anything else in here. No additional amount of prompt engineering additional amount of prompt engineering additional amount of prompt engineering is going to teach it truly new is going to teach it truly new is going to teach it truly new information. you're not going to teach information. you're not going to teach information. you're not going to teach the model anything that's outdated in the model anything that's outdated in the model anything that's outdated in the model. So we've talked about now rag the model. So we've talked about now rag the model. So we've talked about now rag as being one option and we talked about as being one option and we talked about as being one option and we talked about fine tuning as being another one and now fine tuning as being another one and now fine tuning as being another one and now just now we've talked about prompt just now we've talked about prompt just now we've talked about prompt engineering as well and I've really engineering as well and I've really engineering as well and I've really talked about those as three different talked about those as three different talked about those as three different distinct things here but they're distinct things here but they're distinct things here but they're commonly used actually in combination. commonly used actually in combination. commonly used actually in combination. We might use all three together. So We might use all three together. So We might use all three together. So consider a legal AI system. Rag that consider a legal AI system. Rag that consider a legal AI system. Rag that could retrieve specific cases and recent could retrieve specific cases and recent could retrieve specific cases and recent court decisions. Uh the prompt court decisions. Uh the prompt court decisions. Uh the prompt engineering part that could make sure engineering part that could make sure engineering part that could make sure that we follow proper legal document that we follow proper legal document that we follow proper legal document formats by asking for it. And then formats by asking for it. And then formats by asking for it. And then fine-tuning that could help the model fine-tuning that could help the model fine-tuning that could help the model master firm specific policies. I mean master firm specific policies. I mean master firm specific policies. I mean basically we can think of it like this. basically we can think of it like this. basically we can think of it like this. We can think that prompt engineering We can think that prompt engineering We can think that prompt engineering offers flexibility and immediate results offers flexibility and immediate results offers flexibility and immediate results but it can't extend knowledge. Rag that but it can't extend knowledge. Rag that but it can't extend knowledge. Rag that can extend knowledge. It provides can extend knowledge. It provides can extend knowledge. It provides upto-date information but with upto-date information but with upto-date information but with computational overhead and then computational overhead and then computational overhead and then fine-tuning that enables deep domain fine-tuning that enables deep domain fine-tuning that enables deep domain expertise but it requires significant expertise but it requires significant expertise but it requires significant resources and maintenance. Basically it resources and maintenance. Basically it resources and maintenance. Basically it comes down to picking the methods that comes down to picking the methods that comes down to picking the methods that work for you. You know, we've we sure work for you. You know, we've we sure work for you. You know, we've we sure come a long way from vanity searching on come a long way from vanity searching on come a long way from vanity searching on Google",
    "chunks": [
      "Kind: captions Language: en Remember how back in the day people Remember how back in the day people Remember how back in the day people would Google themselves? You type your would Google themselves? You type your would Google themselves? You type your name into a search engine and you see name into a search engine and you see name into a search engine and you see what it knows about you? Well, the what it knows about you? Well, the what it knows about you? Well, the modern equivalent of that is",
      "to do the modern equivalent of that is to do the modern equivalent of that is to do the same thing with a chatbot. So, when I same thing with a chatbot. So, when I same thing with a chatbot. So, when I ask a large language model, who is ask a large language model, who is ask a large language model, who is Martin Keen? Well, the response varies Martin Keen? Well, the response varies Martin Keen? Well, the response varies greatly depending upon which model I'm greatly depending upon which model I'm",
      "greatly depending upon which model I'm asking because different models, they asking because different models, they asking because different models, they have different training data sets. They have different training data sets. They have different training data sets. They have different knowledge cutoff dates. have different knowledge cutoff dates. have different knowledge cutoff dates. So what a given model knows about me? So what a given model knows about me? So what a given model knows about",
      "me? Well, it differs greatly. But how could Well, it differs greatly. But how could Well, it differs greatly. But how could we improve the model's answer? Well, we improve the model's answer? Well, we improve the model's answer? Well, there's three ways. So let's start with there's three ways. So let's start with there's three ways. So let's start with a model here. And we're going to see how a model here. And we're going to see how a model here. And we're going to see how we can improve its",
      "responses. Well, the we can improve its responses. Well, the we can improve its responses. Well, the first thing it could do is it could go first thing it could do is it could go first thing it could do is it could go out and it could perform a search. a out and it could perform a search. a out and it could perform a search. a search for new data that either wasn't search for new data that either wasn't search for new data that either wasn't in its training data set or it was just in its training",
      "data set or it was just in its training data set or it was just data that became available after the data that became available after the data that became available after the model finished training. And then it model finished training. And then it model finished training. And then it could incorporate those results from the could incorporate those results from the could incorporate those results from the search back into its answer. That is search back into its answer. That is search back into",
      "its answer. That is called rag or retrieval augmented called rag or retrieval augmented called rag or retrieval augmented generation. That's one method. Or we generation. That's one method. Or we generation. That's one method. Or we could pick a specialized model, a model could pick a specialized model, a model could pick a specialized model, a model that's been trained on, let's say, that's been trained on, let's say, that's been trained on, let's say, transcripts of these videos. That would",
      "transcripts of these videos. That would transcripts of these videos. That would be an example of something called fine be an example of something called fine be an example of something called fine tuning. Or we could ask the model a tuning. Or we could ask the model a tuning. Or we could ask the model a query that better specifies what we're query that better specifies what we're query that better specifies what we're looking for. So maybe the LLM already looking for. So maybe the LLM already",
      "looking for. So maybe the LLM already knows plenty about the Martin Keen of knows plenty about the Martin Keen of knows plenty about the Martin Keen of the world, but let's tell the model that the world, but let's tell the model that the world, but let's tell the model that we're referring to the Martin Keen who we're referring to the Martin Keen who we're referring to the Martin Keen who works at IBM rather than the Martin Keen works at IBM rather than the Martin Keen works at IBM rather than",
      "the Martin Keen that founded Keen Shoes. That is an that founded Keen Shoes. That is an that founded Keen Shoes. That is an example of prompt example of prompt example of prompt engineering. Three ways to get better engineering. Three ways to get better engineering. Three ways to get better outputs out of large language models, outputs out of large language models, outputs out of large language models, each with their pluses and each with their pluses and each with their pluses and minuses. Let's",
      "start with rag. So, let's minuses. Let's start with rag. So, let's minuses. Let's start with rag. So, let's break it down. First, there's retrieval. break it down. First, there's retrieval. break it down. First, there's retrieval. So, retrieval of external up-to-ate So, retrieval of external up-to-ate So, retrieval of external up-to-ate information. Then, there's augmentation. information. Then, there's augmentation. information. Then, there's augmentation. That's augmentation of the original",
      "That's augmentation of the original That's augmentation of the original prompt with the retrieved information prompt with the retrieved information prompt with the retrieved information added in. And then, finally, there's added in. And then, finally, there's added in. And then, finally, there's generation. That's generation of a generation. That's generation of a generation. That's generation of a response based on all of this enriched response based on all of this enriched response based on all",
      "of this enriched context. So, we can think of it like context. So, we can think of it like context. So, we can think of it like this. So we start with a query and the this. So we start with a query and the this. So we start with a query and the query comes in to a large language query comes in to a large language query comes in to a large language model. Now what rag is going to do is model. Now what rag is going to do is model. Now what rag is going to do is it's first going to go searching",
      "through it's first going to go searching through it's first going to go searching through a corpus of information. So we have this a corpus of information. So we have this a corpus of information. So we have this corpus here full of some sort of data. corpus here full of some sort of data. corpus here full of some sort of data. Now perhaps that's your organization's Now perhaps that's your organization's Now perhaps that's your organization's documents. So it might be spreadsheets, documents. So",
      "it might be spreadsheets, documents. So it might be spreadsheets, PDFs, internal wiks, you know, stuff PDFs, internal wiks, you know, stuff PDFs, internal wiks, you know, stuff like that. But unlike a typical search like that. But unlike a typical search like that. But unlike a typical search engine that just matches keywords, Rag engine that just matches keywords, Rag engine that just matches keywords, Rag converts both your question, the query, converts both your question, the query, converts",
      "both your question, the query, and all of the documents into something and all of the documents into something and all of the documents into something called called called vector vector vector embeddings. So these are all converted embeddings. So these are all converted embeddings. So these are all converted into vectors. essentially turning words into vectors. essentially turning words into vectors. essentially turning words and phrases into long lists of numbers and phrases into long lists of",
      "numbers and phrases into long lists of numbers that capture their meaning. So when you that capture their meaning. So when you that capture their meaning. So when you ask a query like what was our company's ask a query like what was our company's ask a query like what was our company's revenue growth last quarter? Well, Rag revenue growth last quarter? Well, Rag revenue growth last quarter? Well, Rag will find documents that are will find documents that are will find documents that are",
      "mathematically similar in meaning to mathematically similar in meaning to mathematically similar in meaning to your question, even if they don't use your question, even if they don't use your question, even if they don't use the exact same words. So it might find the exact same words. So it might find the exact same words. So it might find documents mentioning fourth quarter documents mentioning fourth quarter documents mentioning fourth quarter performance or quarterly sales. Those performance",
      "or quarterly sales. Those performance or quarterly sales. Those don't contain the keyword revenue don't contain the keyword revenue don't contain the keyword revenue growth, but they are semantically growth, but they are semantically growth, but they are semantically similar. Now, once rag finds the similar. Now, once rag finds the similar. Now, once rag finds the relevant information, it adds this relevant information, it adds this relevant information, it adds this information back into your",
      "original information back into your original information back into your original query before passing it to the language query before passing it to the language query before passing it to the language model. So instead of the model just kind model. So instead of the model just kind model. So instead of the model just kind of guessing based on its training data, of guessing based on its training data, of guessing based on its training data, it can now generate a response that it can now generate a",
      "response that it can now generate a response that incorporates your actual facts and incorporates your actual facts and incorporates your actual facts and figures. So this makes rag particularly figures. So this makes rag particularly figures. So this makes rag particularly valuable when you are looking for valuable when you are looking for valuable when you are looking for information that is up to date and it's information that is up to date and it's information that is up to date and it's also",
      "very valuable when you need in to also very valuable when you need in to also very valuable when you need in to add in information that is domain add in information that is domain add in information that is domain specific as specific as specific as well. But there are some costs to this. well. But there are some costs to this. well. But there are some costs to this. Let's go with the red pen. So one cost Let's go with the red pen. So one cost Let's go with the red pen. So one cost that would be",
      "the cost of performance that would be the cost of performance that would be the cost of performance for performing all of this because you for performing all of this because you for performing all of this because you have this retrieval step here and that have this retrieval step here and that have this retrieval step here and that adds latency to each query compared to a adds latency to each query compared to a adds latency to each query compared to a simple prompt to a model. There are also",
      "simple prompt to a model. There are also simple prompt to a model. There are also costs related to just kind of the the costs related to just kind of the the costs related to just kind of the the processing of this as well. So if we processing of this as well. So if we processing of this as well. So if we think about what we having to do here, think about what we having to do here, think about what we having to do here, we've got documents that need to be we've got documents that need to be we've",
      "got documents that need to be vector embeddings and we need to store vector embeddings and we need to store vector embeddings and we need to store these vector embeddings in a database. these vector embeddings in a database. these vector embeddings in a database. All of this adds to processing costs. It All of this adds to processing costs. It All of this adds to processing costs. It adds to infrastructure costs to make adds to infrastructure costs to make adds to infrastructure costs to make",
      "this solution work. All right, next up, this solution work. All right, next up, this solution work. All right, next up, fine tuning. So remember how we fine tuning. So remember how we fine tuning. So remember how we discussed getting better answers about discussed getting better answers about discussed getting better answers about me by training a model specifically on me by training a model specifically on me by training a model specifically on let's say my video transcripts? Well, let's say my",
      "video transcripts? Well, let's say my video transcripts? Well, that is fine tuning in action. So what that is fine tuning in action. So what that is fine tuning in action. So what we do with fine-tuning is we take a we do with fine-tuning is we take a we do with fine-tuning is we take a model but specifically an existing model model but specifically an existing model model but specifically an existing model and that existing model has broad and that existing model has broad and that existing",
      "model has broad knowledge and then we're going to give knowledge and then we're going to give knowledge and then we're going to give it additional specialized training on a it additional specialized training on a it additional specialized training on a focused data set. So this is now focused data set. So this is now focused data set. So this is now specialized to what we want to develop specialized to what we want to develop specialized to what we want to develop particular expertise on. Now",
      "during particular expertise on. Now during particular expertise on. Now during fine-tuning, we're updating the model's fine-tuning, we're updating the model's fine-tuning, we're updating the model's internal parameters through additional internal parameters through additional internal parameters through additional training. So the model starts out with training. So the model starts out with training. So the model starts out with some weights here like some weights here like some weights here like",
      "this. And those weights were optimized this. And those weights were optimized this. And those weights were optimized during its initial pre-training. And as during its initial pre-training. And as during its initial pre-training. And as we fine-tune, we're making small we fine-tune, we're making small we fine-tune, we're making small adjustments here to the model's weights adjustments here to the model's weights adjustments here to the model's weights using this specialized data set. So this",
      "using this specialized data set. So this using this specialized data set. So this is being is being is being incorporated. Now this process typically incorporated. Now this process typically incorporated. Now this process typically uses supervised learning where we uses supervised learning where we uses supervised learning where we provide input output pairs that provide input output pairs that provide input output pairs that demonstrate the kind of responses we demonstrate the kind of responses",
      "we demonstrate the kind of responses we want. So for example, if we're want. So for example, if we're want. So for example, if we're fine-tuning for technical support, we fine-tuning for technical support, we fine-tuning for technical support, we might provide thousands of examples of might provide thousands of examples of might provide thousands of examples of customer queries and those would be customer queries and those would be customer queries and those would be paired with correct technical",
      "responses. paired with correct technical responses. paired with correct technical responses. The model adjusts its weight through The model adjusts its weight through The model adjusts its weight through back propagation to minimize the back propagation to minimize the back propagation to minimize the difference between its predicted outputs difference between its predicted outputs difference between its predicted outputs and the targeted responses. So we're not and the targeted responses. So",
      "we're not and the targeted responses. So we're not just teaching the model new facts here. just teaching the model new facts here. just teaching the model new facts here. We're actually modifying how it We're actually modifying how it We're actually modifying how it processes information. The model is processes information. The model is processes information. The model is learning to recognize domain specific learning to recognize domain specific learning to recognize domain specific patterns. So",
      "fine-tuning shows its patterns. So fine-tuning shows its patterns. So fine-tuning shows its strength when you particularly need a strength when you particularly need a strength when you particularly need a model that has very deep model that has very deep model that has very deep domain domain domain expertise. That's what we can really add expertise. That's what we can really add expertise. That's what we can really add in with fine-tuning and also it's much in with fine-tuning and also it's",
      "much in with fine-tuning and also it's much faster specifically at inference time. faster specifically at inference time. faster specifically at inference time. So when we are putting the queries in, So when we are putting the queries in, So when we are putting the queries in, it's faster than rag because it doesn't it's faster than rag because it doesn't it's faster than rag because it doesn't need to search through external data. need to search through external data. need to search through",
      "external data. And because the knowledge is kind of And because the knowledge is kind of And because the knowledge is kind of baked into the model's weights, you baked into the model's weights, you baked into the model's weights, you don't need to maintain a separate vector don't need to maintain a separate vector don't need to maintain a separate vector database. But there's some downsides as database. But there's some downsides as database. But there's some downsides as well. Well, there's",
      "certainly issues well. Well, there's certainly issues well. Well, there's certainly issues here with the training complexity of all here with the training complexity of all here with the training complexity of all of this. You're going to need thousands of this. You're going to need thousands of this. You're going to need thousands of high quality training examples. There of high quality training examples. There of high quality training examples. There also issues with computational cost. The",
      "also issues with computational cost. The also issues with computational cost. The computational cost for training this computational cost for training this computational cost for training this model can be substantial and is going to model can be substantial and is going to model can be substantial and is going to require a whole bunch of GPUs. And require a whole bunch of GPUs. And require a whole bunch of GPUs. And there's also challenges related to there's also challenges related to there's",
      "also challenges related to maintenance as well because unlike rag maintenance as well because unlike rag maintenance as well because unlike rag where you can easily add new documents where you can easily add new documents where you can easily add new documents to your knowledge base at any point, to your knowledge base at any point, to your knowledge base at any point, updating a fine-tune model requires updating a fine-tune model requires updating a fine-tune model requires another round of",
      "training. And then another round of training. And then another round of training. And then perhaps most importantly of all, there perhaps most importantly of all, there perhaps most importantly of all, there is a risk of something called is a risk of something called is a risk of something called catastrophic forgetting. Now that's catastrophic forgetting. Now that's catastrophic forgetting. Now that's where the model loses some of its where the model loses some of its where the model loses some",
      "of its general capabilities while it's busy general capabilities while it's busy general capabilities while it's busy learning these specialized ones. So learning these specialized ones. So learning these specialized ones. So finally, let's explore prompt finally, let's explore prompt finally, let's explore prompt engineering. Now specifying Martin Keen engineering. Now specifying Martin Keen engineering. Now specifying Martin Keen who works at IBM versus Martin Keen who who works at IBM versus",
      "Martin Keen who who works at IBM versus Martin Keen who founded Keen Shoes. That's prompt founded Keen Shoes. That's prompt founded Keen Shoes. That's prompt engineering. But at its most basic, engineering. But at its most basic, engineering. But at its most basic, prompt engineering goes far beyond prompt engineering goes far beyond prompt engineering goes far beyond simple clarification. So let's think simple clarification. So let's think simple clarification. So let's think about when we input",
      "a prompt, the model about when we input a prompt, the model about when we input a prompt, the model receives this prompt and it processes it receives this prompt and it processes it receives this prompt and it processes it through a series of through a series of through a series of layers. And these layers are essentially layers. And these layers are essentially layers. And these layers are essentially attention mechanisms. And each one attention mechanisms. And each one attention mechanisms. And",
      "each one focuses on different aspects of your focuses on different aspects of your focuses on different aspects of your prompt text that came in. And by prompt text that came in. And by prompt text that came in. And by including specific elements in your including specific elements in your including specific elements in your prompt, so examples or context or how prompt, so examples or context or how prompt, so examples or context or how you want the format to look, you're you want the format to",
      "look, you're you want the format to look, you're directing the model's attention to directing the model's attention to directing the model's attention to relevant patterns it learned during relevant patterns it learned during relevant patterns it learned during training. So for example, telling a training. So for example, telling a training. So for example, telling a model to think about this step by step model to think about this step by step model to think about this step by step that activates",
      "patterns it learned from that activates patterns it learned from that activates patterns it learned from training data where methodical reasoning training data where methodical reasoning training data where methodical reasoning led to accurate results. So a led to accurate results. So a led to accurate results. So a wellengineered prompt can transform a wellengineered prompt can transform a wellengineered prompt can transform a model's output without any additional model's output without any",
      "additional model's output without any additional training or without data retrieval. So training or without data retrieval. So training or without data retrieval. So take an example of a of a prompt. Let's take an example of a of a prompt. Let's take an example of a of a prompt. Let's say we say is this code secure. It's not say we say is this code secure. It's not say we say is this code secure. It's not a very good prompt. An engineer prompt. a very good prompt. An engineer prompt. a very good",
      "prompt. An engineer prompt. It might read a bit more like this. It's It might read a bit more like this. It's It might read a bit more like this. It's much more detailed. Now we haven't much more detailed. Now we haven't much more detailed. Now we haven't changed the model. We haven't added new changed the model. We haven't added new changed the model. We haven't added new data. we've just better activated its data. we've just better activated its data. we've just better activated its existing",
      "capabilities. Now, I think the existing capabilities. Now, I think the existing capabilities. Now, I think the benefits to this are pretty obvious. One benefits to this are pretty obvious. One benefits to this are pretty obvious. One is that we don't need to change any of is that we don't need to change any of is that we don't need to change any of our backend infrastructure here because our backend infrastructure here because our backend infrastructure here because there are no infrastructure",
      "changes at there are no infrastructure changes at there are no infrastructure changes at all in order to prompt better. It's all all in order to prompt better. It's all all in order to prompt better. It's all on the user. There's also the benefit on the user. There's also the benefit on the user. There's also the benefit that by doing this you get to see that by doing this you get to see that by doing this you get to see immediate immediate immediate responses and immediate results to what",
      "responses and immediate results to what responses and immediate results to what you do. We don't have to add in new you do. We don't have to add in new you do. We don't have to add in new training data or any kind of data training data or any kind of data training data or any kind of data processing. But of course there are some processing. But of course there are some processing. But of course there are some limitations to this as well. Prompt limitations to this as well. Prompt limitations to",
      "this as well. Prompt engineering is as much an art as it is a engineering is as much an art as it is a engineering is as much an art as it is a science. So there is certainly a good science. So there is certainly a good science. So there is certainly a good amount of trial and error in this sort amount of trial and error in this sort amount of trial and error in this sort of process to find effective prompts. of process to find effective prompts. of process to find effective prompts. And you're",
      "also limited in what you can And you're also limited in what you can And you're also limited in what you can do here. You're limited to existing do here. You're limited to existing do here. You're limited to existing knowledge because you're not able to knowledge because you're not able to knowledge because you're not able to actually add anything else in here. No actually add anything else in here. No actually add anything else in here. No additional amount of prompt engineering additional",
      "amount of prompt engineering additional amount of prompt engineering is going to teach it truly new is going to teach it truly new is going to teach it truly new information. you're not going to teach information. you're not going to teach information. you're not going to teach the model anything that's outdated in the model anything that's outdated in the model anything that's outdated in the model. So we've talked about now rag the model. So we've talked about now rag the model. So we've talked",
      "about now rag as being one option and we talked about as being one option and we talked about as being one option and we talked about fine tuning as being another one and now fine tuning as being another one and now fine tuning as being another one and now just now we've talked about prompt just now we've talked about prompt just now we've talked about prompt engineering as well and I've really engineering as well and I've really engineering as well and I've really talked about those as three",
      "different talked about those as three different talked about those as three different distinct things here but they're distinct things here but they're distinct things here but they're commonly used actually in combination. commonly used actually in combination. commonly used actually in combination. We might use all three together. So We might use all three together. So We might use all three together. So consider a legal AI system. Rag that consider a legal AI system. Rag that consider a legal",
      "AI system. Rag that could retrieve specific cases and recent could retrieve specific cases and recent could retrieve specific cases and recent court decisions. Uh the prompt court decisions. Uh the prompt court decisions. Uh the prompt engineering part that could make sure engineering part that could make sure engineering part that could make sure that we follow proper legal document that we follow proper legal document that we follow proper legal document formats by asking for it. And then",
      "formats by asking for it. And then formats by asking for it. And then fine-tuning that could help the model fine-tuning that could help the model fine-tuning that could help the model master firm specific policies. I mean master firm specific policies. I mean master firm specific policies. I mean basically we can think of it like this. basically we can think of it like this. basically we can think of it like this. We can think that prompt engineering We can think that prompt engineering We can",
      "think that prompt engineering offers flexibility and immediate results offers flexibility and immediate results offers flexibility and immediate results but it can't extend knowledge. Rag that but it can't extend knowledge. Rag that but it can't extend knowledge. Rag that can extend knowledge. It provides can extend knowledge. It provides can extend knowledge. It provides upto-date information but with upto-date information but with upto-date information but with computational overhead and then",
      "computational overhead and then computational overhead and then fine-tuning that enables deep domain fine-tuning that enables deep domain fine-tuning that enables deep domain expertise but it requires significant expertise but it requires significant expertise but it requires significant resources and maintenance. Basically it resources and maintenance. Basically it resources and maintenance. Basically it comes down to picking the methods that comes down to picking the methods that comes down to",
      "picking the methods that work for you. You know, we've we sure work for you. You know, we've we sure work for you. You know, we've we sure come a long way from vanity searching on come a long way from vanity searching on come a long way from vanity searching on Google"
    ],
    "chunk_count": 62,
    "content_id": "841be048-7884-49b0-bcd3-54c2ab9ff26c",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554815"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=HdafI0t3sEY&t=2s": {
    "title": "RAG vs. CAG: Solving Knowledge Gaps in AI Models",
    "url": "https://www.youtube.com/watch?v=HdafI0t3sEY&t=2s",
    "description": "Ready to become a certified watsonx Generative AI Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdGtmQ\n\nLearn more about RAG here → https://ibm.biz/BdGtm3\n\nWhat if your AI can't answer who won the Oscars last year? 🎥 Martin Keen explains how RAG (Retrieval-Augmented Generation) and CAG (Cache-Augmented Generation) address knowledge gaps in AI. 🚀 Discover their strengths in real-time retrieval, scalability, and efficient workflows for smarter AI systems. 💻\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdGtmw\n\n#retrievalaugmentedgeneration #aiworkflow #machinelearning",
    "duration": 959,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Left to their own devices, large language models have a bit of a knowledge problem. If a piece of information wasn't in their training set, they won't be able to recall it. Maybe something newsworthy that happened after the model completed training, such as who won the 2025 Best Picture at the Oscars, or it could be something proprietary like a client's purchase history. So to overcome that knowledge problem, we can use augmented generation techniques. For example, retrieval. So retrieval, augmented generation, otherwise known as Now how does that work? Well, essentially we have here a model and the model is going to query an external searchable knowledge base. Here's where we've got our knowledge and that's going to return portions of relevant documents to provide additional context. So we get the documents, we get some context, and we pass that to the LLM model to update its knowledge, if you like, and that updated context, that's used to generate an answer. Anora that won best picture this year, probably got it out of that data set. But retrieval isn't the only augmented generation game in town. Another one is cash augmented generation or CAG. Now CAG is an alternative method. So rather than querying a knowledge database for answers, the core idea of CAG is to preload the entire knowledge base. So we take everything we know and we put it all into the context window. All of it. The Oscar winners, last week's lunch special at the office cafeteria, whatever you want. So rather than feeding the model just curated knowledge, we are feeding the model everything, not just the stuff we deemed relevant to the query. So RAG versus CAG. Let's get into how these two things work. the capabilities of each approach, and an enticing game to test your own knowledge, and let's start with RAG. So RAG is essentially a two-phase system. You've got an offline phase where you ingest and index your knowledge, and then you've got an online phase where you retrieve and generate on demand. And the offline part, pretty straightforward. So you can start with some documents. So this is your knowledge. This could be Word files, PDFs, whatever. and you're going to break them into chunks and create vector embeddings for each chunk using the help of something called an embedding model. Now that embedding model is going to create embeddings and it's going to store them in a database, and specifically this is a vector database where the embeddings are stored. So you've essentially now created a searchable index of your knowledge. So when a user prompt comes in from the user, this is where the online phase of this is going to kick in. So, first thing that's going to happen is we're going to go to a RAG retriever. and that RAG retriever is gonna take the user's question and it's gonna turn it into a vector using the same embedding model that we used earlier, and that's gonna perform a similarity search of your vector database. Now that's gonna return the top K most relevant document chunks from here that are related to this query. There might be something like three to five passages that are most likely to contain the answer to the user's query, and we're gonna take those chunks and we're gonna put them into the context window of the LLM alongside the user's initial query and all of this is then gonna get sent to the large language model. So the model is gonna see the question the user submitted plus these relevant bits of context and use that to generate an answer. We're basically saying to the model, here's the question, here's some potentially useful information to help you answer it that we got out of this vector database, off you go. And the beauty of RAG is that it's very modular, so you can swap out the vector database, you could swap out a different embedding model, or you could change the LLM without rebuilding this entire system. That's RAG. What about CAG? Well, CAG takes a completely different approach. So instead of retrieving knowledge on demand, you front load it all into the model's context all at once. So we'll start again with our documents. This is all of our gathered knowledge. And we're gonna format them into one massive prompt that fits inside of the model's context window. So here it's gonna fit into this. Now, this could be tens or even hundreds of thousands of tokens and then the large language model is going to take this massive amount of input and it's going to process it. So effectively this kind of knowledge blob is going to be processed in a single forward pass and it's going to capture and store the model's internal state after it's digested all of this information. Now this internal state blob, it's actually got a name, it's called the KV cache, or the key value cache, and it's created from each self-attention layer and it represents the model's encoded form of all of your documents, all of your knowledge, so it's kind of like the models already read your documents and now it's memorized it. So when a user submits a query in this situation then we take all of this KV cache and we add the query to it and all of that gets sent into the large language model. And because the Transformers cache has already got all of the knowledge tokens in it, the model can use any relevant information as it generates an answer without having to reprocess all of this text again. So the fundamental distinction between RAG and CAG comes down to when and how knowledge is processed. With RAG, We say, let's fetch only the stuff that we think we're going to actually need. CAG, that says let's load everything, all of our documents up front and then remember it for later. So with RAG, your knowledge base can be really, really large. This could be millions of documents stored in here because you're only retrieving small pieces at a time. The model only sees what's relevant for a particular query. Whereas with CAG you are constrained by the size of the model's context window. Now a typical model today that can have a context window of something like 32 ,000 to 100 ,000 tokens. Some are a bit larger than that but that's pretty standard. It's substantial but it's still finite and everything all of these docs need to fit in that window. So let's talk about capabilities of each approach and we're going to start with accuracy. Now, RAG's accuracy is really intrinsically tied to a particular component. When we talk about accuracy with RAG, we are talking about the retriever. That's what's important here, because if the retriever fails to fetch a relevant document, well then the LLM might not have the facts to answer correctly, but if the retriever works well, then it actually shields the LLM from receiving irrelevant information. Now, CAG, on the other hand, that preloads all potential relevant information. So it guarantees that the information is in there somewhere, I mean, assuming that the knowledge cache actually does contain the question being asked, but with CAG, all of the work is handed over to the model to extract the right piece of information from that large context. So there's the potential here that the LLM might get confused or it might mix in some unrelated information into its answer. So that's accuracy. What about latency? Well, RAG, that introduces an extra step, namely the retrieval into the query workflow and that adds to response time. So when we look at latency with RAG, it's a bit higher, because each query incurs the overhead of embedding the query and then searching the index and then having the LLM process the retrieved text. But with CAG, once the knowledge is cached, answering a query is just one forward pass of the LLM on the user prompt plus the generation. There's no retrieval lookup time. So when it comes to latency, CAG is going to be lower. Alright, what about scalability? Well, RAG can scale to as much as you can fit into your vector database. So we can have some very large data sets when we are using RAG And that's because it only pulled a tiny slice of the data per query. So if you have 10 million documents, you can index them all and you can still retrieve just a few relevant ones for any single question. The LLM is never going to see all 10 million documents at once, but CAG, however, that does have a hard limit. So with CAG, the scalability restriction is basically related to the model context size. We can only put in there what the model will allow us to fit. And as I mentioned earlier, that's typically like 32 to 100K tokens. So that might be a few hundred documents at most. And even as context windows grow, as they are expected to, RAG will likely always maintain a bit of an edge when it comes to scalability. One more, data freshness. Now, when knowledge changes, RAG, that can just, well, it can just update the index very easily. So it doesn't take a lot of work to do that. It can update incrementally as you add new document embeddings or as you remove outdated ones on the fly. It can always use new information with minimal downtime. But CAG, on the other hand, that is going to require some re-computation when anything actually changes. If the data changes frequently, then CAG kind of loses some of its appeal because you're essentially reloading often, which is going to negate the caching benefit. All right, so let's play a game. It's called RAG or CAG. Now I'm gonna give you a use case and you're gonna shout out RAG if you think retrieval augmented generation is the best option, or you'll yell out CAG if you think cache augmented generation is the way to go. Ready? Alright. Scenario one, I am building an IT help desk bot. So users can submit questions and the system's gonna use a product manual to help augment its answers. Now, the product manual is about 200 pages. It's only updated a few times a year. So, RAG or CAG? Don't be shy. Getting acronyms at the screen is an entirely normal process. All right, I'm gonna imagine that most people here are probably saying... CAG for this one. The knowledge base, in this case the product manual, it's small enough to fit in most LLM context windows, the information is pretty static so the caches need to be updated very frequently, and by caching the information we'll be able to answer queries faster than if we had to query a vector database. So I think CAG is probably the answer for this one. What about scenario two? So with this one you're going to be building a research assistant for a law firm. Now the system needs to search through thousands of legal cases that are constantly being updated with new rulings and new amendments. And when lawyers submit queries, they need answers with accurate citations to relevant legal documents. So for this one, RAG or CAG. I think RAG is the way to go here. The knowledge base in this case, it's massive and it's dynamic with this new content been added all the time. So attempting to cache all this information would quickly exceed most models context windows and also that requirement for precise citations to source materials is actually something that RAG naturally supports through its retrieval mechanism. It will tell us where it got its information from. And also the ability to incrementally update the vector database as new legal documents emerge means that the system always has access to the most current information without requiring full cache recomputation. So, rag all the way here. One last one, one last game of RAG or CAG. So, scenario three, you're building a clinical decision support system for hospitals. And the idea here is that doctors need to query patient records and treatment guides and drug interactions. And the responses need to be really comprehensive and of course, very accurate because they're going to be used by doctors during patient consultations. And the doctors are often gonna ask complex follow-up questions. So RAG or CAG for that? Well, how about... Both. Because in this case, the system could first use RAG to retrieve the most relevant subset from the massive knowledge base. So pulling in specific sections of a particular patient's history and some research papers that are based on the doctor's query. And then instead of simply passing those retrieved chunks to the LLM, it could load all that retrieved content into a long context model that uses CAG, creating a temporary working memory, if you like, for the specific patient case. So it's really a hybrid approach. RAG's ability to efficiently search enormous knowledge bases, and then CAG's capability for providing the full breadth of medical knowledge when needed for those follow-up questions without the system repeatedly querying the database. So essentially, RAG and CAG are two strategies for enhancing LLMs with external knowledge, and you'd consider RAG when your knowledge source is very large, or it's frequently updated, or you need citations, or where resources for running long context window models are a bit limited, but you would consider CAG when you have a fixed set of knowledge that can fit within the context window of the model you're using, where latency is important, it needs to be fast, and where you want to simplify deployment. RAG or CAG, the choice is up to you.",
    "chunks": [
      "Kind: captions Language: en Left to their own devices, large language models have a bit of a knowledge problem. If a piece of information wasn't in their training set, they won't be able to recall it. Maybe something newsworthy that happened after the model completed training, such as who won the 2025 Best Picture at the Oscars, or it could be something proprietary like a client's purchase history. So to overcome that knowledge problem, we can use augmented generation techniques. For example,",
      "retrieval. So retrieval, augmented generation, otherwise known as Now how does that work? Well, essentially we have here a model and the model is going to query an external searchable knowledge base. Here's where we've got our knowledge and that's going to return portions of relevant documents to provide additional context. So we get the documents, we get some context, and we pass that to the LLM model to update its knowledge, if you like, and that updated context, that's used to generate an",
      "answer. Anora that won best picture this year, probably got it out of that data set. But retrieval isn't the only augmented generation game in town. Another one is cash augmented generation or CAG. Now CAG is an alternative method. So rather than querying a knowledge database for answers, the core idea of CAG is to preload the entire knowledge base. So we take everything we know and we put it all into the context window. All of it. The Oscar winners, last week's lunch special at the office",
      "cafeteria, whatever you want. So rather than feeding the model just curated knowledge, we are feeding the model everything, not just the stuff we deemed relevant to the query. So RAG versus CAG. Let's get into how these two things work. the capabilities of each approach, and an enticing game to test your own knowledge, and let's start with RAG. So RAG is essentially a two-phase system. You've got an offline phase where you ingest and index your knowledge, and then you've got an online phase where",
      "you retrieve and generate on demand. And the offline part, pretty straightforward. So you can start with some documents. So this is your knowledge. This could be Word files, PDFs, whatever. and you're going to break them into chunks and create vector embeddings for each chunk using the help of something called an embedding model. Now that embedding model is going to create embeddings and it's going to store them in a database, and specifically this is a vector database where the embeddings are",
      "stored. So you've essentially now created a searchable index of your knowledge. So when a user prompt comes in from the user, this is where the online phase of this is going to kick in. So, first thing that's going to happen is we're going to go to a RAG retriever. and that RAG retriever is gonna take the user's question and it's gonna turn it into a vector using the same embedding model that we used earlier, and that's gonna perform a similarity search of your vector database. Now that's gonna",
      "return the top K most relevant document chunks from here that are related to this query. There might be something like three to five passages that are most likely to contain the answer to the user's query, and we're gonna take those chunks and we're gonna put them into the context window of the LLM alongside the user's initial query and all of this is then gonna get sent to the large language model. So the model is gonna see the question the user submitted plus these relevant bits of context and",
      "use that to generate an answer. We're basically saying to the model, here's the question, here's some potentially useful information to help you answer it that we got out of this vector database, off you go. And the beauty of RAG is that it's very modular, so you can swap out the vector database, you could swap out a different embedding model, or you could change the LLM without rebuilding this entire system. That's RAG. What about CAG? Well, CAG takes a completely different approach. So instead",
      "of retrieving knowledge on demand, you front load it all into the model's context all at once. So we'll start again with our documents. This is all of our gathered knowledge. And we're gonna format them into one massive prompt that fits inside of the model's context window. So here it's gonna fit into this. Now, this could be tens or even hundreds of thousands of tokens and then the large language model is going to take this massive amount of input and it's going to process it. So effectively",
      "this kind of knowledge blob is going to be processed in a single forward pass and it's going to capture and store the model's internal state after it's digested all of this information. Now this internal state blob, it's actually got a name, it's called the KV cache, or the key value cache, and it's created from each self-attention layer and it represents the model's encoded form of all of your documents, all of your knowledge, so it's kind of like the models already read your documents and now",
      "it's memorized it. So when a user submits a query in this situation then we take all of this KV cache and we add the query to it and all of that gets sent into the large language model. And because the Transformers cache has already got all of the knowledge tokens in it, the model can use any relevant information as it generates an answer without having to reprocess all of this text again. So the fundamental distinction between RAG and CAG comes down to when and how knowledge is processed. With",
      "RAG, We say, let's fetch only the stuff that we think we're going to actually need. CAG, that says let's load everything, all of our documents up front and then remember it for later. So with RAG, your knowledge base can be really, really large. This could be millions of documents stored in here because you're only retrieving small pieces at a time. The model only sees what's relevant for a particular query. Whereas with CAG you are constrained by the size of the model's context window. Now a",
      "typical model today that can have a context window of something like 32 ,000 to 100 ,000 tokens. Some are a bit larger than that but that's pretty standard. It's substantial but it's still finite and everything all of these docs need to fit in that window. So let's talk about capabilities of each approach and we're going to start with accuracy. Now, RAG's accuracy is really intrinsically tied to a particular component. When we talk about accuracy with RAG, we are talking about the retriever.",
      "That's what's important here, because if the retriever fails to fetch a relevant document, well then the LLM might not have the facts to answer correctly, but if the retriever works well, then it actually shields the LLM from receiving irrelevant information. Now, CAG, on the other hand, that preloads all potential relevant information. So it guarantees that the information is in there somewhere, I mean, assuming that the knowledge cache actually does contain the question being asked, but with",
      "CAG, all of the work is handed over to the model to extract the right piece of information from that large context. So there's the potential here that the LLM might get confused or it might mix in some unrelated information into its answer. So that's accuracy. What about latency? Well, RAG, that introduces an extra step, namely the retrieval into the query workflow and that adds to response time. So when we look at latency with RAG, it's a bit higher, because each query incurs the overhead of",
      "embedding the query and then searching the index and then having the LLM process the retrieved text. But with CAG, once the knowledge is cached, answering a query is just one forward pass of the LLM on the user prompt plus the generation. There's no retrieval lookup time. So when it comes to latency, CAG is going to be lower. Alright, what about scalability? Well, RAG can scale to as much as you can fit into your vector database. So we can have some very large data sets when we are using RAG And",
      "that's because it only pulled a tiny slice of the data per query. So if you have 10 million documents, you can index them all and you can still retrieve just a few relevant ones for any single question. The LLM is never going to see all 10 million documents at once, but CAG, however, that does have a hard limit. So with CAG, the scalability restriction is basically related to the model context size. We can only put in there what the model will allow us to fit. And as I mentioned earlier, that's",
      "typically like 32 to 100K tokens. So that might be a few hundred documents at most. And even as context windows grow, as they are expected to, RAG will likely always maintain a bit of an edge when it comes to scalability. One more, data freshness. Now, when knowledge changes, RAG, that can just, well, it can just update the index very easily. So it doesn't take a lot of work to do that. It can update incrementally as you add new document embeddings or as you remove outdated ones on the fly. It",
      "can always use new information with minimal downtime. But CAG, on the other hand, that is going to require some re-computation when anything actually changes. If the data changes frequently, then CAG kind of loses some of its appeal because you're essentially reloading often, which is going to negate the caching benefit. All right, so let's play a game. It's called RAG or CAG. Now I'm gonna give you a use case and you're gonna shout out RAG if you think retrieval augmented generation is the best",
      "option, or you'll yell out CAG if you think cache augmented generation is the way to go. Ready? Alright. Scenario one, I am building an IT help desk bot. So users can submit questions and the system's gonna use a product manual to help augment its answers. Now, the product manual is about 200 pages. It's only updated a few times a year. So, RAG or CAG? Don't be shy. Getting acronyms at the screen is an entirely normal process. All right, I'm gonna imagine that most people here are probably",
      "saying... CAG for this one. The knowledge base, in this case the product manual, it's small enough to fit in most LLM context windows, the information is pretty static so the caches need to be updated very frequently, and by caching the information we'll be able to answer queries faster than if we had to query a vector database. So I think CAG is probably the answer for this one. What about scenario two? So with this one you're going to be building a research assistant for a law firm. Now the",
      "system needs to search through thousands of legal cases that are constantly being updated with new rulings and new amendments. And when lawyers submit queries, they need answers with accurate citations to relevant legal documents. So for this one, RAG or CAG. I think RAG is the way to go here. The knowledge base in this case, it's massive and it's dynamic with this new content been added all the time. So attempting to cache all this information would quickly exceed most models context windows and",
      "also that requirement for precise citations to source materials is actually something that RAG naturally supports through its retrieval mechanism. It will tell us where it got its information from. And also the ability to incrementally update the vector database as new legal documents emerge means that the system always has access to the most current information without requiring full cache recomputation. So, rag all the way here. One last one, one last game of RAG or CAG. So, scenario three,",
      "you're building a clinical decision support system for hospitals. And the idea here is that doctors need to query patient records and treatment guides and drug interactions. And the responses need to be really comprehensive and of course, very accurate because they're going to be used by doctors during patient consultations. And the doctors are often gonna ask complex follow-up questions. So RAG or CAG for that? Well, how about... Both. Because in this case, the system could first use RAG to",
      "retrieve the most relevant subset from the massive knowledge base. So pulling in specific sections of a particular patient's history and some research papers that are based on the doctor's query. And then instead of simply passing those retrieved chunks to the LLM, it could load all that retrieved content into a long context model that uses CAG, creating a temporary working memory, if you like, for the specific patient case. So it's really a hybrid approach. RAG's ability to efficiently search",
      "enormous knowledge bases, and then CAG's capability for providing the full breadth of medical knowledge when needed for those follow-up questions without the system repeatedly querying the database. So essentially, RAG and CAG are two strategies for enhancing LLMs with external knowledge, and you'd consider RAG when your knowledge source is very large, or it's frequently updated, or you need citations, or where resources for running long context window models are a bit limited, but you would",
      "consider CAG when you have a fixed set of knowledge that can fit within the context window of the model you're using, where latency is important, it needs to be fast, and where you want to simplify deployment. RAG or CAG, the choice is up to you."
    ],
    "chunk_count": 27,
    "content_id": "f88903ea-5038-42ae-93bd-3b147a361501",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554820"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=qppV3n3YlF8&t=1s": {
    "title": "RAG Explained",
    "url": "https://www.youtube.com/watch?v=qppV3n3YlF8&t=1s",
    "description": "Get the interactive demo → https://ibm.biz/BdmPEb\nLearn about the technology → https://ibm.biz/BdmPEp\n\nOftentimes, GAI and RAG discussions are interconnected. Learn more about about RAG is and how it works alongside your databases, LLMs and vector databases for better results with Luv Aggarwal and Shawn Brennan. \n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdmP2c",
    "duration": 483,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en so imagine you're a journalist and you so imagine you're a journalist and you so imagine you're a journalist and you want to write an article on a specific want to write an article on a specific want to write an article on a specific topic now you have a pretty good general topic now you have a pretty good general topic now you have a pretty good general idea about this topic but you'd like to idea about this topic but you'd like to idea about this topic but you'd like to do some more research so you go to your do some more research so you go to your do some more research so you go to your local local local library right now this library right now this library right now this library has thousands of books on library has thousands of books on library has thousands of books on multiple different topics but how do you multiple different topics but how do you multiple different topics but how do you know as the journalist which books are know as the journalist which books are know as the journalist which books are relevant for your topic well you go to relevant for your topic well you go to relevant for your topic well you go to the librarian now the librarian is the the librarian now the librarian is the the librarian now the librarian is the expert on what books contain which expert on what books contain which expert on what books contain which information in the library so our information in the library so our information in the library so our journalist queries the librarian to uh journalist queries the librarian to uh journalist queries the librarian to uh retrieve uh books on certain topics and retrieve uh books on certain topics and retrieve uh books on certain topics and the librarian uh produces those books the librarian uh produces those books the librarian uh produces those books and provides them back to the journalist and provides them back to the journalist and provides them back to the journalist now the librarian isn't the expert on now the librarian isn't the expert on now the librarian isn't the expert on writing the article and the journalist writing the article and the journalist writing the article and the journalist isn't the expert on finding the most isn't the expert on finding the most isn't the expert on finding the most upto-date and relevant information upto-date and relevant information upto-date and relevant information but with the combination of the two we but with the combination of the two we but with the combination of the two we can get the job can get the job can get the job done love this sounds like a lot like done love this sounds like a lot like done love this sounds like a lot like the process of rag or retrieval the process of rag or retrieval the process of rag or retrieval augmented generation where large augmented generation where large augmented generation where large language models call on Vector databases language models call on Vector databases language models call on Vector databases to provide key sources of data and to provide key sources of data and to provide key sources of data and information to answer a question H I'm information to answer a question H I'm information to answer a question H I'm not seeing the connection can you help not seeing the connection can you help not seeing the connection can you help me understand a little bit better me understand a little bit better me understand a little bit better sure so we have a sure so we have a sure so we have a user in your scenario it's that user in your scenario it's that user in your scenario it's that journalist and they have a and they have a and they have a question so what types of questions question so what types of questions question so what types of questions would you want to ask right maybe we can would you want to ask right maybe we can would you want to ask right maybe we can make this more of a business context make this more of a business context make this more of a business context yeah so let's say this is a business yeah so let's say this is a business yeah so let's say this is a business analyst and let's say they want to ask analyst and let's say they want to ask analyst and let's say they want to ask um what was Revenue in q1 from customers um what was Revenue in q1 from customers um what was Revenue in q1 from customers in the Northeast region right so that's in the Northeast region right so that's in the Northeast region right so that's your prompt okay so a couple questions on prompt okay so a couple questions on prompt okay so a couple questions on that user does it have to be a person or that user does it have to be a person or that user does it have to be a person or could it be something else too yeah so could it be something else too yeah so could it be something else too yeah so this doesn't necessarily have to be a this doesn't necessarily have to be a this doesn't necessarily have to be a user it could be a user it could be a user it could be a bot or it could be another bot or it could be another bot or it could be another application even the question that we're application even the question that we're application even the question that we're talking about what was our Revenue in q1 talking about what was our Revenue in q1 talking about what was our Revenue in q1 from the Northeast you know the first from the Northeast you know the first from the Northeast you know the first part of that question it's pretty easy part of that question it's pretty easy part of that question it's pretty easy for you know a general llm to understand for you know a general llm to understand for you know a general llm to understand right what was our Revenue but it's that right what was our Revenue but it's that right what was our Revenue but it's that second part in q1 from customers in the second part in q1 from customers in the second part in q1 from customers in the Northeast that's not something that lln Northeast that's not something that lln Northeast that's not something that lln are trained on right it's very specific are trained on right it's very specific are trained on right it's very specific to our business and it changes over time to our business and it changes over time to our business and it changes over time so we have to treat those separately so so we have to treat those separately so so we have to treat those separately so how do we how do we uh manage that part how do we how do we uh manage that part how do we how do we uh manage that part of the request exactly you'll need of the request exactly you'll need of the request exactly you'll need multiple different sources of data multiple different sources of data multiple different sources of data potentially to answer a specific potentially to answer a specific potentially to answer a specific question right whether that's maybe a question right whether that's maybe a question right whether that's maybe a PDF or another business application or PDF or another business application or PDF or another business application or maybe some some images whatever that maybe some some images whatever that maybe some some images whatever that question is we need the appropriate data question is we need the appropriate data question is we need the appropriate data in order to provide the answer back what in order to provide the answer back what in order to provide the answer back what technology uh allows us to aggregate technology uh allows us to aggregate technology uh allows us to aggregate that data uh and use it for our llm yeah that data uh and use it for our llm yeah that data uh and use it for our llm yeah so we can take this data and we can put so we can take this data and we can put so we can take this data and we can put it into what we call a vector database a vector database is a database a vector database is a database a vector database is a mathematical representation of mathematical representation of mathematical representation of structured and unstructured data similar structured and unstructured data similar structured and unstructured data similar to what we might see in an array gotcha to what we might see in an array gotcha to what we might see in an array gotcha and and these arrays are uh better and and these arrays are uh better and and these arrays are uh better suited or easier to understand for suited or easier to understand for suited or easier to understand for machine learning or generative AI models machine learning or generative AI models machine learning or generative AI models versus just that uh underlying versus just that uh underlying versus just that uh underlying unstructured data exactly we query our unstructured data exactly we query our unstructured data exactly we query our Vector database right and we get back an Vector database right and we get back an Vector database right and we get back an embedding that uh includes uh the the embedding that uh includes uh the the embedding that uh includes uh the the relevant data for which uh we're relevant data for which uh we're relevant data for which uh we're prompting and then we includeed back prompting and then we includeed back prompting and then we includeed back into the original prompt right yeah into the original prompt right yeah into the original prompt right yeah exactly that feeds back into the prompt exactly that feeds back into the prompt exactly that feeds back into the prompt and then once we're at this point we and then once we're at this point we and then once we're at this point we move over to the other side of the move over to the other side of the move over to the other side of the equation which is the large language equation which is the large language equation which is the large language model gotcha so that that prompt that model gotcha so that that prompt that model gotcha so that that prompt that includes the vector embeddings now are includes the vector embeddings now are includes the vector embeddings now are fed into the large language model which fed into the large language model which fed into the large language model which then produces the then produces the then produces the output with the answer to our original output with the answer to our original output with the answer to our original question with sourced upto-date and question with sourced upto-date and question with sourced upto-date and accurate data exactly and that's a accurate data exactly and that's a accurate data exactly and that's a crucial aspect of it as new data comes crucial aspect of it as new data comes crucial aspect of it as new data comes into this Vector into this Vector into this Vector database or things that are updated back database or things that are updated back database or things that are updated back to your relevant question around to your relevant question around to your relevant question around performance in q1 as new data comes in performance in q1 as new data comes in performance in q1 as new data comes in those embeddings are updated ated so those embeddings are updated ated so those embeddings are updated ated so when that question's asked a second time when that question's asked a second time when that question's asked a second time we have more relevant data in order to we have more relevant data in order to we have more relevant data in order to provide back to the llm who then provide back to the llm who then provide back to the llm who then generates the output and the answer okay generates the output and the answer okay generates the output and the answer okay very cool so Sean this sounds a lot like very cool so Sean this sounds a lot like very cool so Sean this sounds a lot like my original analogy there with the my original analogy there with the my original analogy there with the librarian and our journalist right so librarian and our journalist right so librarian and our journalist right so the journalist trusts that the the journalist trusts that the the journalist trusts that the information in the library is accurate information in the library is accurate information in the library is accurate and correct now one of the challenges and correct now one of the challenges and correct now one of the challenges that I see is when I'm talking to that I see is when I'm talking to that I see is when I'm talking to Enterprise customers is they're Enterprise customers is they're Enterprise customers is they're concerned about deploying this kind of concerned about deploying this kind of concerned about deploying this kind of techn techology into customer facing techn techology into customer facing techn techology into customer facing business critical applications so if business critical applications so if business critical applications so if they're building applications taking they're building applications taking they're building applications taking customer orders processing refunds customer orders processing refunds customer orders processing refunds they're worried that uh uh these kinds they're worried that uh uh these kinds they're worried that uh uh these kinds of Technologies can produce of Technologies can produce of Technologies can produce hallucinations or inaccurate results hallucinations or inaccurate results hallucinations or inaccurate results right or perpetuate some kind of bias right or perpetuate some kind of bias right or perpetuate some kind of bias what are some things that uh can be done what are some things that uh can be done what are some things that uh can be done to help mitigate some of these concerns to help mitigate some of these concerns to help mitigate some of these concerns that brings up a great Point love right that brings up a great Point love right that brings up a great Point love right data that comes in on this side but also data that comes in on this side but also data that comes in on this side but also on this side is incredibly important on this side is incredibly important on this side is incredibly important into the output that we get when we go into the output that we get when we go into the output that we get when we go to make that prompt and get that answer to make that prompt and get that answer to make that prompt and get that answer back so it really is true garbage in and back so it really is true garbage in and back so it really is true garbage in and garbage out right so we need to make garbage out right so we need to make garbage out right so we need to make sure we have good data that comes into sure we have good data that comes into sure we have good data that comes into the vector database we need to make sure the vector database we need to make sure the vector database we need to make sure that data is clean governed and managed that data is clean governed and managed that data is clean governed and managed properly gotcha so what I'm hearing is properly gotcha so what I'm hearing is properly gotcha so what I'm hearing is that things like that things like that things like governance and data governance and data governance and data management are of course crucial to the management are of course crucial to the management are of course crucial to the vector database right so making sure vector database right so making sure vector database right so making sure that the actual information that's that the actual information that's that the actual information that's flowing through into the model such as flowing through into the model such as flowing through into the model such as the business results in the sample the business results in the sample the business results in the sample prompt we talked about is governed and prompt we talked about is governed and prompt we talked about is governed and clean but also crucially on the large clean but also crucially on the large clean but also crucially on the large language model side we need to make sure language model side we need to make sure language model side we need to make sure that we're not using a large language that we're not using a large language that we're not using a large language model that takes a blackbox approach model that takes a blackbox approach model that takes a blackbox approach right so a model where you don't right so a model where you don't right so a model where you don't actually know what is the underlying actually know what is the underlying actually know what is the underlying data that went into training it right data that went into training it right data that went into training it right you don't know if there's any you don't know if there's any you don't know if there's any intellectual property in there you don't intellectual property in there you don't intellectual property in there you don't know if there's inaccurate IES in there know if there's inaccurate IES in there know if there's inaccurate IES in there or you don't know if there are pieces of or you don't know if there are pieces of or you don't know if there are pieces of data that will end up perpetuating bias data that will end up perpetuating bias data that will end up perpetuating bias in your output results right so as a in your output results right so as a in your output results right so as a business and as as a business that's business and as as a business that's business and as as a business that's trying to uh uh manage and uphold their trying to uh uh manage and uphold their trying to uh uh manage and uphold their brain reputation it's absolutely brain reputation it's absolutely brain reputation it's absolutely critical to make sure that we're taking critical to make sure that we're taking critical to make sure that we're taking an approach that uh uh uses llms that an approach that uh uh uses llms that an approach that uh uh uses llms that are transparent in how they were trained are transparent in how they were trained are transparent in how they were trained and uh we can be 100% certain that there and uh we can be 100% certain that there and uh we can be 100% certain that there aren't any aren't any aren't any uh inaccuracies or data that's not uh inaccuracies or data that's not uh inaccuracies or data that's not supposed to be in there to be in there supposed to be in there to be in there supposed to be in there to be in there right yeah exactly it's incredibly right yeah exactly it's incredibly right yeah exactly it's incredibly important especially as a brand that we important especially as a brand that we important especially as a brand that we get the right answers we've seen the get the right answers we've seen the get the right answers we've seen the results of impact and especially back to results of impact and especially back to results of impact and especially back to our original question around what was our original question around what was our original question around what was our Revenue in q1 right we don't want our Revenue in q1 right we don't want our Revenue in q1 right we don't want that to be impacted by the results of a that to be impacted by the results of a that to be impacted by the results of a question that comes from you know that question that comes from you know that question that comes from you know that prompts one of our llms exactly exactly prompts one of our llms exactly exactly prompts one of our llms exactly exactly so very powerful technology but it makes so very powerful technology but it makes so very powerful technology but it makes me think back to the the library me think back to the the library me think back to the the library uh our journalists and librarian they uh our journalists and librarian they uh our journalists and librarian they both trust the data and the books that both trust the data and the books that both trust the data and the books that are in the library we have to have that are in the library we have to have that are in the library we have to have that same kind of confidence when we're same kind of confidence when we're same kind of confidence when we're building out these types of gender AI building out these types of gender AI building out these types of gender AI use cases for business as well exactly use cases for business as well exactly use cases for business as well exactly love so governance AI but also data and love so governance AI but also data and love so governance AI but also data and data management are incredibly important data management are incredibly important data management are incredibly important to this process we need all three in to this process we need all three in to this process we need all three in order to get the best result",
    "chunks": [
      "Kind: captions Language: en so imagine you're a journalist and you so imagine you're a journalist and you so imagine you're a journalist and you want to write an article on a specific want to write an article on a specific want to write an article on a specific topic now you have a pretty good general topic now you have a pretty good general topic now you have a pretty good general idea about this topic but you'd like to idea about this topic but you'd like to idea about this topic but you'd",
      "like to do some more research so you go to your do some more research so you go to your do some more research so you go to your local local local library right now this library right now this library right now this library has thousands of books on library has thousands of books on library has thousands of books on multiple different topics but how do you multiple different topics but how do you multiple different topics but how do you know as the journalist which books are know as the journalist",
      "which books are know as the journalist which books are relevant for your topic well you go to relevant for your topic well you go to relevant for your topic well you go to the librarian now the librarian is the the librarian now the librarian is the the librarian now the librarian is the expert on what books contain which expert on what books contain which expert on what books contain which information in the library so our information in the library so our information in the library so our",
      "journalist queries the librarian to uh journalist queries the librarian to uh journalist queries the librarian to uh retrieve uh books on certain topics and retrieve uh books on certain topics and retrieve uh books on certain topics and the librarian uh produces those books the librarian uh produces those books the librarian uh produces those books and provides them back to the journalist and provides them back to the journalist and provides them back to the journalist now the librarian isn't the",
      "expert on now the librarian isn't the expert on now the librarian isn't the expert on writing the article and the journalist writing the article and the journalist writing the article and the journalist isn't the expert on finding the most isn't the expert on finding the most isn't the expert on finding the most upto-date and relevant information upto-date and relevant information upto-date and relevant information but with the combination of the two we but with the combination of the two we but",
      "with the combination of the two we can get the job can get the job can get the job done love this sounds like a lot like done love this sounds like a lot like done love this sounds like a lot like the process of rag or retrieval the process of rag or retrieval the process of rag or retrieval augmented generation where large augmented generation where large augmented generation where large language models call on Vector databases language models call on Vector databases language models call on",
      "Vector databases to provide key sources of data and to provide key sources of data and to provide key sources of data and information to answer a question H I'm information to answer a question H I'm information to answer a question H I'm not seeing the connection can you help not seeing the connection can you help not seeing the connection can you help me understand a little bit better me understand a little bit better me understand a little bit better sure so we have a sure so we have a sure so",
      "we have a user in your scenario it's that user in your scenario it's that user in your scenario it's that journalist and they have a and they have a and they have a question so what types of questions question so what types of questions question so what types of questions would you want to ask right maybe we can would you want to ask right maybe we can would you want to ask right maybe we can make this more of a business context make this more of a business context make this more of a business",
      "context yeah so let's say this is a business yeah so let's say this is a business yeah so let's say this is a business analyst and let's say they want to ask analyst and let's say they want to ask analyst and let's say they want to ask um what was Revenue in q1 from customers um what was Revenue in q1 from customers um what was Revenue in q1 from customers in the Northeast region right so that's in the Northeast region right so that's in the Northeast region right so that's your prompt okay so a",
      "couple questions on prompt okay so a couple questions on prompt okay so a couple questions on that user does it have to be a person or that user does it have to be a person or that user does it have to be a person or could it be something else too yeah so could it be something else too yeah so could it be something else too yeah so this doesn't necessarily have to be a this doesn't necessarily have to be a this doesn't necessarily have to be a user it could be a user it could be a user it could",
      "be a bot or it could be another bot or it could be another bot or it could be another application even the question that we're application even the question that we're application even the question that we're talking about what was our Revenue in q1 talking about what was our Revenue in q1 talking about what was our Revenue in q1 from the Northeast you know the first from the Northeast you know the first from the Northeast you know the first part of that question it's pretty easy part of that",
      "question it's pretty easy part of that question it's pretty easy for you know a general llm to understand for you know a general llm to understand for you know a general llm to understand right what was our Revenue but it's that right what was our Revenue but it's that right what was our Revenue but it's that second part in q1 from customers in the second part in q1 from customers in the second part in q1 from customers in the Northeast that's not something that lln Northeast that's not something",
      "that lln Northeast that's not something that lln are trained on right it's very specific are trained on right it's very specific are trained on right it's very specific to our business and it changes over time to our business and it changes over time to our business and it changes over time so we have to treat those separately so so we have to treat those separately so so we have to treat those separately so how do we how do we uh manage that part how do we how do we uh manage that part how do we",
      "how do we uh manage that part of the request exactly you'll need of the request exactly you'll need of the request exactly you'll need multiple different sources of data multiple different sources of data multiple different sources of data potentially to answer a specific potentially to answer a specific potentially to answer a specific question right whether that's maybe a question right whether that's maybe a question right whether that's maybe a PDF or another business application or PDF or",
      "another business application or PDF or another business application or maybe some some images whatever that maybe some some images whatever that maybe some some images whatever that question is we need the appropriate data question is we need the appropriate data question is we need the appropriate data in order to provide the answer back what in order to provide the answer back what in order to provide the answer back what technology uh allows us to aggregate technology uh allows us to aggregate",
      "technology uh allows us to aggregate that data uh and use it for our llm yeah that data uh and use it for our llm yeah that data uh and use it for our llm yeah so we can take this data and we can put so we can take this data and we can put so we can take this data and we can put it into what we call a vector database a vector database is a database a vector database is a database a vector database is a mathematical representation of mathematical representation of mathematical representation of",
      "structured and unstructured data similar structured and unstructured data similar structured and unstructured data similar to what we might see in an array gotcha to what we might see in an array gotcha to what we might see in an array gotcha and and these arrays are uh better and and these arrays are uh better and and these arrays are uh better suited or easier to understand for suited or easier to understand for suited or easier to understand for machine learning or generative AI models machine",
      "learning or generative AI models machine learning or generative AI models versus just that uh underlying versus just that uh underlying versus just that uh underlying unstructured data exactly we query our unstructured data exactly we query our unstructured data exactly we query our Vector database right and we get back an Vector database right and we get back an Vector database right and we get back an embedding that uh includes uh the the embedding that uh includes uh the the embedding that uh",
      "includes uh the the relevant data for which uh we're relevant data for which uh we're relevant data for which uh we're prompting and then we includeed back prompting and then we includeed back prompting and then we includeed back into the original prompt right yeah into the original prompt right yeah into the original prompt right yeah exactly that feeds back into the prompt exactly that feeds back into the prompt exactly that feeds back into the prompt and then once we're at this point we and",
      "then once we're at this point we and then once we're at this point we move over to the other side of the move over to the other side of the move over to the other side of the equation which is the large language equation which is the large language equation which is the large language model gotcha so that that prompt that model gotcha so that that prompt that model gotcha so that that prompt that includes the vector embeddings now are includes the vector embeddings now are includes the vector",
      "embeddings now are fed into the large language model which fed into the large language model which fed into the large language model which then produces the then produces the then produces the output with the answer to our original output with the answer to our original output with the answer to our original question with sourced upto-date and question with sourced upto-date and question with sourced upto-date and accurate data exactly and that's a accurate data exactly and that's a accurate data",
      "exactly and that's a crucial aspect of it as new data comes crucial aspect of it as new data comes crucial aspect of it as new data comes into this Vector into this Vector into this Vector database or things that are updated back database or things that are updated back database or things that are updated back to your relevant question around to your relevant question around to your relevant question around performance in q1 as new data comes in performance in q1 as new data comes in performance",
      "in q1 as new data comes in those embeddings are updated ated so those embeddings are updated ated so those embeddings are updated ated so when that question's asked a second time when that question's asked a second time when that question's asked a second time we have more relevant data in order to we have more relevant data in order to we have more relevant data in order to provide back to the llm who then provide back to the llm who then provide back to the llm who then generates the output and",
      "the answer okay generates the output and the answer okay generates the output and the answer okay very cool so Sean this sounds a lot like very cool so Sean this sounds a lot like very cool so Sean this sounds a lot like my original analogy there with the my original analogy there with the my original analogy there with the librarian and our journalist right so librarian and our journalist right so librarian and our journalist right so the journalist trusts that the the journalist trusts that the",
      "the journalist trusts that the information in the library is accurate information in the library is accurate information in the library is accurate and correct now one of the challenges and correct now one of the challenges and correct now one of the challenges that I see is when I'm talking to that I see is when I'm talking to that I see is when I'm talking to Enterprise customers is they're Enterprise customers is they're Enterprise customers is they're concerned about deploying this kind of",
      "concerned about deploying this kind of concerned about deploying this kind of techn techology into customer facing techn techology into customer facing techn techology into customer facing business critical applications so if business critical applications so if business critical applications so if they're building applications taking they're building applications taking they're building applications taking customer orders processing refunds customer orders processing refunds customer orders",
      "processing refunds they're worried that uh uh these kinds they're worried that uh uh these kinds they're worried that uh uh these kinds of Technologies can produce of Technologies can produce of Technologies can produce hallucinations or inaccurate results hallucinations or inaccurate results hallucinations or inaccurate results right or perpetuate some kind of bias right or perpetuate some kind of bias right or perpetuate some kind of bias what are some things that uh can be done what are some",
      "things that uh can be done what are some things that uh can be done to help mitigate some of these concerns to help mitigate some of these concerns to help mitigate some of these concerns that brings up a great Point love right that brings up a great Point love right that brings up a great Point love right data that comes in on this side but also data that comes in on this side but also data that comes in on this side but also on this side is incredibly important on this side is incredibly",
      "important on this side is incredibly important into the output that we get when we go into the output that we get when we go into the output that we get when we go to make that prompt and get that answer to make that prompt and get that answer to make that prompt and get that answer back so it really is true garbage in and back so it really is true garbage in and back so it really is true garbage in and garbage out right so we need to make garbage out right so we need to make garbage out right so",
      "we need to make sure we have good data that comes into sure we have good data that comes into sure we have good data that comes into the vector database we need to make sure the vector database we need to make sure the vector database we need to make sure that data is clean governed and managed that data is clean governed and managed that data is clean governed and managed properly gotcha so what I'm hearing is properly gotcha so what I'm hearing is properly gotcha so what I'm hearing is that",
      "things like that things like that things like governance and data governance and data governance and data management are of course crucial to the management are of course crucial to the management are of course crucial to the vector database right so making sure vector database right so making sure vector database right so making sure that the actual information that's that the actual information that's that the actual information that's flowing through into the model such as flowing through into",
      "the model such as flowing through into the model such as the business results in the sample the business results in the sample the business results in the sample prompt we talked about is governed and prompt we talked about is governed and prompt we talked about is governed and clean but also crucially on the large clean but also crucially on the large clean but also crucially on the large language model side we need to make sure language model side we need to make sure language model side we",
      "need to make sure that we're not using a large language that we're not using a large language that we're not using a large language model that takes a blackbox approach model that takes a blackbox approach model that takes a blackbox approach right so a model where you don't right so a model where you don't right so a model where you don't actually know what is the underlying actually know what is the underlying actually know what is the underlying data that went into training it right data that",
      "went into training it right data that went into training it right you don't know if there's any you don't know if there's any you don't know if there's any intellectual property in there you don't intellectual property in there you don't intellectual property in there you don't know if there's inaccurate IES in there know if there's inaccurate IES in there know if there's inaccurate IES in there or you don't know if there are pieces of or you don't know if there are pieces of or you don't know if",
      "there are pieces of data that will end up perpetuating bias data that will end up perpetuating bias data that will end up perpetuating bias in your output results right so as a in your output results right so as a in your output results right so as a business and as as a business that's business and as as a business that's business and as as a business that's trying to uh uh manage and uphold their trying to uh uh manage and uphold their trying to uh uh manage and uphold their brain reputation",
      "it's absolutely brain reputation it's absolutely brain reputation it's absolutely critical to make sure that we're taking critical to make sure that we're taking critical to make sure that we're taking an approach that uh uh uses llms that an approach that uh uh uses llms that an approach that uh uh uses llms that are transparent in how they were trained are transparent in how they were trained are transparent in how they were trained and uh we can be 100% certain that there and uh we can be 100%",
      "certain that there and uh we can be 100% certain that there aren't any aren't any aren't any uh inaccuracies or data that's not uh inaccuracies or data that's not uh inaccuracies or data that's not supposed to be in there to be in there supposed to be in there to be in there supposed to be in there to be in there right yeah exactly it's incredibly right yeah exactly it's incredibly right yeah exactly it's incredibly important especially as a brand that we important especially as a brand that we",
      "important especially as a brand that we get the right answers we've seen the get the right answers we've seen the get the right answers we've seen the results of impact and especially back to results of impact and especially back to results of impact and especially back to our original question around what was our original question around what was our original question around what was our Revenue in q1 right we don't want our Revenue in q1 right we don't want our Revenue in q1 right we don't want",
      "that to be impacted by the results of a that to be impacted by the results of a that to be impacted by the results of a question that comes from you know that question that comes from you know that question that comes from you know that prompts one of our llms exactly exactly prompts one of our llms exactly exactly prompts one of our llms exactly exactly so very powerful technology but it makes so very powerful technology but it makes so very powerful technology but it makes me think back to the",
      "the library me think back to the the library me think back to the the library uh our journalists and librarian they uh our journalists and librarian they uh our journalists and librarian they both trust the data and the books that both trust the data and the books that both trust the data and the books that are in the library we have to have that are in the library we have to have that are in the library we have to have that same kind of confidence when we're same kind of confidence when we're",
      "same kind of confidence when we're building out these types of gender AI building out these types of gender AI building out these types of gender AI use cases for business as well exactly use cases for business as well exactly use cases for business as well exactly love so governance AI but also data and love so governance AI but also data and love so governance AI but also data and data management are incredibly important data management are incredibly important data management are incredibly",
      "important to this process we need all three in to this process we need all three in to this process we need all three in order to get the best result"
    ],
    "chunk_count": 42,
    "content_id": "5f5036fa-a958-4f5b-b28b-1356084763a5",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554824"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=0z9_MhcYvcY&t=1s": {
    "title": "What is Agentic RAG?",
    "url": "https://www.youtube.com/watch?v=0z9_MhcYvcY&t=1s",
    "description": "Want to learn more about AI agents and assistants? Register for Virtual Agents Day here → https://ibm.biz/BdaAVa\nDownload the AI model guide to learn more → https://ibm.biz/Bdaqqb\nLearn more about AI solutions → https://ibm.biz/Bdaqq8\n\nDiscover the future of AI-driven conversations with Agentic RAG. This powerful pipeline enhances responses from large language models by incorporating relevant data retrieved from vector databases. Join David Levy as he discusses how Agentic RAG can create more responsive, accurate, and adaptable AI systems to better service fields like customer service, legal tech, and beyond.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/Bdaqqp",
    "duration": 341,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en So we all know what retrieval augmented generation is. But let's just do a quick refresher. Retrieval augmented generation is a powerful and popular pipeline that enhances responses from a large language model. It does this by incorporating relevant data retrieved from a vector database, adding it as context to the prompt, and sending it to the LLM for generation. What this does is it allows the LLM to ground its response in concrete and accurate information, and that improves the quality and reliability of the response. Let me quickly sketch it out. So let's say we have a user or an application, even. And they send a query. Now without retrieval augment the generation. This query is going to go and get itself interpolated into a prompt. And from there that's going to hit the LLM. And that's going to generate an output. To make this rag. We can add a vector database. So instead of just going directly and getting itself interpolated into the prompt, it's going to hit this vector db. And the response from that vector db is going to be used as context for the prompt. Now in this typical pipeline we call the LLM only once, and we use it solely to generate a response. But what if we could leverage the LLM not just for responses, but also for additional tasks like deciding which vector database to query If we have multiple databases, or even determining the type of response to give? Should an answer with text generate a chart or even provide a code snippet? And that would all be dependent on the context of that query. So this is where the agenetic RAG pipeline comes into play. In agenetic RAG, we use the LLM as an agent and the LLM goes beyond just generating a response. It takes on an active role and can make decisions that will improve both the relevance and accuracy of the retrieved data. Now, let's explore how we can augment the initial process with an agent and a couple of different sources of data. So instead of just one single source, let's add a second. And the first one can be, you know, internal documentation, Right? And the second one can be general industry knowledge. Now in the internal documentation we're going to have things like policies procedures and guidelines. And the general knowledge base will have things like industry standards, best practices and public resources. So how can we get the LLM to use the vector database that contains the data that would be most relevant to the query? Let's add that agent into this pipeline. Now, this agent can intelligently decide which database to query based on the user's question, and the agent isn't making a random guess. It's leveraging the LLMs language, understanding capabilities to interpret the query and determine its context. So if an employee asks what's the company's policy on remote work during the holidays, it would route that to the internal documentation, and that response will be used as context for the prompt. But if the question is more general, like what are the industries standards for remote work in tech companies, the agent's going to route that to the general knowledge database, and that context is going to be used within that prompt powered by an LLM and properly trained, the agent analyzes the query and based on the understanding of the content and the context, decides which database to use. But they're not always going to ask questions that are generally or genuinely relevant to any of this, any of the stuff that we have in our vector DB. So what if someone asks a question that is just totally out of left field? Like who won the World Series in 2015? What the agent can do at that point is it could route it to a failsafe. So because the agent is able to recognize the context of the query, it could recognize that it's not a part of the two databases that we have, could route it to the failsafe and return back. Sorry, I don't have the information in looking for. This agentic RAG pipeline can be used in customer support systems and legal tech. For example, a lawyer can source answers to their questions from like their internal briefs and then in another query, just get stuff from public caseload databases. The agent can be utilized in a ton of ways. Agentic RAG is an evolution in how we enhance the RAG pipeline by moving beyond simple response generation to more intelligent decision making. By allowing an agent to choose the best data sources and potentially even incorporate external information like real timedata or third party services. We can create a pipeline that's more responsive, more accurate, and more adaptable. This approach opens up so many possibilities for applications in customer service, legal, tech, health care, virtually any field as IT technology continues to evolve. We will see AI systems that truly understand context and can deliver amazing values to the end user.",
    "chunks": [
      "Kind: captions Language: en So we all know what retrieval augmented generation is. But let's just do a quick refresher. Retrieval augmented generation is a powerful and popular pipeline that enhances responses from a large language model. It does this by incorporating relevant data retrieved from a vector database, adding it as context to the prompt, and sending it to the LLM for generation. What this does is it allows the LLM to ground its response in concrete and accurate information, and that",
      "improves the quality and reliability of the response. Let me quickly sketch it out. So let's say we have a user or an application, even. And they send a query. Now without retrieval augment the generation. This query is going to go and get itself interpolated into a prompt. And from there that's going to hit the LLM. And that's going to generate an output. To make this rag. We can add a vector database. So instead of just going directly and getting itself interpolated into the prompt, it's going",
      "to hit this vector db. And the response from that vector db is going to be used as context for the prompt. Now in this typical pipeline we call the LLM only once, and we use it solely to generate a response. But what if we could leverage the LLM not just for responses, but also for additional tasks like deciding which vector database to query If we have multiple databases, or even determining the type of response to give? Should an answer with text generate a chart or even provide a code snippet?",
      "And that would all be dependent on the context of that query. So this is where the agenetic RAG pipeline comes into play. In agenetic RAG, we use the LLM as an agent and the LLM goes beyond just generating a response. It takes on an active role and can make decisions that will improve both the relevance and accuracy of the retrieved data. Now, let's explore how we can augment the initial process with an agent and a couple of different sources of data. So instead of just one single source, let's",
      "add a second. And the first one can be, you know, internal documentation, Right? And the second one can be general industry knowledge. Now in the internal documentation we're going to have things like policies procedures and guidelines. And the general knowledge base will have things like industry standards, best practices and public resources. So how can we get the LLM to use the vector database that contains the data that would be most relevant to the query? Let's add that agent into this",
      "pipeline. Now, this agent can intelligently decide which database to query based on the user's question, and the agent isn't making a random guess. It's leveraging the LLMs language, understanding capabilities to interpret the query and determine its context. So if an employee asks what's the company's policy on remote work during the holidays, it would route that to the internal documentation, and that response will be used as context for the prompt. But if the question is more general, like",
      "what are the industries standards for remote work in tech companies, the agent's going to route that to the general knowledge database, and that context is going to be used within that prompt powered by an LLM and properly trained, the agent analyzes the query and based on the understanding of the content and the context, decides which database to use. But they're not always going to ask questions that are generally or genuinely relevant to any of this, any of the stuff that we have in our vector",
      "DB. So what if someone asks a question that is just totally out of left field? Like who won the World Series in 2015? What the agent can do at that point is it could route it to a failsafe. So because the agent is able to recognize the context of the query, it could recognize that it's not a part of the two databases that we have, could route it to the failsafe and return back. Sorry, I don't have the information in looking for. This agentic RAG pipeline can be used in customer support systems",
      "and legal tech. For example, a lawyer can source answers to their questions from like their internal briefs and then in another query, just get stuff from public caseload databases. The agent can be utilized in a ton of ways. Agentic RAG is an evolution in how we enhance the RAG pipeline by moving beyond simple response generation to more intelligent decision making. By allowing an agent to choose the best data sources and potentially even incorporate external information like real timedata or",
      "third party services. We can create a pipeline that's more responsive, more accurate, and more adaptable. This approach opens up so many possibilities for applications in customer service, legal, tech, health care, virtually any field as IT technology continues to evolve. We will see AI systems that truly understand context and can deliver amazing values to the end user."
    ],
    "chunk_count": 10,
    "content_id": "a9365abd-a2ed-4194-a1c7-ac3a53c9fc17",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554828"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=Aw7iQjKAX2k&t=1s": {
    "title": "GraphRAG vs. Traditional RAG: Higher Accuracy & Insight with LLM",
    "url": "https://www.youtube.com/watch?v=Aw7iQjKAX2k&t=1s",
    "description": "Want to learn more about Want to learn more about Generative AI + Machine Learning? Read the ebook here → https://ibm.biz/BdGpyX\nLearn more about GraphRAG here → https://ibm.biz/BdGpyH\n\nWant to learn more about Traditional RAG on a lightboard? Watch here → https://ibm.biz/BdGh97\n\nTransform your data into powerful insights! 🚀 Join Sara Bacha from Converge Technology Solutions as she delves into how GraphRAG outperforms traditional RAG by leveraging knowledge graphs and LLM to enhance data relationships and accuracy. 📊  Learn the benefits in development, production, and governance, making maintenance easier with better explainability and traceability. 🔍\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdGpy4\n\n#llm #ai #datascience",
    "duration": 257,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en imagine you're running a healthcare imagine you're running a healthcare imagine you're running a healthcare support line where patients and support line where patients and support line where patients and providers are calling in with complex providers are calling in with complex providers are calling in with complex multip questions this is where graph rag multip questions this is where graph rag multip questions this is where graph rag comes in it helps map relationships comes in it helps map relationships comes in it helps map relationships providing precise personalized answer providing precise personalized answer providing precise personalized answer faster and this is critical where faster and this is critical where faster and this is critical where accuracy and speed matter today we're accuracy and speed matter today we're accuracy and speed matter today we're going to take a look at how graph rag going to take a look at how graph rag going to take a look at how graph rag helps in delivering higher accuracy and helps in delivering higher accuracy and helps in delivering higher accuracy and more complete answers easier development more complete answers easier development more complete answers easier development and maintenance and enhance governance and maintenance and enhance governance and maintenance and enhance governance we'll go over what is graph frag and we'll go over what is graph frag and we'll go over what is graph frag and uncover the benefits of graph rag uncover the benefits of graph rag uncover the benefits of graph rag relative to traditional rag in relative to traditional rag in relative to traditional rag in development development development production and production and production and governance to understand graph frag governance to understand graph frag governance to understand graph frag let's first break down how Baseline let's first break down how Baseline let's first break down how Baseline graph works we start off with a private graph works we start off with a private graph works we start off with a private data data data set can be both structured and set can be both structured and set can be both structured and unstructured so this is our unstructured so this is our unstructured so this is our traditional and we break them down into traditional and we break them down into traditional and we break them down into text chunks and we store those embeddings in a and we store those embeddings in a and we store those embeddings in a vector database then when we want a query we we use our Vector database to query we we use our Vector database to query we we use our Vector database to extract the context and then we send extract the context and then we send extract the context and then we send that context to our that context to our that context to our llm and then it provides the answer we llm and then it provides the answer we llm and then it provides the answer we all know how traditional rag works now all know how traditional rag works now all know how traditional rag works now graph rag Builds on top of that we start off with leveraging the same we start off with leveraging the same we start off with leveraging the same deex chunks but on top of that we're also chunks but on top of that we're also chunks but on top of that we're also extracting entities and more relative extracting entities and more relative extracting entities and more relative information to be able to map out these information to be able to map out these information to be able to map out these information in a Knowledge Graph this way graph rack doesn't just Graph this way graph rack doesn't just Graph this way graph rack doesn't just retrieve isolated answers it connects retrieve isolated answers it connects retrieve isolated answers it connects relative information which enhances the relative information which enhances the relative information which enhances the quality responses and add accuracy and quality responses and add accuracy and quality responses and add accuracy and insight let's consider an example to insight let's consider an example to insight let's consider an example to demonstrate the capabilities of graph demonstrate the capabilities of graph demonstrate the capabilities of graph rag suppose we have a sentence like this rag suppose we have a sentence like this rag suppose we have a sentence like this an immunologist discussed virus response an immunologist discussed virus response an immunologist discussed virus response strategies with the CEO of a Healthcare strategies with the CEO of a Healthcare strategies with the CEO of a Healthcare Company traditional text analysis might Company traditional text analysis might Company traditional text analysis might have detected immunologist and CEO as have detected immunologist and CEO as have detected immunologist and CEO as named entities however graph rag goes named entities however graph rag goes named entities however graph rag goes further by identifying and mapping the further by identifying and mapping the further by identifying and mapping the relationships between these entities and this provides a deeper entities and this provides a deeper entities and this provides a deeper context and insight into their context and insight into their context and insight into their interaction so graph frag recognizes interaction so graph frag recognizes interaction so graph frag recognizes that the immunologist is deeply that the immunologist is deeply that the immunologist is deeply connected to immunology and the medical connected to immunology and the medical connected to immunology and the medical research whereas the CEO has more of an research whereas the CEO has more of an research whereas the CEO has more of an indirect yet related connection through indirect yet related connection through indirect yet related connection through her readership at the Healthcare company her readership at the Healthcare company her readership at the Healthcare company this analysis goes beyond just simply this analysis goes beyond just simply this analysis goes beyond just simply noting cooccurrences the llm quantifies noting cooccurrences the llm quantifies noting cooccurrences the llm quantifies the strength and nature of these the strength and nature of these the strength and nature of these relationships enabling the construction relationships enabling the construction relationships enabling the construction of weighted graphs that reveal inside F of weighted graphs that reveal inside F of weighted graphs that reveal inside F patterns transforming data into patterns transforming data into patterns transforming data into Knowledge Graph creates a network of Knowledge Graph creates a network of Knowledge Graph creates a network of connected and linked entities and the connected and linked entities and the connected and linked entities and the linked multi-layer Knowledge Graph then linked multi-layer Knowledge Graph then linked multi-layer Knowledge Graph then supports a wide range of applications supports a wide range of applications supports a wide range of applications and generating targeted questions to and generating targeted questions to and generating targeted questions to crafting rich and contextually relevant crafting rich and contextually relevant crafting rich and contextually relevant summaries ultimately providing a depth summaries ultimately providing a depth summaries ultimately providing a depth of insights that traditional rag cannot of insights that traditional rag cannot of insights that traditional rag cannot achieve achieve achieve alone so going back to production alone so going back to production alone so going back to production development and governance graph rack development and governance graph rack development and governance graph rack provides a higher accurac complete answerers a trun complete answerers a trun complete answerers a trun time as from a development perspective time as from a development perspective time as from a development perspective once you've build the graph it's easier once you've build the graph it's easier once you've build the graph it's easier to maintain to maintain to maintain it than it is with a traditional grag it than it is with a traditional grag it than it is with a traditional grag and subsequently once you're querying it and subsequently once you're querying it and subsequently once you're querying it you will get better explainability and traceability explainability and traceability explainability and traceability and access controls thank you for and access controls thank you for and access controls thank you for watching and hope you like this watching and hope you like this watching and hope you like this video if you have any questions or video if you have any questions or video if you have any questions or comments let me know below and don't comments let me know below and don't comments let me know below and don't forget to like And subscribe for more forget to like And subscribe for more forget to like And subscribe for more content like this",
    "chunks": [
      "Kind: captions Language: en imagine you're running a healthcare imagine you're running a healthcare imagine you're running a healthcare support line where patients and support line where patients and support line where patients and providers are calling in with complex providers are calling in with complex providers are calling in with complex multip questions this is where graph rag multip questions this is where graph rag multip questions this is where graph rag comes in it helps map",
      "relationships comes in it helps map relationships comes in it helps map relationships providing precise personalized answer providing precise personalized answer providing precise personalized answer faster and this is critical where faster and this is critical where faster and this is critical where accuracy and speed matter today we're accuracy and speed matter today we're accuracy and speed matter today we're going to take a look at how graph rag going to take a look at how graph rag going to",
      "take a look at how graph rag helps in delivering higher accuracy and helps in delivering higher accuracy and helps in delivering higher accuracy and more complete answers easier development more complete answers easier development more complete answers easier development and maintenance and enhance governance and maintenance and enhance governance and maintenance and enhance governance we'll go over what is graph frag and we'll go over what is graph frag and we'll go over what is graph frag and",
      "uncover the benefits of graph rag uncover the benefits of graph rag uncover the benefits of graph rag relative to traditional rag in relative to traditional rag in relative to traditional rag in development development development production and production and production and governance to understand graph frag governance to understand graph frag governance to understand graph frag let's first break down how Baseline let's first break down how Baseline let's first break down how Baseline graph",
      "works we start off with a private graph works we start off with a private graph works we start off with a private data data data set can be both structured and set can be both structured and set can be both structured and unstructured so this is our unstructured so this is our unstructured so this is our traditional and we break them down into traditional and we break them down into traditional and we break them down into text chunks and we store those embeddings in a and we store those",
      "embeddings in a and we store those embeddings in a vector database then when we want a query we we use our Vector database to query we we use our Vector database to query we we use our Vector database to extract the context and then we send extract the context and then we send extract the context and then we send that context to our that context to our that context to our llm and then it provides the answer we llm and then it provides the answer we llm and then it provides the answer we all know",
      "how traditional rag works now all know how traditional rag works now all know how traditional rag works now graph rag Builds on top of that we start off with leveraging the same we start off with leveraging the same we start off with leveraging the same deex chunks but on top of that we're also chunks but on top of that we're also chunks but on top of that we're also extracting entities and more relative extracting entities and more relative extracting entities and more relative information to be",
      "able to map out these information to be able to map out these information to be able to map out these information in a Knowledge Graph this way graph rack doesn't just Graph this way graph rack doesn't just Graph this way graph rack doesn't just retrieve isolated answers it connects retrieve isolated answers it connects retrieve isolated answers it connects relative information which enhances the relative information which enhances the relative information which enhances the quality responses and",
      "add accuracy and quality responses and add accuracy and quality responses and add accuracy and insight let's consider an example to insight let's consider an example to insight let's consider an example to demonstrate the capabilities of graph demonstrate the capabilities of graph demonstrate the capabilities of graph rag suppose we have a sentence like this rag suppose we have a sentence like this rag suppose we have a sentence like this an immunologist discussed virus response an immunologist",
      "discussed virus response an immunologist discussed virus response strategies with the CEO of a Healthcare strategies with the CEO of a Healthcare strategies with the CEO of a Healthcare Company traditional text analysis might Company traditional text analysis might Company traditional text analysis might have detected immunologist and CEO as have detected immunologist and CEO as have detected immunologist and CEO as named entities however graph rag goes named entities however graph rag goes named",
      "entities however graph rag goes further by identifying and mapping the further by identifying and mapping the further by identifying and mapping the relationships between these entities and this provides a deeper entities and this provides a deeper entities and this provides a deeper context and insight into their context and insight into their context and insight into their interaction so graph frag recognizes interaction so graph frag recognizes interaction so graph frag recognizes that the",
      "immunologist is deeply that the immunologist is deeply that the immunologist is deeply connected to immunology and the medical connected to immunology and the medical connected to immunology and the medical research whereas the CEO has more of an research whereas the CEO has more of an research whereas the CEO has more of an indirect yet related connection through indirect yet related connection through indirect yet related connection through her readership at the Healthcare company her",
      "readership at the Healthcare company her readership at the Healthcare company this analysis goes beyond just simply this analysis goes beyond just simply this analysis goes beyond just simply noting cooccurrences the llm quantifies noting cooccurrences the llm quantifies noting cooccurrences the llm quantifies the strength and nature of these the strength and nature of these the strength and nature of these relationships enabling the construction relationships enabling the construction",
      "relationships enabling the construction of weighted graphs that reveal inside F of weighted graphs that reveal inside F of weighted graphs that reveal inside F patterns transforming data into patterns transforming data into patterns transforming data into Knowledge Graph creates a network of Knowledge Graph creates a network of Knowledge Graph creates a network of connected and linked entities and the connected and linked entities and the connected and linked entities and the linked multi-layer",
      "Knowledge Graph then linked multi-layer Knowledge Graph then linked multi-layer Knowledge Graph then supports a wide range of applications supports a wide range of applications supports a wide range of applications and generating targeted questions to and generating targeted questions to and generating targeted questions to crafting rich and contextually relevant crafting rich and contextually relevant crafting rich and contextually relevant summaries ultimately providing a depth summaries",
      "ultimately providing a depth summaries ultimately providing a depth of insights that traditional rag cannot of insights that traditional rag cannot of insights that traditional rag cannot achieve achieve achieve alone so going back to production alone so going back to production alone so going back to production development and governance graph rack development and governance graph rack development and governance graph rack provides a higher accurac complete answerers a trun complete answerers a",
      "trun complete answerers a trun time as from a development perspective time as from a development perspective time as from a development perspective once you've build the graph it's easier once you've build the graph it's easier once you've build the graph it's easier to maintain to maintain to maintain it than it is with a traditional grag it than it is with a traditional grag it than it is with a traditional grag and subsequently once you're querying it and subsequently once you're querying it",
      "and subsequently once you're querying it you will get better explainability and traceability explainability and traceability explainability and traceability and access controls thank you for and access controls thank you for and access controls thank you for watching and hope you like this watching and hope you like this watching and hope you like this video if you have any questions or video if you have any questions or video if you have any questions or comments let me know below and don't",
      "comments let me know below and don't comments let me know below and don't forget to like And subscribe for more forget to like And subscribe for more forget to like And subscribe for more content like this"
    ],
    "chunk_count": 19,
    "content_id": "4f47c0f9-3ca8-4cf6-9a8e-ca9ec81eaecd",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554831"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=Yq29bZ8Hlrc": {
    "title": "Optimize RAG with AI Agents & Vector Databases",
    "url": "https://www.youtube.com/watch?v=Yq29bZ8Hlrc",
    "description": "Ready to become a certified Administrator on IBM Cloud Pak for Integration? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnHnE\n\nWant to build smarter AI pipelines? Explore the code here → https://ibm.biz/BdnHnX\n\nStruggling with irrelevant data in RAG? 🤖 David Levy shows how AI agent systems and vector databases optimize query accuracy, streamline context retrieval, and enhance response generation. Discover how agentic research builds smarter AI pipelines to tackle complex challenges! 🚀✨\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnHn4\n\n#aiagents #vectordatabase #retrievalaugmentedgeneration",
    "duration": 1977,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Have you ever had a tremendous amount of Have you ever had a tremendous amount of Have you ever had a tremendous amount of data in your vector DB? You're using data in your vector DB? You're using data in your vector DB? You're using that for retrieval augmented generation, that for retrieval augmented generation, that for retrieval augmented generation, but the context you're getting to give but the context you're getting to give but the context you're getting to give to your LLM to produce those great to your LLM to produce those great to your LLM to produce those great results is kind of lacking because it's results is kind of lacking because it's results is kind of lacking because it's pulling in different data that shouldn't pulling in different data that shouldn't pulling in different data that shouldn't really be pulled in with your query. really be pulled in with your query. really be pulled in with your query. Well, today I'm going to show you how to Well, today I'm going to show you how to Well, today I'm going to show you how to seamlessly integrate multiple AI agents seamlessly integrate multiple AI agents seamlessly integrate multiple AI agents into your application to combat this into your application to combat this into your application to combat this kind of problem. We will walk through a kind of problem. We will walk through a kind of problem. We will walk through a practical example that covers query practical example that covers query practical example that covers query categorization, context retrieval from a categorization, context retrieval from a categorization, context retrieval from a vector DB and natural language response vector DB and natural language response vector DB and natural language response generation, all using this multi- aent generation, all using this multi- aent generation, all using this multi- aent approach. This session is designed to approach. This session is designed to approach. This session is designed to give you a clear step-by-step guide on give you a clear step-by-step guide on give you a clear step-by-step guide on working with agents in your projects. working with agents in your projects. working with agents in your projects. Let's dive in and explore how you can Let's dive in and explore how you can Let's dive in and explore how you can leverage these tools to build smarter leverage these tools to build smarter leverage these tools to build smarter applications. So, in the description of applications. So, in the description of applications. So, in the description of the video, you should have a link to the video, you should have a link to the video, you should have a link to this repo. First thing we're going to do this repo. First thing we're going to do this repo. First thing we're going to do is just clone down the repo to our local machine. Okay. And let's go into the machine. Okay. And let's go into the machine. Okay. And let's go into the repo and then into the UI directory. So repo and then into the UI directory. So repo and then into the UI directory. So if you look at the structure of the if you look at the structure of the if you look at the structure of the application, we're going to have an API, application, we're going to have an API, application, we're going to have an API, which is what we're going to be working which is what we're going to be working which is what we're going to be working on today. And then we have a UI, which on today. And then we have a UI, which on today. And then we have a UI, which we're not going to work on, but that's we're not going to work on, but that's we're not going to work on, but that's what's going to render what we're doing what's going to render what we're doing what's going to render what we're doing to the uh to your browser. And so we're to the uh to your browser. And so we're to the uh to your browser. And so we're going to go into the UI and we're going going to go into the UI and we're going going to go into the UI and we're going to install all the dependencies. So the to install all the dependencies. So the to install all the dependencies. So the first thing is just install the root first thing is just install the root first thing is just install the root dependencies and once that's completed dependencies and once that's completed dependencies and once that's completed we're going to run a setup script. So the UI even though we're not script. So the UI even though we're not script. So the UI even though we're not working on it the UI is made with React working on it the UI is made with React working on it the UI is made with React TypeScript and it has an express TypeScript and it has an express TypeScript and it has an express TypeScript server and we're using TypeScript server and we're using TypeScript server and we're using something called carbon components and I something called carbon components and I something called carbon components and I just want to highlight carbon components just want to highlight carbon components just want to highlight carbon components for a second. So, if you search carbon for a second. So, if you search carbon for a second. So, if you search carbon design react, it's going to bring you to design react, it's going to bring you to design react, it's going to bring you to the docs the docs the docs page, and this is where I get all like page, and this is where I get all like page, and this is where I get all like the components to put into the UI, it's the components to put into the UI, it's the components to put into the UI, it's super easy, especially somebody who's super easy, especially somebody who's super easy, especially somebody who's not a particularly uh adept front-end not a particularly uh adept front-end not a particularly uh adept front-end developer. It just makes it super super developer. It just makes it super super developer. It just makes it super super easy and they look good and, you know, easy and they look good and, you know, easy and they look good and, you know, they have the code, everything you need. they have the code, everything you need. they have the code, everything you need. So, my suggestion is even if we're not So, my suggestion is even if we're not So, my suggestion is even if we're not working on the UI today, go look at working on the UI today, go look at working on the UI today, go look at carbon design, look at what we can do, carbon design, look at what we can do, carbon design, look at what we can do, and maybe change the application when and maybe change the application when and maybe change the application when you're done with it. All right, so we're you're done with it. All right, so we're you're done with it. All right, so we're going to wait for the dependencies to going to wait for the dependencies to going to wait for the dependencies to install and we'll be right back. Okay, install and we'll be right back. Okay, install and we'll be right back. Okay, the dependencies have installed. One the dependencies have installed. One the dependencies have installed. One last thing to do within the UI is we're last thing to do within the UI is we're last thing to do within the UI is we're going to copy the client's ENV example going to copy the client's ENV example going to copy the client's ENV example into back into the client and we're into back into the client and we're into back into the client and we're going to create an env. We're going to going to create an env. We're going to going to create an env. We're going to do the same thing for the server. Copy do the same thing for the server. Copy do the same thing for the server. Copy that ENV example and place it right back that ENV example and place it right back that ENV example and place it right back asenv in the server. Uh something we asenv in the server. Uh something we asenv in the server. Uh something we also could do if we want to go into also could do if we want to go into also could do if we want to go into client.mv we've added a way for you to client.mv we've added a way for you to client.mv we've added a way for you to brand the application after if you want brand the application after if you want brand the application after if you want to make something your own. So we have a to make something your own. So we have a to make something your own. So we have a branding and an application name. So for branding and an application name. So for branding and an application name. So for this one we're going to say agents and this one we're going to say agents and this one we're going to say agents and action exclamation point. action exclamation point. action exclamation point. Okay. So now we're done with the Okay. So now we're done with the Okay. So now we're done with the dependencies and we're done totally with dependencies and we're done totally with dependencies and we're done totally with the UI today. So let's go back to the the UI today. So let's go back to the the UI today. So let's go back to the root and let's head over to the API. So root and let's head over to the API. So root and let's head over to the API. So the API is written in Python. So let's the API is written in Python. So let's the API is written in Python. So let's create a virtual create a virtual create a virtual environment. We'll name that AI environment. We'll name that AI environment. We'll name that AI agent. And once that's done, let's activate let's activate the activate let's activate the activate let's activate the uh the virtual environment. And now uh the virtual environment. And now uh the virtual environment. And now we're going to install all the we're going to install all the we're going to install all the dependencies. So, this is going to take a little So, this is going to take a little So, this is going to take a little while. We're installing Crew AI. We're while. We're installing Crew AI. We're while. We're installing Crew AI. We're installing Watson XAI. Ton of installing Watson XAI. Ton of installing Watson XAI. Ton of dependencies. And so, once this is dependencies. And so, once this is dependencies. And so, once this is completed, we'll continue along with the completed, we'll continue along with the completed, we'll continue along with the tutorial. Okay. So, now that the tutorial. Okay. So, now that the tutorial. Okay. So, now that the dependencies have been uh installed, we dependencies have been uh installed, we dependencies have been uh installed, we have to just copy thev have to just copy thev have to just copy thev uh.ample and paste it into the API. uh.ample and paste it into the API. uh.ample and paste it into the API. Let's take a look at what's in that env. Let's take a look at what's in that env. Let's take a look at what's in that env. Uh because we're using Watson XAI, we Uh because we're using Watson XAI, we Uh because we're using Watson XAI, we need to have the connection strings for need to have the connection strings for need to have the connection strings for to connect to Watson XAI. Um so we're to connect to Watson XAI. Um so we're to connect to Watson XAI. Um so we're going to head over to cloud and if you going to head over to cloud and if you going to head over to cloud and if you go into your resource list, just open up go into your resource list, just open up go into your resource list, just open up your Watson Studio and go into IBM your Watson Studio and go into IBM your Watson Studio and go into IBM Watson Watson Watson X and it's just going to log us X and it's just going to log us X and it's just going to log us in and we're going to head over to in and we're going to head over to in and we're going to head over to Prompt Lab. Now I'm sure this is a Prompt Lab. Now I'm sure this is a Prompt Lab. Now I'm sure this is a better way of getting this information better way of getting this information better way of getting this information than the way I do it, but the way I do than the way I do it, but the way I do than the way I do it, but the way I do it is I just go to Prompt Lab. At the it is I just go to Prompt Lab. At the it is I just go to Prompt Lab. At the top right, we have a view code and it top right, we have a view code and it top right, we have a view code and it shows you a curl command and it has all shows you a curl command and it has all shows you a curl command and it has all the most of the stuff that we need in the most of the stuff that we need in the most of the stuff that we need in order to make that configuration with order to make that configuration with order to make that configuration with our API. So, grab from the curl the base our API. So, grab from the curl the base our API. So, grab from the curl the base URL and paste it here in the Watson URL and we go back and we grab the URL and we go back and we grab the URL and we go back and we grab the project ID. Just copy and paste that. And then finally, let's go back to that. And then finally, let's go back to that. And then finally, let's go back to cloud.ibbm. We're going to go at the cloud.ibbm. We're going to go at the cloud.ibbm. We're going to go at the very top. You're going to have a manage. very top. You're going to have a manage. very top. You're going to have a manage. And you're going to want to go to the And you're going to want to go to the And you're going to want to go to the access am. And when we get there, on the access am. And when we get there, on the access am. And when we get there, on the left hand side, you're going to see left hand side, you're going to see left hand side, you're going to see something called API keys. Let's create something called API keys. Let's create something called API keys. Let's create a new one. Call it aentic and one. Call it aentic and one. Call it aentic and create. So, let's just copy create. So, let's just copy create. So, let's just copy that and put it right that and put it right that and put it right here. And that's it. Now our API is set here. And that's it. Now our API is set here. And that's it. Now our API is set up. So the next thing we want to do is up. So the next thing we want to do is up. So the next thing we want to do is check out into a new branch. All right. check out into a new branch. All right. check out into a new branch. All right. So let's check So let's check So let's check out our first branch. It's going to be out our first branch. It's going to be out our first branch. It's going to be step. step. step. Perfect. And what we want to do is we Perfect. And what we want to do is we Perfect. And what we want to do is we want to start up all our services. We want to start up all our services. We want to start up all our services. We have three services. Remember we have have three services. Remember we have have three services. Remember we have the fast API, the React UI, and the the fast API, the React UI, and the the fast API, the React UI, and the express server. So, let's first start up express server. So, let's first start up express server. So, let's first start up uh wait, we have to go into the API uh wait, we have to go into the API uh wait, we have to go into the API directory first and then we could run directory first and then we could run directory first and then we could run our unicorn command. We're just going to our unicorn command. We're just going to our unicorn command. We're just going to run the unicorn server up reload. We're run the unicorn server up reload. We're run the unicorn server up reload. We're going to start our fast API. going to start our fast API. going to start our fast API. Also, let's have two more windows Also, let's have two more windows Also, let's have two more windows because we're going to go back to the UI because we're going to go back to the UI because we're going to go back to the UI and we're going to start up and we're going to start up and we're going to start up the client and then we're going to start the client and then we're going to start the client and then we're going to start up the server. And all these commands are in the uh in And all these commands are in the uh in And all these commands are in the uh in the repo. So you just copy and paste the repo. So you just copy and paste the repo. So you just copy and paste them. So we're just waiting a second for them. So we're just waiting a second for them. So we're just waiting a second for Unicorn for the fast API to get all Unicorn for the fast API to get all Unicorn for the fast API to get all ready. And we'll head over to the ready. And we'll head over to the ready. And we'll head over to the browser and we could already see what browser and we could already see what browser and we could already see what the UI is going to look the UI is going to look the UI is going to look like. Beautiful. It's a chatbot. You like. Beautiful. It's a chatbot. You like. Beautiful. It's a chatbot. You have a chat window, a couple buttons, have a chat window, a couple buttons, have a chat window, a couple buttons, but what it's going to do on the back but what it's going to do on the back but what it's going to do on the back end is going to be pretty cool, I think. end is going to be pretty cool, I think. end is going to be pretty cool, I think. So, if you go to your API directory, So, if you go to your API directory, So, if you go to your API directory, there's going to be a couple of folders there's going to be a couple of folders there's going to be a couple of folders that you're going to be interested in that you're going to be interested in that you're going to be interested in and a questions.txt that I've been and a questions.txt that I've been and a questions.txt that I've been using. So, let me copy and paste this using. So, let me copy and paste this using. So, let me copy and paste this into our chat window. And what we're into our chat window. And what we're into our chat window. And what we're going to want to do is when we hit send, going to want to do is when we hit send, going to want to do is when we hit send, we're going to want the backend to we're going to want the backend to we're going to want the backend to categorize that categorize that categorize that query, grab the correct data from the query, grab the correct data from the query, grab the correct data from the correct collection of vector DB in correct collection of vector DB in correct collection of vector DB in Chroma DB, and then we're going to want Chroma DB, and then we're going to want Chroma DB, and then we're going to want to pass that to a customized prompt, and to pass that to a customized prompt, and to pass that to a customized prompt, and return a nice response. Currently, it's return a nice response. Currently, it's return a nice response. Currently, it's just going to say this will be generated just going to say this will be generated just going to say this will be generated by our multi- aent process, and the by our multi- aent process, and the by our multi- aent process, and the category is something cool, but it will category is something cool, but it will category is something cool, but it will be something cool once we set it up. So be something cool once we set it up. So be something cool once we set it up. So let's go and run the first script that let's go and run the first script that let's go and run the first script that we have. So if you look in API scripts we have. So if you look in API scripts we have. So if you look in API scripts and you look at the process document and you look at the process document and you look at the process document script, this is how we're we're going to script, this is how we're we're going to script, this is how we're we're going to create our Chromma DB uh vector DB, create our Chromma DB uh vector DB, create our Chromma DB uh vector DB, right? We have a directory called docs right? We have a directory called docs right? We have a directory called docs and in that we have three text files. and in that we have three text files. and in that we have three text files. One called accounting, billing and One called accounting, billing and One called accounting, billing and technical. And what I'm trying to show technical. And what I'm trying to show technical. And what I'm trying to show here is that this is imagine you have a here is that this is imagine you have a here is that this is imagine you have a tremendous amount of documentation and tremendous amount of documentation and tremendous amount of documentation and some of those u when we when we query some of those u when we when we query some of those u when we when we query them in a vector to be the the cosign them in a vector to be the the cosign them in a vector to be the the cosign similarity distance going to be pretty similarity distance going to be pretty similarity distance going to be pretty close for stuff that might not be close for stuff that might not be close for stuff that might not be relevant. So we isolate topics right? So relevant. So we isolate topics right? So relevant. So we isolate topics right? So we have an account we have a technical we have an account we have a technical we have an account we have a technical we have the billing and I'm just trying we have the billing and I'm just trying we have the billing and I'm just trying to recreate that locally something very to recreate that locally something very to recreate that locally something very simple. So if you look at the script, simple. So if you look at the script, simple. So if you look at the script, all it's doing is it's saying, \"Okay, all it's doing is it's saying, \"Okay, all it's doing is it's saying, \"Okay, what's the file?\" It's going to loop what's the file?\" It's going to loop what's the file?\" It's going to loop through all the files in Docs. What's through all the files in Docs. What's through all the files in Docs. What's the file name? Create a new collection the file name? Create a new collection the file name? Create a new collection with that file name and insert the with that file name and insert the with that file name and insert the embeddings for that file into that embeddings for that file into that embeddings for that file into that collection. So let's run that script. You'll see first it's going to script. You'll see first it's going to script. You'll see first it's going to say, okay, is there any documents? No say, okay, is there any documents? No say, okay, is there any documents? No existing collections found. So let's existing collections found. So let's existing collections found. So let's going to create three new ones. And going to create three new ones. And going to create three new ones. And we're going to have account, billing, we're going to have account, billing, we're going to have account, billing, technical. Before we could do any of technical. Before we could do any of technical. Before we could do any of that, we have to first categorize the that, we have to first categorize the that, we have to first categorize the query. So this is where we're going to query. So this is where we're going to query. So this is where we're going to create our very first agent. It's going create our very first agent. It's going create our very first agent. It's going to be the categorization agent. So the route we're looking at agent. So the route we're looking at agent. So the route we're looking at that we're going to change is called the that we're going to change is called the that we're going to change is called the agentic route. And here you can see in agentic route. And here you can see in agentic route. And here you can see in the doc string I have what each of the the doc string I have what each of the the doc string I have what each of the each of the agents are going to do. The each of the agents are going to do. The each of the agents are going to do. The first one is going to be the query first one is going to be the query first one is going to be the query categorization. Then we're going to have categorization. Then we're going to have categorization. Then we're going to have the context retrieval. And then we're the context retrieval. And then we're the context retrieval. And then we're going to have the response generation. going to have the response generation. going to have the response generation. And these are all going to be agents. So And these are all going to be agents. So And these are all going to be agents. So let's bring in our agent framework which let's bring in our agent framework which let's bring in our agent framework which is crew AI. So we're going to say from is crew AI. So we're going to say from is crew AI. So we're going to say from crew crew crew AI import the first class we're going to AI import the first class we're going to AI import the first class we're going to bring is agent. We're going to bring in bring is agent. We're going to bring in bring is agent. We're going to bring in a task. We're going to bring in the a task. We're going to bring in the a task. We're going to bring in the crew. We're going to bring in a process crew. We're going to bring in a process crew. We're going to bring in a process and we're bring you can see in the first doc bring you can see in the first doc bring you can see in the first doc string uh it's an LLM powered agent. So string uh it's an LLM powered agent. So string uh it's an LLM powered agent. So this is where we start connecting Wasin this is where we start connecting Wasin this is where we start connecting Wasin XAI to the Crew AI agentic framework. So XAI to the Crew AI agentic framework. So XAI to the Crew AI agentic framework. So let's create our first LLM. When you look at the docs, the only LLM. When you look at the docs, the only LLM. When you look at the docs, the only thing we're really concerned with from thing we're really concerned with from thing we're really concerned with from this is just the model. We're going to this is just the model. We're going to this is just the model. We're going to use the Watsonx AI model. Uh use the Watsonx AI model. Uh use the Watsonx AI model. Uh temperature, max tokens, and then all temperature, max tokens, and then all temperature, max tokens, and then all the connection strings. So if we look in the connection strings. So if we look in the connection strings. So if we look in the server.py, we have a list of the server.py, we have a list of the server.py, we have a list of available models that you could use uh available models that you could use uh available models that you could use uh from Watsonx AI. So, I'm going to use from Watsonx AI. So, I'm going to use from Watsonx AI. So, I'm going to use the granite uh 38 the granite uh 38 the granite uh 38 billion. So, let's bring that billion. So, let's bring that billion. So, let's bring that in. And we just have to append Watson X in. And we just have to append Watson X in. And we just have to append Watson X to the front of to the front of to the front of it. Then, we're going to set the it. Then, we're going to set the it. Then, we're going to set the temperature. Now, this is from trial and temperature. Now, this is from trial and temperature. Now, this is from trial and error, but 0.7 works well. And the max error, but 0.7 works well. And the max error, but 0.7 works well. And the max tokens is 50 because again, all this is tokens is 50 because again, all this is tokens is 50 because again, all this is doing is just categorizing a query, doing is just categorizing a query, doing is just categorizing a query, right? We're not we're not doing any uh right? We're not we're not doing any uh right? We're not we're not doing any uh massive return or anything like that. So massive return or anything like that. So massive return or anything like that. So the next thing we have to do is we're the next thing we have to do is we're the next thing we have to do is we're going to bring in the connection string going to bring in the connection string going to bring in the connection string Watson. So we're going to need the API Watson. So we're going to need the API Watson. So we're going to need the API key, the project ID, and the URL. So key, the project ID, and the URL. So key, the project ID, and the URL. So let's say URL is going to equal let's say URL is going to equal let's say URL is going to equal OS and get the key. Let's just paste OS and get the key. Let's just paste OS and get the key. Let's just paste that in. And then we're going to have that in. And then we're going to have that in. And then we're going to have the API the API the API key. We're going to get that from our env. And then finally, we're going to env. And then finally, we're going to env. And then finally, we're going to get the get the get the project ID, which is going to again just project ID, which is going to again just project ID, which is going to again just bring it in. Perfect. So, let's bring it into the in. Perfect. So, let's bring it into the in. Perfect. So, let's bring it into the uh the Crew AI LM class. Uh so, we're uh the Crew AI LM class. Uh so, we're uh the Crew AI LM class. Uh so, we're going to have the base URL is going to going to have the base URL is going to going to have the base URL is going to be the URL. The API key is going to be be the URL. The API key is going to be be the URL. The API key is going to be the API key and the project ID is going the API key and the project ID is going the API key and the project ID is going to be the project ID. to be the project ID. to be the project ID. Perfect. So now we have to create our Perfect. So now we have to create our Perfect. So now we have to create our first agent. Let's do it. We're going to first agent. Let's do it. We're going to first agent. Let's do it. We're going to name we're going to name him name we're going to name him name we're going to name him categorization agent. A very clever categorization agent. A very clever categorization agent. A very clever name. And if we look at the docs name. And if we look at the docs name. And if we look at the docs again, we can see exactly what we're again, we can see exactly what we're again, we can see exactly what we're going to be doing here. going to be doing here. going to be doing here. So the top three are particularly cool, So the top three are particularly cool, So the top three are particularly cool, right? to me. You have a role, a goal, right? to me. You have a role, a goal, right? to me. You have a role, a goal, and a backstory. And if you go to the and a backstory. And if you go to the and a backstory. And if you go to the crew AI crew AI crew AI docs, you have really, you know, they docs, you have really, you know, they docs, you have really, you know, they they are really good about explaining they are really good about explaining they are really good about explaining exactly what all those attributes do, exactly what all those attributes do, exactly what all those attributes do, but I just want to read them to you. So, but I just want to read them to you. So, but I just want to read them to you. So, the the role, it's defining the agents the the role, it's defining the agents the the role, it's defining the agents function and its expertise within the function and its expertise within the function and its expertise within the crew. Remember, it's a crew of agents. crew. Remember, it's a crew of agents. crew. Remember, it's a crew of agents. Uh the goal is we're going to it gives Uh the goal is we're going to it gives Uh the goal is we're going to it gives you the clear defined goal of what it's you the clear defined goal of what it's you the clear defined goal of what it's going to do. And then the backstory is going to do. And then the backstory is going to do. And then the backstory is great. It's just provides context and great. It's just provides context and great. It's just provides context and personality to the agent which I find personality to the agent which I find personality to the agent which I find very very cool. So let's start very very cool. So let's start very very cool. So let's start with the uh let's start with the role with the uh let's start with the role with the uh let's start with the role and so for the categorization agent the role is going to the one I've agent the role is going to the one I've agent the role is going to the one I've come up with is collection selector. come up with is collection selector. come up with is collection selector. Now, if you've worked with LLMs, you Now, if you've worked with LLMs, you Now, if you've worked with LLMs, you know, a lot of these, and this probably know, a lot of these, and this probably know, a lot of these, and this probably looks like prompt engineering, too, looks like prompt engineering, too, looks like prompt engineering, too, because it kind of is. It's just trial because it kind of is. It's just trial because it kind of is. It's just trial and error. This is what worked out for and error. This is what worked out for and error. This is what worked out for me. So, I gave it the role of a me. So, I gave it the role of a me. So, I gave it the role of a collection selector because it's collection selector because it's collection selector because it's selecting the collection. The goal is to selecting the collection. The goal is to selecting the collection. The goal is to analyze analyze analyze uh it's going to analyze the user uh it's going to analyze the user uh it's going to analyze the user queries and queries and queries and determine the most determine the most determine the most relevant relevant relevant Chromod collection. Chromod collection. Chromod collection. And then we're going to give the And then we're going to give the And then we're going to give the backstory. Uh he is an expert in query classification. And he routes questions classification. And he routes questions classification. And he routes questions to the to the to the correct domain. All right. Finally, we're going domain. All right. Finally, we're going domain. All right. Finally, we're going to do add a couple of the other things to do add a couple of the other things to do add a couple of the other things that we need. Verose I'm going to set to that we need. Verose I'm going to set to that we need. Verose I'm going to set to true because we want to see what what true because we want to see what what true because we want to see what what it's up to in the logs. Um allow it's up to in the logs. Um allow it's up to in the logs. Um allow delegation. So this is interesting. delegation. So this is interesting. delegation. So this is interesting. Remember, we're going to have multiple Remember, we're going to have multiple Remember, we're going to have multiple agents and they could all have different agents and they could all have different agents and they could all have different goals and different expertises and it goals and different expertises and it goals and different expertises and it can make the decision on on what it can make the decision on on what it can make the decision on on what it wants to do based on that, right? Like, wants to do based on that, right? Like, wants to do based on that, right? Like, okay, this is not for me. Let me send it okay, this is not for me. Let me send it okay, this is not for me. Let me send it to another agent. It's going to allow to to another agent. It's going to allow to to another agent. It's going to allow to delegate. But in our case, we don't. delegate. But in our case, we don't. delegate. But in our case, we don't. We're just going sequentially, right? We We're just going sequentially, right? We We're just going sequentially, right? We have three agents. We need them to do have three agents. We need them to do have three agents. We need them to do exactly what we want them to do. So, we exactly what we want them to do. So, we exactly what we want them to do. So, we turn delegation off. And in line with turn delegation off. And in line with turn delegation off. And in line with that, there's also something called max that, there's also something called max that, there's also something called max iterations. So, it defaults to 20. But iterations. So, it defaults to 20. But iterations. So, it defaults to 20. But in our case again like if something is in our case again like if something is in our case again like if something is not working because these are pretty not working because these are pretty not working because these are pretty simple tasks. If something's not working simple tasks. If something's not working simple tasks. If something's not working it's just going to try to try it over it's just going to try to try it over it's just going to try to try it over and over and over again. We just have to and over and over again. We just have to and over and over again. We just have to fix the code that at least in my fix the code that at least in my fix the code that at least in my experience is what's happening. And experience is what's happening. And experience is what's happening. And finally we have to give it its brain finally we have to give it its brain finally we have to give it its brain right. We're going to give it the right. We're going to give it the right. We're going to give it the categorization. This is its reasoning categorization. This is its reasoning categorization. This is its reasoning capability. This is how it's going to capability. This is how it's going to capability. This is how it's going to actually do what it needs to do. And actually do what it needs to do. And actually do what it needs to do. And that's the categorization alm that's that's the categorization alm that's that's the categorization alm that's using granite. So now we have an agent using granite. So now we have an agent using granite. So now we have an agent who has a brain and has a role and has a who has a brain and has a role and has a who has a brain and has a role and has a backstory and has a whole life story. we backstory and has a whole life story. we backstory and has a whole life story. we have to give it a task. We we're we're have to give it a task. We we're we're have to give it a task. We we're we're going to ask the agent to do something. going to ask the agent to do something. going to ask the agent to do something. So let's create our task and again let's look at the docs. task and again let's look at the docs. task and again let's look at the docs. So the things we're going to be So the things we're going to be So the things we're going to be concerned with obviously is agent. We concerned with obviously is agent. We concerned with obviously is agent. We have to assign this task to our have to assign this task to our have to assign this task to our categorization agent. Um description categorization agent. Um description categorization agent. Um description which is going to be really just a which is going to be really just a which is going to be really just a prompt. Uh, and then output JSON, which prompt. Uh, and then output JSON, which prompt. Uh, and then output JSON, which is really important for what we're going is really important for what we're going is really important for what we're going to do because we're going to send this to do because we're going to send this to do because we're going to send this first agent's response directly back to first agent's response directly back to first agent's response directly back to the UI. So, it has to be formatted in a the UI. So, it has to be formatted in a the UI. So, it has to be formatted in a particular way. So, I'm just going to particular way. So, I'm just going to particular way. So, I'm just going to copy and paste the description because copy and paste the description because copy and paste the description because it's just a it's a prompt, right? Like, it's just a it's a prompt, right? Like, it's just a it's a prompt, right? Like, if anyone's used prompts before, this if anyone's used prompts before, this if anyone's used prompts before, this took me a while to get took me a while to get took me a while to get correct, correct, correct, but you can see exactly what I mean, but you can see exactly what I mean, but you can see exactly what I mean, right? This is a prompt. We're saying, right? This is a prompt. We're saying, right? This is a prompt. We're saying, look, look at the query and determine look, look at the query and determine look, look at the query and determine the best category. you must only return the best category. you must only return the best category. you must only return one word because again we're going to be one word because again we're going to be one word because again we're going to be using this later down the line as the using this later down the line as the using this later down the line as the classification agent. And then we give classification agent. And then we give classification agent. And then we give it category definitions and we're really it category definitions and we're really it category definitions and we're really kind of broad with it because we want to kind of broad with it because we want to kind of broad with it because we want to give agency to the agent. So we're just give agency to the agent. So we're just give agency to the agent. So we're just saying okay this is what a technical saying okay this is what a technical saying okay this is what a technical query could look like. This is what query could look like. This is what query could look like. This is what billing we're giving that we're giving billing we're giving that we're giving billing we're giving that we're giving the agent agency here. And then finally the agent agency here. And then finally the agent agency here. And then finally I just really want to hammer home please I just really want to hammer home please I just really want to hammer home please just only one word from this list. just only one word from this list. just only one word from this list. And then we also have a expected output And then we also have a expected output And then we also have a expected output and this is important because we we need and this is important because we we need and this is important because we we need something explicit. So we want a JSON something explicit. So we want a JSON something explicit. So we want a JSON object with category field and it has to object with category field and it has to object with category field and it has to either be technical billing or account. either be technical billing or account. either be technical billing or account. The agent we're assigning to it The agent we're assigning to it The agent we're assigning to it obviously is the categorization agent. obviously is the categorization agent. obviously is the categorization agent. And finally the last thing that I And finally the last thing that I And finally the last thing that I mentioned was the output mentioned was the output mentioned was the output JSON. And I find this really nice. So JSON. And I find this really nice. So JSON. And I find this really nice. So the output JSON takes in a pyantic the output JSON takes in a pyantic the output JSON takes in a pyantic model. So let me copy and paste the model. So let me copy and paste the model. So let me copy and paste the pyantic model I have. and I'll show you pyantic model I have. and I'll show you pyantic model I have. and I'll show you what I did. Paste it in did. Paste it in did. Paste it in here. So, we have a category response. here. So, we have a category response. here. So, we have a category response. I'm expecting a JSON object with a I'm expecting a JSON object with a I'm expecting a JSON object with a category field. Uh, and the value is category field. Uh, and the value is category field. Uh, and the value is going to be either technical, billing, going to be either technical, billing, going to be either technical, billing, or account. Now, I added this or account. Now, I added this or account. Now, I added this description because I have a feeling description because I have a feeling description because I have a feeling that the agent is actually looking at that the agent is actually looking at that the agent is actually looking at the descriptions of these models before the descriptions of these models before the descriptions of these models before it's responding on it. I not don't don't it's responding on it. I not don't don't it's responding on it. I not don't don't quote me on it, but that's what I think quote me on it, but that's what I think quote me on it, but that's what I think it's doing. So I added it uh and it it's doing. So I added it uh and it it's doing. So I added it uh and it worked well. So from all I can tell it worked well. So from all I can tell it worked well. So from all I can tell it is working the way it's supposed to be is working the way it's supposed to be is working the way it's supposed to be working. All right. So now we have a working. All right. So now we have a working. All right. So now we have a agent who is powered by our LLM and has agent who is powered by our LLM and has agent who is powered by our LLM and has a task to follow. So let's create the a task to follow. So let's create the a task to follow. So let's create the first crew. You look at the docs here. We have crew. You look at the docs here. We have crew. You look at the docs here. We have tasks. We have agents. We're going to tasks. We have agents. We're going to tasks. We have agents. We're going to we're going to have we're going to use we're going to have we're going to use we're going to have we're going to use the process and we're going to we're the process and we're going to we're the process and we're going to we're going to have verbosity because we want going to have verbosity because we want going to have verbosity because we want to have some responses. So first thing to have some responses. So first thing to have some responses. So first thing to first is let's add the agent and right now we only have agent and right now we only have agent and right now we only have one. Then we have to add the tasks and that's going to be the tasks and that's going to be the tasks and that's going to be the categorization task. Remember this is categorization task. Remember this is categorization task. Remember this is going to be a crew. It's going to be a going to be a crew. It's going to be a going to be a crew. It's going to be a couple of agents here. And then we're couple of agents here. And then we're couple of agents here. And then we're going to have both. We're going to set going to have both. We're going to set going to have both. We're going to set it to true again because we want to see it to true again because we want to see it to true again because we want to see what it's doing. And then finally we what it's doing. And then finally we what it's doing. And then finally we have a process. And the process is going have a process. And the process is going have a process. And the process is going to be sequential because uh if you look to be sequential because uh if you look to be sequential because uh if you look at the dock string, we're just going at the dock string, we're just going at the dock string, we're just going step by step. So now that we have the step by step. So now that we have the step by step. So now that we have the crew, let's have the crew kickoff and we're going to call the kickoff and we're going to call the kickoff and we're going to call the kickoff method from the crew. And if you kickoff method from the crew. And if you kickoff method from the crew. And if you remember from the category response, remember from the category response, remember from the category response, we're expecting it to be a JSON object we're expecting it to be a JSON object we're expecting it to be a JSON object with category as a field. So, what we're going to So, what we're going to So, what we're going to do is we're going to instead of send do is we're going to instead of send do is we're going to instead of send back something cool to the UI, we're back something cool to the UI, we're back something cool to the UI, we're going to grab the category going to grab the category going to grab the category result and we're going to try to grab result and we're going to try to grab result and we're going to try to grab that category from the response. Make that category from the response. Make that category from the response. Make sure nothing broke. Looks good. We have the category broke. Looks good. We have the category broke. Looks good. We have the category result. Let's test it out. result. Let's test it out. result. Let's test it out. So let's copy and paste what we already So let's copy and paste what we already So let's copy and paste what we already sent and hopefully it returns back sent and hopefully it returns back sent and hopefully it returns back category something category technical category something category technical category something category technical perfect. So it was a and if we look at perfect. So it was a and if we look at perfect. So it was a and if we look at the logs we can see exactly what it's the logs we can see exactly what it's the logs we can see exactly what it's doing right we are using the the doing right we are using the the doing right we are using the the collection selector that first agent you collection selector that first agent you collection selector that first agent you could see the test that we're giving it could see the test that we're giving it could see the test that we're giving it we pass in that user query the one that we pass in that user query the one that we pass in that user query the one that we sent from the UI and then we get the we sent from the UI and then we get the we sent from the UI and then we get the final answers is exactly the structure final answers is exactly the structure final answers is exactly the structure that we were looking for so the UI could that we were looking for so the UI could that we were looking for so the UI could ingest it and render it nicely. All ingest it and render it nicely. All ingest it and render it nicely. All right. So now that we have the basic right. So now that we have the basic right. So now that we have the basic categorization agent in place, let's categorization agent in place, let's categorization agent in place, let's move on and enhance our pipeline. So move on and enhance our pipeline. So move on and enhance our pipeline. So let's just commit our let's just commit our let's just commit our changes and let's check out the second changes and let's check out the second changes and let's check out the second step branch. Perfect. Nothing broke. step branch. Perfect. Nothing broke. step branch. Perfect. Nothing broke. Great. Okay. So the next step here if we Great. Okay. So the next step here if we Great. Okay. So the next step here if we go to the uh the doc string is now to go to the uh the doc string is now to go to the uh the doc string is now to retrieve that data from that vector db. retrieve that data from that vector db. retrieve that data from that vector db. Right. Let's make it the only. So, we're Right. Let's make it the only. So, we're Right. Let's make it the only. So, we're going to do the same process. We're going to do the same process. We're going to do the same process. We're going to copy and paste the going to copy and paste the going to copy and paste the categorization LLM. We're going to categorization LLM. We're going to categorization LLM. We're going to create a new LM, and this is going to be create a new LM, and this is going to be create a new LM, and this is going to be the retriever LLM. It's going to necessitate more LLM. It's going to necessitate more LLM. It's going to necessitate more tokens. Like, I set it to a th00and, but tokens. Like, I set it to a th00and, but tokens. Like, I set it to a th00and, but everything else is going to stay the everything else is going to stay the everything else is going to stay the same. And then we're just going to grab same. And then we're just going to grab same. And then we're just going to grab two more two more two more uh an agent and another task. So, this uh an agent and another task. So, this uh an agent and another task. So, this is the retriever agent and the retriever is the retriever agent and the retriever is the retriever agent and the retriever task. I'm going to copy and paste this task. I'm going to copy and paste this task. I'm going to copy and paste this from our notes. from our notes. from our notes. And I'll explain exactly what they're And I'll explain exactly what they're And I'll explain exactly what they're doing. There is going to be a doing. There is going to be a doing. There is going to be a significant difference here and you'll significant difference here and you'll significant difference here and you'll see the error right away is that it's see the error right away is that it's see the error right away is that it's using a tool. And I'll explain exactly using a tool. And I'll explain exactly using a tool. And I'll explain exactly what we're doing there. So the retriever what we're doing there. So the retriever what we're doing there. So the retriever agent has a job, right? It's going to agent has a job, right? It's going to agent has a job, right? It's going to take that category that it's receiving take that category that it's receiving take that category that it's receiving from the uh categorization agent and from the uh categorization agent and from the uh categorization agent and it's going to it's going to pass it to a it's going to it's going to pass it to a it's going to it's going to pass it to a function that is going to query our uh function that is going to query our uh function that is going to query our uh vector DB. And so that function is going vector DB. And so that function is going vector DB. And so that function is going to be that tool. So let's create our to be that tool. So let's create our to be that tool. So let's create our first first first tool. We're going to name it the query tool. We're going to name it the query tool. We're going to name it the query collection tool. Let's define it. It's going to take what does it it. It's going to take what does it it. It's going to take what does it take? It's only taking the category and take? It's only taking the category and take? It's only taking the category and it's going to take the uh it's going to it's going to take the uh it's going to it's going to take the uh it's going to take the the query to embed. So query take the the query to embed. So query take the the query to embed. So query string and it's going to string and it's going to string and it's going to return a dictionary. Perfect. Let's add return a dictionary. Perfect. Let's add return a dictionary. Perfect. Let's add a doc string. This is going to be the a doc string. This is going to be the a doc string. This is going to be the tool query chroma tool query chroma tool query chroma uh based on uh based on uh based on category and return relevant documents. category and return relevant documents. category and return relevant documents. Now, if you ever worked with uh rag, if Now, if you ever worked with uh rag, if Now, if you ever worked with uh rag, if you ever worked with vector db, the the you ever worked with vector db, the the you ever worked with vector db, the the functionality of this is going to be functionality of this is going to be functionality of this is going to be very familiar, right? So, let me just very familiar, right? So, let me just very familiar, right? So, let me just copy and paste what that actual tool is copy and paste what that actual tool is copy and paste what that actual tool is going to going to going to do. Uh we're using the Watsonxi do. Uh we're using the Watsonxi do. Uh we're using the Watsonxi embeddings. Don't worry, like if you embeddings. Don't worry, like if you embeddings. Don't worry, like if you don't have that, you could use your own don't have that, you could use your own don't have that, you could use your own embeddings model if you have it locally. embeddings model if you have it locally. embeddings model if you have it locally. My my computer is not capable of it at My my computer is not capable of it at My my computer is not capable of it at the moment. Um the interesting thing the moment. Um the interesting thing the moment. Um the interesting thing here though is this part. We're grabbing here though is this part. We're grabbing here though is this part. We're grabbing the category that was returned by the the category that was returned by the the category that was returned by the categorization task and we're using that categorization task and we're using that categorization task and we're using that to query the data like query the vector to query the data like query the vector to query the data like query the vector DB which is fascinating because you're DB which is fascinating because you're DB which is fascinating because you're you have you're just saying this is what you have you're just saying this is what you have you're just saying this is what you do and the LM is doing it the agents you do and the LM is doing it the agents you do and the LM is doing it the agents are doing it. So that that is very very are doing it. So that that is very very are doing it. So that that is very very cool to me. So now that we have that we cool to me. So now that we have that we cool to me. So now that we have that we have that tool you can see what the have that tool you can see what the have that tool you can see what the retriever agent is doing and we give it retriever agent is doing and we give it retriever agent is doing and we give it the task you know we're passing in that the task you know we're passing in that the task you know we're passing in that query from the route. We have an query from the route. We have an query from the route. We have an expected output. We're not worried too expected output. We're not worried too expected output. We're not worried too much about this because we're not ever much about this because we're not ever much about this because we're not ever going to send back the context to the going to send back the context to the going to send back the context to the UI. So, we're not really enforcing that UI. So, we're not really enforcing that UI. So, we're not really enforcing that output output output JSON. But the only other thing I want to JSON. But the only other thing I want to JSON. But the only other thing I want to mention here is that we had to add a mention here is that we had to add a mention here is that we had to add a context and context is we're giving context and context is we're giving context and context is we're giving access to that categorization tasks like access to that categorization tasks like access to that categorization tasks like uh what its output was. So, it knew uh what its output was. So, it knew uh what its output was. So, it knew that's how we're getting that category that's how we're getting that category that's how we're getting that category and that's how we're telling the agent and that's how we're telling the agent and that's how we're telling the agent look at this category. you have the look at this category. you have the look at this category. you have the query, pass it, use this function, and query, pass it, use this function, and query, pass it, use this function, and then call it and return back the then call it and return back the then call it and return back the context. So for us now, all we're going context. So for us now, all we're going context. So for us now, all we're going to do is we're going to add the new to do is we're going to add the new to do is we're going to add the new agent to agent to agent to our crew. Welcome, and we're going to our crew. Welcome, and we're going to our crew. Welcome, and we're going to add the new add the new add the new task. Process is still going to be uh task. Process is still going to be uh task. Process is still going to be uh sequential. This time, let's just remove sequential. This time, let's just remove sequential. This time, let's just remove the category, like actually grabbing it, the category, like actually grabbing it, the category, like actually grabbing it, because we're not going to be returning because we're not going to be returning because we're not going to be returning that anymore. So we'll just say bye for that anymore. So we'll just say bye for that anymore. So we'll just say bye for now. But what we are going to do is now. But what we are going to do is now. But what we are going to do is we're going to print out that category result. result. result. Okay. Make sure everything comes up Okay. Make sure everything comes up Okay. Make sure everything comes up good. We're going to print out that good. We're going to print out that good. We're going to print out that category result and we're going to see category result and we're going to see category result and we're going to see exactly what happens when we send over exactly what happens when we send over exactly what happens when we send over that that uh request that query from that that uh request that query from that that uh request that query from here. here. here. Hopefully, we'll watch the new agent do Hopefully, we'll watch the new agent do Hopefully, we'll watch the new agent do exactly what we want it to do. Okay, exactly what we want it to do. Okay, exactly what we want it to do. Okay, let's get there. Okay, so okay, it let's get there. Okay, so okay, it let's get there. Okay, so okay, it already it already has the correct already it already has the correct already it already has the correct category. So now that it's going to take category. So now that it's going to take category. So now that it's going to take the collection uh look at that, we got the collection uh look at that, we got the collection uh look at that, we got the uh the the result from the rag. It the uh the the result from the rag. It the uh the the result from the rag. It used that category and it passed our used that category and it passed our used that category and it passed our Chromb collection. So we got that Chromb collection. So we got that Chromb collection. So we got that collection and then we queried it and collection and then we queried it and collection and then we queried it and now it returns back all the context for now it returns back all the context for now it returns back all the context for that query by basically by itself. We that query by basically by itself. We that query by basically by itself. We just told it. Yeah. And so we have you just told it. Yeah. And so we have you just told it. Yeah. And so we have you could see almost exactly what we're could see almost exactly what we're could see almost exactly what we're going to send to the final agent, right? going to send to the final agent, right? going to send to the final agent, right? We're going to send it the category We're going to send it the category We're going to send it the category because we want to return it to a UI. because we want to return it to a UI. because we want to return it to a UI. We're sending it the query. And now we We're sending it the query. And now we We're sending it the query. And now we have the context from our vector DB to have the context from our vector DB to have the context from our vector DB to augment retrieval augment and generate augment retrieval augment and generate augment retrieval augment and generate our our response. I don't know. I think our our response. I don't know. I think our our response. I don't know. I think that is particularly that is particularly that is particularly fascinating. So that's just a way that fascinating. So that's just a way that fascinating. So that's just a way that you could use tools with agents. And you could use tools with agents. And you could use tools with agents. And that was really just our retriever that was really just our retriever that was really just our retriever agent, right? Like we're able to use agent, right? Like we're able to use agent, right? Like we're able to use that function. And tool in this in this that function. And tool in this in this that function. And tool in this in this in this case means like we're using a in this case means like we're using a in this case means like we're using a function. We're giving an agent tools to function. We're giving an agent tools to function. We're giving an agent tools to use a function. And that is very very use a function. And that is very very use a function. And that is very very cool to make. So we're done with the uh cool to make. So we're done with the uh cool to make. So we're done with the uh retriever agent and we're going to move retriever agent and we're going to move retriever agent and we're going to move on to the generation agent. And this is on to the generation agent. And this is on to the generation agent. And this is the final step of the application. So the final step of the application. So the final step of the application. So let's just commit our changes and let's check out the final changes and let's check out the final changes and let's check out the final step. Let's go back to our API. And if we look Let's go back to our API. And if we look Let's go back to our API. And if we look back at our doc string, we know what the back at our doc string, we know what the back at our doc string, we know what the final step is. We're going to uh we're final step is. We're going to uh we're final step is. We're going to uh we're going to create an agent that creates a going to create an agent that creates a going to create an agent that creates a nice response for the nice response for the nice response for the user. Basically, everything that we just user. Basically, everything that we just user. Basically, everything that we just did, we're going to do one more time. did, we're going to do one more time. did, we're going to do one more time. And I really like this this And I really like this this And I really like this this uh this pattern, right? like of creating uh this pattern, right? like of creating uh this pattern, right? like of creating their own LLMs for each of the agents. their own LLMs for each of the agents. their own LLMs for each of the agents. I'd find that to be very u very nice I'd find that to be very u very nice I'd find that to be very u very nice because we could set different like for because we could set different like for because we could set different like for us we didn't really set any the only us we didn't really set any the only us we didn't really set any the only thing we're changing is the MAC tokens. thing we're changing is the MAC tokens. thing we're changing is the MAC tokens. Obviously, we want the response to have Obviously, we want the response to have Obviously, we want the response to have more leeway, but other than that, we're more leeway, but other than that, we're more leeway, but other than that, we're just like we could we could make it just like we could we could make it just like we could we could make it drastically different. Each LLM could be drastically different. Each LLM could be drastically different. Each LLM could be uh that we power could have a different uh that we power could have a different uh that we power could have a different model if we're using Watson Xi. We could model if we're using Watson Xi. We could model if we're using Watson Xi. We could use Mistral, we could use uh Llama, we use Mistral, we could use uh Llama, we use Mistral, we could use uh Llama, we could use whatever we want. So let's add could use whatever we want. So let's add could use whatever we want. So let's add the final the final to uh the task and the final the final to uh the task and the final the final to uh the task and the final agent which is going to be our the final agent which is going to be our the final agent which is going to be our generation generation generation agent. And once again we're going to be agent. And once again we're going to be agent. And once again we're going to be using a tool and I'll explain why in a using a tool and I'll explain why in a using a tool and I'll explain why in a second. So again we gave a role we gave second. So again we gave a role we gave second. So again we gave a role we gave a backstory. We uh we have a backstory. We uh we have a backstory. We uh we have a an LLM. Hold on. Let me just make sure a an LLM. Hold on. Let me just make sure a an LLM. Hold on. Let me just make sure I named it correctly. Oh yeah, there's a generation all not Oh yeah, there's a generation all not Oh yeah, there's a generation all not not the response. Let me just update not the response. Let me just update not the response. Let me just update that. that. that. Okay. Okay. But we have we're missing Okay. Okay. But we have we're missing Okay. Okay. But we have we're missing one last tool. And this tool, what I'm one last tool. And this tool, what I'm one last tool. And this tool, what I'm going to show you is how I found the going to show you is how I found the going to show you is how I found the prompt for this. So this tool is going prompt for this. So this tool is going prompt for this. So this tool is going to interpolate that query and that to interpolate that query and that to interpolate that query and that context into a nice prompt. And where I context into a nice prompt. And where I context into a nice prompt. And where I got the prompt is if you go to your got the prompt is if you go to your got the prompt is if you go to your projects, you can create this projects, you can create this projects, you can create this accelerator. just like look up Watson accelerator. just like look up Watson accelerator. just like look up Watson XAI rag and it'll give you this XAI rag and it'll give you this XAI rag and it'll give you this accelerator that you could just create accelerator that you could just create accelerator that you could just create and within there they have prompt and within there they have prompt and within there they have prompt templates written by the people who templates written by the people who templates written by the people who train the models you know or work with train the models you know or work with train the models you know or work with it a tremendous amount. So this prompt it a tremendous amount. So this prompt it a tremendous amount. So this prompt template is just I'm going to take this template is just I'm going to take this template is just I'm going to take this because they're they wrote it better because they're they wrote it better because they're they wrote it better than I'm really not a particularly good than I'm really not a particularly good than I'm really not a particularly good prompt engineer to be totally honest. So prompt engineer to be totally honest. So prompt engineer to be totally honest. So I just copy and paste this and I want to I just copy and paste this and I want to I just copy and paste this and I want to then interpolate the context that we then interpolate the context that we then interpolate the context that we received from the Chroma DB into the received from the Chroma DB into the received from the Chroma DB into the context and the question from the query. context and the question from the query. context and the question from the query. Right? So what we're going to do is Right? So what we're going to do is Right? So what we're going to do is we're going to create another tool and we're going to create another tool and we're going to create another tool and I'm just going to copy and paste the uh I'm just going to copy and paste the uh I'm just going to copy and paste the uh the tool and the process and the uh and the tool and the process and the uh and the tool and the process and the uh and the prompt. I'm going to give access to the prompt. I'm going to give access to the prompt. I'm going to give access to the generation agent. So this generation the generation agent. So this generation the generation agent. So this generation response tool the the generate response response tool the the generate response response tool the the generate response tool you can see exactly what I'm doing. tool you can see exactly what I'm doing. tool you can see exactly what I'm doing. It's grabbing the context. It's grabbing It's grabbing the context. It's grabbing It's grabbing the context. It's grabbing the query and it's sending it to uh this the query and it's sending it to uh this the query and it's sending it to uh this prompt. And finally, there's one last prompt. And finally, there's one last prompt. And finally, there's one last thing we have to do, which is create a thing we have to do, which is create a thing we have to do, which is create a pyantic model for the output because now pyantic model for the output because now pyantic model for the output because now we're sending back the entire thing to we're sending back the entire thing to we're sending back the entire thing to the UI. I really want to enforce that the UI. I really want to enforce that the UI. I really want to enforce that it's just going to be a JSON. I really it's just going to be a JSON. I really it's just going to be a JSON. I really want that category and I really want want that category and I really want want that category and I really want that I believe I call it response. I'll that I believe I call it response. I'll that I believe I call it response. I'll figure out let me let me look at what figure out let me let me look at what figure out let me let me look at what actually I call. But I need both of actually I call. But I need both of actually I call. But I need both of those to be there in order for it not those to be there in order for it not those to be there in order for it not to, you know, blow up on response. So, to, you know, blow up on response. So, to, you know, blow up on response. So, let me copy this let me copy this let me copy this model. Let's add it to the top over model. Let's add it to the top over model. Let's add it to the top over here. Okay. And this is going to be So let me just copy this model and pl be So let me just copy this model and pl be So let me just copy this model and pl paste it right here. This going to be paste it right here. This going to be paste it right here. This going to be the final response. This is going to be the final response. This is going to be the final response. This is going to be the JSON object that we're looking for the JSON object that we're looking for the JSON object that we're looking for that has a category field and has a that has a category field and has a that has a category field and has a response. And we're going to send response. And we're going to send response. And we're going to send this back straight through directly to this back straight through directly to this back straight through directly to our UI. So that's why we have this uh in our UI. So that's why we have this uh in our UI. So that's why we have this uh in the generation task. That's why we have the generation task. That's why we have the generation task. That's why we have we're trying to say, okay, this is what we're trying to say, okay, this is what we're trying to say, okay, this is what we want. This is what we want it to look we want. This is what we want it to look we want. This is what we want it to look like and this is what you need to like and this is what you need to like and this is what you need to return. return. return. So, let's add our final agent to So, let's add our final agent to So, let's add our final agent to the to the the to the the to the crew. Let's give him his final task. Okay, we have the crew kickoff. task. Okay, we have the crew kickoff. task. Okay, we have the crew kickoff. Let's just call this crew result now Let's just call this crew result now Let's just call this crew result now because we no longer need to send back because we no longer need to send back because we no longer need to send back that that that hard-coded response. Get rid of this. hard-coded response. Get rid of this. hard-coded response. Get rid of this. We're not sending this. Okay. Make sure nothing Okay. Make sure nothing Okay. Make sure nothing broke. broke. broke. Perfect. And let's see if we get a nice response. Okay. All right. It grabbed response. Okay. All right. It grabbed response. Okay. All right. It grabbed the correct category. Okay. It sent that the correct category. Okay. It sent that the correct category. Okay. It sent that category to the retriever who she category to the retriever who she category to the retriever who she returned back all of the context from returned back all of the context from returned back all of the context from the rag uh from the vector the rag uh from the vector the rag uh from the vector DB. Now she's going to send it to the DB. Now she's going to send it to the DB. Now she's going to send it to the final agent who's going to interpolate final agent who's going to interpolate final agent who's going to interpolate that into that prompt we we uh cribed that into that prompt we we uh cribed that into that prompt we we uh cribed notes from from Watson Studio. Let me notes from from Watson Studio. Let me notes from from Watson Studio. Let me it. Come it. Come it. Come on. Perfect. Okay. Yeah. So you see it on. Perfect. Okay. Yeah. So you see it on. Perfect. Okay. Yeah. So you see it the response has Okay. We have we have the response has Okay. We have we have the response has Okay. We have we have everything set up. It has the context everything set up. It has the context everything set up. It has the context Look at that. Huh? It worked. I'm not Look at that. Huh? It worked. I'm not Look at that. Huh? It worked. I'm not surprised. It worked before. I I've surprised. It worked before. I I've surprised. It worked before. I I've built this. But still, it's always kind built this. But still, it's always kind built this. But still, it's always kind of surprising. It's an amazing it's an of surprising. It's an amazing it's an of surprising. It's an amazing it's an amazing technology. So, you see we have amazing technology. So, you see we have amazing technology. So, you see we have the this rag response. Let's actually the this rag response. Let's actually the this rag response. Let's actually double check to make sure everything double check to make sure everything double check to make sure everything looks right. So, we have uh it's it's looks right. So, we have uh it's it's looks right. So, we have uh it's it's referencing error 01. So, let's look at referencing error 01. So, let's look at referencing error 01. So, let's look at our docs and let's make sure that we our docs and let's make sure that we our docs and let's make sure that we have the correct stuff. Error1 session have the correct stuff. Error1 session have the correct stuff. Error1 session expired. Clear your browser. Let's see expired. Clear your browser. Let's see expired. Clear your browser. Let's see what that says. Perfect. Yeah. So, it's what that says. Perfect. Yeah. So, it's what that says. Perfect. Yeah. So, it's it it worked exactly the way we wanted it it worked exactly the way we wanted it it worked exactly the way we wanted to. Something I really like is it's able to. Something I really like is it's able to. Something I really like is it's able to uh give me back like different steps to uh give me back like different steps to uh give me back like different steps like responses in the in the pipeline. like responses in the in the pipeline. like responses in the in the pipeline. I'm able to pass them along during the I'm able to pass them along during the I'm able to pass them along during the agent. So, I'm able to categorize the agent. So, I'm able to categorize the agent. So, I'm able to categorize the query. I'm able to show a really really query. I'm able to show a really really query. I'm able to show a really really nice message uh and a good accurate nice message uh and a good accurate nice message uh and a good accurate response. And it's all done with these response. And it's all done with these response. And it's all done with these agents. I think it's very cool. Um yeah. agents. I think it's very cool. Um yeah. agents. I think it's very cool. Um yeah. And so, obviously, we could we could And so, obviously, we could we could And so, obviously, we could we could enhance this. We could refactor it. We enhance this. We could refactor it. We enhance this. We could refactor it. We could change the parameters that we're could change the parameters that we're could change the parameters that we're using for the LLM to make it do using for the LLM to make it do using for the LLM to make it do different things. We could use a totally different things. We could use a totally different things. We could use a totally different LLM, totally different models different LLM, totally different models different LLM, totally different models to have whatever we want. What I really to have whatever we want. What I really to have whatever we want. What I really wanted to do, the two new agents that I wanted to do, the two new agents that I wanted to do, the two new agents that I really want to make, I'm probably going really want to make, I'm probably going really want to make, I'm probably going to do it later, is to route queries uh to do it later, is to route queries uh to do it later, is to route queries uh to the web, if it's not part of the to the web, if it's not part of the to the web, if it's not part of the Chromb collections, if it's able to Chromb collections, if it's able to Chromb collections, if it's able to categorize and say, \"Okay, this is out categorize and say, \"Okay, this is out categorize and say, \"Okay, this is out of the blue.\" Um, and also I want to of the blue.\" Um, and also I want to of the blue.\" Um, and also I want to really I want to I want to format that really I want to I want to format that really I want to I want to format that response when we get it back to the uh response when we get it back to the uh response when we get it back to the uh to the UI to maybe maybe format it to the UI to maybe maybe format it to the UI to maybe maybe format it within HTML and have another agent do within HTML and have another agent do within HTML and have another agent do that, right? Like look at this and put that, right? Like look at this and put that, right? Like look at this and put this into a nice HTML package and post this into a nice HTML package and post this into a nice HTML package and post it on uh as a response. Awesome. We've it on uh as a response. Awesome. We've it on uh as a response. Awesome. We've built a pretty sophisticated multi-agent built a pretty sophisticated multi-agent built a pretty sophisticated multi-agent pipeline here. So let's just recap. We pipeline here. So let's just recap. We pipeline here. So let's just recap. We built the back end to an agentic rag built the back end to an agentic rag built the back end to an agentic rag chatbot that is able to identify the chatbot that is able to identify the chatbot that is able to identify the queries category, target the correct queries category, target the correct queries category, target the correct chromabb collection and interpolate the chromabb collection and interpolate the chromabb collection and interpolate the query in the context into a custom query in the context into a custom query in the context into a custom prompt and generate a natural language prompt and generate a natural language prompt and generate a natural language response. So with this application and response. So with this application and response. So with this application and this process, we would love for you to this process, we would love for you to this process, we would love for you to explore additional use cases, customize explore additional use cases, customize explore additional use cases, customize the UI and experiment with the Crew AI the UI and experiment with the Crew AI the UI and experiment with the Crew AI framework and build something really framework and build something really framework and build something really cool. Maybe add a route that makes a web cool. Maybe add a route that makes a web cool. Maybe add a route that makes a web search if the query is just totally out search if the query is just totally out search if the query is just totally out of bounds. Maybe create an agent whose of bounds. Maybe create an agent whose of bounds. Maybe create an agent whose only job it is is to format the response only job it is is to format the response only job it is is to format the response in a particular way. We would love to in a particular way. We would love to in a particular way. We would love to see anything you do with it. Dive into see anything you do with it. Dive into see anything you do with it. Dive into the code, have fun, build something the code, have fun, build something the code, have fun, build something cool, refactor it, make it better. Just cool, refactor it, make it better. Just cool, refactor it, make it better. Just be creative.",
    "chunks": [
      "Kind: captions Language: en Have you ever had a tremendous amount of Have you ever had a tremendous amount of Have you ever had a tremendous amount of data in your vector DB? You're using data in your vector DB? You're using data in your vector DB? You're using that for retrieval augmented generation, that for retrieval augmented generation, that for retrieval augmented generation, but the context you're getting to give but the context you're getting to give but the context you're getting to",
      "give to your LLM to produce those great to your LLM to produce those great to your LLM to produce those great results is kind of lacking because it's results is kind of lacking because it's results is kind of lacking because it's pulling in different data that shouldn't pulling in different data that shouldn't pulling in different data that shouldn't really be pulled in with your query. really be pulled in with your query. really be pulled in with your query. Well, today I'm going to show you how",
      "to Well, today I'm going to show you how to Well, today I'm going to show you how to seamlessly integrate multiple AI agents seamlessly integrate multiple AI agents seamlessly integrate multiple AI agents into your application to combat this into your application to combat this into your application to combat this kind of problem. We will walk through a kind of problem. We will walk through a kind of problem. We will walk through a practical example that covers query practical example that covers",
      "query practical example that covers query categorization, context retrieval from a categorization, context retrieval from a categorization, context retrieval from a vector DB and natural language response vector DB and natural language response vector DB and natural language response generation, all using this multi- aent generation, all using this multi- aent generation, all using this multi- aent approach. This session is designed to approach. This session is designed to approach. This session",
      "is designed to give you a clear step-by-step guide on give you a clear step-by-step guide on give you a clear step-by-step guide on working with agents in your projects. working with agents in your projects. working with agents in your projects. Let's dive in and explore how you can Let's dive in and explore how you can Let's dive in and explore how you can leverage these tools to build smarter leverage these tools to build smarter leverage these tools to build smarter applications. So, in the",
      "description of applications. So, in the description of applications. So, in the description of the video, you should have a link to the video, you should have a link to the video, you should have a link to this repo. First thing we're going to do this repo. First thing we're going to do this repo. First thing we're going to do is just clone down the repo to our local machine. Okay. And let's go into the machine. Okay. And let's go into the machine. Okay. And let's go into the repo and then into",
      "the UI directory. So repo and then into the UI directory. So repo and then into the UI directory. So if you look at the structure of the if you look at the structure of the if you look at the structure of the application, we're going to have an API, application, we're going to have an API, application, we're going to have an API, which is what we're going to be working which is what we're going to be working which is what we're going to be working on today. And then we have a UI, which on today.",
      "And then we have a UI, which on today. And then we have a UI, which we're not going to work on, but that's we're not going to work on, but that's we're not going to work on, but that's what's going to render what we're doing what's going to render what we're doing what's going to render what we're doing to the uh to your browser. And so we're to the uh to your browser. And so we're to the uh to your browser. And so we're going to go into the UI and we're going going to go into the UI and we're",
      "going going to go into the UI and we're going to install all the dependencies. So the to install all the dependencies. So the to install all the dependencies. So the first thing is just install the root first thing is just install the root first thing is just install the root dependencies and once that's completed dependencies and once that's completed dependencies and once that's completed we're going to run a setup script. So the UI even though we're not script. So the UI even though we're not",
      "script. So the UI even though we're not working on it the UI is made with React working on it the UI is made with React working on it the UI is made with React TypeScript and it has an express TypeScript and it has an express TypeScript and it has an express TypeScript server and we're using TypeScript server and we're using TypeScript server and we're using something called carbon components and I something called carbon components and I something called carbon components and I just want to",
      "highlight carbon components just want to highlight carbon components just want to highlight carbon components for a second. So, if you search carbon for a second. So, if you search carbon for a second. So, if you search carbon design react, it's going to bring you to design react, it's going to bring you to design react, it's going to bring you to the docs the docs the docs page, and this is where I get all like page, and this is where I get all like page, and this is where I get all like the",
      "components to put into the UI, it's the components to put into the UI, it's the components to put into the UI, it's super easy, especially somebody who's super easy, especially somebody who's super easy, especially somebody who's not a particularly uh adept front-end not a particularly uh adept front-end not a particularly uh adept front-end developer. It just makes it super super developer. It just makes it super super developer. It just makes it super super easy and they look good and, you",
      "know, easy and they look good and, you know, easy and they look good and, you know, they have the code, everything you need. they have the code, everything you need. they have the code, everything you need. So, my suggestion is even if we're not So, my suggestion is even if we're not So, my suggestion is even if we're not working on the UI today, go look at working on the UI today, go look at working on the UI today, go look at carbon design, look at what we can do, carbon design, look at what we",
      "can do, carbon design, look at what we can do, and maybe change the application when and maybe change the application when and maybe change the application when you're done with it. All right, so we're you're done with it. All right, so we're you're done with it. All right, so we're going to wait for the dependencies to going to wait for the dependencies to going to wait for the dependencies to install and we'll be right back. Okay, install and we'll be right back. Okay, install and we'll be",
      "right back. Okay, the dependencies have installed. One the dependencies have installed. One the dependencies have installed. One last thing to do within the UI is we're last thing to do within the UI is we're last thing to do within the UI is we're going to copy the client's ENV example going to copy the client's ENV example going to copy the client's ENV example into back into the client and we're into back into the client and we're into back into the client and we're going to create an env.",
      "We're going to going to create an env. We're going to going to create an env. We're going to do the same thing for the server. Copy do the same thing for the server. Copy do the same thing for the server. Copy that ENV example and place it right back that ENV example and place it right back that ENV example and place it right back asenv in the server. Uh something we asenv in the server. Uh something we asenv in the server. Uh something we also could do if we want to go into also could do if we",
      "want to go into also could do if we want to go into client.mv we've added a way for you to client.mv we've added a way for you to client.mv we've added a way for you to brand the application after if you want brand the application after if you want brand the application after if you want to make something your own. So we have a to make something your own. So we have a to make something your own. So we have a branding and an application name. So for branding and an application name. So for",
      "branding and an application name. So for this one we're going to say agents and this one we're going to say agents and this one we're going to say agents and action exclamation point. action exclamation point. action exclamation point. Okay. So now we're done with the Okay. So now we're done with the Okay. So now we're done with the dependencies and we're done totally with dependencies and we're done totally with dependencies and we're done totally with the UI today. So let's go back to the the",
      "UI today. So let's go back to the the UI today. So let's go back to the root and let's head over to the API. So root and let's head over to the API. So root and let's head over to the API. So the API is written in Python. So let's the API is written in Python. So let's the API is written in Python. So let's create a virtual create a virtual create a virtual environment. We'll name that AI environment. We'll name that AI environment. We'll name that AI agent. And once that's done, let's activate",
      "let's activate the activate let's activate the activate let's activate the uh the virtual environment. And now uh the virtual environment. And now uh the virtual environment. And now we're going to install all the we're going to install all the we're going to install all the dependencies. So, this is going to take a little So, this is going to take a little So, this is going to take a little while. We're installing Crew AI. We're while. We're installing Crew AI. We're while. We're installing Crew",
      "AI. We're installing Watson XAI. Ton of installing Watson XAI. Ton of installing Watson XAI. Ton of dependencies. And so, once this is dependencies. And so, once this is dependencies. And so, once this is completed, we'll continue along with the completed, we'll continue along with the completed, we'll continue along with the tutorial. Okay. So, now that the tutorial. Okay. So, now that the tutorial. Okay. So, now that the dependencies have been uh installed, we dependencies have been uh",
      "installed, we dependencies have been uh installed, we have to just copy thev have to just copy thev have to just copy thev uh.ample and paste it into the API. uh.ample and paste it into the API. uh.ample and paste it into the API. Let's take a look at what's in that env. Let's take a look at what's in that env. Let's take a look at what's in that env. Uh because we're using Watson XAI, we Uh because we're using Watson XAI, we Uh because we're using Watson XAI, we need to have the connection",
      "strings for need to have the connection strings for need to have the connection strings for to connect to Watson XAI. Um so we're to connect to Watson XAI. Um so we're to connect to Watson XAI. Um so we're going to head over to cloud and if you going to head over to cloud and if you going to head over to cloud and if you go into your resource list, just open up go into your resource list, just open up go into your resource list, just open up your Watson Studio and go into IBM your Watson Studio",
      "and go into IBM your Watson Studio and go into IBM Watson Watson Watson X and it's just going to log us X and it's just going to log us X and it's just going to log us in and we're going to head over to in and we're going to head over to in and we're going to head over to Prompt Lab. Now I'm sure this is a Prompt Lab. Now I'm sure this is a Prompt Lab. Now I'm sure this is a better way of getting this information better way of getting this information better way of getting this information than",
      "the way I do it, but the way I do than the way I do it, but the way I do than the way I do it, but the way I do it is I just go to Prompt Lab. At the it is I just go to Prompt Lab. At the it is I just go to Prompt Lab. At the top right, we have a view code and it top right, we have a view code and it top right, we have a view code and it shows you a curl command and it has all shows you a curl command and it has all shows you a curl command and it has all the most of the stuff that we need in the",
      "most of the stuff that we need in the most of the stuff that we need in order to make that configuration with order to make that configuration with order to make that configuration with our API. So, grab from the curl the base our API. So, grab from the curl the base our API. So, grab from the curl the base URL and paste it here in the Watson URL and we go back and we grab the URL and we go back and we grab the URL and we go back and we grab the project ID. Just copy and paste that. And then",
      "finally, let's go back to that. And then finally, let's go back to that. And then finally, let's go back to cloud.ibbm. We're going to go at the cloud.ibbm. We're going to go at the cloud.ibbm. We're going to go at the very top. You're going to have a manage. very top. You're going to have a manage. very top. You're going to have a manage. And you're going to want to go to the And you're going to want to go to the And you're going to want to go to the access am. And when we get there, on the",
      "access am. And when we get there, on the access am. And when we get there, on the left hand side, you're going to see left hand side, you're going to see left hand side, you're going to see something called API keys. Let's create something called API keys. Let's create something called API keys. Let's create a new one. Call it aentic and one. Call it aentic and one. Call it aentic and create. So, let's just copy create. So, let's just copy create. So, let's just copy that and put it right that",
      "and put it right that and put it right here. And that's it. Now our API is set here. And that's it. Now our API is set here. And that's it. Now our API is set up. So the next thing we want to do is up. So the next thing we want to do is up. So the next thing we want to do is check out into a new branch. All right. check out into a new branch. All right. check out into a new branch. All right. So let's check So let's check So let's check out our first branch. It's going to be out our first branch.",
      "It's going to be out our first branch. It's going to be step. step. step. Perfect. And what we want to do is we Perfect. And what we want to do is we Perfect. And what we want to do is we want to start up all our services. We want to start up all our services. We want to start up all our services. We have three services. Remember we have have three services. Remember we have have three services. Remember we have the fast API, the React UI, and the the fast API, the React UI, and the the fast API,",
      "the React UI, and the express server. So, let's first start up express server. So, let's first start up express server. So, let's first start up uh wait, we have to go into the API uh wait, we have to go into the API uh wait, we have to go into the API directory first and then we could run directory first and then we could run directory first and then we could run our unicorn command. We're just going to our unicorn command. We're just going to our unicorn command. We're just going to run the",
      "unicorn server up reload. We're run the unicorn server up reload. We're run the unicorn server up reload. We're going to start our fast API. going to start our fast API. going to start our fast API. Also, let's have two more windows Also, let's have two more windows Also, let's have two more windows because we're going to go back to the UI because we're going to go back to the UI because we're going to go back to the UI and we're going to start up and we're going to start up and we're going to",
      "start up the client and then we're going to start the client and then we're going to start the client and then we're going to start up the server. And all these commands are in the uh in And all these commands are in the uh in And all these commands are in the uh in the repo. So you just copy and paste the repo. So you just copy and paste the repo. So you just copy and paste them. So we're just waiting a second for them. So we're just waiting a second for them. So we're just waiting a second for",
      "Unicorn for the fast API to get all Unicorn for the fast API to get all Unicorn for the fast API to get all ready. And we'll head over to the ready. And we'll head over to the ready. And we'll head over to the browser and we could already see what browser and we could already see what browser and we could already see what the UI is going to look the UI is going to look the UI is going to look like. Beautiful. It's a chatbot. You like. Beautiful. It's a chatbot. You like. Beautiful. It's a",
      "chatbot. You have a chat window, a couple buttons, have a chat window, a couple buttons, have a chat window, a couple buttons, but what it's going to do on the back but what it's going to do on the back but what it's going to do on the back end is going to be pretty cool, I think. end is going to be pretty cool, I think. end is going to be pretty cool, I think. So, if you go to your API directory, So, if you go to your API directory, So, if you go to your API directory, there's going to be a",
      "couple of folders there's going to be a couple of folders there's going to be a couple of folders that you're going to be interested in that you're going to be interested in that you're going to be interested in and a questions.txt that I've been and a questions.txt that I've been and a questions.txt that I've been using. So, let me copy and paste this using. So, let me copy and paste this using. So, let me copy and paste this into our chat window. And what we're into our chat window. And what",
      "we're into our chat window. And what we're going to want to do is when we hit send, going to want to do is when we hit send, going to want to do is when we hit send, we're going to want the backend to we're going to want the backend to we're going to want the backend to categorize that categorize that categorize that query, grab the correct data from the query, grab the correct data from the query, grab the correct data from the correct collection of vector DB in correct collection of vector DB",
      "in correct collection of vector DB in Chroma DB, and then we're going to want Chroma DB, and then we're going to want Chroma DB, and then we're going to want to pass that to a customized prompt, and to pass that to a customized prompt, and to pass that to a customized prompt, and return a nice response. Currently, it's return a nice response. Currently, it's return a nice response. Currently, it's just going to say this will be generated just going to say this will be generated just going to say",
      "this will be generated by our multi- aent process, and the by our multi- aent process, and the by our multi- aent process, and the category is something cool, but it will category is something cool, but it will category is something cool, but it will be something cool once we set it up. So be something cool once we set it up. So be something cool once we set it up. So let's go and run the first script that let's go and run the first script that let's go and run the first script that we have. So",
      "if you look in API scripts we have. So if you look in API scripts we have. So if you look in API scripts and you look at the process document and you look at the process document and you look at the process document script, this is how we're we're going to script, this is how we're we're going to script, this is how we're we're going to create our Chromma DB uh vector DB, create our Chromma DB uh vector DB, create our Chromma DB uh vector DB, right? We have a directory called docs right? We have",
      "a directory called docs right? We have a directory called docs and in that we have three text files. and in that we have three text files. and in that we have three text files. One called accounting, billing and One called accounting, billing and One called accounting, billing and technical. And what I'm trying to show technical. And what I'm trying to show technical. And what I'm trying to show here is that this is imagine you have a here is that this is imagine you have a here is that this is",
      "imagine you have a tremendous amount of documentation and tremendous amount of documentation and tremendous amount of documentation and some of those u when we when we query some of those u when we when we query some of those u when we when we query them in a vector to be the the cosign them in a vector to be the the cosign them in a vector to be the the cosign similarity distance going to be pretty similarity distance going to be pretty similarity distance going to be pretty close for stuff that",
      "might not be close for stuff that might not be close for stuff that might not be relevant. So we isolate topics right? So relevant. So we isolate topics right? So relevant. So we isolate topics right? So we have an account we have a technical we have an account we have a technical we have an account we have a technical we have the billing and I'm just trying we have the billing and I'm just trying we have the billing and I'm just trying to recreate that locally something very to recreate that",
      "locally something very to recreate that locally something very simple. So if you look at the script, simple. So if you look at the script, simple. So if you look at the script, all it's doing is it's saying, \"Okay, all it's doing is it's saying, \"Okay, all it's doing is it's saying, \"Okay, what's the file?\" It's going to loop what's the file?\" It's going to loop what's the file?\" It's going to loop through all the files in Docs. What's through all the files in Docs. What's through all the files",
      "in Docs. What's the file name? Create a new collection the file name? Create a new collection the file name? Create a new collection with that file name and insert the with that file name and insert the with that file name and insert the embeddings for that file into that embeddings for that file into that embeddings for that file into that collection. So let's run that script. You'll see first it's going to script. You'll see first it's going to script. You'll see first it's going to say, okay,",
      "is there any documents? No say, okay, is there any documents? No say, okay, is there any documents? No existing collections found. So let's existing collections found. So let's existing collections found. So let's going to create three new ones. And going to create three new ones. And going to create three new ones. And we're going to have account, billing, we're going to have account, billing, we're going to have account, billing, technical. Before we could do any of technical. Before we could",
      "do any of technical. Before we could do any of that, we have to first categorize the that, we have to first categorize the that, we have to first categorize the query. So this is where we're going to query. So this is where we're going to query. So this is where we're going to create our very first agent. It's going create our very first agent. It's going create our very first agent. It's going to be the categorization agent. So the route we're looking at agent. So the route we're looking at",
      "agent. So the route we're looking at that we're going to change is called the that we're going to change is called the that we're going to change is called the agentic route. And here you can see in agentic route. And here you can see in agentic route. And here you can see in the doc string I have what each of the the doc string I have what each of the the doc string I have what each of the each of the agents are going to do. The each of the agents are going to do. The each of the agents are",
      "going to do. The first one is going to be the query first one is going to be the query first one is going to be the query categorization. Then we're going to have categorization. Then we're going to have categorization. Then we're going to have the context retrieval. And then we're the context retrieval. And then we're the context retrieval. And then we're going to have the response generation. going to have the response generation. going to have the response generation. And these are all going",
      "to be agents. So And these are all going to be agents. So And these are all going to be agents. So let's bring in our agent framework which let's bring in our agent framework which let's bring in our agent framework which is crew AI. So we're going to say from is crew AI. So we're going to say from is crew AI. So we're going to say from crew crew crew AI import the first class we're going to AI import the first class we're going to AI import the first class we're going to bring is agent. We're",
      "going to bring in bring is agent. We're going to bring in bring is agent. We're going to bring in a task. We're going to bring in the a task. We're going to bring in the a task. We're going to bring in the crew. We're going to bring in a process crew. We're going to bring in a process crew. We're going to bring in a process and we're bring you can see in the first doc bring you can see in the first doc bring you can see in the first doc string uh it's an LLM powered agent. So string uh it's an",
      "LLM powered agent. So string uh it's an LLM powered agent. So this is where we start connecting Wasin this is where we start connecting Wasin this is where we start connecting Wasin XAI to the Crew AI agentic framework. So XAI to the Crew AI agentic framework. So XAI to the Crew AI agentic framework. So let's create our first LLM. When you look at the docs, the only LLM. When you look at the docs, the only LLM. When you look at the docs, the only thing we're really concerned with from thing we're",
      "really concerned with from thing we're really concerned with from this is just the model. We're going to this is just the model. We're going to this is just the model. We're going to use the Watsonx AI model. Uh use the Watsonx AI model. Uh use the Watsonx AI model. Uh temperature, max tokens, and then all temperature, max tokens, and then all temperature, max tokens, and then all the connection strings. So if we look in the connection strings. So if we look in the connection strings. So if we",
      "look in the server.py, we have a list of the server.py, we have a list of the server.py, we have a list of available models that you could use uh available models that you could use uh available models that you could use uh from Watsonx AI. So, I'm going to use from Watsonx AI. So, I'm going to use from Watsonx AI. So, I'm going to use the granite uh 38 the granite uh 38 the granite uh 38 billion. So, let's bring that billion. So, let's bring that billion. So, let's bring that in. And we just",
      "have to append Watson X in. And we just have to append Watson X in. And we just have to append Watson X to the front of to the front of to the front of it. Then, we're going to set the it. Then, we're going to set the it. Then, we're going to set the temperature. Now, this is from trial and temperature. Now, this is from trial and temperature. Now, this is from trial and error, but 0.7 works well. And the max error, but 0.7 works well. And the max error, but 0.7 works well. And the max tokens is",
      "50 because again, all this is tokens is 50 because again, all this is tokens is 50 because again, all this is doing is just categorizing a query, doing is just categorizing a query, doing is just categorizing a query, right? We're not we're not doing any uh right? We're not we're not doing any uh right? We're not we're not doing any uh massive return or anything like that. So massive return or anything like that. So massive return or anything like that. So the next thing we have to do is we're",
      "the next thing we have to do is we're the next thing we have to do is we're going to bring in the connection string going to bring in the connection string going to bring in the connection string Watson. So we're going to need the API Watson. So we're going to need the API Watson. So we're going to need the API key, the project ID, and the URL. So key, the project ID, and the URL. So key, the project ID, and the URL. So let's say URL is going to equal let's say URL is going to equal let's say URL",
      "is going to equal OS and get the key. Let's just paste OS and get the key. Let's just paste OS and get the key. Let's just paste that in. And then we're going to have that in. And then we're going to have that in. And then we're going to have the API the API the API key. We're going to get that from our env. And then finally, we're going to env. And then finally, we're going to env. And then finally, we're going to get the get the get the project ID, which is going to again just project ID, which",
      "is going to again just project ID, which is going to again just bring it in. Perfect. So, let's bring it into the in. Perfect. So, let's bring it into the in. Perfect. So, let's bring it into the uh the Crew AI LM class. Uh so, we're uh the Crew AI LM class. Uh so, we're uh the Crew AI LM class. Uh so, we're going to have the base URL is going to going to have the base URL is going to going to have the base URL is going to be the URL. The API key is going to be be the URL. The API key is going to",
      "be be the URL. The API key is going to be the API key and the project ID is going the API key and the project ID is going the API key and the project ID is going to be the project ID. to be the project ID. to be the project ID. Perfect. So now we have to create our Perfect. So now we have to create our Perfect. So now we have to create our first agent. Let's do it. We're going to first agent. Let's do it. We're going to first agent. Let's do it. We're going to name we're going to name him name",
      "we're going to name him name we're going to name him categorization agent. A very clever categorization agent. A very clever categorization agent. A very clever name. And if we look at the docs name. And if we look at the docs name. And if we look at the docs again, we can see exactly what we're again, we can see exactly what we're again, we can see exactly what we're going to be doing here. going to be doing here. going to be doing here. So the top three are particularly cool, So the top three",
      "are particularly cool, So the top three are particularly cool, right? to me. You have a role, a goal, right? to me. You have a role, a goal, right? to me. You have a role, a goal, and a backstory. And if you go to the and a backstory. And if you go to the and a backstory. And if you go to the crew AI crew AI crew AI docs, you have really, you know, they docs, you have really, you know, they docs, you have really, you know, they they are really good about explaining they are really good about",
      "explaining they are really good about explaining exactly what all those attributes do, exactly what all those attributes do, exactly what all those attributes do, but I just want to read them to you. So, but I just want to read them to you. So, but I just want to read them to you. So, the the role, it's defining the agents the the role, it's defining the agents the the role, it's defining the agents function and its expertise within the function and its expertise within the function and its",
      "expertise within the crew. Remember, it's a crew of agents. crew. Remember, it's a crew of agents. crew. Remember, it's a crew of agents. Uh the goal is we're going to it gives Uh the goal is we're going to it gives Uh the goal is we're going to it gives you the clear defined goal of what it's you the clear defined goal of what it's you the clear defined goal of what it's going to do. And then the backstory is going to do. And then the backstory is going to do. And then the backstory is great.",
      "It's just provides context and great. It's just provides context and great. It's just provides context and personality to the agent which I find personality to the agent which I find personality to the agent which I find very very cool. So let's start very very cool. So let's start very very cool. So let's start with the uh let's start with the role with the uh let's start with the role with the uh let's start with the role and so for the categorization agent the role is going to the one I've",
      "agent the role is going to the one I've agent the role is going to the one I've come up with is collection selector. come up with is collection selector. come up with is collection selector. Now, if you've worked with LLMs, you Now, if you've worked with LLMs, you Now, if you've worked with LLMs, you know, a lot of these, and this probably know, a lot of these, and this probably know, a lot of these, and this probably looks like prompt engineering, too, looks like prompt engineering, too, looks",
      "like prompt engineering, too, because it kind of is. It's just trial because it kind of is. It's just trial because it kind of is. It's just trial and error. This is what worked out for and error. This is what worked out for and error. This is what worked out for me. So, I gave it the role of a me. So, I gave it the role of a me. So, I gave it the role of a collection selector because it's collection selector because it's collection selector because it's selecting the collection. The goal is to",
      "selecting the collection. The goal is to selecting the collection. The goal is to analyze analyze analyze uh it's going to analyze the user uh it's going to analyze the user uh it's going to analyze the user queries and queries and queries and determine the most determine the most determine the most relevant relevant relevant Chromod collection. Chromod collection. Chromod collection. And then we're going to give the And then we're going to give the And then we're going to give the backstory. Uh",
      "he is an expert in query classification. And he routes questions classification. And he routes questions classification. And he routes questions to the to the to the correct domain. All right. Finally, we're going domain. All right. Finally, we're going domain. All right. Finally, we're going to do add a couple of the other things to do add a couple of the other things to do add a couple of the other things that we need. Verose I'm going to set to that we need. Verose I'm going to set to that we",
      "need. Verose I'm going to set to true because we want to see what what true because we want to see what what true because we want to see what what it's up to in the logs. Um allow it's up to in the logs. Um allow it's up to in the logs. Um allow delegation. So this is interesting. delegation. So this is interesting. delegation. So this is interesting. Remember, we're going to have multiple Remember, we're going to have multiple Remember, we're going to have multiple agents and they could all have",
      "different agents and they could all have different agents and they could all have different goals and different expertises and it goals and different expertises and it goals and different expertises and it can make the decision on on what it can make the decision on on what it can make the decision on on what it wants to do based on that, right? Like, wants to do based on that, right? Like, wants to do based on that, right? Like, okay, this is not for me. Let me send it okay, this is not for me.",
      "Let me send it okay, this is not for me. Let me send it to another agent. It's going to allow to to another agent. It's going to allow to to another agent. It's going to allow to delegate. But in our case, we don't. delegate. But in our case, we don't. delegate. But in our case, we don't. We're just going sequentially, right? We We're just going sequentially, right? We We're just going sequentially, right? We have three agents. We need them to do have three agents. We need them to do have three",
      "agents. We need them to do exactly what we want them to do. So, we exactly what we want them to do. So, we exactly what we want them to do. So, we turn delegation off. And in line with turn delegation off. And in line with turn delegation off. And in line with that, there's also something called max that, there's also something called max that, there's also something called max iterations. So, it defaults to 20. But iterations. So, it defaults to 20. But iterations. So, it defaults to 20. But in",
      "our case again like if something is in our case again like if something is in our case again like if something is not working because these are pretty not working because these are pretty not working because these are pretty simple tasks. If something's not working simple tasks. If something's not working simple tasks. If something's not working it's just going to try to try it over it's just going to try to try it over it's just going to try to try it over and over and over again. We just have",
      "to and over and over again. We just have to and over and over again. We just have to fix the code that at least in my fix the code that at least in my fix the code that at least in my experience is what's happening. And experience is what's happening. And experience is what's happening. And finally we have to give it its brain finally we have to give it its brain finally we have to give it its brain right. We're going to give it the right. We're going to give it the right. We're going to give it",
      "the categorization. This is its reasoning categorization. This is its reasoning categorization. This is its reasoning capability. This is how it's going to capability. This is how it's going to capability. This is how it's going to actually do what it needs to do. And actually do what it needs to do. And actually do what it needs to do. And that's the categorization alm that's that's the categorization alm that's that's the categorization alm that's using granite. So now we have an agent using",
      "granite. So now we have an agent using granite. So now we have an agent who has a brain and has a role and has a who has a brain and has a role and has a who has a brain and has a role and has a backstory and has a whole life story. we backstory and has a whole life story. we backstory and has a whole life story. we have to give it a task. We we're we're have to give it a task. We we're we're have to give it a task. We we're we're going to ask the agent to do something. going to ask the agent to",
      "do something. going to ask the agent to do something. So let's create our task and again let's look at the docs. task and again let's look at the docs. task and again let's look at the docs. So the things we're going to be So the things we're going to be So the things we're going to be concerned with obviously is agent. We concerned with obviously is agent. We concerned with obviously is agent. We have to assign this task to our have to assign this task to our have to assign this task to our",
      "categorization agent. Um description categorization agent. Um description categorization agent. Um description which is going to be really just a which is going to be really just a which is going to be really just a prompt. Uh, and then output JSON, which prompt. Uh, and then output JSON, which prompt. Uh, and then output JSON, which is really important for what we're going is really important for what we're going is really important for what we're going to do because we're going to send this to",
      "do because we're going to send this to do because we're going to send this first agent's response directly back to first agent's response directly back to first agent's response directly back to the UI. So, it has to be formatted in a the UI. So, it has to be formatted in a the UI. So, it has to be formatted in a particular way. So, I'm just going to particular way. So, I'm just going to particular way. So, I'm just going to copy and paste the description because copy and paste the description",
      "because copy and paste the description because it's just a it's a prompt, right? Like, it's just a it's a prompt, right? Like, it's just a it's a prompt, right? Like, if anyone's used prompts before, this if anyone's used prompts before, this if anyone's used prompts before, this took me a while to get took me a while to get took me a while to get correct, correct, correct, but you can see exactly what I mean, but you can see exactly what I mean, but you can see exactly what I mean, right? This",
      "is a prompt. We're saying, right? This is a prompt. We're saying, right? This is a prompt. We're saying, look, look at the query and determine look, look at the query and determine look, look at the query and determine the best category. you must only return the best category. you must only return the best category. you must only return one word because again we're going to be one word because again we're going to be one word because again we're going to be using this later down the line as the",
      "using this later down the line as the using this later down the line as the classification agent. And then we give classification agent. And then we give classification agent. And then we give it category definitions and we're really it category definitions and we're really it category definitions and we're really kind of broad with it because we want to kind of broad with it because we want to kind of broad with it because we want to give agency to the agent. So we're just give agency to the",
      "agent. So we're just give agency to the agent. So we're just saying okay this is what a technical saying okay this is what a technical saying okay this is what a technical query could look like. This is what query could look like. This is what query could look like. This is what billing we're giving that we're giving billing we're giving that we're giving billing we're giving that we're giving the agent agency here. And then finally the agent agency here. And then finally the agent agency here.",
      "And then finally I just really want to hammer home please I just really want to hammer home please I just really want to hammer home please just only one word from this list. just only one word from this list. just only one word from this list. And then we also have a expected output And then we also have a expected output And then we also have a expected output and this is important because we we need and this is important because we we need and this is important because we we need something",
      "explicit. So we want a JSON something explicit. So we want a JSON something explicit. So we want a JSON object with category field and it has to object with category field and it has to object with category field and it has to either be technical billing or account. either be technical billing or account. either be technical billing or account. The agent we're assigning to it The agent we're assigning to it The agent we're assigning to it obviously is the categorization agent. obviously is the",
      "categorization agent. obviously is the categorization agent. And finally the last thing that I And finally the last thing that I And finally the last thing that I mentioned was the output mentioned was the output mentioned was the output JSON. And I find this really nice. So JSON. And I find this really nice. So JSON. And I find this really nice. So the output JSON takes in a pyantic the output JSON takes in a pyantic the output JSON takes in a pyantic model. So let me copy and paste the model.",
      "So let me copy and paste the model. So let me copy and paste the pyantic model I have. and I'll show you pyantic model I have. and I'll show you pyantic model I have. and I'll show you what I did. Paste it in did. Paste it in did. Paste it in here. So, we have a category response. here. So, we have a category response. here. So, we have a category response. I'm expecting a JSON object with a I'm expecting a JSON object with a I'm expecting a JSON object with a category field. Uh, and the value is",
      "category field. Uh, and the value is category field. Uh, and the value is going to be either technical, billing, going to be either technical, billing, going to be either technical, billing, or account. Now, I added this or account. Now, I added this or account. Now, I added this description because I have a feeling description because I have a feeling description because I have a feeling that the agent is actually looking at that the agent is actually looking at that the agent is actually",
      "looking at the descriptions of these models before the descriptions of these models before the descriptions of these models before it's responding on it. I not don't don't it's responding on it. I not don't don't it's responding on it. I not don't don't quote me on it, but that's what I think quote me on it, but that's what I think quote me on it, but that's what I think it's doing. So I added it uh and it it's doing. So I added it uh and it it's doing. So I added it uh and it worked well. So",
      "from all I can tell it worked well. So from all I can tell it worked well. So from all I can tell it is working the way it's supposed to be is working the way it's supposed to be is working the way it's supposed to be working. All right. So now we have a working. All right. So now we have a working. All right. So now we have a agent who is powered by our LLM and has agent who is powered by our LLM and has agent who is powered by our LLM and has a task to follow. So let's create the a task to",
      "follow. So let's create the a task to follow. So let's create the first crew. You look at the docs here. We have crew. You look at the docs here. We have crew. You look at the docs here. We have tasks. We have agents. We're going to tasks. We have agents. We're going to tasks. We have agents. We're going to we're going to have we're going to use we're going to have we're going to use we're going to have we're going to use the process and we're going to we're the process and we're going to we're",
      "the process and we're going to we're going to have verbosity because we want going to have verbosity because we want going to have verbosity because we want to have some responses. So first thing to have some responses. So first thing to have some responses. So first thing to first is let's add the agent and right now we only have agent and right now we only have agent and right now we only have one. Then we have to add the tasks and that's going to be the tasks and that's going to be the tasks",
      "and that's going to be the categorization task. Remember this is categorization task. Remember this is categorization task. Remember this is going to be a crew. It's going to be a going to be a crew. It's going to be a going to be a crew. It's going to be a couple of agents here. And then we're couple of agents here. And then we're couple of agents here. And then we're going to have both. We're going to set going to have both. We're going to set going to have both. We're going to set it to true",
      "again because we want to see it to true again because we want to see it to true again because we want to see what it's doing. And then finally we what it's doing. And then finally we what it's doing. And then finally we have a process. And the process is going have a process. And the process is going have a process. And the process is going to be sequential because uh if you look to be sequential because uh if you look to be sequential because uh if you look at the dock string, we're just going",
      "at the dock string, we're just going at the dock string, we're just going step by step. So now that we have the step by step. So now that we have the step by step. So now that we have the crew, let's have the crew kickoff and we're going to call the kickoff and we're going to call the kickoff and we're going to call the kickoff method from the crew. And if you kickoff method from the crew. And if you kickoff method from the crew. And if you remember from the category response, remember from the",
      "category response, remember from the category response, we're expecting it to be a JSON object we're expecting it to be a JSON object we're expecting it to be a JSON object with category as a field. So, what we're going to So, what we're going to So, what we're going to do is we're going to instead of send do is we're going to instead of send do is we're going to instead of send back something cool to the UI, we're back something cool to the UI, we're back something cool to the UI, we're going to",
      "grab the category going to grab the category going to grab the category result and we're going to try to grab result and we're going to try to grab result and we're going to try to grab that category from the response. Make that category from the response. Make that category from the response. Make sure nothing broke. Looks good. We have the category broke. Looks good. We have the category broke. Looks good. We have the category result. Let's test it out. result. Let's test it out. result. Let's",
      "test it out. So let's copy and paste what we already So let's copy and paste what we already So let's copy and paste what we already sent and hopefully it returns back sent and hopefully it returns back sent and hopefully it returns back category something category technical category something category technical category something category technical perfect. So it was a and if we look at perfect. So it was a and if we look at perfect. So it was a and if we look at the logs we can see exactly what",
      "it's the logs we can see exactly what it's the logs we can see exactly what it's doing right we are using the the doing right we are using the the doing right we are using the the collection selector that first agent you collection selector that first agent you collection selector that first agent you could see the test that we're giving it could see the test that we're giving it could see the test that we're giving it we pass in that user query the one that we pass in that user query the one",
      "that we pass in that user query the one that we sent from the UI and then we get the we sent from the UI and then we get the we sent from the UI and then we get the final answers is exactly the structure final answers is exactly the structure final answers is exactly the structure that we were looking for so the UI could that we were looking for so the UI could that we were looking for so the UI could ingest it and render it nicely. All ingest it and render it nicely. All ingest it and render it",
      "nicely. All right. So now that we have the basic right. So now that we have the basic right. So now that we have the basic categorization agent in place, let's categorization agent in place, let's categorization agent in place, let's move on and enhance our pipeline. So move on and enhance our pipeline. So move on and enhance our pipeline. So let's just commit our let's just commit our let's just commit our changes and let's check out the second changes and let's check out the second changes and",
      "let's check out the second step branch. Perfect. Nothing broke. step branch. Perfect. Nothing broke. step branch. Perfect. Nothing broke. Great. Okay. So the next step here if we Great. Okay. So the next step here if we Great. Okay. So the next step here if we go to the uh the doc string is now to go to the uh the doc string is now to go to the uh the doc string is now to retrieve that data from that vector db. retrieve that data from that vector db. retrieve that data from that vector db. Right.",
      "Let's make it the only. So, we're Right. Let's make it the only. So, we're Right. Let's make it the only. So, we're going to do the same process. We're going to do the same process. We're going to do the same process. We're going to copy and paste the going to copy and paste the going to copy and paste the categorization LLM. We're going to categorization LLM. We're going to categorization LLM. We're going to create a new LM, and this is going to be create a new LM, and this is going to be create",
      "a new LM, and this is going to be the retriever LLM. It's going to necessitate more LLM. It's going to necessitate more LLM. It's going to necessitate more tokens. Like, I set it to a th00and, but tokens. Like, I set it to a th00and, but tokens. Like, I set it to a th00and, but everything else is going to stay the everything else is going to stay the everything else is going to stay the same. And then we're just going to grab same. And then we're just going to grab same. And then we're just going",
      "to grab two more two more two more uh an agent and another task. So, this uh an agent and another task. So, this uh an agent and another task. So, this is the retriever agent and the retriever is the retriever agent and the retriever is the retriever agent and the retriever task. I'm going to copy and paste this task. I'm going to copy and paste this task. I'm going to copy and paste this from our notes. from our notes. from our notes. And I'll explain exactly what they're And I'll explain",
      "exactly what they're And I'll explain exactly what they're doing. There is going to be a doing. There is going to be a doing. There is going to be a significant difference here and you'll significant difference here and you'll significant difference here and you'll see the error right away is that it's see the error right away is that it's see the error right away is that it's using a tool. And I'll explain exactly using a tool. And I'll explain exactly using a tool. And I'll explain exactly what",
      "we're doing there. So the retriever what we're doing there. So the retriever what we're doing there. So the retriever agent has a job, right? It's going to agent has a job, right? It's going to agent has a job, right? It's going to take that category that it's receiving take that category that it's receiving take that category that it's receiving from the uh categorization agent and from the uh categorization agent and from the uh categorization agent and it's going to it's going to pass it to a",
      "it's going to it's going to pass it to a it's going to it's going to pass it to a function that is going to query our uh function that is going to query our uh function that is going to query our uh vector DB. And so that function is going vector DB. And so that function is going vector DB. And so that function is going to be that tool. So let's create our to be that tool. So let's create our to be that tool. So let's create our first first first tool. We're going to name it the query tool. We're",
      "going to name it the query tool. We're going to name it the query collection tool. Let's define it. It's going to take what does it it. It's going to take what does it it. It's going to take what does it take? It's only taking the category and take? It's only taking the category and take? It's only taking the category and it's going to take the uh it's going to it's going to take the uh it's going to it's going to take the uh it's going to take the the query to embed. So query take the the query",
      "to embed. So query take the the query to embed. So query string and it's going to string and it's going to string and it's going to return a dictionary. Perfect. Let's add return a dictionary. Perfect. Let's add return a dictionary. Perfect. Let's add a doc string. This is going to be the a doc string. This is going to be the a doc string. This is going to be the tool query chroma tool query chroma tool query chroma uh based on uh based on uh based on category and return relevant documents.",
      "category and return relevant documents. category and return relevant documents. Now, if you ever worked with uh rag, if Now, if you ever worked with uh rag, if Now, if you ever worked with uh rag, if you ever worked with vector db, the the you ever worked with vector db, the the you ever worked with vector db, the the functionality of this is going to be functionality of this is going to be functionality of this is going to be very familiar, right? So, let me just very familiar, right? So, let me",
      "just very familiar, right? So, let me just copy and paste what that actual tool is copy and paste what that actual tool is copy and paste what that actual tool is going to going to going to do. Uh we're using the Watsonxi do. Uh we're using the Watsonxi do. Uh we're using the Watsonxi embeddings. Don't worry, like if you embeddings. Don't worry, like if you embeddings. Don't worry, like if you don't have that, you could use your own don't have that, you could use your own don't have that, you",
      "could use your own embeddings model if you have it locally. embeddings model if you have it locally. embeddings model if you have it locally. My my computer is not capable of it at My my computer is not capable of it at My my computer is not capable of it at the moment. Um the interesting thing the moment. Um the interesting thing the moment. Um the interesting thing here though is this part. We're grabbing here though is this part. We're grabbing here though is this part. We're grabbing the",
      "category that was returned by the the category that was returned by the the category that was returned by the categorization task and we're using that categorization task and we're using that categorization task and we're using that to query the data like query the vector to query the data like query the vector to query the data like query the vector DB which is fascinating because you're DB which is fascinating because you're DB which is fascinating because you're you have you're just saying",
      "this is what you have you're just saying this is what you have you're just saying this is what you do and the LM is doing it the agents you do and the LM is doing it the agents you do and the LM is doing it the agents are doing it. So that that is very very are doing it. So that that is very very are doing it. So that that is very very cool to me. So now that we have that we cool to me. So now that we have that we cool to me. So now that we have that we have that tool you can see what the have",
      "that tool you can see what the have that tool you can see what the retriever agent is doing and we give it retriever agent is doing and we give it retriever agent is doing and we give it the task you know we're passing in that the task you know we're passing in that the task you know we're passing in that query from the route. We have an query from the route. We have an query from the route. We have an expected output. We're not worried too expected output. We're not worried too expected output.",
      "We're not worried too much about this because we're not ever much about this because we're not ever much about this because we're not ever going to send back the context to the going to send back the context to the going to send back the context to the UI. So, we're not really enforcing that UI. So, we're not really enforcing that UI. So, we're not really enforcing that output output output JSON. But the only other thing I want to JSON. But the only other thing I want to JSON. But the only other",
      "thing I want to mention here is that we had to add a mention here is that we had to add a mention here is that we had to add a context and context is we're giving context and context is we're giving context and context is we're giving access to that categorization tasks like access to that categorization tasks like access to that categorization tasks like uh what its output was. So, it knew uh what its output was. So, it knew uh what its output was. So, it knew that's how we're getting that",
      "category that's how we're getting that category that's how we're getting that category and that's how we're telling the agent and that's how we're telling the agent and that's how we're telling the agent look at this category. you have the look at this category. you have the look at this category. you have the query, pass it, use this function, and query, pass it, use this function, and query, pass it, use this function, and then call it and return back the then call it and return back the then",
      "call it and return back the context. So for us now, all we're going context. So for us now, all we're going context. So for us now, all we're going to do is we're going to add the new to do is we're going to add the new to do is we're going to add the new agent to agent to agent to our crew. Welcome, and we're going to our crew. Welcome, and we're going to our crew. Welcome, and we're going to add the new add the new add the new task. Process is still going to be uh task. Process is still going",
      "to be uh task. Process is still going to be uh sequential. This time, let's just remove sequential. This time, let's just remove sequential. This time, let's just remove the category, like actually grabbing it, the category, like actually grabbing it, the category, like actually grabbing it, because we're not going to be returning because we're not going to be returning because we're not going to be returning that anymore. So we'll just say bye for that anymore. So we'll just say bye for that",
      "anymore. So we'll just say bye for now. But what we are going to do is now. But what we are going to do is now. But what we are going to do is we're going to print out that category result. result. result. Okay. Make sure everything comes up Okay. Make sure everything comes up Okay. Make sure everything comes up good. We're going to print out that good. We're going to print out that good. We're going to print out that category result and we're going to see category result and we're going to see",
      "category result and we're going to see exactly what happens when we send over exactly what happens when we send over exactly what happens when we send over that that uh request that query from that that uh request that query from that that uh request that query from here. here. here. Hopefully, we'll watch the new agent do Hopefully, we'll watch the new agent do Hopefully, we'll watch the new agent do exactly what we want it to do. Okay, exactly what we want it to do. Okay, exactly what we want",
      "it to do. Okay, let's get there. Okay, so okay, it let's get there. Okay, so okay, it let's get there. Okay, so okay, it already it already has the correct already it already has the correct already it already has the correct category. So now that it's going to take category. So now that it's going to take category. So now that it's going to take the collection uh look at that, we got the collection uh look at that, we got the collection uh look at that, we got the uh the the result from the rag.",
      "It the uh the the result from the rag. It the uh the the result from the rag. It used that category and it passed our used that category and it passed our used that category and it passed our Chromb collection. So we got that Chromb collection. So we got that Chromb collection. So we got that collection and then we queried it and collection and then we queried it and collection and then we queried it and now it returns back all the context for now it returns back all the context for now it",
      "returns back all the context for that query by basically by itself. We that query by basically by itself. We that query by basically by itself. We just told it. Yeah. And so we have you just told it. Yeah. And so we have you just told it. Yeah. And so we have you could see almost exactly what we're could see almost exactly what we're could see almost exactly what we're going to send to the final agent, right? going to send to the final agent, right? going to send to the final agent, right? We're",
      "going to send it the category We're going to send it the category We're going to send it the category because we want to return it to a UI. because we want to return it to a UI. because we want to return it to a UI. We're sending it the query. And now we We're sending it the query. And now we We're sending it the query. And now we have the context from our vector DB to have the context from our vector DB to have the context from our vector DB to augment retrieval augment and generate augment",
      "retrieval augment and generate augment retrieval augment and generate our our response. I don't know. I think our our response. I don't know. I think our our response. I don't know. I think that is particularly that is particularly that is particularly fascinating. So that's just a way that fascinating. So that's just a way that fascinating. So that's just a way that you could use tools with agents. And you could use tools with agents. And you could use tools with agents. And that was really just",
      "our retriever that was really just our retriever that was really just our retriever agent, right? Like we're able to use agent, right? Like we're able to use agent, right? Like we're able to use that function. And tool in this in this that function. And tool in this in this that function. And tool in this in this in this case means like we're using a in this case means like we're using a in this case means like we're using a function. We're giving an agent tools to function. We're giving an agent",
      "tools to function. We're giving an agent tools to use a function. And that is very very use a function. And that is very very use a function. And that is very very cool to make. So we're done with the uh cool to make. So we're done with the uh cool to make. So we're done with the uh retriever agent and we're going to move retriever agent and we're going to move retriever agent and we're going to move on to the generation agent. And this is on to the generation agent. And this is on to the",
      "generation agent. And this is the final step of the application. So the final step of the application. So the final step of the application. So let's just commit our changes and let's check out the final changes and let's check out the final changes and let's check out the final step. Let's go back to our API. And if we look Let's go back to our API. And if we look Let's go back to our API. And if we look back at our doc string, we know what the back at our doc string, we know what the back at",
      "our doc string, we know what the final step is. We're going to uh we're final step is. We're going to uh we're final step is. We're going to uh we're going to create an agent that creates a going to create an agent that creates a going to create an agent that creates a nice response for the nice response for the nice response for the user. Basically, everything that we just user. Basically, everything that we just user. Basically, everything that we just did, we're going to do one more time. did,",
      "we're going to do one more time. did, we're going to do one more time. And I really like this this And I really like this this And I really like this this uh this pattern, right? like of creating uh this pattern, right? like of creating uh this pattern, right? like of creating their own LLMs for each of the agents. their own LLMs for each of the agents. their own LLMs for each of the agents. I'd find that to be very u very nice I'd find that to be very u very nice I'd find that to be very u very",
      "nice because we could set different like for because we could set different like for because we could set different like for us we didn't really set any the only us we didn't really set any the only us we didn't really set any the only thing we're changing is the MAC tokens. thing we're changing is the MAC tokens. thing we're changing is the MAC tokens. Obviously, we want the response to have Obviously, we want the response to have Obviously, we want the response to have more leeway, but other",
      "than that, we're more leeway, but other than that, we're more leeway, but other than that, we're just like we could we could make it just like we could we could make it just like we could we could make it drastically different. Each LLM could be drastically different. Each LLM could be drastically different. Each LLM could be uh that we power could have a different uh that we power could have a different uh that we power could have a different model if we're using Watson Xi. We could model if",
      "we're using Watson Xi. We could model if we're using Watson Xi. We could use Mistral, we could use uh Llama, we use Mistral, we could use uh Llama, we use Mistral, we could use uh Llama, we could use whatever we want. So let's add could use whatever we want. So let's add could use whatever we want. So let's add the final the final to uh the task and the final the final to uh the task and the final the final to uh the task and the final agent which is going to be our the final agent which is going",
      "to be our the final agent which is going to be our generation generation generation agent. And once again we're going to be agent. And once again we're going to be agent. And once again we're going to be using a tool and I'll explain why in a using a tool and I'll explain why in a using a tool and I'll explain why in a second. So again we gave a role we gave second. So again we gave a role we gave second. So again we gave a role we gave a backstory. We uh we have a backstory. We uh we have a",
      "backstory. We uh we have a an LLM. Hold on. Let me just make sure a an LLM. Hold on. Let me just make sure a an LLM. Hold on. Let me just make sure I named it correctly. Oh yeah, there's a generation all not Oh yeah, there's a generation all not Oh yeah, there's a generation all not not the response. Let me just update not the response. Let me just update not the response. Let me just update that. that. that. Okay. Okay. But we have we're missing Okay. Okay. But we have we're missing Okay. Okay.",
      "But we have we're missing one last tool. And this tool, what I'm one last tool. And this tool, what I'm one last tool. And this tool, what I'm going to show you is how I found the going to show you is how I found the going to show you is how I found the prompt for this. So this tool is going prompt for this. So this tool is going prompt for this. So this tool is going to interpolate that query and that to interpolate that query and that to interpolate that query and that context into a nice",
      "prompt. And where I context into a nice prompt. And where I context into a nice prompt. And where I got the prompt is if you go to your got the prompt is if you go to your got the prompt is if you go to your projects, you can create this projects, you can create this projects, you can create this accelerator. just like look up Watson accelerator. just like look up Watson accelerator. just like look up Watson XAI rag and it'll give you this XAI rag and it'll give you this XAI rag and it'll give",
      "you this accelerator that you could just create accelerator that you could just create accelerator that you could just create and within there they have prompt and within there they have prompt and within there they have prompt templates written by the people who templates written by the people who templates written by the people who train the models you know or work with train the models you know or work with train the models you know or work with it a tremendous amount. So this prompt it a",
      "tremendous amount. So this prompt it a tremendous amount. So this prompt template is just I'm going to take this template is just I'm going to take this template is just I'm going to take this because they're they wrote it better because they're they wrote it better because they're they wrote it better than I'm really not a particularly good than I'm really not a particularly good than I'm really not a particularly good prompt engineer to be totally honest. So prompt engineer to be totally",
      "honest. So prompt engineer to be totally honest. So I just copy and paste this and I want to I just copy and paste this and I want to I just copy and paste this and I want to then interpolate the context that we then interpolate the context that we then interpolate the context that we received from the Chroma DB into the received from the Chroma DB into the received from the Chroma DB into the context and the question from the query. context and the question from the query. context and the",
      "question from the query. Right? So what we're going to do is Right? So what we're going to do is Right? So what we're going to do is we're going to create another tool and we're going to create another tool and we're going to create another tool and I'm just going to copy and paste the uh I'm just going to copy and paste the uh I'm just going to copy and paste the uh the tool and the process and the uh and the tool and the process and the uh and the tool and the process and the uh and the prompt.",
      "I'm going to give access to the prompt. I'm going to give access to the prompt. I'm going to give access to the generation agent. So this generation the generation agent. So this generation the generation agent. So this generation response tool the the generate response response tool the the generate response response tool the the generate response tool you can see exactly what I'm doing. tool you can see exactly what I'm doing. tool you can see exactly what I'm doing. It's grabbing the context.",
      "It's grabbing It's grabbing the context. It's grabbing It's grabbing the context. It's grabbing the query and it's sending it to uh this the query and it's sending it to uh this the query and it's sending it to uh this prompt. And finally, there's one last prompt. And finally, there's one last prompt. And finally, there's one last thing we have to do, which is create a thing we have to do, which is create a thing we have to do, which is create a pyantic model for the output because now pyantic",
      "model for the output because now pyantic model for the output because now we're sending back the entire thing to we're sending back the entire thing to we're sending back the entire thing to the UI. I really want to enforce that the UI. I really want to enforce that the UI. I really want to enforce that it's just going to be a JSON. I really it's just going to be a JSON. I really it's just going to be a JSON. I really want that category and I really want want that category and I really want want",
      "that category and I really want that I believe I call it response. I'll that I believe I call it response. I'll that I believe I call it response. I'll figure out let me let me look at what figure out let me let me look at what figure out let me let me look at what actually I call. But I need both of actually I call. But I need both of actually I call. But I need both of those to be there in order for it not those to be there in order for it not those to be there in order for it not to, you know,",
      "blow up on response. So, to, you know, blow up on response. So, to, you know, blow up on response. So, let me copy this let me copy this let me copy this model. Let's add it to the top over model. Let's add it to the top over model. Let's add it to the top over here. Okay. And this is going to be So let me just copy this model and pl be So let me just copy this model and pl be So let me just copy this model and pl paste it right here. This going to be paste it right here. This going to be paste",
      "it right here. This going to be the final response. This is going to be the final response. This is going to be the final response. This is going to be the JSON object that we're looking for the JSON object that we're looking for the JSON object that we're looking for that has a category field and has a that has a category field and has a that has a category field and has a response. And we're going to send response. And we're going to send response. And we're going to send this back straight",
      "through directly to this back straight through directly to this back straight through directly to our UI. So that's why we have this uh in our UI. So that's why we have this uh in our UI. So that's why we have this uh in the generation task. That's why we have the generation task. That's why we have the generation task. That's why we have we're trying to say, okay, this is what we're trying to say, okay, this is what we're trying to say, okay, this is what we want. This is what we want it to look",
      "we want. This is what we want it to look we want. This is what we want it to look like and this is what you need to like and this is what you need to like and this is what you need to return. return. return. So, let's add our final agent to So, let's add our final agent to So, let's add our final agent to the to the the to the the to the crew. Let's give him his final task. Okay, we have the crew kickoff. task. Okay, we have the crew kickoff. task. Okay, we have the crew kickoff. Let's just call",
      "this crew result now Let's just call this crew result now Let's just call this crew result now because we no longer need to send back because we no longer need to send back because we no longer need to send back that that that hard-coded response. Get rid of this. hard-coded response. Get rid of this. hard-coded response. Get rid of this. We're not sending this. Okay. Make sure nothing Okay. Make sure nothing Okay. Make sure nothing broke. broke. broke. Perfect. And let's see if we get a nice",
      "response. Okay. All right. It grabbed response. Okay. All right. It grabbed response. Okay. All right. It grabbed the correct category. Okay. It sent that the correct category. Okay. It sent that the correct category. Okay. It sent that category to the retriever who she category to the retriever who she category to the retriever who she returned back all of the context from returned back all of the context from returned back all of the context from the rag uh from the vector the rag uh from the",
      "vector the rag uh from the vector DB. Now she's going to send it to the DB. Now she's going to send it to the DB. Now she's going to send it to the final agent who's going to interpolate final agent who's going to interpolate final agent who's going to interpolate that into that prompt we we uh cribed that into that prompt we we uh cribed that into that prompt we we uh cribed notes from from Watson Studio. Let me notes from from Watson Studio. Let me notes from from Watson Studio. Let me it. Come",
      "it. Come it. Come on. Perfect. Okay. Yeah. So you see it on. Perfect. Okay. Yeah. So you see it on. Perfect. Okay. Yeah. So you see it the response has Okay. We have we have the response has Okay. We have we have the response has Okay. We have we have everything set up. It has the context everything set up. It has the context everything set up. It has the context Look at that. Huh? It worked. I'm not Look at that. Huh? It worked. I'm not Look at that. Huh? It worked. I'm not surprised. It worked",
      "before. I I've surprised. It worked before. I I've surprised. It worked before. I I've built this. But still, it's always kind built this. But still, it's always kind built this. But still, it's always kind of surprising. It's an amazing it's an of surprising. It's an amazing it's an of surprising. It's an amazing it's an amazing technology. So, you see we have amazing technology. So, you see we have amazing technology. So, you see we have the this rag response. Let's actually the this rag",
      "response. Let's actually the this rag response. Let's actually double check to make sure everything double check to make sure everything double check to make sure everything looks right. So, we have uh it's it's looks right. So, we have uh it's it's looks right. So, we have uh it's it's referencing error 01. So, let's look at referencing error 01. So, let's look at referencing error 01. So, let's look at our docs and let's make sure that we our docs and let's make sure that we our docs and let's",
      "make sure that we have the correct stuff. Error1 session have the correct stuff. Error1 session have the correct stuff. Error1 session expired. Clear your browser. Let's see expired. Clear your browser. Let's see expired. Clear your browser. Let's see what that says. Perfect. Yeah. So, it's what that says. Perfect. Yeah. So, it's what that says. Perfect. Yeah. So, it's it it worked exactly the way we wanted it it worked exactly the way we wanted it it worked exactly the way we wanted to.",
      "Something I really like is it's able to. Something I really like is it's able to. Something I really like is it's able to uh give me back like different steps to uh give me back like different steps to uh give me back like different steps like responses in the in the pipeline. like responses in the in the pipeline. like responses in the in the pipeline. I'm able to pass them along during the I'm able to pass them along during the I'm able to pass them along during the agent. So, I'm able to",
      "categorize the agent. So, I'm able to categorize the agent. So, I'm able to categorize the query. I'm able to show a really really query. I'm able to show a really really query. I'm able to show a really really nice message uh and a good accurate nice message uh and a good accurate nice message uh and a good accurate response. And it's all done with these response. And it's all done with these response. And it's all done with these agents. I think it's very cool. Um yeah. agents. I think it's",
      "very cool. Um yeah. agents. I think it's very cool. Um yeah. And so, obviously, we could we could And so, obviously, we could we could And so, obviously, we could we could enhance this. We could refactor it. We enhance this. We could refactor it. We enhance this. We could refactor it. We could change the parameters that we're could change the parameters that we're could change the parameters that we're using for the LLM to make it do using for the LLM to make it do using for the LLM to make it do",
      "different things. We could use a totally different things. We could use a totally different things. We could use a totally different LLM, totally different models different LLM, totally different models different LLM, totally different models to have whatever we want. What I really to have whatever we want. What I really to have whatever we want. What I really wanted to do, the two new agents that I wanted to do, the two new agents that I wanted to do, the two new agents that I really want to",
      "make, I'm probably going really want to make, I'm probably going really want to make, I'm probably going to do it later, is to route queries uh to do it later, is to route queries uh to do it later, is to route queries uh to the web, if it's not part of the to the web, if it's not part of the to the web, if it's not part of the Chromb collections, if it's able to Chromb collections, if it's able to Chromb collections, if it's able to categorize and say, \"Okay, this is out categorize and say,",
      "\"Okay, this is out categorize and say, \"Okay, this is out of the blue.\" Um, and also I want to of the blue.\" Um, and also I want to of the blue.\" Um, and also I want to really I want to I want to format that really I want to I want to format that really I want to I want to format that response when we get it back to the uh response when we get it back to the uh response when we get it back to the uh to the UI to maybe maybe format it to the UI to maybe maybe format it to the UI to maybe maybe",
      "format it within HTML and have another agent do within HTML and have another agent do within HTML and have another agent do that, right? Like look at this and put that, right? Like look at this and put that, right? Like look at this and put this into a nice HTML package and post this into a nice HTML package and post this into a nice HTML package and post it on uh as a response. Awesome. We've it on uh as a response. Awesome. We've it on uh as a response. Awesome. We've built a pretty",
      "sophisticated multi-agent built a pretty sophisticated multi-agent built a pretty sophisticated multi-agent pipeline here. So let's just recap. We pipeline here. So let's just recap. We pipeline here. So let's just recap. We built the back end to an agentic rag built the back end to an agentic rag built the back end to an agentic rag chatbot that is able to identify the chatbot that is able to identify the chatbot that is able to identify the queries category, target the correct queries category,",
      "target the correct queries category, target the correct chromabb collection and interpolate the chromabb collection and interpolate the chromabb collection and interpolate the query in the context into a custom query in the context into a custom query in the context into a custom prompt and generate a natural language prompt and generate a natural language prompt and generate a natural language response. So with this application and response. So with this application and response. So with this",
      "application and this process, we would love for you to this process, we would love for you to this process, we would love for you to explore additional use cases, customize explore additional use cases, customize explore additional use cases, customize the UI and experiment with the Crew AI the UI and experiment with the Crew AI the UI and experiment with the Crew AI framework and build something really framework and build something really framework and build something really cool. Maybe add a",
      "route that makes a web cool. Maybe add a route that makes a web cool. Maybe add a route that makes a web search if the query is just totally out search if the query is just totally out search if the query is just totally out of bounds. Maybe create an agent whose of bounds. Maybe create an agent whose of bounds. Maybe create an agent whose only job it is is to format the response only job it is is to format the response only job it is is to format the response in a particular way. We would love",
      "to in a particular way. We would love to in a particular way. We would love to see anything you do with it. Dive into see anything you do with it. Dive into see anything you do with it. Dive into the code, have fun, build something the code, have fun, build something the code, have fun, build something cool, refactor it, make it better. Just cool, refactor it, make it better. Just cool, refactor it, make it better. Just be creative."
    ],
    "chunk_count": 172,
    "content_id": "df5d7ed8-005b-4cf4-a641-ee2db1714ea1",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554835"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=y7sXDpffzQQ": {
    "title": "What is a Knowledge Graph?",
    "url": "https://www.youtube.com/watch?v=y7sXDpffzQQ",
    "description": "Learn more about Knowledge Graph → http://ibm.biz/knowledge-graph-guide\nWatch \"What is Natural Language Processing?\" lightboard video → https://youtu.be/fLvJ8VdHLA0\nBuild a domain specific Knowledge Graph → http://ibm.biz/build-knowledge-graph\nCheck out IBM Watson Discovery → http://ibm.biz/prod-watson-discovery\n\nKnowledge graphs represent a network of real-world entities, such as people, places, and things in the world, and illustrates the relationship between them. \n\nIn this lightboard video, Martin Keen with IBM visually explains the fundamentals of knowledge graphs and then demonstrates how they can turn your data into machine understandable language. \n\nGet started on IBM Cloud at no cost → http://ibm.biz/BdftGB\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#KnowledgeGraph #NaturalLanguageProcessing #DataIntegration",
    "duration": 336,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en perhaps you're unfamiliar with the term perhaps you're unfamiliar with the term perhaps you're unfamiliar with the term of knowledge graph of knowledge graph of knowledge graph i suspect you have benefited from one i suspect you have benefited from one i suspect you have benefited from one maybe even today maybe even today maybe even today take your favorite virtual assistant did take your favorite virtual assistant did take your favorite virtual assistant did you know that when you ask a question you know that when you ask a question you know that when you ask a question like what is the capital of canada like what is the capital of canada like what is the capital of canada the assistant is pulling the response the assistant is pulling the response the assistant is pulling the response ottawa in this case from information in ottawa in this case from information in ottawa in this case from information in a knowledge graph knowledge graphs can a knowledge graph knowledge graphs can a knowledge graph knowledge graphs can be seen as a way of representing be seen as a way of representing be seen as a way of representing semantic information between two semantic information between two semantic information between two entities and what's really cool about entities and what's really cool about entities and what's really cool about them is that modern applications allow them is that modern applications allow them is that modern applications allow almost any entity you could imagine to almost any entity you could imagine to almost any entity you could imagine to be described within one for example we be described within one for example we be described within one for example we could have a knowledge graph of movies could have a knowledge graph of movies could have a knowledge graph of movies and actors or we can describe and actors or we can describe and actors or we can describe ingredients for recipes as well as the ingredients for recipes as well as the ingredients for recipes as well as the steps required to cook them and this steps required to cook them and this steps required to cook them and this means machines are able to understand means machines are able to understand means machines are able to understand how these entities relate to each other how these entities relate to each other how these entities relate to each other along with their shared attributes and along with their shared attributes and along with their shared attributes and this allows us to draw connections this allows us to draw connections this allows us to draw connections between different things in the world between different things in the world between different things in the world around us now a knowledge graph is made around us now a knowledge graph is made around us now a knowledge graph is made up of nodes connected by edges connected by edges connected by edges nodes describe any object or person or nodes describe any object or person or nodes describe any object or person or place and an edge defines the place and an edge defines the place and an edge defines the relationship between the nodes relationship between the nodes relationship between the nodes so for example a node of ottawa and a node and a node and a node of canada might be connected by the edge might be connected by the edge might be connected by the edge of capital and the thing here is that the pair of and the thing here is that the pair of and the thing here is that the pair of nodes they can be connected by more than nodes they can be connected by more than nodes they can be connected by more than one relation if the two are related in one relation if the two are related in one relation if the two are related in multiple ways so for example let's take multiple ways so for example let's take multiple ways so for example let's take another city let's take paris now paris is the capital of is the capital of is the capital of france but it is also part of the roman empire but it is also part of the roman empire but it is also part of the roman empire or it did used to be so in this case the edges are paris to so in this case the edges are paris to so in this case the edges are paris to france is capital france is capital france is capital and then and then and then paris to roman empire paris to roman empire paris to roman empire city of city of city of and we can see then how that these nodes and we can see then how that these nodes and we can see then how that these nodes can be connected with multiple edges as can be connected with multiple edges as can be connected with multiple edges as we expand this we expand this we expand this and knowledge graphs can sort of build and knowledge graphs can sort of build and knowledge graphs can sort of build different data sources and bind them different data sources and bind them different data sources and bind them together to infer missing facts so let's together to infer missing facts so let's together to infer missing facts so let's say you're trying to predict the number say you're trying to predict the number say you're trying to predict the number of chinese restaurants in new york city of chinese restaurants in new york city of chinese restaurants in new york city you could use one data source let's say you could use one data source let's say you could use one data source let's say census data but that might not tell you census data but that might not tell you census data but that might not tell you the whole story it might be out of date the whole story it might be out of date the whole story it might be out of date it might not classify everything it might not classify everything it might not classify everything correctly and so forth so if we add a correctly and so forth so if we add a correctly and so forth so if we add a second data source like say online second data source like say online second data source like say online reviews about all the different reviews about all the different reviews about all the different restaurants and put them all in a restaurants and put them all in a restaurants and put them all in a knowledge graph then we can use knowledge graph then we can use knowledge graph then we can use statistical methods to infer that statistical methods to infer that statistical methods to infer that actually there are 2 900 restaurants actually there are 2 900 restaurants actually there are 2 900 restaurants serving chinese food in new york city serving chinese food in new york city serving chinese food in new york city which may be a lot different than what which may be a lot different than what which may be a lot different than what was reported in the census data alone was reported in the census data alone was reported in the census data alone now knowledge graphs utilize something now knowledge graphs utilize something now knowledge graphs utilize something called natural called natural called natural language language language processing abbreviated to nlp abbreviated to nlp abbreviated to nlp to construct a view of nodes and edges to construct a view of nodes and edges to construct a view of nodes and edges through a process called semantic through a process called semantic through a process called semantic enrichment i can take some unstructured enrichment i can take some unstructured enrichment i can take some unstructured text say a white paper and classify that text say a white paper and classify that text say a white paper and classify that text using natural language processing text using natural language processing text using natural language processing to really sort of create data sets which to really sort of create data sets which to really sort of create data sets which are correlated and are correlated and are correlated and related to that information related to that information related to that information and what that builds me is a knowledge and what that builds me is a knowledge and what that builds me is a knowledge graph graph graph and beyond sort of helping with question and beyond sort of helping with question and beyond sort of helping with question and answer queries there are a lot of and answer queries there are a lot of and answer queries there are a lot of other commercial applications for other commercial applications for other commercial applications for knowledge graphs so for example the knowledge graphs so for example the knowledge graphs so for example the recommended videos that are probably recommended videos that are probably recommended videos that are probably appearing alongside this one in youtube appearing alongside this one in youtube appearing alongside this one in youtube right now well they leverage a knowledge right now well they leverage a knowledge right now well they leverage a knowledge graph based on queries people are graph based on queries people are graph based on queries people are searching for and other videos that they searching for and other videos that they searching for and other videos that they might enjoy might enjoy might enjoy in insurance you can use knowledge in insurance you can use knowledge in insurance you can use knowledge graphs to sort of and make sure that a graphs to sort of and make sure that a graphs to sort of and make sure that a given claim for damage is actually a given claim for damage is actually a given claim for damage is actually a true claim or whether it's been one true claim or whether it's been one true claim or whether it's been one that's reported by a policy holder for that's reported by a policy holder for that's reported by a policy holder for fraud and in retail knowledge graphs can fraud and in retail knowledge graphs can fraud and in retail knowledge graphs can assist understanding the relationship assist understanding the relationship assist understanding the relationship between products so that companies can between products so that companies can between products so that companies can recommend different pairings that might recommend different pairings that might recommend different pairings that might be of interest to customers be of interest to customers be of interest to customers and i'll leave you here with some wisdom and i'll leave you here with some wisdom and i'll leave you here with some wisdom that i was acutely aware of last night that i was acutely aware of last night that i was acutely aware of last night and i'll share it in knowledge graph and i'll share it in knowledge graph and i'll share it in knowledge graph form it consists of three nodes there's form it consists of three nodes there's form it consists of three nodes there's human then there is coffee and then there is sleep and then there is sleep and then there is sleep and these nodes are connected of course and these nodes are connected of course and these nodes are connected of course by edges by edges by edges the edge between human the edge between human the edge between human and coffee is consume the edge between the edge between the edge between human and sleep is needs and what i learned last night the edge and what i learned last night the edge and what i learned last night the edge between coffee and sleep between coffee and sleep between coffee and sleep is prevents avoid caffeine after 5 pm folks avoid caffeine after 5 pm folks avoid caffeine after 5 pm folks if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching",
    "chunks": [
      "Kind: captions Language: en perhaps you're unfamiliar with the term perhaps you're unfamiliar with the term perhaps you're unfamiliar with the term of knowledge graph of knowledge graph of knowledge graph i suspect you have benefited from one i suspect you have benefited from one i suspect you have benefited from one maybe even today maybe even today maybe even today take your favorite virtual assistant did take your favorite virtual assistant did take your favorite virtual assistant did you",
      "know that when you ask a question you know that when you ask a question you know that when you ask a question like what is the capital of canada like what is the capital of canada like what is the capital of canada the assistant is pulling the response the assistant is pulling the response the assistant is pulling the response ottawa in this case from information in ottawa in this case from information in ottawa in this case from information in a knowledge graph knowledge graphs can a knowledge",
      "graph knowledge graphs can a knowledge graph knowledge graphs can be seen as a way of representing be seen as a way of representing be seen as a way of representing semantic information between two semantic information between two semantic information between two entities and what's really cool about entities and what's really cool about entities and what's really cool about them is that modern applications allow them is that modern applications allow them is that modern applications allow almost",
      "any entity you could imagine to almost any entity you could imagine to almost any entity you could imagine to be described within one for example we be described within one for example we be described within one for example we could have a knowledge graph of movies could have a knowledge graph of movies could have a knowledge graph of movies and actors or we can describe and actors or we can describe and actors or we can describe ingredients for recipes as well as the ingredients for recipes as",
      "well as the ingredients for recipes as well as the steps required to cook them and this steps required to cook them and this steps required to cook them and this means machines are able to understand means machines are able to understand means machines are able to understand how these entities relate to each other how these entities relate to each other how these entities relate to each other along with their shared attributes and along with their shared attributes and along with their shared",
      "attributes and this allows us to draw connections this allows us to draw connections this allows us to draw connections between different things in the world between different things in the world between different things in the world around us now a knowledge graph is made around us now a knowledge graph is made around us now a knowledge graph is made up of nodes connected by edges connected by edges connected by edges nodes describe any object or person or nodes describe any object or person or",
      "nodes describe any object or person or place and an edge defines the place and an edge defines the place and an edge defines the relationship between the nodes relationship between the nodes relationship between the nodes so for example a node of ottawa and a node and a node and a node of canada might be connected by the edge might be connected by the edge might be connected by the edge of capital and the thing here is that the pair of and the thing here is that the pair of and the thing here is",
      "that the pair of nodes they can be connected by more than nodes they can be connected by more than nodes they can be connected by more than one relation if the two are related in one relation if the two are related in one relation if the two are related in multiple ways so for example let's take multiple ways so for example let's take multiple ways so for example let's take another city let's take paris now paris is the capital of is the capital of is the capital of france but it is also part of",
      "the roman empire but it is also part of the roman empire but it is also part of the roman empire or it did used to be so in this case the edges are paris to so in this case the edges are paris to so in this case the edges are paris to france is capital france is capital france is capital and then and then and then paris to roman empire paris to roman empire paris to roman empire city of city of city of and we can see then how that these nodes and we can see then how that these nodes and we can",
      "see then how that these nodes can be connected with multiple edges as can be connected with multiple edges as can be connected with multiple edges as we expand this we expand this we expand this and knowledge graphs can sort of build and knowledge graphs can sort of build and knowledge graphs can sort of build different data sources and bind them different data sources and bind them different data sources and bind them together to infer missing facts so let's together to infer missing facts so",
      "let's together to infer missing facts so let's say you're trying to predict the number say you're trying to predict the number say you're trying to predict the number of chinese restaurants in new york city of chinese restaurants in new york city of chinese restaurants in new york city you could use one data source let's say you could use one data source let's say you could use one data source let's say census data but that might not tell you census data but that might not tell you census data",
      "but that might not tell you the whole story it might be out of date the whole story it might be out of date the whole story it might be out of date it might not classify everything it might not classify everything it might not classify everything correctly and so forth so if we add a correctly and so forth so if we add a correctly and so forth so if we add a second data source like say online second data source like say online second data source like say online reviews about all the different",
      "reviews about all the different reviews about all the different restaurants and put them all in a restaurants and put them all in a restaurants and put them all in a knowledge graph then we can use knowledge graph then we can use knowledge graph then we can use statistical methods to infer that statistical methods to infer that statistical methods to infer that actually there are 2 900 restaurants actually there are 2 900 restaurants actually there are 2 900 restaurants serving chinese food in",
      "new york city serving chinese food in new york city serving chinese food in new york city which may be a lot different than what which may be a lot different than what which may be a lot different than what was reported in the census data alone was reported in the census data alone was reported in the census data alone now knowledge graphs utilize something now knowledge graphs utilize something now knowledge graphs utilize something called natural called natural called natural language language",
      "language processing abbreviated to nlp abbreviated to nlp abbreviated to nlp to construct a view of nodes and edges to construct a view of nodes and edges to construct a view of nodes and edges through a process called semantic through a process called semantic through a process called semantic enrichment i can take some unstructured enrichment i can take some unstructured enrichment i can take some unstructured text say a white paper and classify that text say a white paper and classify that",
      "text say a white paper and classify that text using natural language processing text using natural language processing text using natural language processing to really sort of create data sets which to really sort of create data sets which to really sort of create data sets which are correlated and are correlated and are correlated and related to that information related to that information related to that information and what that builds me is a knowledge and what that builds me is a knowledge",
      "and what that builds me is a knowledge graph graph graph and beyond sort of helping with question and beyond sort of helping with question and beyond sort of helping with question and answer queries there are a lot of and answer queries there are a lot of and answer queries there are a lot of other commercial applications for other commercial applications for other commercial applications for knowledge graphs so for example the knowledge graphs so for example the knowledge graphs so for example",
      "the recommended videos that are probably recommended videos that are probably recommended videos that are probably appearing alongside this one in youtube appearing alongside this one in youtube appearing alongside this one in youtube right now well they leverage a knowledge right now well they leverage a knowledge right now well they leverage a knowledge graph based on queries people are graph based on queries people are graph based on queries people are searching for and other videos that they",
      "searching for and other videos that they searching for and other videos that they might enjoy might enjoy might enjoy in insurance you can use knowledge in insurance you can use knowledge in insurance you can use knowledge graphs to sort of and make sure that a graphs to sort of and make sure that a graphs to sort of and make sure that a given claim for damage is actually a given claim for damage is actually a given claim for damage is actually a true claim or whether it's been one true claim or",
      "whether it's been one true claim or whether it's been one that's reported by a policy holder for that's reported by a policy holder for that's reported by a policy holder for fraud and in retail knowledge graphs can fraud and in retail knowledge graphs can fraud and in retail knowledge graphs can assist understanding the relationship assist understanding the relationship assist understanding the relationship between products so that companies can between products so that companies can between",
      "products so that companies can recommend different pairings that might recommend different pairings that might recommend different pairings that might be of interest to customers be of interest to customers be of interest to customers and i'll leave you here with some wisdom and i'll leave you here with some wisdom and i'll leave you here with some wisdom that i was acutely aware of last night that i was acutely aware of last night that i was acutely aware of last night and i'll share it in",
      "knowledge graph and i'll share it in knowledge graph and i'll share it in knowledge graph form it consists of three nodes there's form it consists of three nodes there's form it consists of three nodes there's human then there is coffee and then there is sleep and then there is sleep and then there is sleep and these nodes are connected of course and these nodes are connected of course and these nodes are connected of course by edges by edges by edges the edge between human the edge between human",
      "the edge between human and coffee is consume the edge between the edge between the edge between human and sleep is needs and what i learned last night the edge and what i learned last night the edge and what i learned last night the edge between coffee and sleep between coffee and sleep between coffee and sleep is prevents avoid caffeine after 5 pm folks avoid caffeine after 5 pm folks avoid caffeine after 5 pm folks if you have any questions please drop us if you have any questions please drop",
      "us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching"
    ],
    "chunk_count": 24,
    "content_id": "b354d07f-312e-4983-92dc-01162b98e774",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554838"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=Za7aG-ooGLQ": {
    "title": "GraphRAG Explained: AI Retrieval with Knowledge Graphs & Cypher",
    "url": "https://www.youtube.com/watch?v=Za7aG-ooGLQ",
    "description": "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdngMV\n\n🚀 Try GraphRAG now! Access the code here → https://ibm.biz/BdngaC\n\nLearn more about GraphRAG here → https://ibm.biz/BdngM9\n\n🤖 Can AI turn text into structured knowledge? Discover how GraphRAG leverages knowledge graphs, graph databases, and Cypher queries to transform unstructured data into actionable insights. See how LLMs enable intelligent retrieval and automation, reshaping workflows across industries. 🚀\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdngMU\n\n#knowledgegraph #cypher #ai",
    "duration": 866,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Today I'm going to show you how to Today I'm going to show you how to Today I'm going to show you how to populate a knowledge graph and query it populate a knowledge graph and query it populate a knowledge graph and query it using an LLM. Graph retrieval augmented using an LLM. Graph retrieval augmented using an LLM. Graph retrieval augmented generation or graph rag is emerging as a generation or graph rag is emerging as a generation or graph rag is emerging as a powerful alternative to vector search powerful alternative to vector search powerful alternative to vector search methods. Instead of using a vector methods. Instead of using a vector methods. Instead of using a vector database, graph rag systems store data database, graph rag systems store data database, graph rag systems store data in the format of a knowledge graph using in the format of a knowledge graph using in the format of a knowledge graph using a graph database. In a knowledge graph, a graph database. In a knowledge graph, a graph database. In a knowledge graph, the relationships between data points the relationships between data points the relationships between data points called edges are as meaningful as the called edges are as meaningful as the called edges are as meaningful as the connections between data points called connections between data points called connections between data points called vertices or sometimes nodes. A graph rag vertices or sometimes nodes. A graph rag vertices or sometimes nodes. A graph rag approach leverages the structured nature approach leverages the structured nature approach leverages the structured nature of graph databases to give greater depth of graph databases to give greater depth of graph databases to give greater depth and context of retrieved information and context of retrieved information and context of retrieved information about networks or complex relationships. The first step in setting relationships. The first step in setting relationships. The first step in setting up our system is creating and populating up our system is creating and populating up our system is creating and populating the knowledge graph. We'll be using an the knowledge graph. We'll be using an the knowledge graph. We'll be using an LLM to assist in creating the knowledge LLM to assist in creating the knowledge LLM to assist in creating the knowledge graph. Given unstructured text data, the graph. Given unstructured text data, the graph. Given unstructured text data, the LLM will extract entities and LLM will extract entities and LLM will extract entities and relationships from the data, relationships from the data, relationships from the data, transforming the data into structured transforming the data into structured transforming the data into structured data, which can then be inserted into data, which can then be inserted into data, which can then be inserted into the knowledge the knowledge the knowledge graph. After the knowledge graph is graph. After the knowledge graph is graph. After the knowledge graph is created, we'll be using the LLM to query created, we'll be using the LLM to query created, we'll be using the LLM to query data from the knowledge graph and return data from the knowledge graph and return data from the knowledge graph and return the response in natural language. Cipher the response in natural language. Cipher the response in natural language. Cipher is the query language for a graph is the query language for a graph is the query language for a graph database. When a user asks a question in database. When a user asks a question in database. When a user asks a question in natural language, the LLM will generate natural language, the LLM will generate natural language, the LLM will generate the cipher query to extract that the cipher query to extract that the cipher query to extract that information from the knowledge graph. information from the knowledge graph. information from the knowledge graph. The cipher query then gets executed on The cipher query then gets executed on The cipher query then gets executed on the database and the results are the database and the results are the database and the results are returned to the LLM. The last step is returned to the LLM. The last step is returned to the LLM. The last step is for the LLM to interpret the results of for the LLM to interpret the results of for the LLM to interpret the results of the cipher query in the context of the the cipher query in the context of the the cipher query in the context of the natural language question and return a natural language question and return a natural language question and return a natural language response. For this natural language response. For this natural language response. For this example, we'll need an API key and example, we'll need an API key and example, we'll need an API key and project ID. The link to this notebook is project ID. The link to this notebook is project ID. The link to this notebook is in the description below. In the in the description below. In the in the description below. In the notebook, you'll find instructions for notebook, you'll find instructions for notebook, you'll find instructions for retrieving these credentials. We'll be using Neo4j, an open-source We'll be using Neo4j, an open-source We'll be using Neo4j, an open-source graph database, but any graph database graph database, but any graph database graph database, but any graph database can be used to create the knowledge can be used to create the knowledge can be used to create the knowledge graph. We'll create a local instance of graph. We'll create a local instance of graph. We'll create a local instance of the database using a containerization the database using a containerization the database using a containerization tool. I'll be using Podman, but you can tool. I'll be using Podman, but you can tool. I'll be using Podman, but you can use any containerization tool, for use any containerization tool, for use any containerization tool, for example, Docker, as long as it allows example, Docker, as long as it allows example, Docker, as long as it allows you to create a Neo4j instance. If you you to create a Neo4j instance. If you you to create a Neo4j instance. If you don't have a containerization tool don't have a containerization tool don't have a containerization tool already, take a moment to install already, take a moment to install already, take a moment to install one. one. one. After installing, initialize and start a After installing, initialize and start a After installing, initialize and start a machine. My machine's already machine. My machine's already machine. My machine's already initialized, so I'm just starting it here. Once you have this running, you here. Once you have this running, you here. Once you have this running, you can start a database instance with this can start a database instance with this can start a database instance with this configuration. We need credentials to configuration. We need credentials to configuration. We need credentials to access the database. So, I'm setting a access the database. So, I'm setting a access the database. So, I'm setting a name and password here. We also need to name and password here. We also need to name and password here. We also need to include the Apoch library as a plugin in include the Apoch library as a plugin in include the Apoch library as a plugin in order to enable additional functionality order to enable additional functionality order to enable additional functionality for working with data and for working with data and for working with data and graphs. It looks like our graph database graphs. It looks like our graph database graphs. It looks like our graph database is up and running. Now, it's a good practice to create a Now, it's a good practice to create a Now, it's a good practice to create a fresh virtual environment for this fresh virtual environment for this fresh virtual environment for this project. I'm using Python project. I'm using Python project. I'm using Python 3.11.3 here. In the Python environment 3.11.3 here. In the Python environment 3.11.3 here. In the Python environment for your notebook, install the following for your notebook, install the following for your notebook, install the following Python Python Python libraries. We'll be using the OS and get libraries. We'll be using the OS and get libraries. We'll be using the OS and get pass modules to set up credentials. pass modules to set up credentials. pass modules to set up credentials. We'll use Langchain's document class to We'll use Langchain's document class to We'll use Langchain's document class to store the text for input into our graph store the text for input into our graph store the text for input into our graph database and the LLM graph transformer database and the LLM graph transformer database and the LLM graph transformer to create a graph from our text input. to create a graph from our text input. to create a graph from our text input. To interact with and query the graph To interact with and query the graph To interact with and query the graph database, we'll use the langchain neo4j database, we'll use the langchain neo4j database, we'll use the langchain neo4j module and its accompanying graph cipher module and its accompanying graph cipher module and its accompanying graph cipher QA chain class. To craft our prompts for QA chain class. To craft our prompts for QA chain class. To craft our prompts for the LLM, we'll use Langchain's prompt the LLM, we'll use Langchain's prompt the LLM, we'll use Langchain's prompt template and fshot prompt template. template and fshot prompt template. template and fshot prompt template. We'll use the Langchain IBM and IBM We'll use the Langchain IBM and IBM We'll use the Langchain IBM and IBM Watsonx.ai modules to interact with the Watsonx.ai modules to interact with the Watsonx.ai modules to interact with the LLM and to set the parameters for our LLM and to set the parameters for our LLM and to set the parameters for our models. We'll need to set up our credentials We'll need to set up our credentials We'll need to set up our credentials using the API key and project ID that we using the API key and project ID that we using the API key and project ID that we retrieved earlier. We'll also need to retrieved earlier. We'll also need to retrieved earlier. We'll also need to set the URL from which we'll access set the URL from which we'll access set the URL from which we'll access these services. Now that our environment these services. Now that our environment these services. Now that our environment set up, we can create the knowledge set up, we can create the knowledge set up, we can create the knowledge graph. First, we need to create a graph. First, we need to create a graph. First, we need to create a connection to the local database connection to the local database connection to the local database instance that we started instance that we started instance that we started earlier. Next, we define our data for earlier. Next, we define our data for earlier. Next, we define our data for input into the knowledge graph. In this input into the knowledge graph. In this input into the knowledge graph. In this case, the text describes employees at a case, the text describes employees at a case, the text describes employees at a company, groups they work in, and their company, groups they work in, and their company, groups they work in, and their job titles. We'll use this set of job titles. We'll use this set of job titles. We'll use this set of relationships to test the graph relationships to test the graph relationships to test the graph generating capabilities of the LLM. But generating capabilities of the LLM. But generating capabilities of the LLM. But you don't have to limit your data to you don't have to limit your data to you don't have to limit your data to straightforward examples of relationship straightforward examples of relationship straightforward examples of relationship data. Graph rank systems have been shown data. Graph rank systems have been shown data. Graph rank systems have been shown to be successful in retrieval and to be successful in retrieval and to be successful in retrieval and summarization tasks for far more complex summarization tasks for far more complex summarization tasks for far more complex narrative and connected data. Now we'll configure our Now we'll configure our Now we'll configure our LLM which will generate text describing LLM which will generate text describing LLM which will generate text describing the graph. The LLM temperature should be the graph. The LLM temperature should be the graph. The LLM temperature should be fairly low and the number of tokens high fairly low and the number of tokens high fairly low and the number of tokens high to encourage the model to generate as to encourage the model to generate as to encourage the model to generate as much detail as possible without much detail as possible without much detail as possible without hallucinating entities or relationships hallucinating entities or relationships hallucinating entities or relationships that aren't present. One of the most powerful LLM use cases One of the most powerful LLM use cases One of the most powerful LLM use cases is transforming unstructured text data is transforming unstructured text data is transforming unstructured text data into structured data. The LLM will into structured data. The LLM will into structured data. The LLM will transform our text input string into a transform our text input string into a transform our text input string into a structure of nodes and relationships structure of nodes and relationships structure of nodes and relationships that we can use to populate the that we can use to populate the that we can use to populate the knowledge graph. knowledge graph. knowledge graph. The LLM graph transformer allows you to The LLM graph transformer allows you to The LLM graph transformer allows you to set the kinds of nodes and relationships set the kinds of nodes and relationships set the kinds of nodes and relationships you'd like the LLM to generate. you'd like the LLM to generate. you'd like the LLM to generate. Restricting the LLM to just those Restricting the LLM to just those Restricting the LLM to just those entities makes it more likely that entities makes it more likely that entities makes it more likely that you'll get a good representation of the you'll get a good representation of the you'll get a good representation of the knowledge in a graph. Given our text knowledge in a graph. Given our text knowledge in a graph. Given our text input, we set the allowed nodes to input, we set the allowed nodes to input, we set the allowed nodes to person, title, and group. We also set person, title, and group. We also set person, title, and group. We also set the allowed relationships to title, the allowed relationships to title, the allowed relationships to title, collaborates, and group. We use the collaborates, and group. We use the collaborates, and group. We use the document class to prepare our text to be document class to prepare our text to be document class to prepare our text to be added to the graph documents. The call added to the graph documents. The call added to the graph documents. The call to convert to graph documents generates to convert to graph documents generates to convert to graph documents generates text in a format that represents the text in a format that represents the text in a format that represents the entities in the graph. We can inspect this graph graph. We can inspect this graph graph. We can inspect this graph documents object to see how the LLM documents object to see how the LLM documents object to see how the LLM generated nodes and relationships from generated nodes and relationships from generated nodes and relationships from the text representing the relevant the text representing the relevant the text representing the relevant context and relevant entities. Now that we have the data in the correct Now that we have the data in the correct Now that we have the data in the correct format, we can insert these nodes and format, we can insert these nodes and format, we can insert these nodes and edges into the graph database using the edges into the graph database using the edges into the graph database using the add graph documents method. Once the graph data is created, method. Once the graph data is created, method. Once the graph data is created, we can visualize it using our browser. In order to query our graph browser. In order to query our graph browser. In order to query our graph database, we'll use cipher queries. Cipher is for a graph database what SQL Cipher is for a graph database what SQL Cipher is for a graph database what SQL is for a relational database. Instead of is for a relational database. Instead of is for a relational database. Instead of operating on tables, Cipher queries operating on tables, Cipher queries operating on tables, Cipher queries operate on the nodes, relationships, and operate on the nodes, relationships, and operate on the nodes, relationships, and paths in the graph database. To paths in the graph database. To paths in the graph database. To visualize the graph in the browser, I visualize the graph in the browser, I visualize the graph in the browser, I ran this query which shows us all the ran this query which shows us all the ran this query which shows us all the nodes and relationships in the graph. On nodes and relationships in the graph. On nodes and relationships in the graph. On a larger knowledge graph, this a larger knowledge graph, this a larger knowledge graph, this visualization might be too complex, but visualization might be too complex, but visualization might be too complex, but for our example, it works to verify the for our example, it works to verify the for our example, it works to verify the structure of the structure of the structure of the graph. It looks like the relationships graph. It looks like the relationships graph. It looks like the relationships in our input text have been correctly in our input text have been correctly in our input text have been correctly represented here in the knowledge graph. We can also examine the schema graph. We can also examine the schema graph. We can also examine the schema and data types in the database using the and data types in the database using the and data types in the database using the get schema property of the graph. get schema property of the graph. get schema property of the graph. Without the LLM, creating the knowledge Without the LLM, creating the knowledge Without the LLM, creating the knowledge graph might be a manual process to graph might be a manual process to graph might be a manual process to diagram entities and relationships from diagram entities and relationships from diagram entities and relationships from unstructured text. Now that we have our unstructured text. Now that we have our unstructured text. Now that we have our knowledge graph, we can query it, taking knowledge graph, we can query it, taking knowledge graph, we can query it, taking advantage of the graph structure and advantage of the graph structure and advantage of the graph structure and graph database retrieval capabilities to graph database retrieval capabilities to graph database retrieval capabilities to derive valuable information over the derive valuable information over the derive valuable information over the data in a more holistic way than data in a more holistic way than data in a more holistic way than semantic search can perform on a vector semantic search can perform on a vector semantic search can perform on a vector database. Now, we'll use natural database. Now, we'll use natural database. Now, we'll use natural language to query the knowledge graph. language to query the knowledge graph. language to query the knowledge graph. The natural language query will be The natural language query will be The natural language query will be passed to the LLM which is going to passed to the LLM which is going to passed to the LLM which is going to translate the query into cipher syntax. translate the query into cipher syntax. translate the query into cipher syntax. This cipher query will be executed on This cipher query will be executed on This cipher query will be executed on the database and the result will be the database and the result will be the database and the result will be returned to the LLM using natural returned to the LLM using natural returned to the LLM using natural language. Prompting the LLM correctly language. Prompting the LLM correctly language. Prompting the LLM correctly requires some prompt engineering. We'll requires some prompt engineering. We'll requires some prompt engineering. We'll think of the prompting step in two think of the prompting step in two think of the prompting step in two parts. So, we'll need to set up two parts. So, we'll need to set up two parts. So, we'll need to set up two different prompts. The first prompt different prompts. The first prompt different prompts. The first prompt gives the LLM instructions for gives the LLM instructions for gives the LLM instructions for generating a correct cipher query from generating a correct cipher query from generating a correct cipher query from the user's natural language query. the user's natural language query. the user's natural language query. Langchain provides a few prompt template Langchain provides a few prompt template Langchain provides a few prompt template that can be used to give examples to the that can be used to give examples to the that can be used to give examples to the LLM in the prompt, encouraging the LLM LLM in the prompt, encouraging the LLM LLM in the prompt, encouraging the LLM to write correct and succinct cipher to write correct and succinct cipher to write correct and succinct cipher syntax. This code block gives several syntax. This code block gives several syntax. This code block gives several examples of questions and corresponding examples of questions and corresponding examples of questions and corresponding cipher queries that the LLM should use cipher queries that the LLM should use cipher queries that the LLM should use as a guide. It also constrains the as a guide. It also constrains the as a guide. It also constrains the output of the model to only the query. output of the model to only the query. output of the model to only the query. An overly chatty LLM might add in extra An overly chatty LLM might add in extra An overly chatty LLM might add in extra information that would lead to invalid information that would lead to invalid information that would lead to invalid cipher cipher cipher queries. Using a prefix with a specified queries. Using a prefix with a specified queries. Using a prefix with a specified task and instructions also helps to task and instructions also helps to task and instructions also helps to constrain the model behavior and makes constrain the model behavior and makes constrain the model behavior and makes it more likely that the LLM will output it more likely that the LLM will output it more likely that the LLM will output correct cipher syntax. The second prompt provides the LLM The second prompt provides the LLM The second prompt provides the LLM instructions for translating the result instructions for translating the result instructions for translating the result of the cipher query into natural of the cipher query into natural of the cipher query into natural language given the original natural language given the original natural language given the original natural language question from the user. We language question from the user. We language question from the user. We employ a few prompting strategy here employ a few prompting strategy here employ a few prompting strategy here too, providing examples to the LLM for too, providing examples to the LLM for too, providing examples to the LLM for how to do this. We call this prompt the how to do this. We call this prompt the how to do this. We call this prompt the QA prompt. Essentially, it describes how QA prompt. Essentially, it describes how QA prompt. Essentially, it describes how the LLM should answer the question with the LLM should answer the question with the LLM should answer the question with the information returned from the graph the information returned from the graph the information returned from the graph database. Now we'll bundle together our cipher Now we'll bundle together our cipher Now we'll bundle together our cipher prompt, our QA prompt, our knowledge prompt, our QA prompt, our knowledge prompt, our QA prompt, our knowledge graph, and an LLM to create the question graph, and an LLM to create the question graph, and an LLM to create the question answering chain using the graph cipher answering chain using the graph cipher answering chain using the graph cipher QA chain class. We're implementing a QA chain class. We're implementing a QA chain class. We're implementing a simple retrieval procedure here, but simple retrieval procedure here, but simple retrieval procedure here, but there are ways to improve on this there are ways to improve on this there are ways to improve on this strategy by providing additional context strategy by providing additional context strategy by providing additional context to the LLM about groupings and summaries to the LLM about groupings and summaries to the LLM about groupings and summaries of like nodes within the knowledge of like nodes within the knowledge of like nodes within the knowledge graph. graph. graph. Using a temperature of zero and a length Using a temperature of zero and a length Using a temperature of zero and a length penalty encourages the LLM to keep the penalty encourages the LLM to keep the penalty encourages the LLM to keep the cipher prompt short and straightforward. cipher prompt short and straightforward. cipher prompt short and straightforward. If you're wondering why we're If you're wondering why we're If you're wondering why we're configuring a different LLM here, it's configuring a different LLM here, it's configuring a different LLM here, it's because we're setting different because we're setting different because we're setting different parameters for retrieval of information parameters for retrieval of information parameters for retrieval of information from the graph than we used earlier for from the graph than we used earlier for from the graph than we used earlier for constructing the graph. Now, we can constructing the graph. Now, we can constructing the graph. Now, we can query the data by invoking the chain query the data by invoking the chain query the data by invoking the chain with a natural language question. If you with a natural language question. If you with a natural language question. If you try this out, your responses may be try this out, your responses may be try this out, your responses may be slightly different than what we're slightly different than what we're slightly different than what we're seeing here because LLMs are not seeing here because LLMs are not seeing here because LLMs are not strictly strictly strictly deterministic. Here's our first deterministic. Here's our first deterministic. Here's our first question. What is John's title? We can question. What is John's title? We can question. What is John's title? We can see the cipher query generated by this see the cipher query generated by this see the cipher query generated by this LLM to retrieve the information, the LLM to retrieve the information, the LLM to retrieve the information, the result of the cipher query, and the result of the cipher query, and the result of the cipher query, and the natural language response from the LLM natural language response from the LLM natural language response from the LLM as director of the digital marketing as director of the digital marketing as director of the digital marketing group. Looks good. Let's try a slightly group. Looks good. Let's try a slightly group. Looks good. Let's try a slightly more complex question. Who does John more complex question. Who does John more complex question. Who does John collaborate collaborate collaborate with? Again, the LLM generates a cipher with? Again, the LLM generates a cipher with? Again, the LLM generates a cipher query to retrieve the correct query to retrieve the correct query to retrieve the correct information from the graph database and information from the graph database and information from the graph database and returns the correct returns the correct returns the correct response. John collaborates with Jane. response. John collaborates with Jane. response. John collaborates with Jane. This looks good. Let's ask the chain This looks good. Let's ask the chain This looks good. Let's ask the chain about a group relationship. What group about a group relationship. What group about a group relationship. What group is Jane is Jane is Jane in? Jane is in the executive group. in? Jane is in the executive group. in? Jane is in the executive group. Okay, let's try one more that requires Okay, let's try one more that requires Okay, let's try one more that requires the LLM to give us two outputs. Who does the LLM to give us two outputs. Who does the LLM to give us two outputs. Who does Jane collaborate with? Jane collaborates Jane collaborate with? Jane collaborates Jane collaborate with? Jane collaborates with Sharon and John. Even for this more with Sharon and John. Even for this more with Sharon and John. Even for this more difficult query, we can see the chain difficult query, we can see the chain difficult query, we can see the chain correctly identifies both of the correctly identifies both of the correctly identifies both of the collaborators. Beyond retrieving the collaborators. Beyond retrieving the collaborators. Beyond retrieving the simple titles and relationships from our simple titles and relationships from our simple titles and relationships from our input string in this example, graph rag input string in this example, graph rag input string in this example, graph rag can summarize and retrieve contextual can summarize and retrieve contextual can summarize and retrieve contextual information over the whole structure of information over the whole structure of information over the whole structure of the knowledge graph. So how is this the knowledge graph. So how is this the knowledge graph. So how is this different from a vector rag system? different from a vector rag system? different from a vector rag system? Firstly, instead of calculating Firstly, instead of calculating Firstly, instead of calculating embeddings and storing the resulting embeddings and storing the resulting embeddings and storing the resulting embedded information in a vector embedded information in a vector embedded information in a vector database, a graph rag system transforms database, a graph rag system transforms database, a graph rag system transforms unstructured text data into structured unstructured text data into structured unstructured text data into structured data using an LLM and a knowledge graph data using an LLM and a knowledge graph data using an LLM and a knowledge graph is populated with this data. The second is populated with this data. The second is populated with this data. The second difference is in the retrieval step. difference is in the retrieval step. difference is in the retrieval step. Instead of performing semantic search Instead of performing semantic search Instead of performing semantic search and returning results with semantic and returning results with semantic and returning results with semantic similarity, the LLM generates a cipher similarity, the LLM generates a cipher similarity, the LLM generates a cipher query in response to the user's natural query in response to the user's natural query in response to the user's natural language query which gets executed on language query which gets executed on language query which gets executed on the graph database containing the the graph database containing the the graph database containing the knowledge graph. The graph rag system knowledge graph. The graph rag system knowledge graph. The graph rag system avoids one of the limitations of vector avoids one of the limitations of vector avoids one of the limitations of vector rag. If you think about the way vector rag. If you think about the way vector rag. If you think about the way vector rag returns top semantic search results rag returns top semantic search results rag returns top semantic search results to a query, you can recognize that to a query, you can recognize that to a query, you can recognize that vector rag can't provide the LLM with vector rag can't provide the LLM with vector rag can't provide the LLM with knowledge over the whole text corpus in knowledge over the whole text corpus in knowledge over the whole text corpus in response to one query. It's limited to response to one query. It's limited to response to one query. It's limited to the top semantic search results. Graphra the top semantic search results. Graphra the top semantic search results. Graphra can leverage graph indexes which store can leverage graph indexes which store can leverage graph indexes which store summaries about groupings of like nodes summaries about groupings of like nodes summaries about groupings of like nodes to provide summarization over the whole to provide summarization over the whole to provide summarization over the whole corpus of text within one query result. corpus of text within one query result. corpus of text within one query result. In practice, you may want both the In practice, you may want both the In practice, you may want both the capabilities of retrieval from a capabilities of retrieval from a capabilities of retrieval from a semantic search on a vector database and semantic search on a vector database and semantic search on a vector database and a graph search over a knowledge graph. a graph search over a knowledge graph. a graph search over a knowledge graph. It's possible to build these sort of It's possible to build these sort of It's possible to build these sort of hybrid rag systems using both vector hybrid rag systems using both vector hybrid rag systems using both vector databases and graph databases and graph databases and graph databases. Check out the GitHub link in databases. Check out the GitHub link in databases. Check out the GitHub link in the description below to try out graph the description below to try out graph the description below to try out graph rag for yourself.",
    "chunks": [
      "Kind: captions Language: en Today I'm going to show you how to Today I'm going to show you how to Today I'm going to show you how to populate a knowledge graph and query it populate a knowledge graph and query it populate a knowledge graph and query it using an LLM. Graph retrieval augmented using an LLM. Graph retrieval augmented using an LLM. Graph retrieval augmented generation or graph rag is emerging as a generation or graph rag is emerging as a generation or graph rag is emerging as a",
      "powerful alternative to vector search powerful alternative to vector search powerful alternative to vector search methods. Instead of using a vector methods. Instead of using a vector methods. Instead of using a vector database, graph rag systems store data database, graph rag systems store data database, graph rag systems store data in the format of a knowledge graph using in the format of a knowledge graph using in the format of a knowledge graph using a graph database. In a knowledge graph, a",
      "graph database. In a knowledge graph, a graph database. In a knowledge graph, the relationships between data points the relationships between data points the relationships between data points called edges are as meaningful as the called edges are as meaningful as the called edges are as meaningful as the connections between data points called connections between data points called connections between data points called vertices or sometimes nodes. A graph rag vertices or sometimes nodes. A graph",
      "rag vertices or sometimes nodes. A graph rag approach leverages the structured nature approach leverages the structured nature approach leverages the structured nature of graph databases to give greater depth of graph databases to give greater depth of graph databases to give greater depth and context of retrieved information and context of retrieved information and context of retrieved information about networks or complex relationships. The first step in setting relationships. The first step in",
      "setting relationships. The first step in setting up our system is creating and populating up our system is creating and populating up our system is creating and populating the knowledge graph. We'll be using an the knowledge graph. We'll be using an the knowledge graph. We'll be using an LLM to assist in creating the knowledge LLM to assist in creating the knowledge LLM to assist in creating the knowledge graph. Given unstructured text data, the graph. Given unstructured text data, the graph.",
      "Given unstructured text data, the LLM will extract entities and LLM will extract entities and LLM will extract entities and relationships from the data, relationships from the data, relationships from the data, transforming the data into structured transforming the data into structured transforming the data into structured data, which can then be inserted into data, which can then be inserted into data, which can then be inserted into the knowledge the knowledge the knowledge graph. After the",
      "knowledge graph is graph. After the knowledge graph is graph. After the knowledge graph is created, we'll be using the LLM to query created, we'll be using the LLM to query created, we'll be using the LLM to query data from the knowledge graph and return data from the knowledge graph and return data from the knowledge graph and return the response in natural language. Cipher the response in natural language. Cipher the response in natural language. Cipher is the query language for a graph is the",
      "query language for a graph is the query language for a graph database. When a user asks a question in database. When a user asks a question in database. When a user asks a question in natural language, the LLM will generate natural language, the LLM will generate natural language, the LLM will generate the cipher query to extract that the cipher query to extract that the cipher query to extract that information from the knowledge graph. information from the knowledge graph. information from the",
      "knowledge graph. The cipher query then gets executed on The cipher query then gets executed on The cipher query then gets executed on the database and the results are the database and the results are the database and the results are returned to the LLM. The last step is returned to the LLM. The last step is returned to the LLM. The last step is for the LLM to interpret the results of for the LLM to interpret the results of for the LLM to interpret the results of the cipher query in the context of",
      "the the cipher query in the context of the the cipher query in the context of the natural language question and return a natural language question and return a natural language question and return a natural language response. For this natural language response. For this natural language response. For this example, we'll need an API key and example, we'll need an API key and example, we'll need an API key and project ID. The link to this notebook is project ID. The link to this notebook is project",
      "ID. The link to this notebook is in the description below. In the in the description below. In the in the description below. In the notebook, you'll find instructions for notebook, you'll find instructions for notebook, you'll find instructions for retrieving these credentials. We'll be using Neo4j, an open-source We'll be using Neo4j, an open-source We'll be using Neo4j, an open-source graph database, but any graph database graph database, but any graph database graph database, but any graph",
      "database can be used to create the knowledge can be used to create the knowledge can be used to create the knowledge graph. We'll create a local instance of graph. We'll create a local instance of graph. We'll create a local instance of the database using a containerization the database using a containerization the database using a containerization tool. I'll be using Podman, but you can tool. I'll be using Podman, but you can tool. I'll be using Podman, but you can use any containerization tool,",
      "for use any containerization tool, for use any containerization tool, for example, Docker, as long as it allows example, Docker, as long as it allows example, Docker, as long as it allows you to create a Neo4j instance. If you you to create a Neo4j instance. If you you to create a Neo4j instance. If you don't have a containerization tool don't have a containerization tool don't have a containerization tool already, take a moment to install already, take a moment to install already, take a moment",
      "to install one. one. one. After installing, initialize and start a After installing, initialize and start a After installing, initialize and start a machine. My machine's already machine. My machine's already machine. My machine's already initialized, so I'm just starting it here. Once you have this running, you here. Once you have this running, you here. Once you have this running, you can start a database instance with this can start a database instance with this can start a database instance",
      "with this configuration. We need credentials to configuration. We need credentials to configuration. We need credentials to access the database. So, I'm setting a access the database. So, I'm setting a access the database. So, I'm setting a name and password here. We also need to name and password here. We also need to name and password here. We also need to include the Apoch library as a plugin in include the Apoch library as a plugin in include the Apoch library as a plugin in order to enable",
      "additional functionality order to enable additional functionality order to enable additional functionality for working with data and for working with data and for working with data and graphs. It looks like our graph database graphs. It looks like our graph database graphs. It looks like our graph database is up and running. Now, it's a good practice to create a Now, it's a good practice to create a Now, it's a good practice to create a fresh virtual environment for this fresh virtual environment",
      "for this fresh virtual environment for this project. I'm using Python project. I'm using Python project. I'm using Python 3.11.3 here. In the Python environment 3.11.3 here. In the Python environment 3.11.3 here. In the Python environment for your notebook, install the following for your notebook, install the following for your notebook, install the following Python Python Python libraries. We'll be using the OS and get libraries. We'll be using the OS and get libraries. We'll be using the OS and",
      "get pass modules to set up credentials. pass modules to set up credentials. pass modules to set up credentials. We'll use Langchain's document class to We'll use Langchain's document class to We'll use Langchain's document class to store the text for input into our graph store the text for input into our graph store the text for input into our graph database and the LLM graph transformer database and the LLM graph transformer database and the LLM graph transformer to create a graph from our text",
      "input. to create a graph from our text input. to create a graph from our text input. To interact with and query the graph To interact with and query the graph To interact with and query the graph database, we'll use the langchain neo4j database, we'll use the langchain neo4j database, we'll use the langchain neo4j module and its accompanying graph cipher module and its accompanying graph cipher module and its accompanying graph cipher QA chain class. To craft our prompts for QA chain class. To",
      "craft our prompts for QA chain class. To craft our prompts for the LLM, we'll use Langchain's prompt the LLM, we'll use Langchain's prompt the LLM, we'll use Langchain's prompt template and fshot prompt template. template and fshot prompt template. template and fshot prompt template. We'll use the Langchain IBM and IBM We'll use the Langchain IBM and IBM We'll use the Langchain IBM and IBM Watsonx.ai modules to interact with the Watsonx.ai modules to interact with the Watsonx.ai modules to",
      "interact with the LLM and to set the parameters for our LLM and to set the parameters for our LLM and to set the parameters for our models. We'll need to set up our credentials We'll need to set up our credentials We'll need to set up our credentials using the API key and project ID that we using the API key and project ID that we using the API key and project ID that we retrieved earlier. We'll also need to retrieved earlier. We'll also need to retrieved earlier. We'll also need to set the URL",
      "from which we'll access set the URL from which we'll access set the URL from which we'll access these services. Now that our environment these services. Now that our environment these services. Now that our environment set up, we can create the knowledge set up, we can create the knowledge set up, we can create the knowledge graph. First, we need to create a graph. First, we need to create a graph. First, we need to create a connection to the local database connection to the local database",
      "connection to the local database instance that we started instance that we started instance that we started earlier. Next, we define our data for earlier. Next, we define our data for earlier. Next, we define our data for input into the knowledge graph. In this input into the knowledge graph. In this input into the knowledge graph. In this case, the text describes employees at a case, the text describes employees at a case, the text describes employees at a company, groups they work in, and their",
      "company, groups they work in, and their company, groups they work in, and their job titles. We'll use this set of job titles. We'll use this set of job titles. We'll use this set of relationships to test the graph relationships to test the graph relationships to test the graph generating capabilities of the LLM. But generating capabilities of the LLM. But generating capabilities of the LLM. But you don't have to limit your data to you don't have to limit your data to you don't have to limit your",
      "data to straightforward examples of relationship straightforward examples of relationship straightforward examples of relationship data. Graph rank systems have been shown data. Graph rank systems have been shown data. Graph rank systems have been shown to be successful in retrieval and to be successful in retrieval and to be successful in retrieval and summarization tasks for far more complex summarization tasks for far more complex summarization tasks for far more complex narrative and",
      "connected data. Now we'll configure our Now we'll configure our Now we'll configure our LLM which will generate text describing LLM which will generate text describing LLM which will generate text describing the graph. The LLM temperature should be the graph. The LLM temperature should be the graph. The LLM temperature should be fairly low and the number of tokens high fairly low and the number of tokens high fairly low and the number of tokens high to encourage the model to generate as to",
      "encourage the model to generate as to encourage the model to generate as much detail as possible without much detail as possible without much detail as possible without hallucinating entities or relationships hallucinating entities or relationships hallucinating entities or relationships that aren't present. One of the most powerful LLM use cases One of the most powerful LLM use cases One of the most powerful LLM use cases is transforming unstructured text data is transforming unstructured text",
      "data is transforming unstructured text data into structured data. The LLM will into structured data. The LLM will into structured data. The LLM will transform our text input string into a transform our text input string into a transform our text input string into a structure of nodes and relationships structure of nodes and relationships structure of nodes and relationships that we can use to populate the that we can use to populate the that we can use to populate the knowledge graph. knowledge",
      "graph. knowledge graph. The LLM graph transformer allows you to The LLM graph transformer allows you to The LLM graph transformer allows you to set the kinds of nodes and relationships set the kinds of nodes and relationships set the kinds of nodes and relationships you'd like the LLM to generate. you'd like the LLM to generate. you'd like the LLM to generate. Restricting the LLM to just those Restricting the LLM to just those Restricting the LLM to just those entities makes it more likely that",
      "entities makes it more likely that entities makes it more likely that you'll get a good representation of the you'll get a good representation of the you'll get a good representation of the knowledge in a graph. Given our text knowledge in a graph. Given our text knowledge in a graph. Given our text input, we set the allowed nodes to input, we set the allowed nodes to input, we set the allowed nodes to person, title, and group. We also set person, title, and group. We also set person, title, and",
      "group. We also set the allowed relationships to title, the allowed relationships to title, the allowed relationships to title, collaborates, and group. We use the collaborates, and group. We use the collaborates, and group. We use the document class to prepare our text to be document class to prepare our text to be document class to prepare our text to be added to the graph documents. The call added to the graph documents. The call added to the graph documents. The call to convert to graph",
      "documents generates to convert to graph documents generates to convert to graph documents generates text in a format that represents the text in a format that represents the text in a format that represents the entities in the graph. We can inspect this graph graph. We can inspect this graph graph. We can inspect this graph documents object to see how the LLM documents object to see how the LLM documents object to see how the LLM generated nodes and relationships from generated nodes and",
      "relationships from generated nodes and relationships from the text representing the relevant the text representing the relevant the text representing the relevant context and relevant entities. Now that we have the data in the correct Now that we have the data in the correct Now that we have the data in the correct format, we can insert these nodes and format, we can insert these nodes and format, we can insert these nodes and edges into the graph database using the edges into the graph database",
      "using the edges into the graph database using the add graph documents method. Once the graph data is created, method. Once the graph data is created, method. Once the graph data is created, we can visualize it using our browser. In order to query our graph browser. In order to query our graph browser. In order to query our graph database, we'll use cipher queries. Cipher is for a graph database what SQL Cipher is for a graph database what SQL Cipher is for a graph database what SQL is for a",
      "relational database. Instead of is for a relational database. Instead of is for a relational database. Instead of operating on tables, Cipher queries operating on tables, Cipher queries operating on tables, Cipher queries operate on the nodes, relationships, and operate on the nodes, relationships, and operate on the nodes, relationships, and paths in the graph database. To paths in the graph database. To paths in the graph database. To visualize the graph in the browser, I visualize the graph in",
      "the browser, I visualize the graph in the browser, I ran this query which shows us all the ran this query which shows us all the ran this query which shows us all the nodes and relationships in the graph. On nodes and relationships in the graph. On nodes and relationships in the graph. On a larger knowledge graph, this a larger knowledge graph, this a larger knowledge graph, this visualization might be too complex, but visualization might be too complex, but visualization might be too complex,",
      "but for our example, it works to verify the for our example, it works to verify the for our example, it works to verify the structure of the structure of the structure of the graph. It looks like the relationships graph. It looks like the relationships graph. It looks like the relationships in our input text have been correctly in our input text have been correctly in our input text have been correctly represented here in the knowledge graph. We can also examine the schema graph. We can also",
      "examine the schema graph. We can also examine the schema and data types in the database using the and data types in the database using the and data types in the database using the get schema property of the graph. get schema property of the graph. get schema property of the graph. Without the LLM, creating the knowledge Without the LLM, creating the knowledge Without the LLM, creating the knowledge graph might be a manual process to graph might be a manual process to graph might be a manual",
      "process to diagram entities and relationships from diagram entities and relationships from diagram entities and relationships from unstructured text. Now that we have our unstructured text. Now that we have our unstructured text. Now that we have our knowledge graph, we can query it, taking knowledge graph, we can query it, taking knowledge graph, we can query it, taking advantage of the graph structure and advantage of the graph structure and advantage of the graph structure and graph database",
      "retrieval capabilities to graph database retrieval capabilities to graph database retrieval capabilities to derive valuable information over the derive valuable information over the derive valuable information over the data in a more holistic way than data in a more holistic way than data in a more holistic way than semantic search can perform on a vector semantic search can perform on a vector semantic search can perform on a vector database. Now, we'll use natural database. Now, we'll use",
      "natural database. Now, we'll use natural language to query the knowledge graph. language to query the knowledge graph. language to query the knowledge graph. The natural language query will be The natural language query will be The natural language query will be passed to the LLM which is going to passed to the LLM which is going to passed to the LLM which is going to translate the query into cipher syntax. translate the query into cipher syntax. translate the query into cipher syntax. This",
      "cipher query will be executed on This cipher query will be executed on This cipher query will be executed on the database and the result will be the database and the result will be the database and the result will be returned to the LLM using natural returned to the LLM using natural returned to the LLM using natural language. Prompting the LLM correctly language. Prompting the LLM correctly language. Prompting the LLM correctly requires some prompt engineering. We'll requires some prompt",
      "engineering. We'll requires some prompt engineering. We'll think of the prompting step in two think of the prompting step in two think of the prompting step in two parts. So, we'll need to set up two parts. So, we'll need to set up two parts. So, we'll need to set up two different prompts. The first prompt different prompts. The first prompt different prompts. The first prompt gives the LLM instructions for gives the LLM instructions for gives the LLM instructions for generating a correct cipher",
      "query from generating a correct cipher query from generating a correct cipher query from the user's natural language query. the user's natural language query. the user's natural language query. Langchain provides a few prompt template Langchain provides a few prompt template Langchain provides a few prompt template that can be used to give examples to the that can be used to give examples to the that can be used to give examples to the LLM in the prompt, encouraging the LLM LLM in the prompt,",
      "encouraging the LLM LLM in the prompt, encouraging the LLM to write correct and succinct cipher to write correct and succinct cipher to write correct and succinct cipher syntax. This code block gives several syntax. This code block gives several syntax. This code block gives several examples of questions and corresponding examples of questions and corresponding examples of questions and corresponding cipher queries that the LLM should use cipher queries that the LLM should use cipher queries that",
      "the LLM should use as a guide. It also constrains the as a guide. It also constrains the as a guide. It also constrains the output of the model to only the query. output of the model to only the query. output of the model to only the query. An overly chatty LLM might add in extra An overly chatty LLM might add in extra An overly chatty LLM might add in extra information that would lead to invalid information that would lead to invalid information that would lead to invalid cipher cipher cipher",
      "queries. Using a prefix with a specified queries. Using a prefix with a specified queries. Using a prefix with a specified task and instructions also helps to task and instructions also helps to task and instructions also helps to constrain the model behavior and makes constrain the model behavior and makes constrain the model behavior and makes it more likely that the LLM will output it more likely that the LLM will output it more likely that the LLM will output correct cipher syntax. The second",
      "prompt provides the LLM The second prompt provides the LLM The second prompt provides the LLM instructions for translating the result instructions for translating the result instructions for translating the result of the cipher query into natural of the cipher query into natural of the cipher query into natural language given the original natural language given the original natural language given the original natural language question from the user. We language question from the user. We language",
      "question from the user. We employ a few prompting strategy here employ a few prompting strategy here employ a few prompting strategy here too, providing examples to the LLM for too, providing examples to the LLM for too, providing examples to the LLM for how to do this. We call this prompt the how to do this. We call this prompt the how to do this. We call this prompt the QA prompt. Essentially, it describes how QA prompt. Essentially, it describes how QA prompt. Essentially, it describes how the",
      "LLM should answer the question with the LLM should answer the question with the LLM should answer the question with the information returned from the graph the information returned from the graph the information returned from the graph database. Now we'll bundle together our cipher Now we'll bundle together our cipher Now we'll bundle together our cipher prompt, our QA prompt, our knowledge prompt, our QA prompt, our knowledge prompt, our QA prompt, our knowledge graph, and an LLM to create the",
      "question graph, and an LLM to create the question graph, and an LLM to create the question answering chain using the graph cipher answering chain using the graph cipher answering chain using the graph cipher QA chain class. We're implementing a QA chain class. We're implementing a QA chain class. We're implementing a simple retrieval procedure here, but simple retrieval procedure here, but simple retrieval procedure here, but there are ways to improve on this there are ways to improve on this",
      "there are ways to improve on this strategy by providing additional context strategy by providing additional context strategy by providing additional context to the LLM about groupings and summaries to the LLM about groupings and summaries to the LLM about groupings and summaries of like nodes within the knowledge of like nodes within the knowledge of like nodes within the knowledge graph. graph. graph. Using a temperature of zero and a length Using a temperature of zero and a length Using a",
      "temperature of zero and a length penalty encourages the LLM to keep the penalty encourages the LLM to keep the penalty encourages the LLM to keep the cipher prompt short and straightforward. cipher prompt short and straightforward. cipher prompt short and straightforward. If you're wondering why we're If you're wondering why we're If you're wondering why we're configuring a different LLM here, it's configuring a different LLM here, it's configuring a different LLM here, it's because we're setting",
      "different because we're setting different because we're setting different parameters for retrieval of information parameters for retrieval of information parameters for retrieval of information from the graph than we used earlier for from the graph than we used earlier for from the graph than we used earlier for constructing the graph. Now, we can constructing the graph. Now, we can constructing the graph. Now, we can query the data by invoking the chain query the data by invoking the chain query",
      "the data by invoking the chain with a natural language question. If you with a natural language question. If you with a natural language question. If you try this out, your responses may be try this out, your responses may be try this out, your responses may be slightly different than what we're slightly different than what we're slightly different than what we're seeing here because LLMs are not seeing here because LLMs are not seeing here because LLMs are not strictly strictly strictly",
      "deterministic. Here's our first deterministic. Here's our first deterministic. Here's our first question. What is John's title? We can question. What is John's title? We can question. What is John's title? We can see the cipher query generated by this see the cipher query generated by this see the cipher query generated by this LLM to retrieve the information, the LLM to retrieve the information, the LLM to retrieve the information, the result of the cipher query, and the result of the cipher",
      "query, and the result of the cipher query, and the natural language response from the LLM natural language response from the LLM natural language response from the LLM as director of the digital marketing as director of the digital marketing as director of the digital marketing group. Looks good. Let's try a slightly group. Looks good. Let's try a slightly group. Looks good. Let's try a slightly more complex question. Who does John more complex question. Who does John more complex question. Who",
      "does John collaborate collaborate collaborate with? Again, the LLM generates a cipher with? Again, the LLM generates a cipher with? Again, the LLM generates a cipher query to retrieve the correct query to retrieve the correct query to retrieve the correct information from the graph database and information from the graph database and information from the graph database and returns the correct returns the correct returns the correct response. John collaborates with Jane. response. John",
      "collaborates with Jane. response. John collaborates with Jane. This looks good. Let's ask the chain This looks good. Let's ask the chain This looks good. Let's ask the chain about a group relationship. What group about a group relationship. What group about a group relationship. What group is Jane is Jane is Jane in? Jane is in the executive group. in? Jane is in the executive group. in? Jane is in the executive group. Okay, let's try one more that requires Okay, let's try one more that requires",
      "Okay, let's try one more that requires the LLM to give us two outputs. Who does the LLM to give us two outputs. Who does the LLM to give us two outputs. Who does Jane collaborate with? Jane collaborates Jane collaborate with? Jane collaborates Jane collaborate with? Jane collaborates with Sharon and John. Even for this more with Sharon and John. Even for this more with Sharon and John. Even for this more difficult query, we can see the chain difficult query, we can see the chain difficult query,",
      "we can see the chain correctly identifies both of the correctly identifies both of the correctly identifies both of the collaborators. Beyond retrieving the collaborators. Beyond retrieving the collaborators. Beyond retrieving the simple titles and relationships from our simple titles and relationships from our simple titles and relationships from our input string in this example, graph rag input string in this example, graph rag input string in this example, graph rag can summarize and retrieve",
      "contextual can summarize and retrieve contextual can summarize and retrieve contextual information over the whole structure of information over the whole structure of information over the whole structure of the knowledge graph. So how is this the knowledge graph. So how is this the knowledge graph. So how is this different from a vector rag system? different from a vector rag system? different from a vector rag system? Firstly, instead of calculating Firstly, instead of calculating Firstly,",
      "instead of calculating embeddings and storing the resulting embeddings and storing the resulting embeddings and storing the resulting embedded information in a vector embedded information in a vector embedded information in a vector database, a graph rag system transforms database, a graph rag system transforms database, a graph rag system transforms unstructured text data into structured unstructured text data into structured unstructured text data into structured data using an LLM and a",
      "knowledge graph data using an LLM and a knowledge graph data using an LLM and a knowledge graph is populated with this data. The second is populated with this data. The second is populated with this data. The second difference is in the retrieval step. difference is in the retrieval step. difference is in the retrieval step. Instead of performing semantic search Instead of performing semantic search Instead of performing semantic search and returning results with semantic and returning results",
      "with semantic and returning results with semantic similarity, the LLM generates a cipher similarity, the LLM generates a cipher similarity, the LLM generates a cipher query in response to the user's natural query in response to the user's natural query in response to the user's natural language query which gets executed on language query which gets executed on language query which gets executed on the graph database containing the the graph database containing the the graph database containing",
      "the knowledge graph. The graph rag system knowledge graph. The graph rag system knowledge graph. The graph rag system avoids one of the limitations of vector avoids one of the limitations of vector avoids one of the limitations of vector rag. If you think about the way vector rag. If you think about the way vector rag. If you think about the way vector rag returns top semantic search results rag returns top semantic search results rag returns top semantic search results to a query, you can",
      "recognize that to a query, you can recognize that to a query, you can recognize that vector rag can't provide the LLM with vector rag can't provide the LLM with vector rag can't provide the LLM with knowledge over the whole text corpus in knowledge over the whole text corpus in knowledge over the whole text corpus in response to one query. It's limited to response to one query. It's limited to response to one query. It's limited to the top semantic search results. Graphra the top semantic search",
      "results. Graphra the top semantic search results. Graphra can leverage graph indexes which store can leverage graph indexes which store can leverage graph indexes which store summaries about groupings of like nodes summaries about groupings of like nodes summaries about groupings of like nodes to provide summarization over the whole to provide summarization over the whole to provide summarization over the whole corpus of text within one query result. corpus of text within one query result. corpus",
      "of text within one query result. In practice, you may want both the In practice, you may want both the In practice, you may want both the capabilities of retrieval from a capabilities of retrieval from a capabilities of retrieval from a semantic search on a vector database and semantic search on a vector database and semantic search on a vector database and a graph search over a knowledge graph. a graph search over a knowledge graph. a graph search over a knowledge graph. It's possible to build",
      "these sort of It's possible to build these sort of It's possible to build these sort of hybrid rag systems using both vector hybrid rag systems using both vector hybrid rag systems using both vector databases and graph databases and graph databases and graph databases. Check out the GitHub link in databases. Check out the GitHub link in databases. Check out the GitHub link in the description below to try out graph the description below to try out graph the description below to try out graph rag",
      "for yourself."
    ],
    "chunk_count": 71,
    "content_id": "3435bfa0-44c9-472a-9fb6-7e8ffb96b0b1",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554841"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=jcgaNrC4ElU": {
    "title": "Five Steps to Create a New AI Model",
    "url": "https://www.youtube.com/watch?v=jcgaNrC4ElU",
    "description": "Earn a Generative AI certificate today → https://ibm.biz/BdKUNX\nLearn more about watsonx: https://ibm.biz/BdvDnr\n\nAI promises to touch every aspect of work and life, but how do they get made?\n\rIn this video Martin keen walks through a five step framework for how to build and deploy AI models.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdKUNr",
    "duration": 416,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Deep learning has enabled us tobuild detailed specialized AI models, and we can do that provided we gather enough data, label it, and usethat to train and deploy those models. Models like customer service chatbots or fraud detection inbanking. Now, in the past if you wanted to build a new model for your specialization - so, say a model forpredictive maintenance in manufacturing - well, you’d need to start again with data selectionand curation, labeling, model development,training, and validation. But foundation modelsare changing that paradigm. So what is a foundation model? A foundation model is a more focused, centralized effort to createa base model. And, through fine tuning, that base foundation model can be adapted to a specializedmodel. Need an AI model for programming language translation? Well, start with a foundational model andthen fine tune it with programming language data. Fine tuning and adapting base foundationmodels rapidly speeds up AI model development. So, how do we do that? Let’s look at the fivestages of the workflow to create an AI model. Stage 1 is to prepare the data. Now in this stage we need to trainour AI model with the data we're going to use, and we're going to need a lot of data. Potentially petabytes of data acrossdozens of domains. The data can combine both available open source data and proprietarydata. Now this stage performs a series of data processing tasks. Those includecategorization which describes what the data is. So which data is English, which is German? Whichis Ansible which is Java? That sort of thing. Then the data is also applied with a filtere. So filtering allows us to, for example, apply filters for hate speech, and profanity and abuse, and that sort of thing. Stuff we want to filter out of the system that we don't train the model on it. Other filters may flag copyrighted material, private or sensitive information. Something else we're going to take out is  duplicate data as well. So we're going to remove that from there. And then that leaves us with something called a base data pile. So that's really the output of stage one. And this base data pile can be versioned and tagged. Andthat allows us to say, \"This is what I’m training the AI model on, and here are thefilters I used\". It's perfect for governance. Now, Stage 2 is to train the model. And we're going to train the model on those base data piles. So we startthis stage by picking the foundational model we want to use. So we will select our model. Now, there are many types of foundation models. There are generative foundationmodels, encoder-only models, lightweight models, high parameter models. Are you looking to build anAI model to use as a chatbot, or as a classifier? So pick the foundational model that matches your usecase, then match the data pile with that model. Next we take the data pile and we tokenize it. Foundation models work with tokens rather than words, and a data pile could resultin potentially trillions of tokens. And now we can engage the process of training using all of those tokens. This process can take a long time, depending on the size of the model. Large scale foundationmodels can take months with many thousands of GPUs. But, once it’s done, the longest andhighest computational costs are behind us. Stage 3 is \"validate\". When training isfinished we benchmark the model. And this involves running the model and assessing itsperformance against a set of benchmarks that help define the quality of the model. And then from here we can create a model card that says this is the model I’ve trained andthese are the benchmark scores it has achieved. Now up until this point the main persona thathas performed these tasks is the data scientist. Now Stage 4 is \"tune\", and this is where we bring in thepersona of the application developer. This persona does not need to be an AI expert. They engage withthe model, generating - for example - prompts that elicit good performance from the model. Theycan provide additional local data to fine tune the model to improve its performance. And this stage is something that you can do in hours or days - much quickerthan building a model from scratch. And now we’re ready for Stage 5, which is to deployment the model. Now thismodel could run as as service offering deployed to a public cloud. Or we could, alternatively, embed the model into anapplication that runs much closer to the edgeof the network. Either way we can continueto iterate and improve the model over time. Now here at IBM we’ve announced a platformthat enables all 5 of the stages of this workflow. And It’s called watsonx and it’s composed ofthree elements. So we have: watsonx.data, watsonx.governance, and watsonx.ai., and this all built on IBM’shybrid cloud platform which is Red Hat OpenShift. Now Watsonx.data is a modern data lakehouse and establishes connections with the datarepositories that make up the data inStage 1. Watsonx.governance manages the data cards from Stage 1 and model cards fromStage 3 enabling a collection of fact sheets that ensure a well-governed AI process andlifecycle. And watsonx.ai provides a means for the application developer personato engage with the model in Stage 4. Overall foundation models are changingthe way we build specialized AI models and this 5-stage workflow allows teams tocreate AI and AI-derived applications with greater sophistication while rapidlyspeeding up AI model development.",
    "chunks": [
      "Kind: captions Language: en Deep learning has enabled us tobuild detailed specialized AI models, and we can do that provided we gather enough data, label it, and usethat to train and deploy those models. Models like customer service chatbots or fraud detection inbanking. Now, in the past if you wanted to build a new model for your specialization - so, say a model forpredictive maintenance in manufacturing - well, you’d need to start again with data selectionand curation, labeling, model",
      "development,training, and validation. But foundation modelsare changing that paradigm. So what is a foundation model? A foundation model is a more focused, centralized effort to createa base model. And, through fine tuning, that base foundation model can be adapted to a specializedmodel. Need an AI model for programming language translation? Well, start with a foundational model andthen fine tune it with programming language data. Fine tuning and adapting base foundationmodels rapidly speeds up",
      "AI model development. So, how do we do that? Let’s look at the fivestages of the workflow to create an AI model. Stage 1 is to prepare the data. Now in this stage we need to trainour AI model with the data we're going to use, and we're going to need a lot of data. Potentially petabytes of data acrossdozens of domains. The data can combine both available open source data and proprietarydata. Now this stage performs a series of data processing tasks. Those includecategorization which describes what",
      "the data is. So which data is English, which is German? Whichis Ansible which is Java? That sort of thing. Then the data is also applied with a filtere. So filtering allows us to, for example, apply filters for hate speech, and profanity and abuse, and that sort of thing. Stuff we want to filter out of the system that we don't train the model on it. Other filters may flag copyrighted material, private or sensitive information. Something else we're going to take out is duplicate data as well. So",
      "we're going to remove that from there. And then that leaves us with something called a base data pile. So that's really the output of stage one. And this base data pile can be versioned and tagged. Andthat allows us to say, \"This is what I’m training the AI model on, and here are thefilters I used\". It's perfect for governance. Now, Stage 2 is to train the model. And we're going to train the model on those base data piles. So we startthis stage by picking the foundational model we want to use. So",
      "we will select our model. Now, there are many types of foundation models. There are generative foundationmodels, encoder-only models, lightweight models, high parameter models. Are you looking to build anAI model to use as a chatbot, or as a classifier? So pick the foundational model that matches your usecase, then match the data pile with that model. Next we take the data pile and we tokenize it. Foundation models work with tokens rather than words, and a data pile could resultin potentially",
      "trillions of tokens. And now we can engage the process of training using all of those tokens. This process can take a long time, depending on the size of the model. Large scale foundationmodels can take months with many thousands of GPUs. But, once it’s done, the longest andhighest computational costs are behind us. Stage 3 is \"validate\". When training isfinished we benchmark the model. And this involves running the model and assessing itsperformance against a set of benchmarks that help define",
      "the quality of the model. And then from here we can create a model card that says this is the model I’ve trained andthese are the benchmark scores it has achieved. Now up until this point the main persona thathas performed these tasks is the data scientist. Now Stage 4 is \"tune\", and this is where we bring in thepersona of the application developer. This persona does not need to be an AI expert. They engage withthe model, generating - for example - prompts that elicit good performance from the",
      "model. Theycan provide additional local data to fine tune the model to improve its performance. And this stage is something that you can do in hours or days - much quickerthan building a model from scratch. And now we’re ready for Stage 5, which is to deployment the model. Now thismodel could run as as service offering deployed to a public cloud. Or we could, alternatively, embed the model into anapplication that runs much closer to the edgeof the network. Either way we can continueto iterate and",
      "improve the model over time. Now here at IBM we’ve announced a platformthat enables all 5 of the stages of this workflow. And It’s called watsonx and it’s composed ofthree elements. So we have: watsonx.data, watsonx.governance, and watsonx.ai., and this all built on IBM’shybrid cloud platform which is Red Hat OpenShift. Now Watsonx.data is a modern data lakehouse and establishes connections with the datarepositories that make up the data inStage 1. Watsonx.governance manages the data cards from",
      "Stage 1 and model cards fromStage 3 enabling a collection of fact sheets that ensure a well-governed AI process andlifecycle. And watsonx.ai provides a means for the application developer personato engage with the model in Stage 4. Overall foundation models are changingthe way we build specialized AI models and this 5-stage workflow allows teams tocreate AI and AI-derived applications with greater sophistication while rapidlyspeeding up AI model development."
    ],
    "chunk_count": 11,
    "content_id": "86e32a51-f54f-4086-acd9-fd347e2a9731",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554845"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=ZXiruGOCn9s": {
    "title": "What are Transformers (Machine Learning Model)?",
    "url": "https://www.youtube.com/watch?v=ZXiruGOCn9s",
    "description": "Learn more about Transformers → http://ibm.biz/ML-Transformers\nLearn more about AI → http://ibm.biz/more-about-ai\nCheck out IBM Watson → http://ibm.biz/more-about-watson\n\nTransformers? In this case, we're talking about a machine learning model, and in this video Martin Keen explains what transformers are, what they're good for, and maybe ... what they're not so good at for.\n\nDownload a free AI ebook → http://ibm.biz/ai-ebook-free\nRead about the Journey to AI → http://ibm.biz/ai-journey-blog\n\nGet started for free on IBM Cloud → http://ibm.biz/Bdf7QA\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#AI #Software #ITModernization",
    "duration": 350,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en no it's no it's no it's it it's not those transformers but but it it's not those transformers but but it it's not those transformers but but they can do some pretty cool things let they can do some pretty cool things let they can do some pretty cool things let me show you so me show you so me show you so why did the banana cross the road why did the banana cross the road why did the banana cross the road because it was sick of being mashed because it was sick of being mashed because it was sick of being mashed yeah i'm not sure that i quite get that yeah i'm not sure that i quite get that yeah i'm not sure that i quite get that one and that's because it was created by one and that's because it was created by one and that's because it was created by a computer i literally asked it to tell a computer i literally asked it to tell a computer i literally asked it to tell me a joke me a joke me a joke and this is what it came up with and this is what it came up with and this is what it came up with specifically i used a gpt-3 specifically i used a gpt-3 specifically i used a gpt-3 or a generative pre-trained transformer or a generative pre-trained transformer or a generative pre-trained transformer model the three here means that this is model the three here means that this is model the three here means that this is the third generation the third generation the third generation gpt-3 is an auto-regressive language gpt-3 is an auto-regressive language gpt-3 is an auto-regressive language model that produces text that looks like model that produces text that looks like model that produces text that looks like it was written by a human it was written by a human it was written by a human gpt3 can write poetry craft emails and gpt3 can write poetry craft emails and gpt3 can write poetry craft emails and evidently come up with its own jokes evidently come up with its own jokes evidently come up with its own jokes off you go off you go off you go while our banana joke isn't exactly while our banana joke isn't exactly while our banana joke isn't exactly funny it does fit the typical pattern of funny it does fit the typical pattern of funny it does fit the typical pattern of a joke with a setup and a punch line and a joke with a setup and a punch line and a joke with a setup and a punch line and sort of kind of makes sense i mean who sort of kind of makes sense i mean who sort of kind of makes sense i mean who wouldn't cross the road to avoid getting wouldn't cross the road to avoid getting wouldn't cross the road to avoid getting mashed but look gpt3 is just one example mashed but look gpt3 is just one example mashed but look gpt3 is just one example a transformer something that transforms from one something that transforms from one something that transforms from one sequence into another and language sequence into another and language sequence into another and language translation is just a great example translation is just a great example translation is just a great example perhaps we want to take a sentence of perhaps we want to take a sentence of perhaps we want to take a sentence of why did the banana cross the road and we want to take that and we want to take that and we want to take that english phrase and translate it into english phrase and translate it into english phrase and translate it into french french french well transformers consist of two parts well transformers consist of two parts well transformers consist of two parts there is an encoder and there is a decoder the encoder works on the input the encoder works on the input the encoder works on the input sequence sequence sequence and the and the and the decoder operates on the target decoder operates on the target decoder operates on the target output sequence output sequence output sequence now on the face of it translation seems now on the face of it translation seems now on the face of it translation seems like little more than just like a basic like little more than just like a basic like little more than just like a basic lookup task so lookup task so lookup task so convert the y convert the y convert the y here of our english sentence to the here of our english sentence to the here of our english sentence to the french equivalent of porcua french equivalent of porcua french equivalent of porcua but of course but of course but of course language translation doesn't really work language translation doesn't really work language translation doesn't really work that way things like word order in terms that way things like word order in terms that way things like word order in terms of phrase often mix things up and the of phrase often mix things up and the of phrase often mix things up and the way transformers work is through way transformers work is through way transformers work is through sequence to sequence learning where the sequence to sequence learning where the sequence to sequence learning where the transformer takes a sequence of tokens transformer takes a sequence of tokens transformer takes a sequence of tokens in this case words in a sentence and in this case words in a sentence and in this case words in a sentence and predicts the next word in the output predicts the next word in the output predicts the next word in the output sequence sequence sequence it does this through iterating through it does this through iterating through it does this through iterating through encoder layers so the encoder generates encoder layers so the encoder generates encoder layers so the encoder generates encodings that define which part of the encodings that define which part of the encodings that define which part of the input sequence are relevant to each input sequence are relevant to each input sequence are relevant to each other and then passes these encodings to other and then passes these encodings to other and then passes these encodings to the next encoder layer the decoder takes the next encoder layer the decoder takes the next encoder layer the decoder takes all of these encodings and uses their all of these encodings and uses their all of these encodings and uses their derived context to generate the output derived context to generate the output derived context to generate the output sequence sequence sequence now transformers are a form of semi now transformers are a form of semi now transformers are a form of semi supervised learning by semi sequence semi-supervised we mean by semi sequence semi-supervised we mean by semi sequence semi-supervised we mean that they are pre-trained in an that they are pre-trained in an that they are pre-trained in an unsupervised manner with a large unsupervised manner with a large unsupervised manner with a large unlabeled data set and then they're unlabeled data set and then they're unlabeled data set and then they're fine-tuned through supervised training fine-tuned through supervised training fine-tuned through supervised training to get them to perform better now in to get them to perform better now in to get them to perform better now in previous videos i've talked about other previous videos i've talked about other previous videos i've talked about other machine learning algorithms that handle machine learning algorithms that handle machine learning algorithms that handle sequential input like natural language sequential input like natural language sequential input like natural language for example there are recurrent neural for example there are recurrent neural for example there are recurrent neural networks or rnns networks or rnns networks or rnns what makes transformers a little bit what makes transformers a little bit what makes transformers a little bit different is they do not necessarily different is they do not necessarily different is they do not necessarily process data in order process data in order process data in order transformers use something called an transformers use something called an transformers use something called an attention mechanism and this provides context around items and this provides context around items and this provides context around items in the input sequence so rather than in the input sequence so rather than in the input sequence so rather than starting our translation with the word starting our translation with the word starting our translation with the word why because it's at the start of the why because it's at the start of the why because it's at the start of the sentence the transformer attempts to sentence the transformer attempts to sentence the transformer attempts to identify the context that bring meaning identify the context that bring meaning identify the context that bring meaning in each word in the sequence and it's in each word in the sequence and it's in each word in the sequence and it's this attention mechanism that gives this attention mechanism that gives this attention mechanism that gives transformers a huge leg up over transformers a huge leg up over transformers a huge leg up over algorithms like rnn that must run in algorithms like rnn that must run in algorithms like rnn that must run in sequence sequence sequence transformers run multiple sequences transformers run multiple sequences transformers run multiple sequences parallel parallel parallel and this vastly speeds up training times and this vastly speeds up training times and this vastly speeds up training times so beyond translations what can so beyond translations what can so beyond translations what can transformers be applied to well document transformers be applied to well document transformers be applied to well document summaries they're another great example summaries they're another great example summaries they're another great example you can like feed in a whole article as you can like feed in a whole article as you can like feed in a whole article as the input sequence and then generate an the input sequence and then generate an the input sequence and then generate an output sequence output sequence output sequence that's going to really just be a couple that's going to really just be a couple that's going to really just be a couple of sentences that summarize the main of sentences that summarize the main of sentences that summarize the main points points points transformers can create whole new transformers can create whole new transformers can create whole new documents of their own for example like documents of their own for example like documents of their own for example like write a whole blog post and beyond just write a whole blog post and beyond just write a whole blog post and beyond just language transformers have done things language transformers have done things language transformers have done things like learn to play chess and perform like learn to play chess and perform like learn to play chess and perform image processing that even rivals the image processing that even rivals the image processing that even rivals the capabilities of convolutional neural capabilities of convolutional neural capabilities of convolutional neural networks networks networks look transformers are a powerful deep look transformers are a powerful deep look transformers are a powerful deep learning model and thanks to how the learning model and thanks to how the learning model and thanks to how the attention mechanism can be paralyzed are attention mechanism can be paralyzed are attention mechanism can be paralyzed are getting better all the time and who getting better all the time and who getting better all the time and who knows pretty soon maybe they'll even be knows pretty soon maybe they'll even be knows pretty soon maybe they'll even be able to pull off banana jokes that able to pull off banana jokes that able to pull off banana jokes that are actually funny if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching",
    "chunks": [
      "Kind: captions Language: en no it's no it's no it's it it's not those transformers but but it it's not those transformers but but it it's not those transformers but but they can do some pretty cool things let they can do some pretty cool things let they can do some pretty cool things let me show you so me show you so me show you so why did the banana cross the road why did the banana cross the road why did the banana cross the road because it was sick of being mashed because it was sick of being",
      "mashed because it was sick of being mashed yeah i'm not sure that i quite get that yeah i'm not sure that i quite get that yeah i'm not sure that i quite get that one and that's because it was created by one and that's because it was created by one and that's because it was created by a computer i literally asked it to tell a computer i literally asked it to tell a computer i literally asked it to tell me a joke me a joke me a joke and this is what it came up with and this is what it came up with",
      "and this is what it came up with specifically i used a gpt-3 specifically i used a gpt-3 specifically i used a gpt-3 or a generative pre-trained transformer or a generative pre-trained transformer or a generative pre-trained transformer model the three here means that this is model the three here means that this is model the three here means that this is the third generation the third generation the third generation gpt-3 is an auto-regressive language gpt-3 is an auto-regressive language gpt-3",
      "is an auto-regressive language model that produces text that looks like model that produces text that looks like model that produces text that looks like it was written by a human it was written by a human it was written by a human gpt3 can write poetry craft emails and gpt3 can write poetry craft emails and gpt3 can write poetry craft emails and evidently come up with its own jokes evidently come up with its own jokes evidently come up with its own jokes off you go off you go off you go while",
      "our banana joke isn't exactly while our banana joke isn't exactly while our banana joke isn't exactly funny it does fit the typical pattern of funny it does fit the typical pattern of funny it does fit the typical pattern of a joke with a setup and a punch line and a joke with a setup and a punch line and a joke with a setup and a punch line and sort of kind of makes sense i mean who sort of kind of makes sense i mean who sort of kind of makes sense i mean who wouldn't cross the road to avoid",
      "getting wouldn't cross the road to avoid getting wouldn't cross the road to avoid getting mashed but look gpt3 is just one example mashed but look gpt3 is just one example mashed but look gpt3 is just one example a transformer something that transforms from one something that transforms from one something that transforms from one sequence into another and language sequence into another and language sequence into another and language translation is just a great example translation is just a great",
      "example translation is just a great example perhaps we want to take a sentence of perhaps we want to take a sentence of perhaps we want to take a sentence of why did the banana cross the road and we want to take that and we want to take that and we want to take that english phrase and translate it into english phrase and translate it into english phrase and translate it into french french french well transformers consist of two parts well transformers consist of two parts well transformers",
      "consist of two parts there is an encoder and there is a decoder the encoder works on the input the encoder works on the input the encoder works on the input sequence sequence sequence and the and the and the decoder operates on the target decoder operates on the target decoder operates on the target output sequence output sequence output sequence now on the face of it translation seems now on the face of it translation seems now on the face of it translation seems like little more than just like",
      "a basic like little more than just like a basic like little more than just like a basic lookup task so lookup task so lookup task so convert the y convert the y convert the y here of our english sentence to the here of our english sentence to the here of our english sentence to the french equivalent of porcua french equivalent of porcua french equivalent of porcua but of course but of course but of course language translation doesn't really work language translation doesn't really work language",
      "translation doesn't really work that way things like word order in terms that way things like word order in terms that way things like word order in terms of phrase often mix things up and the of phrase often mix things up and the of phrase often mix things up and the way transformers work is through way transformers work is through way transformers work is through sequence to sequence learning where the sequence to sequence learning where the sequence to sequence learning where the transformer",
      "takes a sequence of tokens transformer takes a sequence of tokens transformer takes a sequence of tokens in this case words in a sentence and in this case words in a sentence and in this case words in a sentence and predicts the next word in the output predicts the next word in the output predicts the next word in the output sequence sequence sequence it does this through iterating through it does this through iterating through it does this through iterating through encoder layers so the encoder",
      "generates encoder layers so the encoder generates encoder layers so the encoder generates encodings that define which part of the encodings that define which part of the encodings that define which part of the input sequence are relevant to each input sequence are relevant to each input sequence are relevant to each other and then passes these encodings to other and then passes these encodings to other and then passes these encodings to the next encoder layer the decoder takes the next encoder",
      "layer the decoder takes the next encoder layer the decoder takes all of these encodings and uses their all of these encodings and uses their all of these encodings and uses their derived context to generate the output derived context to generate the output derived context to generate the output sequence sequence sequence now transformers are a form of semi now transformers are a form of semi now transformers are a form of semi supervised learning by semi sequence semi-supervised we mean by semi",
      "sequence semi-supervised we mean by semi sequence semi-supervised we mean that they are pre-trained in an that they are pre-trained in an that they are pre-trained in an unsupervised manner with a large unsupervised manner with a large unsupervised manner with a large unlabeled data set and then they're unlabeled data set and then they're unlabeled data set and then they're fine-tuned through supervised training fine-tuned through supervised training fine-tuned through supervised training to get",
      "them to perform better now in to get them to perform better now in to get them to perform better now in previous videos i've talked about other previous videos i've talked about other previous videos i've talked about other machine learning algorithms that handle machine learning algorithms that handle machine learning algorithms that handle sequential input like natural language sequential input like natural language sequential input like natural language for example there are recurrent neural",
      "for example there are recurrent neural for example there are recurrent neural networks or rnns networks or rnns networks or rnns what makes transformers a little bit what makes transformers a little bit what makes transformers a little bit different is they do not necessarily different is they do not necessarily different is they do not necessarily process data in order process data in order process data in order transformers use something called an transformers use something called an",
      "transformers use something called an attention mechanism and this provides context around items and this provides context around items and this provides context around items in the input sequence so rather than in the input sequence so rather than in the input sequence so rather than starting our translation with the word starting our translation with the word starting our translation with the word why because it's at the start of the why because it's at the start of the why because it's at the",
      "start of the sentence the transformer attempts to sentence the transformer attempts to sentence the transformer attempts to identify the context that bring meaning identify the context that bring meaning identify the context that bring meaning in each word in the sequence and it's in each word in the sequence and it's in each word in the sequence and it's this attention mechanism that gives this attention mechanism that gives this attention mechanism that gives transformers a huge leg up over",
      "transformers a huge leg up over transformers a huge leg up over algorithms like rnn that must run in algorithms like rnn that must run in algorithms like rnn that must run in sequence sequence sequence transformers run multiple sequences transformers run multiple sequences transformers run multiple sequences parallel parallel parallel and this vastly speeds up training times and this vastly speeds up training times and this vastly speeds up training times so beyond translations what can so beyond",
      "translations what can so beyond translations what can transformers be applied to well document transformers be applied to well document transformers be applied to well document summaries they're another great example summaries they're another great example summaries they're another great example you can like feed in a whole article as you can like feed in a whole article as you can like feed in a whole article as the input sequence and then generate an the input sequence and then generate an the",
      "input sequence and then generate an output sequence output sequence output sequence that's going to really just be a couple that's going to really just be a couple that's going to really just be a couple of sentences that summarize the main of sentences that summarize the main of sentences that summarize the main points points points transformers can create whole new transformers can create whole new transformers can create whole new documents of their own for example like documents of their own",
      "for example like documents of their own for example like write a whole blog post and beyond just write a whole blog post and beyond just write a whole blog post and beyond just language transformers have done things language transformers have done things language transformers have done things like learn to play chess and perform like learn to play chess and perform like learn to play chess and perform image processing that even rivals the image processing that even rivals the image processing",
      "that even rivals the capabilities of convolutional neural capabilities of convolutional neural capabilities of convolutional neural networks networks networks look transformers are a powerful deep look transformers are a powerful deep look transformers are a powerful deep learning model and thanks to how the learning model and thanks to how the learning model and thanks to how the attention mechanism can be paralyzed are attention mechanism can be paralyzed are attention mechanism can be",
      "paralyzed are getting better all the time and who getting better all the time and who getting better all the time and who knows pretty soon maybe they'll even be knows pretty soon maybe they'll even be knows pretty soon maybe they'll even be able to pull off banana jokes that able to pull off banana jokes that able to pull off banana jokes that are actually funny if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below",
      "and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching"
    ],
    "chunk_count": 25,
    "content_id": "b15ec046-f6d1-4767-8bcb-d21efd566739",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554848"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=0Wwn5IEqFcg": {
    "title": "Small vs. Large AI Models: Trade-offs & Use Cases Explained",
    "url": "https://www.youtube.com/watch?v=0Wwn5IEqFcg",
    "description": "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnuNp\n\nLearn more about Small Language Models here → https://ibm.biz/BdnuNg\n\nLearn more about Large Language Models (LLMs) here → https://ibm.biz/BdnuNA\n\nBig or small? 🤖 Martin Keen explores trade-offs between small AI models like Mistral 7B and massive LLMs with billions of parameters. Learn how benchmarks like MMLU and use cases like on-device AI and multilingual translation are reshaping AI innovation with smarter, efficient models!\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnuN9\n\n#aimodels #llm #machinelearning",
    "duration": 571,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en The first L in LLM stands for large. But how large is large? Well, today's language models cover a huge range of sizes, from lightweight networks that have maybe 300 million parameters that can run entirely on a smartphone to titanic systems with hundreds of billions, or perhaps even approaching a trillion parameters that require racks of GPUs in a hyperscale data center. And yeah, size in this context, it is measured in parameters. That's how we measure the size of an LLM and parameters are the individual floating point weights that a neural network tweaks while it trains. And collectively these parameters encode everything the model can recall or reason about. Well let's talk about some specific models. So for example Mistral 7B that is an example of a small model, the seven B there that says it contains roughly 7 billion of those weights or those parameters. By comparison. And we could take a look at llama three for example from meta. Now this one is a much bigger model.. 400B. So we would put this in the large LLM category. And in fact some frontier models there much bigger than that. The room to push well beyond half a trillion parameters. And in broad strokes extra parameters buys extra capability. Larger models have more room to memorize more facts and support more languages and carry out more intricate chains of reasoning. But the trade off, of course, with these guys is cost. They demand exponentially more compute and energy and memory, both to train them in the first place and then to run them in production. So the story isn't simply bigger is better. Smaller models are catching up and are punching far above their weight class. And let me give you an example. Well, we measure progress in language model capability with benchmarks. And one of the most enduring benchmarks is the m m l u. That's massive multitask language understanding. Now the MMLU it contains more than 15,000 multiple choice questions across all sorts of domains sub subjects like math and history and law and medicine and anybody taking the test needs to combine both factual recall with problem solving across many fields. So the test is a convenient, if somewhat imperfect snapshot of kind of broad general purpose ability. Now, if you took the MMLU, you and you were just guessing at random, you would score around 25% on the test. But if you weren't guessing at random, if you're just kind of a regular Joe, just a regular human, and you took the test, you might score somewhere around 35%. It's a it's a pretty hard test, but what about a domain expert? Well, a domain expert would score far higher, something like around 90% on questions that are within their specialty. So that's humans. What about AI models? Well, when GPT three came out in 2020, this is a 175 billion parameter model. It posted a score on the MMLU view of 44%. I mean, that's pretty respectable. It's better than the average Joe, but it's far from mastery. What about today's models? Well, if we take a look at today's frontier models, kind of the best models we have, they can score in the high 80s, maybe 88% on the test. But let's use a different benchmark. Let's use a benchmark of 60%. And we can say that is a practical cutoff because above that line, a model begins to look like a like a competent generalist that can answer everyday questions. And what is striking is how quickly that 60% barrier has fallen to ever smaller models. So in February of 2023, the smallest model that could score above 60% was Llama 1-65B 65 B, meaning 65 billion parameters. But just a few months later, by July of the same year, Llama 2 - 34B. They could do it with barely half the parameters. Then if we fast forward to September of the same year that saw Misteral 7B join the cloud, which we know is a 7 billion parameter model, and then in March of 2024, Qwen 1.5 MOE became the first model with fewer than 3 billion active parameters to clear 60%. In other words, month by month, we are learning to squeeze competent generalist behavior into smaller and smaller footprints. So smaller models are getting smarter. And I think the next natural question becomes which model should I put into production, large or small? And the answer, of course, depends on your workload, your latency, your privacy constraints. And let's be honest, the size of your GPU budget. Now I'm generalizing here. Your case may be different, but certain tasks do still reward sheer scale. So let's talk about some large model use cases. And one of the first really comes down broad spectrum code generation. So a small model can master a handful of programing languages. But a a frontier model has room for dozens of ecosystems and can reason across multi file projects and unfamiliar APIs and weird edge cases. Another good example is when you have document heavy work that you need to process. So we might need to ingest a very large contract and a medical guideline and a technical standard. And a large model's longer context window means it can keep more of the source text in mind, reducing hallucinations and improving citation quality. And the same scale advantage appears in high fidelity multilingual translation as well, where we're going from one language to another, and the extra parameters that the network carve out richer subspaces for each language. Finally, capturing idioms and nuance that smaller models might kind of gloss over. But look, there are some cases where small models are not only good enough, but they are outright preferable. So let's talk about some of those use cases. And one of those comes down to on device a AI. So keyboard prediction or voice commands that offline search that stuff lives or dies by sub 100 millisecond latency and strict data privacy and small models that run on device. Well, they're great for that. Also, when it just comes down to everyday summarization, that's another sweet spot. In an in news summarization study, Mistral 7B instruct achieved ROGUE and Bert score metrics that were statistically indistinguishable from a much larger model GPT 3.5 turbo. And that's despite the model running 30 times cheaper and faster. And another good use case comes down to enterprise chat bots. So with these, a business can fine tune a seven or a 13 billion parameter model on its own manuals, and it can reach near expert accuracy. And IBM found that the the granite 13 B family match the performance of models that were five times larger on typical enterprise Q and A task. So the rule of thumb is for expansive, open ended reasoning. Bigger does still buy more headroom for focused skills like summarizing and classifying. A carefully trained small model delivers perhaps 90% of the quality at a fraction of the cost. So go big. Stay small. In the end, it's your use case that will drive the decision.",
    "chunks": [
      "Kind: captions Language: en The first L in LLM stands for large. But how large is large? Well, today's language models cover a huge range of sizes, from lightweight networks that have maybe 300 million parameters that can run entirely on a smartphone to titanic systems with hundreds of billions, or perhaps even approaching a trillion parameters that require racks of GPUs in a hyperscale data center. And yeah, size in this context, it is measured in parameters. That's how we measure the size of",
      "an LLM and parameters are the individual floating point weights that a neural network tweaks while it trains. And collectively these parameters encode everything the model can recall or reason about. Well let's talk about some specific models. So for example Mistral 7B that is an example of a small model, the seven B there that says it contains roughly 7 billion of those weights or those parameters. By comparison. And we could take a look at llama three for example from meta. Now this one is a",
      "much bigger model.. 400B. So we would put this in the large LLM category. And in fact some frontier models there much bigger than that. The room to push well beyond half a trillion parameters. And in broad strokes extra parameters buys extra capability. Larger models have more room to memorize more facts and support more languages and carry out more intricate chains of reasoning. But the trade off, of course, with these guys is cost. They demand exponentially more compute and energy and memory,",
      "both to train them in the first place and then to run them in production. So the story isn't simply bigger is better. Smaller models are catching up and are punching far above their weight class. And let me give you an example. Well, we measure progress in language model capability with benchmarks. And one of the most enduring benchmarks is the m m l u. That's massive multitask language understanding. Now the MMLU it contains more than 15,000 multiple choice questions across all sorts of domains",
      "sub subjects like math and history and law and medicine and anybody taking the test needs to combine both factual recall with problem solving across many fields. So the test is a convenient, if somewhat imperfect snapshot of kind of broad general purpose ability. Now, if you took the MMLU, you and you were just guessing at random, you would score around 25% on the test. But if you weren't guessing at random, if you're just kind of a regular Joe, just a regular human, and you took the test, you",
      "might score somewhere around 35%. It's a it's a pretty hard test, but what about a domain expert? Well, a domain expert would score far higher, something like around 90% on questions that are within their specialty. So that's humans. What about AI models? Well, when GPT three came out in 2020, this is a 175 billion parameter model. It posted a score on the MMLU view of 44%. I mean, that's pretty respectable. It's better than the average Joe, but it's far from mastery. What about today's models?",
      "Well, if we take a look at today's frontier models, kind of the best models we have, they can score in the high 80s, maybe 88% on the test. But let's use a different benchmark. Let's use a benchmark of 60%. And we can say that is a practical cutoff because above that line, a model begins to look like a like a competent generalist that can answer everyday questions. And what is striking is how quickly that 60% barrier has fallen to ever smaller models. So in February of 2023, the smallest model",
      "that could score above 60% was Llama 1-65B 65 B, meaning 65 billion parameters. But just a few months later, by July of the same year, Llama 2 - 34B. They could do it with barely half the parameters. Then if we fast forward to September of the same year that saw Misteral 7B join the cloud, which we know is a 7 billion parameter model, and then in March of 2024, Qwen 1.5 MOE became the first model with fewer than 3 billion active parameters to clear 60%. In other words, month by month, we are",
      "learning to squeeze competent generalist behavior into smaller and smaller footprints. So smaller models are getting smarter. And I think the next natural question becomes which model should I put into production, large or small? And the answer, of course, depends on your workload, your latency, your privacy constraints. And let's be honest, the size of your GPU budget. Now I'm generalizing here. Your case may be different, but certain tasks do still reward sheer scale. So let's talk about some",
      "large model use cases. And one of the first really comes down broad spectrum code generation. So a small model can master a handful of programing languages. But a a frontier model has room for dozens of ecosystems and can reason across multi file projects and unfamiliar APIs and weird edge cases. Another good example is when you have document heavy work that you need to process. So we might need to ingest a very large contract and a medical guideline and a technical standard. And a large model's",
      "longer context window means it can keep more of the source text in mind, reducing hallucinations and improving citation quality. And the same scale advantage appears in high fidelity multilingual translation as well, where we're going from one language to another, and the extra parameters that the network carve out richer subspaces for each language. Finally, capturing idioms and nuance that smaller models might kind of gloss over. But look, there are some cases where small models are not only",
      "good enough, but they are outright preferable. So let's talk about some of those use cases. And one of those comes down to on device a AI. So keyboard prediction or voice commands that offline search that stuff lives or dies by sub 100 millisecond latency and strict data privacy and small models that run on device. Well, they're great for that. Also, when it just comes down to everyday summarization, that's another sweet spot. In an in news summarization study, Mistral 7B instruct achieved ROGUE",
      "and Bert score metrics that were statistically indistinguishable from a much larger model GPT 3.5 turbo. And that's despite the model running 30 times cheaper and faster. And another good use case comes down to enterprise chat bots. So with these, a business can fine tune a seven or a 13 billion parameter model on its own manuals, and it can reach near expert accuracy. And IBM found that the the granite 13 B family match the performance of models that were five times larger on typical enterprise",
      "Q and A task. So the rule of thumb is for expansive, open ended reasoning. Bigger does still buy more headroom for focused skills like summarizing and classifying. A carefully trained small model delivers perhaps 90% of the quality at a fraction of the cost. So go big. Stay small. In the end, it's your use case that will drive the decision."
    ],
    "chunk_count": 14,
    "content_id": "53318fd5-ae77-4ffc-9683-b5216fa0d3ba",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554893"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=QPQy7jUpmyA&t=1s": {
    "title": "Why Are There So Many Foundation Models?",
    "url": "https://www.youtube.com/watch?v=QPQy7jUpmyA&t=1s",
    "description": "Check out watsonx: hhttps://ibm.biz/BdvyLa\n\nThere are a lot of foundation models available, and more being released all the time, but why is that? Why do we need so many different models? Martin Keen explores the reason by demonstrating the value of one of these many models, the \"IBM NASA Geospatial Model.\"",
    "duration": 313,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en If you head over to Huggingface,  you will find literally thousands of foundation models available for download. And that's just the open source ones. So this does beg the question, why are there so many foundation models? Well, to help answer that, we're going to look to NASA. But first, we should probably define what a foundation model actually is. And look, I have a whole video on that topic. So for now, let's just say that foundation models, which is what this represents here, are large scale neural networks trained on vast amounts of data, and they serve as a base or a \"foundation\" for a multitude of applications. And a foundation model can apply information. It's learned about one situation to a different situation it was not trained on. And we call that transfer learning. Pre-trained a foundation model, and you can teach it an entirely new task with a limited set of hand-labeled examples. So if we pick a foundation model that has ingested the right data and we provide the right fine tuning, we can put it to work in our own specific applications. Which brings us to NASA. If you're looking for huge amounts of data, well, look no further than NASA. Today we are sitting on about 70 petabytes of earth science data captured from satellite images, which which sounds like a lot, but by 2030, with the launch of a dozen or so new space missions, that number is expected to be closer to 300 petabytes of data. So we have a vast, vast amount of data. and we may be able to use that to provide insights to, well, all sorts of climate-related discoveries. But how can we possibly utilize it? Well, through a foundation model, of course! Now, for the last six months, NASA has been working with IBM to create an AI foundation model for Earth observations. And now you and, well, anybody who wants it can download the whole thing. It's called the \"IBM NASA Geospatial model,\" and it's an open source model available on Huggingface. And this geospatial foundation model really does help us to answer the question of why are there so many foundation models? Look, underpinning all foundation models is the concept of a transformer. That's an AI architecture that can turn heaps of raw data, be that text or audio, or in this case, satellite images into a compressed representation that captures the data's basic structure. That represents this. And then we can use this with a foundation model for a wide variety of tasks with some extra label data and tuning. Now, look, traditionally, analyzing satellite data like this has been a tedious process because of the time required for human experts to annotate features. So in each satellite image we label-- let's see this group of pixels, that's crops --and then we would say this group of pixels. Yeah, that's trees and so forth. And a human is having to go through this takes a lot of time. So foundation models can cut out a lot of this manual effort by extracting the structure of raw natural images so that fewer label examples are needed. Then the foundation model has been fine tuned to allow users to map the extent of past US floods and wildfires. Why do that? Because these measurements then can be used to predict future areas of risk. So we have a flood and wildfire prediction model. Pretty cool. But look, foundation models are well, foundational. We can take that model and apply our own fine tuning to build upon the model to perform different tasks entirely. So with additional fine tuning, our flood and wildfire prediction model can be redeployed for tasks like tracking deforestation or predicting crop yields, or even looking at detecting and monitoring greenhouse gases. In fact, Clark University are adapting this very model for other applications, including time series segmentation and similarity search. So in this case, foundation models are multiplying the usefulness of NASA data where fine tuning can adapt these models to new use cases. And look, that's just NASA Earth science data. Those thousands of open source foundation models that I mentioned at the beginning, they are trained and tuned on a wide variety of other data, like code generation or foundation models related to a specific industry. So by selecting the right foundation model and adapting it, we can put that model to work in new ways to meet our needs. And that is why there are so many foundation models available and why there are so many more to come.",
    "chunks": [
      "Kind: captions Language: en If you head over to Huggingface, you will find literally thousands of foundation models available for download. And that's just the open source ones. So this does beg the question, why are there so many foundation models? Well, to help answer that, we're going to look to NASA. But first, we should probably define what a foundation model actually is. And look, I have a whole video on that topic. So for now, let's just say that foundation models, which is what this",
      "represents here, are large scale neural networks trained on vast amounts of data, and they serve as a base or a \"foundation\" for a multitude of applications. And a foundation model can apply information. It's learned about one situation to a different situation it was not trained on. And we call that transfer learning. Pre-trained a foundation model, and you can teach it an entirely new task with a limited set of hand-labeled examples. So if we pick a foundation model that has ingested the right",
      "data and we provide the right fine tuning, we can put it to work in our own specific applications. Which brings us to NASA. If you're looking for huge amounts of data, well, look no further than NASA. Today we are sitting on about 70 petabytes of earth science data captured from satellite images, which which sounds like a lot, but by 2030, with the launch of a dozen or so new space missions, that number is expected to be closer to 300 petabytes of data. So we have a vast, vast amount of data. and",
      "we may be able to use that to provide insights to, well, all sorts of climate-related discoveries. But how can we possibly utilize it? Well, through a foundation model, of course! Now, for the last six months, NASA has been working with IBM to create an AI foundation model for Earth observations. And now you and, well, anybody who wants it can download the whole thing. It's called the \"IBM NASA Geospatial model,\" and it's an open source model available on Huggingface. And this geospatial",
      "foundation model really does help us to answer the question of why are there so many foundation models? Look, underpinning all foundation models is the concept of a transformer. That's an AI architecture that can turn heaps of raw data, be that text or audio, or in this case, satellite images into a compressed representation that captures the data's basic structure. That represents this. And then we can use this with a foundation model for a wide variety of tasks with some extra label data and",
      "tuning. Now, look, traditionally, analyzing satellite data like this has been a tedious process because of the time required for human experts to annotate features. So in each satellite image we label-- let's see this group of pixels, that's crops --and then we would say this group of pixels. Yeah, that's trees and so forth. And a human is having to go through this takes a lot of time. So foundation models can cut out a lot of this manual effort by extracting the structure of raw natural images",
      "so that fewer label examples are needed. Then the foundation model has been fine tuned to allow users to map the extent of past US floods and wildfires. Why do that? Because these measurements then can be used to predict future areas of risk. So we have a flood and wildfire prediction model. Pretty cool. But look, foundation models are well, foundational. We can take that model and apply our own fine tuning to build upon the model to perform different tasks entirely. So with additional fine",
      "tuning, our flood and wildfire prediction model can be redeployed for tasks like tracking deforestation or predicting crop yields, or even looking at detecting and monitoring greenhouse gases. In fact, Clark University are adapting this very model for other applications, including time series segmentation and similarity search. So in this case, foundation models are multiplying the usefulness of NASA data where fine tuning can adapt these models to new use cases. And look, that's just NASA Earth",
      "science data. Those thousands of open source foundation models that I mentioned at the beginning, they are trained and tuned on a wide variety of other data, like code generation or foundation models related to a specific industry. So by selecting the right foundation model and adapting it, we can put that model to work in new ways to meet our needs. And that is why there are so many foundation models available and why there are so many more to come."
    ],
    "chunk_count": 9,
    "content_id": "fc24cea6-be6d-47cf-a65c-e99373e16c2d",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554899"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=W01tIRP_Rqs": {
    "title": "Supervised vs. Unsupervised Learning",
    "url": "https://www.youtube.com/watch?v=W01tIRP_Rqs",
    "description": "Learn more about WatsonX: https://ibm.biz/BdPuCJ\n\nMore about supervised & unsupervised learning → https://ibm.biz/Blog-Supervised-vs-Unsupervised\nLearn about IBM Watson Studio → https://ibm.biz/learn-watson-studio\nExplore: IBM Cloud Pak for Data → https://ibm.biz/explore-pak-for-data\n\nWhat's the best type of machine learning model for you - supervised or Unsupervised learning?\nIn this video, Martin Keen explains what the difference is between these 2 types, the pros and cons of each, and ... presents a 3rd possibility.\n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-tier\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#AI #Software #ITModernization #IBM #MachineLearning #ml #watsonX",
    "duration": 428,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Supervised and unsupervised learning are two corecomponents in building machine learning models. So what's the difference? Well, just to cut to the chase: supervised learning, that uses labeled input and output data, while an unsupervised learning model doesn't. But what does that really mean? Well, let's better define both learning models, go deeper into the differences between them andthen answer the question of which is best for you. Now, in supervised learning,the machine learning algorithm istrained on a labeled dataset. So this meansthat each example in the training dataset, the algorithm knows what the correct output is. And the algorithm uses this knowledge to try to generalize to new examples that it's never seenbefore. Now, using labeled inputs and outputs, the model can measure its accuracy and learn overtime. Supervised learning can be actually divided into a couple of subcategories. Firstly,there is a category of classification. And classification talks about whether the outputis a discrete class label such as \"spam\" and \"not spam\". Linear classifiers, support vectormachines, or SPMs, decision trees, random forests - they're all common examples of classificationalgorithms. The other example is regression. The output here is a continuousvalue, such as price or probability. Linear regression and logistic regression aretwo common types of regression algorithms. Now, unsupervised learning is where the machinelearning algorithm is not really given any labelsat all. And these algorithms discover hiddenpatterns in data without the need for human intervention. They're unsupervised. Unsupervised learning models are used for three main tasks, such as clustering, association and dimensionalityreduction. So let's take a look at each one of those, starting with clustering. Now clusteringis where the algorithm groups similar experiences together. So a common application ofclustering is customer segmentation, where businesses might group customers togetherbased on similarities like, I don't know, age or location or spending habits, somethinglike that. Then you have association. And association is where the algorithm looksfor relationships between variables in the data. Now association rules are often used in marketbasket analysis, where businesses want to knowwhich items are often bought together. You know,something along the lines of, \"customers who boughtthis item also bought \",  that sort of thing. The final one to talk about is dimensional ... dimensional reduction. And this is where thealgorithm reduces the number of variables in the data, while still preserving as much ofthe information as possible. Now, often this technique is used in the pre-processing datastage, such as when autoencoders remove noisefrom visual images to improve picture quality. Okay, so let's talk about the differences between these two types of learning. In supervisedlearning, the algorithm learns from training datasets by iteratively making predictions on thedata and then adjusting for the correct answer. While supervised learning models tend to bemore accurate than unsupervised learning models, they do require all of this up-front humanintervention to label the data appropriately. For example, a supervised learning modelcan predict how long your commute will be on the time of day and thinking aboutthe weather conditions and so forth. But first you'll have to train it to know thingslike rainy weather extends the driving time. By contrast, unsupervised learning models workon their own to discover the inherent structureof unlabeled data. These models don't needhumans to intervene. They can automatically find patterns in data and group them together. So, for example, an unsupervised learning model can cluster images by the objects they contain - things like people and animals and buildings - without being told what those objects were aheadof time. Now, an important distinction to make is that unsupervised learning models don't makepredictions. They only group data together. So if you were to use an unsupervised learning modelon that same commute dataset, it would group together commutes with similar conditions like the time of day and the weather, but it wouldn't be able to predict how long eachcommute would take. Okay, so which of these twooptions is right for you? In general, supervisedlearning is more commonly used than unsupervised learning, and that's really because it's moreaccurate and efficient. But that being said, unsupervised learning has its own advantages.There's two that I can think of. Firstly, unsupervised learning can be used on data that isnot labeled, which is often the case in real world datasets. And then secondly, unsupervised learningcan be used to find hidden patterns in data that supervised learning models just wouldn't find. Classifying big data can be a real challenge in supervised learning, but the results are highlyaccurate and trustworthy. And in contrast, unsupervised learning can handle large volumesof data in real time. But there's a lack of transparency into how that data is clustered anda high risk given accurate results. But wait, it is not an \"either/or\" choice. May I presentto you the middle ground known as semi-supervised learning. This is, well, a happy medium whereyou use a training data set with both labeled and unlabeled data. And it's particularly useful whenit's difficult to extract relevant features from data when you have a high volume of data. So, for example, you could use a semi-supervised learning algorithm on a data set with millions ofimages where only a few thousand of those imagesare actually labeled. Semi-supervisedlearning is ideal for medical images, where a small amount of training data could leadto a significant improvement in accuracy. For example, a radiologist can look at and label somesmall subset of CT scans for tumors or diseases, and then the machine can more accuratelypredict which patients might require more medical attention without going throughand labeling the entire set. Machine learning models are a powerful way to gainthe data insights that improve our world. The right model for your data depends on thetype of data that you have and what you want to do with it. And the choice between supervised andunsupervised learning is only the first step. If you have any questions, please drop us a line below. And if you want to see more videos like this in the future, please like and subscribe. Thanks for watching!",
    "chunks": [
      "Kind: captions Language: en Supervised and unsupervised learning are two corecomponents in building machine learning models. So what's the difference? Well, just to cut to the chase: supervised learning, that uses labeled input and output data, while an unsupervised learning model doesn't. But what does that really mean? Well, let's better define both learning models, go deeper into the differences between them andthen answer the question of which is best for you. Now, in supervised learning,the",
      "machine learning algorithm istrained on a labeled dataset. So this meansthat each example in the training dataset, the algorithm knows what the correct output is. And the algorithm uses this knowledge to try to generalize to new examples that it's never seenbefore. Now, using labeled inputs and outputs, the model can measure its accuracy and learn overtime. Supervised learning can be actually divided into a couple of subcategories. Firstly,there is a category of classification. And classification",
      "talks about whether the outputis a discrete class label such as \"spam\" and \"not spam\". Linear classifiers, support vectormachines, or SPMs, decision trees, random forests - they're all common examples of classificationalgorithms. The other example is regression. The output here is a continuousvalue, such as price or probability. Linear regression and logistic regression aretwo common types of regression algorithms. Now, unsupervised learning is where the machinelearning algorithm is not really",
      "given any labelsat all. And these algorithms discover hiddenpatterns in data without the need for human intervention. They're unsupervised. Unsupervised learning models are used for three main tasks, such as clustering, association and dimensionalityreduction. So let's take a look at each one of those, starting with clustering. Now clusteringis where the algorithm groups similar experiences together. So a common application ofclustering is customer segmentation, where businesses might group",
      "customers togetherbased on similarities like, I don't know, age or location or spending habits, somethinglike that. Then you have association. And association is where the algorithm looksfor relationships between variables in the data. Now association rules are often used in marketbasket analysis, where businesses want to knowwhich items are often bought together. You know,something along the lines of, \"customers who boughtthis item also bought \", that sort of thing. The final one to talk about",
      "is dimensional ... dimensional reduction. And this is where thealgorithm reduces the number of variables in the data, while still preserving as much ofthe information as possible. Now, often this technique is used in the pre-processing datastage, such as when autoencoders remove noisefrom visual images to improve picture quality. Okay, so let's talk about the differences between these two types of learning. In supervisedlearning, the algorithm learns from training datasets by iteratively making",
      "predictions on thedata and then adjusting for the correct answer. While supervised learning models tend to bemore accurate than unsupervised learning models, they do require all of this up-front humanintervention to label the data appropriately. For example, a supervised learning modelcan predict how long your commute will be on the time of day and thinking aboutthe weather conditions and so forth. But first you'll have to train it to know thingslike rainy weather extends the driving time. By",
      "contrast, unsupervised learning models workon their own to discover the inherent structureof unlabeled data. These models don't needhumans to intervene. They can automatically find patterns in data and group them together. So, for example, an unsupervised learning model can cluster images by the objects they contain - things like people and animals and buildings - without being told what those objects were aheadof time. Now, an important distinction to make is that unsupervised learning models",
      "don't makepredictions. They only group data together. So if you were to use an unsupervised learning modelon that same commute dataset, it would group together commutes with similar conditions like the time of day and the weather, but it wouldn't be able to predict how long eachcommute would take. Okay, so which of these twooptions is right for you? In general, supervisedlearning is more commonly used than unsupervised learning, and that's really because it's moreaccurate and efficient. But that",
      "being said, unsupervised learning has its own advantages.There's two that I can think of. Firstly, unsupervised learning can be used on data that isnot labeled, which is often the case in real world datasets. And then secondly, unsupervised learningcan be used to find hidden patterns in data that supervised learning models just wouldn't find. Classifying big data can be a real challenge in supervised learning, but the results are highlyaccurate and trustworthy. And in contrast, unsupervised",
      "learning can handle large volumesof data in real time. But there's a lack of transparency into how that data is clustered anda high risk given accurate results. But wait, it is not an \"either/or\" choice. May I presentto you the middle ground known as semi-supervised learning. This is, well, a happy medium whereyou use a training data set with both labeled and unlabeled data. And it's particularly useful whenit's difficult to extract relevant features from data when you have a high volume of data.",
      "So, for example, you could use a semi-supervised learning algorithm on a data set with millions ofimages where only a few thousand of those imagesare actually labeled. Semi-supervisedlearning is ideal for medical images, where a small amount of training data could leadto a significant improvement in accuracy. For example, a radiologist can look at and label somesmall subset of CT scans for tumors or diseases, and then the machine can more accuratelypredict which patients might require more",
      "medical attention without going throughand labeling the entire set. Machine learning models are a powerful way to gainthe data insights that improve our world. The right model for your data depends on thetype of data that you have and what you want to do with it. And the choice between supervised andunsupervised learning is only the first step. If you have any questions, please drop us a line below. And if you want to see more videos like this in the future, please like and subscribe. Thanks for",
      "watching!"
    ],
    "chunk_count": 14,
    "content_id": "4b9c6709-9793-4ac9-b2a5-8f8c455804b0",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554903"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=S5AGN9XfPK4": {
    "title": "What is Back Propagation",
    "url": "https://www.youtube.com/watch?v=S5AGN9XfPK4",
    "description": "Learn about watsonx→ https://ibm.biz/BdyEjK\n\nNeural networks are great for predictive modeling — everything from stock trends to language translations. But what if the answer is wrong, how do they “learn” to do better? Martin Keen explains that during a process called backward propagation, the generated output is compared to the expected output, and then the error contributed by each neuron (or “node”) is examined. By adjusting the node’s weights and biases, error is reduced and thus the overall accuracy improved.\n\nGet started for free on IBM Cloud → https://ibm.biz/sign-up-now\n\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now",
    "duration": 479,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en We're going to take a look at back propagation. It's central to the functioning of neural networks, helping them to learn and adapt. And we're going to cover it in simple but instructive terms. So even if your only knowledge of neural networks is \"Isn't that something to do with chatGPT?\" Well, we've got you covered. Now, a neural network fundamentally comprises multiple layers of neurons interconnected by weights. So I'm going to draw some neurons here, and I'm organizing them in layers. And these neurons are also known as nodes. Now, the layers here are categorized. So that's let's do that, the categorization. We have a layer here called the input layer. These two layers in the middle here are the hidden layer and the layer on the end here, that is the output layer. And these neurons are all interconnected with each other across the layers. So each neuron is connected to each other neuron in the next layer. So you can see that here. Okay, so now we have our basic neural network. And during a process called forward propagation, the input data traverses through these layers where the weights, biases and activation functions transform the data until an output is produced. So, let's define those terms. Weights, what is that when we're talking about a neural network? Well, the weights define the strength of the connections between each of the neurons. Then we have the activation function, and the activation function is applied to the weighted sum of the inputs at each neuron to introduce non-linearity into the network, and that allows it to make complex relationships. And that's really where we can use activation functions. Commonly, you'll see activation functions used such as sigmoid, for example. And then finally, biases. So biases really are the additional parameter that shift the activation function to the left or the right, and that aids the network's flexibility. So, consider a single training instance with its associated input data. Now, this data propagates forward through the network, causing every neutron to calculate a weighted sum of the inputs, which is then passed through its activation function. And the final result is the network's output. Great! So where does back propagation come in? Well, the initial output might not be accurate. The network needs to learn from its mistakes and adjust its weights to improve. And back propagation is essentially an algorithm used to train neural networks, applying the principle of error correction. So, after forward propagation, the output error, which is the difference between the network's output and the actual output, is computed. Now that's something called a loss function. And the error is distributed back through the network, providing each neuron in the network a measure of its contribution to total error. Using these measures, back propagation adjusts the weights and the biases of the network to minimize that error. And the objective here is to improve the accuracy of the network's output during subsequent forward propagation. It's a process of optimization, often employing a technique known as gradient descent. Now, gradient descent, that's the topic of a whole video of its own, but essentially, gradient descent is an algorithm used to find the optimal weights and biases that minimize the lost function. It iteratively adjusts the weights and biases in the direction that reduces the error most rapidly. And that means the steepest descent. Now, back propagation is widely used in many neural networks. So let's consider a speech recognition system. We provide as input a spoken word, and it outputs a written transcript of that word. Now, if during training our spoken inputs, it turns out that it doesn't match the written outputs, then back propagation may be able to help. Look, I speak with a British accent, but I've lived in the US for years. But when locals here ask for my name-- Martin --they often hear it as something different entirely, like Marvin or Morton or Mark. If this neural network had made the same mistake, we'd calculate the error by using the loss function to quantify the difference between the predicted output \"Marvin\" and the actual output \"Martin\". We'd compute the gradient of the loss function with respect to the weight and biases in the network and update the weighting biases in the network accordingly. Then we'd undergo multiple iterations of forward propagation and back propagation, tinkering with those weights and biases until we reach convergence-- a time where the network could reliably translate Martin into M-A-R-T-I-N. This is can't be applied to people, can it? Well, but anyway, let's just talk about one more thing with back propagation, and that's the distinction between static and recurrent back propagation networks. Let's start with static. So static back propagation is employed in a feed-forward neural networks where the data moves in a single direction from input layer to output layer. Some example use cases of this, well, we can think of OCR, or optical character recognition, where the goal is to identify and classify the letters and numbers in a given image. Another common example is with spam detection, and here we are looking to use a neural network to learn from features such as the emails, content and the sender's email address to classify an email as spam or not spam. Now back propagation can also be applied to recurrent neural networks as well, or RNNs. Now these networks have loops, and this type of back propagation is slightly more complex given the recursive nature of these networks. Now, some use cases? If we think about sentiment analysis, that's a common use case for this. And that's a good example of where RNNs are used to analyze the sentiment of a piece of text, like a customer product review. Another good example is time series prediction. So predicting things like stock prices or weather patterns. Ultimately, back propagation is the backbone of the learning in neural networks. It tests for errors, working its way back from the output layer to the input layer, adjusting the weights as it goes with the goal to minimize future errors. Errors like how to pronounce Martin in a passible American accent. MART-EN... MAR-EN... MART-ENNE.",
    "chunks": [
      "Kind: captions Language: en We're going to take a look at back propagation. It's central to the functioning of neural networks, helping them to learn and adapt. And we're going to cover it in simple but instructive terms. So even if your only knowledge of neural networks is \"Isn't that something to do with chatGPT?\" Well, we've got you covered. Now, a neural network fundamentally comprises multiple layers of neurons interconnected by weights. So I'm going to draw some neurons here, and I'm",
      "organizing them in layers. And these neurons are also known as nodes. Now, the layers here are categorized. So that's let's do that, the categorization. We have a layer here called the input layer. These two layers in the middle here are the hidden layer and the layer on the end here, that is the output layer. And these neurons are all interconnected with each other across the layers. So each neuron is connected to each other neuron in the next layer. So you can see that here. Okay, so now we",
      "have our basic neural network. And during a process called forward propagation, the input data traverses through these layers where the weights, biases and activation functions transform the data until an output is produced. So, let's define those terms. Weights, what is that when we're talking about a neural network? Well, the weights define the strength of the connections between each of the neurons. Then we have the activation function, and the activation function is applied to the weighted",
      "sum of the inputs at each neuron to introduce non-linearity into the network, and that allows it to make complex relationships. And that's really where we can use activation functions. Commonly, you'll see activation functions used such as sigmoid, for example. And then finally, biases. So biases really are the additional parameter that shift the activation function to the left or the right, and that aids the network's flexibility. So, consider a single training instance with its associated input",
      "data. Now, this data propagates forward through the network, causing every neutron to calculate a weighted sum of the inputs, which is then passed through its activation function. And the final result is the network's output. Great! So where does back propagation come in? Well, the initial output might not be accurate. The network needs to learn from its mistakes and adjust its weights to improve. And back propagation is essentially an algorithm used to train neural networks, applying the",
      "principle of error correction. So, after forward propagation, the output error, which is the difference between the network's output and the actual output, is computed. Now that's something called a loss function. And the error is distributed back through the network, providing each neuron in the network a measure of its contribution to total error. Using these measures, back propagation adjusts the weights and the biases of the network to minimize that error. And the objective here is to improve",
      "the accuracy of the network's output during subsequent forward propagation. It's a process of optimization, often employing a technique known as gradient descent. Now, gradient descent, that's the topic of a whole video of its own, but essentially, gradient descent is an algorithm used to find the optimal weights and biases that minimize the lost function. It iteratively adjusts the weights and biases in the direction that reduces the error most rapidly. And that means the steepest descent. Now,",
      "back propagation is widely used in many neural networks. So let's consider a speech recognition system. We provide as input a spoken word, and it outputs a written transcript of that word. Now, if during training our spoken inputs, it turns out that it doesn't match the written outputs, then back propagation may be able to help. Look, I speak with a British accent, but I've lived in the US for years. But when locals here ask for my name-- Martin --they often hear it as something different",
      "entirely, like Marvin or Morton or Mark. If this neural network had made the same mistake, we'd calculate the error by using the loss function to quantify the difference between the predicted output \"Marvin\" and the actual output \"Martin\". We'd compute the gradient of the loss function with respect to the weight and biases in the network and update the weighting biases in the network accordingly. Then we'd undergo multiple iterations of forward propagation and back propagation, tinkering with",
      "those weights and biases until we reach convergence-- a time where the network could reliably translate Martin into M-A-R-T-I-N. This is can't be applied to people, can it? Well, but anyway, let's just talk about one more thing with back propagation, and that's the distinction between static and recurrent back propagation networks. Let's start with static. So static back propagation is employed in a feed-forward neural networks where the data moves in a single direction from input layer to output",
      "layer. Some example use cases of this, well, we can think of OCR, or optical character recognition, where the goal is to identify and classify the letters and numbers in a given image. Another common example is with spam detection, and here we are looking to use a neural network to learn from features such as the emails, content and the sender's email address to classify an email as spam or not spam. Now back propagation can also be applied to recurrent neural networks as well, or RNNs. Now these",
      "networks have loops, and this type of back propagation is slightly more complex given the recursive nature of these networks. Now, some use cases? If we think about sentiment analysis, that's a common use case for this. And that's a good example of where RNNs are used to analyze the sentiment of a piece of text, like a customer product review. Another good example is time series prediction. So predicting things like stock prices or weather patterns. Ultimately, back propagation is the backbone of",
      "the learning in neural networks. It tests for errors, working its way back from the output layer to the input layer, adjusting the weights as it goes with the goal to minimize future errors. Errors like how to pronounce Martin in a passible American accent. MART-EN... MAR-EN... MART-ENNE."
    ],
    "chunk_count": 13,
    "content_id": "35efe8ac-bc90-45f8-960e-eee1a7eafdcf",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554908"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=zqv1eELa7fs": {
    "title": "Training AI Models with Federated Learning",
    "url": "https://www.youtube.com/watch?v=zqv1eELa7fs",
    "description": "Explore  watsonx.ai → https://ibm.biz/Bdy4qU\n\nFederated learning is a way to train AI models without anyone seeing or touching your data, offering a way to unlock information to feed new AI applications. In this video, Martin Keen discusses the forms, benefits and challenges of federated learning. \n\nGet started for free on IBM Cloud → https://ibm.biz/sign-up-now\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#ai #watsonx #llm",
    "duration": 387,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en let's unpack the concept of Federated let's unpack the concept of Federated let's unpack the concept of Federated learning a method for training AI models learning a method for training AI models learning a method for training AI models that is all about keeping your sensitive that is all about keeping your sensitive that is all about keeping your sensitive data right where it should be with you data right where it should be with you data right where it should be with you now ai applications like chat Bots now ai applications like chat Bots now ai applications like chat Bots recommendation systems and spam filters recommendation systems and spam filters recommendation systems and spam filters they're all very data hungry and they they're all very data hungry and they they're all very data hungry and they have been fed tons of examples have been fed tons of examples have been fed tons of examples mountains of information which they use mountains of information which they use mountains of information which they use to learn their specific tasks to build to learn their specific tasks to build to learn their specific tasks to build an AI model an AI model an AI model now normally in machine learning We now normally in machine learning We now normally in machine learning We Gather all of this data from different Gather all of this data from different Gather all of this data from different sources and bring it to one place all of sources and bring it to one place all of sources and bring it to one place all of this will reside this will reside this will reside in a central server and that's where the in a central server and that's where the in a central server and that's where the actual training of the model takes place actual training of the model takes place actual training of the model takes place Federated learning turns this process on Federated learning turns this process on Federated learning turns this process on its head instead of bringing the data to its head instead of bringing the data to its head instead of bringing the data to the model we take the model to the data the model we take the model to the data the model we take the model to the data so here's how it works think every so here's how it works think every so here's how it works think every device like a smartphone or a laptop or device like a smartphone or a laptop or device like a smartphone or a laptop or a server it has its own local version of a server it has its own local version of a server it has its own local version of a model a model a model so each of these are reporting into so each of these are reporting into so each of these are reporting into their own model and this model learns their own model and this model learns their own model and this model learns from the data right there on the device from the data right there on the device from the data right there on the device itself now after the model has learned itself now after the model has learned itself now after the model has learned from the local data it sends only the from the local data it sends only the from the local data it sends only the model updates back to the central server model updates back to the central server model updates back to the central server not the actual raw data so this all goes not the actual raw data so this all goes not the actual raw data so this all goes here here here to the central server and then that to the central server and then that to the central server and then that server Aggregates all of these updates server Aggregates all of these updates server Aggregates all of these updates from all the devices to create what is from all the devices to create what is from all the devices to create what is called the global called the global called the global model now why bother with this level of now why bother with this level of now why bother with this level of decentralization well this concept was decentralization well this concept was decentralization well this concept was first introduced by Google in 2016 at a first introduced by Google in 2016 at a first introduced by Google in 2016 at a time when Global attention was focused time when Global attention was focused time when Global attention was focused on the use and misuse of personal data on the use and misuse of personal data on the use and misuse of personal data concerns about data privacy and security concerns about data privacy and security concerns about data privacy and security prompted the search for alternatives to prompted the search for alternatives to prompted the search for alternatives to traditional centralized AI training traditional centralized AI training traditional centralized AI training methods giving birth to Federated methods giving birth to Federated methods giving birth to Federated learning so let's imagine a scenario learning so let's imagine a scenario learning so let's imagine a scenario involving a group of companies that want involving a group of companies that want involving a group of companies that want to collaborate on building a model to to collaborate on building a model to to collaborate on building a model to predict market trends but each company predict market trends but each company predict market trends but each company has sensitive sales data they want to has sensitive sales data they want to has sensitive sales data they want to keep private so each company has access keep private so each company has access keep private so each company has access to an initial Baseline predictive Global to an initial Baseline predictive Global to an initial Baseline predictive Global model here's our Global model up here model here's our Global model up here model here's our Global model up here and this resides in a central and this resides in a central and this resides in a central server server server now in their individual environments now in their individual environments now in their individual environments each company trains the instances of the each company trains the instances of the each company trains the instances of the model using their own sensitive sales model using their own sensitive sales model using their own sensitive sales data so we have the global model here data so we have the global model here data so we have the global model here and then these individual models with and then these individual models with and then these individual models with each company and here is their sensitive each company and here is their sensitive each company and here is their sensitive sales data along the bottom and they're sales data along the bottom and they're sales data along the bottom and they're tweaking and refining their model based tweaking and refining their model based tweaking and refining their model based on their unique data so the companies do on their unique data so the companies do on their unique data so the companies do not share their sensitive sales data not share their sensitive sales data not share their sensitive sales data instead they only share the updates they instead they only share the updates they instead they only share the updates they made to the model now these updates they made to the model now these updates they made to the model now these updates they don't contain any raw sales data but don't contain any raw sales data but don't contain any raw sales data but they do reflect the insights gained from they do reflect the insights gained from they do reflect the insights gained from the data the model updates are then sent the data the model updates are then sent the data the model updates are then sent back back back to the central server to the central server to the central server and here they're integrated into the and here they're integrated into the and here they're integrated into the global model now this iterative process global model now this iterative process global model now this iterative process continues with each company refining the continues with each company refining the continues with each company refining the model based on their private data and model based on their private data and model based on their private data and sharing only the model updates over time sharing only the model updates over time sharing only the model updates over time this model becomes increasingly accurate this model becomes increasingly accurate this model becomes increasingly accurate at predicting market trends even though at predicting market trends even though at predicting market trends even though no company had to share their sensitive no company had to share their sensitive no company had to share their sensitive data each Company benefits from the data each Company benefits from the data each Company benefits from the collective intelligence of the group collective intelligence of the group collective intelligence of the group while maintaining their data privacy while maintaining their data privacy while maintaining their data privacy that is the essence of Federated that is the essence of Federated that is the essence of Federated learning allowing for Collaborative learning allowing for Collaborative learning allowing for Collaborative Learning from shared model updates while Learning from shared model updates while Learning from shared model updates while keeping the actual data distributed and keeping the actual data distributed and keeping the actual data distributed and private private private now we can think of Federated learning now we can think of Federated learning now we can think of Federated learning as coming in three flavors so there's as coming in three flavors so there's as coming in three flavors so there's horizontal and horizontal Federated horizontal and horizontal Federated horizontal and horizontal Federated learning describes the forecasting model learning describes the forecasting model learning describes the forecasting model example we've just discussed where the example we've just discussed where the example we've just discussed where the data sets were all similar in this case data sets were all similar in this case data sets were all similar in this case the similarity was this was all sales the similarity was this was all sales the similarity was this was all sales data data data now another one is now another one is now another one is called called called vertical Federated learning so instead vertical Federated learning so instead vertical Federated learning so instead of using similar data sets we're dealing of using similar data sets we're dealing of using similar data sets we're dealing with complementary data using movie and with complementary data using movie and with complementary data using movie and book reviews for example to predict book reviews for example to predict book reviews for example to predict someone's music preferences someone's music preferences someone's music preferences and then the Third Kind is called and then the Third Kind is called and then the Third Kind is called Federated Federated Federated transfer learning here we start with a transfer learning here we start with a transfer learning here we start with a model that's already been trained to do model that's already been trained to do model that's already been trained to do one task and then adapt it to do one task and then adapt it to do one task and then adapt it to do something slightly different like like something slightly different like like something slightly different like like how a pre-trained foundation model how a pre-trained foundation model how a pre-trained foundation model designed to perform a task like designed to perform a task like designed to perform a task like detecting cars is trained on another detecting cars is trained on another detecting cars is trained on another data set to do something else entirely data set to do something else entirely data set to do something else entirely like identify cats now the use cases for like identify cats now the use cases for like identify cats now the use cases for Federated learning are far-reaching and Federated learning are far-reaching and Federated learning are far-reaching and impactful just consider the healthcare impactful just consider the healthcare impactful just consider the healthcare industry where Federated learning allows industry where Federated learning allows industry where Federated learning allows medical institutions to collaboratively medical institutions to collaboratively medical institutions to collaboratively train their models on their sensitive train their models on their sensitive train their models on their sensitive data without sharing the actual medical data without sharing the actual medical data without sharing the actual medical records or how financial institutions records or how financial institutions records or how financial institutions can improve their fraud detection can improve their fraud detection can improve their fraud detection mechanisms and credit scoring systems mechanisms and credit scoring systems mechanisms and credit scoring systems without compromising on customer privacy without compromising on customer privacy without compromising on customer privacy however Federated learning is not however Federated learning is not however Federated learning is not without its challenges there is the risk without its challenges there is the risk without its challenges there is the risk of inference attacks where adversaries of inference attacks where adversaries of inference attacks where adversaries may try to extract information about the may try to extract information about the may try to extract information about the data from the shared model updates when data from the shared model updates when data from the shared model updates when we put them up there now to counter this we put them up there now to counter this we put them up there now to counter this researchers are looking into strategies researchers are looking into strategies researchers are looking into strategies like secure multi-party computation to like secure multi-party computation to like secure multi-party computation to ensure privacy by encrypting model ensure privacy by encrypting model ensure privacy by encrypting model updates or by adding a degree of noise updates or by adding a degree of noise updates or by adding a degree of noise to the data to mislead potential to the data to mislead potential to the data to mislead potential attackers other challenges include attackers other challenges include attackers other challenges include computational efficiency because we do computational efficiency because we do computational efficiency because we do have all of this work going on locally have all of this work going on locally have all of this work going on locally here and maintaining transparency in here and maintaining transparency in here and maintaining transparency in model training and creating incentives model training and creating incentives model training and creating incentives for truthful participation look in the for truthful participation look in the for truthful participation look in the end Federated learning offers a end Federated learning offers a end Federated learning offers a promising path towards a new generation promising path towards a new generation promising path towards a new generation of AI applications by addressing privacy of AI applications by addressing privacy of AI applications by addressing privacy concerns and leveraging the power of concerns and leveraging the power of concerns and leveraging the power of distributed computing distributed computing distributed computing Federated learning holds the potential Federated learning holds the potential Federated learning holds the potential to revolutionize how AI models are to revolutionize how AI models are to revolutionize how AI models are trained trained trained if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like And subscribe thanks for watching",
    "chunks": [
      "Kind: captions Language: en let's unpack the concept of Federated let's unpack the concept of Federated let's unpack the concept of Federated learning a method for training AI models learning a method for training AI models learning a method for training AI models that is all about keeping your sensitive that is all about keeping your sensitive that is all about keeping your sensitive data right where it should be with you data right where it should be with you data right where it should be with",
      "you now ai applications like chat Bots now ai applications like chat Bots now ai applications like chat Bots recommendation systems and spam filters recommendation systems and spam filters recommendation systems and spam filters they're all very data hungry and they they're all very data hungry and they they're all very data hungry and they have been fed tons of examples have been fed tons of examples have been fed tons of examples mountains of information which they use mountains of information",
      "which they use mountains of information which they use to learn their specific tasks to build to learn their specific tasks to build to learn their specific tasks to build an AI model an AI model an AI model now normally in machine learning We now normally in machine learning We now normally in machine learning We Gather all of this data from different Gather all of this data from different Gather all of this data from different sources and bring it to one place all of sources and bring it to one",
      "place all of sources and bring it to one place all of this will reside this will reside this will reside in a central server and that's where the in a central server and that's where the in a central server and that's where the actual training of the model takes place actual training of the model takes place actual training of the model takes place Federated learning turns this process on Federated learning turns this process on Federated learning turns this process on its head instead of",
      "bringing the data to its head instead of bringing the data to its head instead of bringing the data to the model we take the model to the data the model we take the model to the data the model we take the model to the data so here's how it works think every so here's how it works think every so here's how it works think every device like a smartphone or a laptop or device like a smartphone or a laptop or device like a smartphone or a laptop or a server it has its own local version of a server it",
      "has its own local version of a server it has its own local version of a model a model a model so each of these are reporting into so each of these are reporting into so each of these are reporting into their own model and this model learns their own model and this model learns their own model and this model learns from the data right there on the device from the data right there on the device from the data right there on the device itself now after the model has learned itself now after the model",
      "has learned itself now after the model has learned from the local data it sends only the from the local data it sends only the from the local data it sends only the model updates back to the central server model updates back to the central server model updates back to the central server not the actual raw data so this all goes not the actual raw data so this all goes not the actual raw data so this all goes here here here to the central server and then that to the central server and then that to",
      "the central server and then that server Aggregates all of these updates server Aggregates all of these updates server Aggregates all of these updates from all the devices to create what is from all the devices to create what is from all the devices to create what is called the global called the global called the global model now why bother with this level of now why bother with this level of now why bother with this level of decentralization well this concept was decentralization well this",
      "concept was decentralization well this concept was first introduced by Google in 2016 at a first introduced by Google in 2016 at a first introduced by Google in 2016 at a time when Global attention was focused time when Global attention was focused time when Global attention was focused on the use and misuse of personal data on the use and misuse of personal data on the use and misuse of personal data concerns about data privacy and security concerns about data privacy and security concerns about",
      "data privacy and security prompted the search for alternatives to prompted the search for alternatives to prompted the search for alternatives to traditional centralized AI training traditional centralized AI training traditional centralized AI training methods giving birth to Federated methods giving birth to Federated methods giving birth to Federated learning so let's imagine a scenario learning so let's imagine a scenario learning so let's imagine a scenario involving a group of companies",
      "that want involving a group of companies that want involving a group of companies that want to collaborate on building a model to to collaborate on building a model to to collaborate on building a model to predict market trends but each company predict market trends but each company predict market trends but each company has sensitive sales data they want to has sensitive sales data they want to has sensitive sales data they want to keep private so each company has access keep private so each",
      "company has access keep private so each company has access to an initial Baseline predictive Global to an initial Baseline predictive Global to an initial Baseline predictive Global model here's our Global model up here model here's our Global model up here model here's our Global model up here and this resides in a central and this resides in a central and this resides in a central server server server now in their individual environments now in their individual environments now in their",
      "individual environments each company trains the instances of the each company trains the instances of the each company trains the instances of the model using their own sensitive sales model using their own sensitive sales model using their own sensitive sales data so we have the global model here data so we have the global model here data so we have the global model here and then these individual models with and then these individual models with and then these individual models with each company",
      "and here is their sensitive each company and here is their sensitive each company and here is their sensitive sales data along the bottom and they're sales data along the bottom and they're sales data along the bottom and they're tweaking and refining their model based tweaking and refining their model based tweaking and refining their model based on their unique data so the companies do on their unique data so the companies do on their unique data so the companies do not share their sensitive",
      "sales data not share their sensitive sales data not share their sensitive sales data instead they only share the updates they instead they only share the updates they instead they only share the updates they made to the model now these updates they made to the model now these updates they made to the model now these updates they don't contain any raw sales data but don't contain any raw sales data but don't contain any raw sales data but they do reflect the insights gained from they do reflect",
      "the insights gained from they do reflect the insights gained from the data the model updates are then sent the data the model updates are then sent the data the model updates are then sent back back back to the central server to the central server to the central server and here they're integrated into the and here they're integrated into the and here they're integrated into the global model now this iterative process global model now this iterative process global model now this iterative process",
      "continues with each company refining the continues with each company refining the continues with each company refining the model based on their private data and model based on their private data and model based on their private data and sharing only the model updates over time sharing only the model updates over time sharing only the model updates over time this model becomes increasingly accurate this model becomes increasingly accurate this model becomes increasingly accurate at predicting",
      "market trends even though at predicting market trends even though at predicting market trends even though no company had to share their sensitive no company had to share their sensitive no company had to share their sensitive data each Company benefits from the data each Company benefits from the data each Company benefits from the collective intelligence of the group collective intelligence of the group collective intelligence of the group while maintaining their data privacy while maintaining",
      "their data privacy while maintaining their data privacy that is the essence of Federated that is the essence of Federated that is the essence of Federated learning allowing for Collaborative learning allowing for Collaborative learning allowing for Collaborative Learning from shared model updates while Learning from shared model updates while Learning from shared model updates while keeping the actual data distributed and keeping the actual data distributed and keeping the actual data distributed",
      "and private private private now we can think of Federated learning now we can think of Federated learning now we can think of Federated learning as coming in three flavors so there's as coming in three flavors so there's as coming in three flavors so there's horizontal and horizontal Federated horizontal and horizontal Federated horizontal and horizontal Federated learning describes the forecasting model learning describes the forecasting model learning describes the forecasting model example",
      "we've just discussed where the example we've just discussed where the example we've just discussed where the data sets were all similar in this case data sets were all similar in this case data sets were all similar in this case the similarity was this was all sales the similarity was this was all sales the similarity was this was all sales data data data now another one is now another one is now another one is called called called vertical Federated learning so instead vertical Federated",
      "learning so instead vertical Federated learning so instead of using similar data sets we're dealing of using similar data sets we're dealing of using similar data sets we're dealing with complementary data using movie and with complementary data using movie and with complementary data using movie and book reviews for example to predict book reviews for example to predict book reviews for example to predict someone's music preferences someone's music preferences someone's music preferences and",
      "then the Third Kind is called and then the Third Kind is called and then the Third Kind is called Federated Federated Federated transfer learning here we start with a transfer learning here we start with a transfer learning here we start with a model that's already been trained to do model that's already been trained to do model that's already been trained to do one task and then adapt it to do one task and then adapt it to do one task and then adapt it to do something slightly different like",
      "like something slightly different like like something slightly different like like how a pre-trained foundation model how a pre-trained foundation model how a pre-trained foundation model designed to perform a task like designed to perform a task like designed to perform a task like detecting cars is trained on another detecting cars is trained on another detecting cars is trained on another data set to do something else entirely data set to do something else entirely data set to do something",
      "else entirely like identify cats now the use cases for like identify cats now the use cases for like identify cats now the use cases for Federated learning are far-reaching and Federated learning are far-reaching and Federated learning are far-reaching and impactful just consider the healthcare impactful just consider the healthcare impactful just consider the healthcare industry where Federated learning allows industry where Federated learning allows industry where Federated learning allows",
      "medical institutions to collaboratively medical institutions to collaboratively medical institutions to collaboratively train their models on their sensitive train their models on their sensitive train their models on their sensitive data without sharing the actual medical data without sharing the actual medical data without sharing the actual medical records or how financial institutions records or how financial institutions records or how financial institutions can improve their fraud detection",
      "can improve their fraud detection can improve their fraud detection mechanisms and credit scoring systems mechanisms and credit scoring systems mechanisms and credit scoring systems without compromising on customer privacy without compromising on customer privacy without compromising on customer privacy however Federated learning is not however Federated learning is not however Federated learning is not without its challenges there is the risk without its challenges there is the risk without its",
      "challenges there is the risk of inference attacks where adversaries of inference attacks where adversaries of inference attacks where adversaries may try to extract information about the may try to extract information about the may try to extract information about the data from the shared model updates when data from the shared model updates when data from the shared model updates when we put them up there now to counter this we put them up there now to counter this we put them up there now to",
      "counter this researchers are looking into strategies researchers are looking into strategies researchers are looking into strategies like secure multi-party computation to like secure multi-party computation to like secure multi-party computation to ensure privacy by encrypting model ensure privacy by encrypting model ensure privacy by encrypting model updates or by adding a degree of noise updates or by adding a degree of noise updates or by adding a degree of noise to the data to mislead",
      "potential to the data to mislead potential to the data to mislead potential attackers other challenges include attackers other challenges include attackers other challenges include computational efficiency because we do computational efficiency because we do computational efficiency because we do have all of this work going on locally have all of this work going on locally have all of this work going on locally here and maintaining transparency in here and maintaining transparency in here and",
      "maintaining transparency in model training and creating incentives model training and creating incentives model training and creating incentives for truthful participation look in the for truthful participation look in the for truthful participation look in the end Federated learning offers a end Federated learning offers a end Federated learning offers a promising path towards a new generation promising path towards a new generation promising path towards a new generation of AI applications by",
      "addressing privacy of AI applications by addressing privacy of AI applications by addressing privacy concerns and leveraging the power of concerns and leveraging the power of concerns and leveraging the power of distributed computing distributed computing distributed computing Federated learning holds the potential Federated learning holds the potential Federated learning holds the potential to revolutionize how AI models are to revolutionize how AI models are to revolutionize how AI models are",
      "trained trained trained if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like And subscribe thanks for watching"
    ],
    "chunk_count": 33,
    "content_id": "23d658c9-6313-4c91-8555-2b38cd7d0a5d",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554912"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=pePAAGfh-IU": {
    "title": "How to Pick the Right AI Foundation Model",
    "url": "https://www.youtube.com/watch?v=pePAAGfh-IU",
    "description": "Test foundation models on watsonx → https://ibm.biz/BdvGb6\n\nThere are so many foundation models available for AI Development, but how do you pick the right one? Picking the wrong one might cost you money, time, accuracy, and reliability. Martin Keen, Master Inventor, walks through his six step approach to picking the right model for your next project.\n\nGet started for free on IBM Cloud → https://ibm.biz/sign-up-now\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now",
    "duration": 474,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en If you have a use case for generative AI, how do you decide on which foundation model to pick to run it? With the huge number of foundation models out there, It's not an easy question. Different models are trained on different data and have different parameter counts, and picking the wrong model can have severe unwanted impact, like biases originating from the training data or hallucinations that are just plain wrong. Now, one approach is to just pick the largest, most massive model out there to execute every task. The largest models have huge parameter counts and are usually pretty good generalists, but with large models come costs, costs of compute, cost of complexity and costs of variability. So often the better approach is to pick the right size model for the specific use case you have. So let me propose to you an AI model selection framework. It has six pretty simple stages. Let's take a look at what they areand then give some examples of how this might work. Now, stage one, that is to clearly articulate your use case. What exactly are you planning to use generative A.I. for? From there you'll list some of the model options available to you. Perhaps there are already a subset of foundation models running that you have access to. With a short list of models you'll next want to identify each model's size, performance costs, risks, and deployment methods. Next, evaluate those model characteristics for your specific use case. Run some tests. That's the next stage, testing options based on your previously identified use case and deployment needs. And then finally, choose the option that provides the most value. So let's put this framework to the test. Now, my use case, we're going to say that is a use case for text generation. I need the AI to write personalized emails for my awesome marketing campaign. That's stage one. Now, my organization is already using two foundation models for other things, so I'll evaluate those. First of all, we've got Llama 2 and specifically the Llama 2 70 model. a fairly large model, 70 billion parameters. It's from meta and I know it's quite good at some text generation use cases. Then there's also Granite that we have deployed. Granite is a smaller general purpose model and that's from IBM. And I know there is a 13 billion parameter model that I've heard does quite well with text generation as well. So those are the models I'm going to evaluate, Llama 2 and Granite. Next, we need to evaluate model size, performance, and risks. And a good place to start here is with the model card. The model cards might tell us if the model has been trained on data specifically for our purposes. Pre-trained Foundation models are fine tuned for specific use cases such as sentiment analysis or document summarization or maybe text generation. And that's important to know because if a model is pre trained on a use case close to ours, it may perform better when processing our prompts and enable us to use zero shot prompting to obtain our desired results. And that means we can simply ask the model to perform tasks without having to provide multiple completed examples first. Now, when it comes to evaluating model performance for our use case, we can consider three factors. The first factor that we would consider is accuracy. Now, accuracy denotes how close the generated output is to the desired output, and it can be measured objectively and repeatedly by choosing evaluation metrics that are relevant to your use cases. So for example, if your use case related to text translation, the B.L.E.U. - that's the BiLingual Evaluation Understudy benchmark, can be used to indicate the quality of the generated translations. Now the second factor relates to reliably of the model. Now that's a function of several factors actually, such as consistency, explainability and trustworthiness, as well as how well a model avoids toxicity like hate speech. Reliability comes down to trust, and trust is built through transparency and traceability of the training data and accuracy and reliability of the output. And then the third factor that is speed. And specifically we're saying how quickly does a user get a response to a submitted prompt? Now, speed and accuracy are often a trade off here. Larger models may be slower, but perhaps deliver a more accurate answer. Or then again, maybe the smaller model is faster and has minimal differences in accuracy to the larger model. It really comes down to finding the sweet spot between performance, speed and cost. A smaller, less expensive model may not offer performance or accuracy metrics on par with an expensive one, but it would still be preferable over the latter. If you consider any additional benefits, the model might deliver like lower latency and greater transparency into the model inputs and outputs. The way to find out is to simply select the model that's likely to deliver the desired output and well, test it. Test that model with your prompts to see if it works, and then assess the model, performance and quality of the output using metrics. Now, I've mentioned deployment in passing, so a quick word on that. As a decision factor, we need to evaluate where and how we want the model and data to be deployed. So let's say that we're leaning towards Llama 2 as our chosen model based on our testing. Right, cool. Llama 2. That's an open source model and we could inference with it on a public cloud. So we've got a public cloud already out here. It's got an element of choice in it, which is limited to we can just inference to that. But if we decide we want to fine tune the model with our own enterprise data, we might need to deploy it on prem. So this is where we have our own version of Llama two and we are going to provide fine tuning to it. Now, deploying on premise gives you greater control, and more security benefits compared to a public cloud environment. But it's an expensive proposition, especially when factoring model size and compute power, including the number of GPUs it takes to run a single large language model. Now, everything we've discussed here is tied to a specific use case, but of course it's quite likely that any given organization will have multiple use cases. And as we run through this model selection framework, we might find that each use case is better suited to a different foundation model. That's called a multi model approach. Essentially, not all A.I. models are the same, and neither are your use cases. And this framework might be just what you need to pair the models and the use cases together to find a winning combination of both.",
    "chunks": [
      "Kind: captions Language: en If you have a use case for generative AI, how do you decide on which foundation model to pick to run it? With the huge number of foundation models out there, It's not an easy question. Different models are trained on different data and have different parameter counts, and picking the wrong model can have severe unwanted impact, like biases originating from the training data or hallucinations that are just plain wrong. Now, one approach is to just pick the largest,",
      "most massive model out there to execute every task. The largest models have huge parameter counts and are usually pretty good generalists, but with large models come costs, costs of compute, cost of complexity and costs of variability. So often the better approach is to pick the right size model for the specific use case you have. So let me propose to you an AI model selection framework. It has six pretty simple stages. Let's take a look at what they areand then give some examples of how this",
      "might work. Now, stage one, that is to clearly articulate your use case. What exactly are you planning to use generative A.I. for? From there you'll list some of the model options available to you. Perhaps there are already a subset of foundation models running that you have access to. With a short list of models you'll next want to identify each model's size, performance costs, risks, and deployment methods. Next, evaluate those model characteristics for your specific use case. Run some tests.",
      "That's the next stage, testing options based on your previously identified use case and deployment needs. And then finally, choose the option that provides the most value. So let's put this framework to the test. Now, my use case, we're going to say that is a use case for text generation. I need the AI to write personalized emails for my awesome marketing campaign. That's stage one. Now, my organization is already using two foundation models for other things, so I'll evaluate those. First of all,",
      "we've got Llama 2 and specifically the Llama 2 70 model. a fairly large model, 70 billion parameters. It's from meta and I know it's quite good at some text generation use cases. Then there's also Granite that we have deployed. Granite is a smaller general purpose model and that's from IBM. And I know there is a 13 billion parameter model that I've heard does quite well with text generation as well. So those are the models I'm going to evaluate, Llama 2 and Granite. Next, we need to evaluate",
      "model size, performance, and risks. And a good place to start here is with the model card. The model cards might tell us if the model has been trained on data specifically for our purposes. Pre-trained Foundation models are fine tuned for specific use cases such as sentiment analysis or document summarization or maybe text generation. And that's important to know because if a model is pre trained on a use case close to ours, it may perform better when processing our prompts and enable us to use",
      "zero shot prompting to obtain our desired results. And that means we can simply ask the model to perform tasks without having to provide multiple completed examples first. Now, when it comes to evaluating model performance for our use case, we can consider three factors. The first factor that we would consider is accuracy. Now, accuracy denotes how close the generated output is to the desired output, and it can be measured objectively and repeatedly by choosing evaluation metrics that are",
      "relevant to your use cases. So for example, if your use case related to text translation, the B.L.E.U. - that's the BiLingual Evaluation Understudy benchmark, can be used to indicate the quality of the generated translations. Now the second factor relates to reliably of the model. Now that's a function of several factors actually, such as consistency, explainability and trustworthiness, as well as how well a model avoids toxicity like hate speech. Reliability comes down to trust, and trust is",
      "built through transparency and traceability of the training data and accuracy and reliability of the output. And then the third factor that is speed. And specifically we're saying how quickly does a user get a response to a submitted prompt? Now, speed and accuracy are often a trade off here. Larger models may be slower, but perhaps deliver a more accurate answer. Or then again, maybe the smaller model is faster and has minimal differences in accuracy to the larger model. It really comes down to",
      "finding the sweet spot between performance, speed and cost. A smaller, less expensive model may not offer performance or accuracy metrics on par with an expensive one, but it would still be preferable over the latter. If you consider any additional benefits, the model might deliver like lower latency and greater transparency into the model inputs and outputs. The way to find out is to simply select the model that's likely to deliver the desired output and well, test it. Test that model with your",
      "prompts to see if it works, and then assess the model, performance and quality of the output using metrics. Now, I've mentioned deployment in passing, so a quick word on that. As a decision factor, we need to evaluate where and how we want the model and data to be deployed. So let's say that we're leaning towards Llama 2 as our chosen model based on our testing. Right, cool. Llama 2. That's an open source model and we could inference with it on a public cloud. So we've got a public cloud already",
      "out here. It's got an element of choice in it, which is limited to we can just inference to that. But if we decide we want to fine tune the model with our own enterprise data, we might need to deploy it on prem. So this is where we have our own version of Llama two and we are going to provide fine tuning to it. Now, deploying on premise gives you greater control, and more security benefits compared to a public cloud environment. But it's an expensive proposition, especially when factoring model",
      "size and compute power, including the number of GPUs it takes to run a single large language model. Now, everything we've discussed here is tied to a specific use case, but of course it's quite likely that any given organization will have multiple use cases. And as we run through this model selection framework, we might find that each use case is better suited to a different foundation model. That's called a multi model approach. Essentially, not all A.I. models are the same, and neither are your",
      "use cases. And this framework might be just what you need to pair the models and the use cases together to find a winning combination of both."
    ],
    "chunk_count": 14,
    "content_id": "b6285984-3681-44bd-95be-4332cbeec5f5",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554915"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=pYax2rupKEY": {
    "title": "How to Choose Large Language Models: A Developer’s Guide to LLMs",
    "url": "https://www.youtube.com/watch?v=pYax2rupKEY",
    "description": "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnKPy\n\nCurious about running large language models locally? Check out the code here → https://ibm.biz/Bdna8C\n\nLearn more about Large Language Models (LLMs) here → https://ibm.biz/BdnKPS\n\nStruggling to choose the right Large Language Model? 🤔 Cedric Clyburn explains how to evaluate LLMs like GPT, Llama, and Granite for tasks like summarization and coding. Discover benchmarks, RAG workflows, and open-source tools to optimize performance, cost, and flexibility for production use! 🚀\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://www.ibm.com/account/reg/us-en/signup?formid=news-urx-52120\n\n#largelanguagemodels #llm #aidevelopment",
    "duration": 416,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en With the huge amount of large language With the huge amount of large language With the huge amount of large language models out there today, it can be a bit models out there today, it can be a bit models out there today, it can be a bit overwhelming to choose the perfect one overwhelming to choose the perfect one overwhelming to choose the perfect one for your use case. Plus, the decision for your use case. Plus, the decision for your use case. Plus, the decision you make might have an impact on the you make might have an impact on the you make might have an impact on the accuracy of your results, as well as accuracy of your results, as well as accuracy of your results, as well as cost and performance. But don't worry, cost and performance. But don't worry, cost and performance. But don't worry, in the next few minutes, I'll show you in the next few minutes, I'll show you in the next few minutes, I'll show you as a developer, how I independently as a developer, how I independently as a developer, how I independently evaluate different models, both evaluate different models, both evaluate different models, both proprietary and open source, and walk proprietary and open source, and walk proprietary and open source, and walk you through different demos of model use you through different demos of model use you through different demos of model use cases like summarization, questioning, cases like summarization, questioning, cases like summarization, questioning, answering on your data, and more. Now, answering on your data, and more. Now, answering on your data, and more. Now, some people start off by looking at some people start off by looking at some people start off by looking at benchmarks or leaderboards. But for me, benchmarks or leaderboards. But for me, benchmarks or leaderboards. But for me, the biggest consideration for model the biggest consideration for model the biggest consideration for model selection is the problem that you're selection is the problem that you're selection is the problem that you're trying to solve. Because while GPT and trying to solve. Because while GPT and trying to solve. Because while GPT and other SASbased models are an easy and other SASbased models are an easy and other SASbased models are an easy and fast way to begin prototyping, many fast way to begin prototyping, many fast way to begin prototyping, many organizations need the full control, organizations need the full control, organizations need the full control, customization, and flexibility that an customization, and flexibility that an customization, and flexibility that an open-source model like Llama or Mistl open-source model like Llama or Mistl open-source model like Llama or Mistl provides. But no matter what you choose, provides. But no matter what you choose, provides. But no matter what you choose, you'll need to consider the performance, you'll need to consider the performance, you'll need to consider the performance, speed, and price of the model. And speed, and price of the model. And speed, and price of the model. And there's a lot of tools to help out with there's a lot of tools to help out with there's a lot of tools to help out with this. So, let's get started here. I'm this. So, let's get started here. I'm this. So, let's get started here. I'm starting off at artificial analysis, starting off at artificial analysis, starting off at artificial analysis, comparing the entire landscape of comparing the entire landscape of comparing the entire landscape of models, both proprietary and open- models, both proprietary and open- models, both proprietary and open- source. And you're probably going to see source. And you're probably going to see source. And you're probably going to see some familiar names here. But something some familiar names here. But something some familiar names here. But something I do want to note is that there are some I do want to note is that there are some I do want to note is that there are some trends. For example, with higher trends. For example, with higher trends. For example, with higher intelligence typically results in a intelligence typically results in a intelligence typically results in a higher price or higher cost, while at higher price or higher cost, while at higher price or higher cost, while at the same time, smaller models might the same time, smaller models might the same time, smaller models might result in faster speeds and lower cost result in faster speeds and lower cost result in faster speeds and lower cost at the same time. Let's take at the same time. Let's take at the same time. Let's take intelligence as an example. So the intelligence as an example. So the intelligence as an example. So the numbers that they calculated actually numbers that they calculated actually numbers that they calculated actually result from a variety of benchmarks on result from a variety of benchmarks on result from a variety of benchmarks on MMLU Pro and similar evaluations. But MMLU Pro and similar evaluations. But MMLU Pro and similar evaluations. But let's say that you're scaling things up let's say that you're scaling things up let's say that you're scaling things up to millions of queries to your model. to millions of queries to your model. to millions of queries to your model. You probably don't need a PhD level AI You probably don't need a PhD level AI You probably don't need a PhD level AI for a simple task like that. But one of for a simple task like that. But one of for a simple task like that. But one of my favorite community-based platforms to my favorite community-based platforms to my favorite community-based platforms to valet models is the chatbot arena valet models is the chatbot arena valet models is the chatbot arena leaderboard by UC Berkeley and ALM leaderboard by UC Berkeley and ALM leaderboard by UC Berkeley and ALM Arena, which combines over a million Arena, which combines over a million Arena, which combines over a million blind user votes on models to rank them blind user votes on models to rank them blind user votes on models to rank them and essentially provide a vibe score. and essentially provide a vibe score. and essentially provide a vibe score. Because benchmarks sometimes can be Because benchmarks sometimes can be Because benchmarks sometimes can be reverse engineered by models, the reverse engineered by models, the reverse engineered by models, the chatbot arena is a great way to chatbot arena is a great way to chatbot arena is a great way to understand what the general AI community understand what the general AI community understand what the general AI community thinks is the best model. And this thinks is the best model. And this thinks is the best model. And this directly correlates to its abilities on directly correlates to its abilities on directly correlates to its abilities on reasoning, math, writing, and more. reasoning, math, writing, and more. reasoning, math, writing, and more. Plus, let's say, for example, that you Plus, let's say, for example, that you Plus, let's say, for example, that you want to compare two different models. want to compare two different models. want to compare two different models. You can actually do so in the interface. You can actually do so in the interface. You can actually do so in the interface. For example, I tried out this prompt to For example, I tried out this prompt to For example, I tried out this prompt to write an example customer response for a write an example customer response for a write an example customer response for a bank in JSON, and we're able to compare bank in JSON, and we're able to compare bank in JSON, and we're able to compare between granite 8 billion and Llama 8 between granite 8 billion and Llama 8 between granite 8 billion and Llama 8 billion. So, it's pretty cool, right? billion. So, it's pretty cool, right? billion. So, it's pretty cool, right? And finally, for simply open-source And finally, for simply open-source And finally, for simply open-source foundation and fine-tune models, the foundation and fine-tune models, the foundation and fine-tune models, the open LLM leaderboard has a wide variety open LLM leaderboard has a wide variety open LLM leaderboard has a wide variety of model metrics and filters in order to of model metrics and filters in order to of model metrics and filters in order to understand which model might be best for understand which model might be best for understand which model might be best for your specific use case. for example, if your specific use case. for example, if your specific use case. for example, if you have a GPU or you want to run it you have a GPU or you want to run it you have a GPU or you want to run it locally on your machine or even do locally on your machine or even do locally on your machine or even do real-time inferencing on a mobile or real-time inferencing on a mobile or real-time inferencing on a mobile or edge device. So, what's great is that edge device. So, what's great is that edge device. So, what's great is that you can easily select these filters and you can easily select these filters and you can easily select these filters and see the model directly on hugging face. see the model directly on hugging face. see the model directly on hugging face. For example, the number three result For example, the number three result For example, the number three result here is the granite model. And on here is the granite model. And on here is the granite model. And on hugging face, you can understand the hugging face, you can understand the hugging face, you can understand the millions of models and data sets that millions of models and data sets that millions of models and data sets that are on there and understand how you can are on there and understand how you can are on there and understand how you can use it on your machine. Now, we've taken use it on your machine. Now, we've taken use it on your machine. Now, we've taken a look at the general model landscape a look at the general model landscape a look at the general model landscape here, but let's start testing these out here, but let's start testing these out here, but let's start testing these out locally with our data. For example, this locally with our data. For example, this locally with our data. For example, this granite model that we have here on granite model that we have here on granite model that we have here on hugging face. In order to test out hugging face. In order to test out hugging face. In order to test out different models and their use cases, different models and their use cases, different models and their use cases, we're going to use Olama, which is a we're going to use Olama, which is a we're going to use Olama, which is a popular developer tool that enables popular developer tool that enables popular developer tool that enables everybody to run their own large everybody to run their own large everybody to run their own large language models on their own system. language models on their own system. language models on their own system. It's open source and it has a model It's open source and it has a model It's open source and it has a model repository, meaning that we can run repository, meaning that we can run repository, meaning that we can run chat, vision, tool calling, and even rag chat, vision, tool calling, and even rag chat, vision, tool calling, and even rag embedding models locally. So to start, embedding models locally. So to start, embedding models locally. So to start, we're going to run Granite, specifically we're going to run Granite, specifically we're going to run Granite, specifically that Granite 3.1 model that we took a that Granite 3.1 model that we took a that Granite 3.1 model that we took a look at earlier on HuggingFace. And here look at earlier on HuggingFace. And here look at earlier on HuggingFace. And here it's already quantized or optimized and it's already quantized or optimized and it's already quantized or optimized and compressed for our machine. And we're compressed for our machine. And we're compressed for our machine. And we're going to give it a quick question to going to give it a quick question to going to give it a quick question to make sure it's running. Talk like a make sure it's running. Talk like a make sure it's running. Talk like a pirate. Let's make sure. And there we pirate. Let's make sure. And there we pirate. Let's make sure. And there we go. We've got a funny response from our go. We've got a funny response from our go. We've got a funny response from our model. But now with the model running model. But now with the model running model. But now with the model running locally on our machine, I want to use it locally on our machine, I want to use it locally on our machine, I want to use it with my own data to understand what it with my own data to understand what it with my own data to understand what it can do and its possibilities. We're can do and its possibilities. We're can do and its possibilities. We're going to use rag or retrieval augmented going to use rag or retrieval augmented going to use rag or retrieval augmented generation in order to do this. Here's generation in order to do this. Here's generation in order to do this. Here's an open- source AI interface called open an open- source AI interface called open an open- source AI interface called open web UI and it's going to allow us to use web UI and it's going to allow us to use web UI and it's going to allow us to use a local model that we have running for a local model that we have running for a local model that we have running for example granite with Olama or maybe any example granite with Olama or maybe any example granite with Olama or maybe any open AI compatible API model remotely as open AI compatible API model remotely as open AI compatible API model remotely as well. But let's think about it as an AI well. But let's think about it as an AI well. But let's think about it as an AI application, right? The backend could be application, right? The backend could be application, right? The backend could be our model and model server and the front our model and model server and the front our model and model server and the front end could be a user interface like this end could be a user interface like this end could be a user interface like this that allows us to take in our own custom that allows us to take in our own custom that allows us to take in our own custom data to search the web or to build a data to search the web or to build a data to search the web or to build a gentic applications all with AI. So gentic applications all with AI. So gentic applications all with AI. So let's start off with rag by attaching a let's start off with rag by attaching a let's start off with rag by attaching a file of something the model file of something the model file of something the model traditionally wouldn't know. This is traditionally wouldn't know. This is traditionally wouldn't know. This is specific enterprise data, right? Stuff specific enterprise data, right? Stuff specific enterprise data, right? Stuff that the model wasn't trained on that the model wasn't trained on that the model wasn't trained on originally and specifically about Marty originally and specifically about Marty originally and specifically about Marty McFly. And we're going to provide this McFly. And we're going to provide this McFly. And we're going to provide this to the model and ask a specific to the model and ask a specific to the model and ask a specific question. What happened to Marty McFly question. What happened to Marty McFly question. What happened to Marty McFly in the 1955 accident from the claim? in the 1955 accident from the claim? in the 1955 accident from the claim? Now, traditionally, a model wouldn't Now, traditionally, a model wouldn't Now, traditionally, a model wouldn't know about this information, but by know about this information, but by know about this information, but by using an embedding model in the using an embedding model in the using an embedding model in the background as well as a vector database, background as well as a vector database, background as well as a vector database, we're able to pull certain information we're able to pull certain information we're able to pull certain information from that source document and even from that source document and even from that source document and even provide that in uh the citations here to provide that in uh the citations here to provide that in uh the citations here to have a clear source of truth for our have a clear source of truth for our have a clear source of truth for our model's answers. So, we're able to try model's answers. So, we're able to try model's answers. So, we're able to try out Rag here, but also different agentic out Rag here, but also different agentic out Rag here, but also different agentic functions as well, and it's a great functions as well, and it's a great functions as well, and it's a great place to get started with your own place to get started with your own place to get started with your own unique data. Now, let's say that you're unique data. Now, let's say that you're unique data. Now, let's say that you're building applications and you want to building applications and you want to building applications and you want to use a free coding assistant within your use a free coding assistant within your use a free coding assistant within your IDE. Well, traditionally, you need to IDE. Well, traditionally, you need to IDE. Well, traditionally, you need to use a SAS offering or a specifically use a SAS offering or a specifically use a SAS offering or a specifically fine-tuned coding model. But now, more fine-tuned coding model. But now, more fine-tuned coding model. But now, more recently, one model can now work with a recently, one model can now work with a recently, one model can now work with a variety of languages, including your variety of languages, including your variety of languages, including your code. So what I've set up here is code. So what I've set up here is code. So what I've set up here is continue and it's a open- source and continue and it's a open- source and continue and it's a open- source and free extension from the VS code free extension from the VS code free extension from the VS code marketplace or Intelligj and specified marketplace or Intelligj and specified marketplace or Intelligj and specified it to use a local model that I have it to use a local model that I have it to use a local model that I have running with Olama that granite model running with Olama that granite model running with Olama that granite model from earlier. So what we're able to do from earlier. So what we're able to do from earlier. So what we're able to do is to chat with our codebase explain is to chat with our codebase explain is to chat with our codebase explain entire files and make edits for us. So entire files and make edits for us. So entire files and make edits for us. So here I think we should add comments and here I think we should add comments and here I think we should add comments and some quick documentation on what this some quick documentation on what this some quick documentation on what this class is doing so that other developers class is doing so that other developers class is doing so that other developers can understand it as well. So I'm going can understand it as well. So I'm going can understand it as well. So I'm going to ask to add java.com comments to ask to add java.com comments to ask to add java.com comments describing the service and it's going to describing the service and it's going to describing the service and it's going to go in and add this necessary and go in and add this necessary and go in and add this necessary and important documentation to my project in important documentation to my project in important documentation to my project in line and be able to ask me to approve it line and be able to ask me to approve it line and be able to ask me to approve it or to deny it. So I think it's pretty or to deny it. So I think it's pretty or to deny it. So I think it's pretty cool and it's a great way to use an AI cool and it's a great way to use an AI cool and it's a great way to use an AI model with your codebase as well. Okay, model with your codebase as well. Okay, model with your codebase as well. Okay, great. So now you know the various ways great. So now you know the various ways great. So now you know the various ways to evaluate and test models both from to evaluate and test models both from to evaluate and test models both from online leaderboards and benchmarks as online leaderboards and benchmarks as online leaderboards and benchmarks as well as from your own machine. But well as from your own machine. But well as from your own machine. But remember, it all comes down to your use remember, it all comes down to your use remember, it all comes down to your use case. And there's even hybrid approaches case. And there's even hybrid approaches case. And there's even hybrid approaches of using a more powerful model in of using a more powerful model in of using a more powerful model in conjunction with a small model on conjunction with a small model on conjunction with a small model on device. But we're just getting started device. But we're just getting started device. But we're just getting started because after experimenting with models because after experimenting with models because after experimenting with models comes the stage of building something comes the stage of building something comes the stage of building something great with AI. Now, what are you working great with AI. Now, what are you working great with AI. Now, what are you working on these days? Please let us know in the on these days? Please let us know in the on these days? Please let us know in the comments below. But as always, thank you comments below. But as always, thank you comments below. But as always, thank you so much for watching. Be sure to leave a so much for watching. Be sure to leave a so much for watching. Be sure to leave a like if you learned something today.",
    "chunks": [
      "Kind: captions Language: en With the huge amount of large language With the huge amount of large language With the huge amount of large language models out there today, it can be a bit models out there today, it can be a bit models out there today, it can be a bit overwhelming to choose the perfect one overwhelming to choose the perfect one overwhelming to choose the perfect one for your use case. Plus, the decision for your use case. Plus, the decision for your use case. Plus, the decision you",
      "make might have an impact on the you make might have an impact on the you make might have an impact on the accuracy of your results, as well as accuracy of your results, as well as accuracy of your results, as well as cost and performance. But don't worry, cost and performance. But don't worry, cost and performance. But don't worry, in the next few minutes, I'll show you in the next few minutes, I'll show you in the next few minutes, I'll show you as a developer, how I independently as a",
      "developer, how I independently as a developer, how I independently evaluate different models, both evaluate different models, both evaluate different models, both proprietary and open source, and walk proprietary and open source, and walk proprietary and open source, and walk you through different demos of model use you through different demos of model use you through different demos of model use cases like summarization, questioning, cases like summarization, questioning, cases like",
      "summarization, questioning, answering on your data, and more. Now, answering on your data, and more. Now, answering on your data, and more. Now, some people start off by looking at some people start off by looking at some people start off by looking at benchmarks or leaderboards. But for me, benchmarks or leaderboards. But for me, benchmarks or leaderboards. But for me, the biggest consideration for model the biggest consideration for model the biggest consideration for model selection is the",
      "problem that you're selection is the problem that you're selection is the problem that you're trying to solve. Because while GPT and trying to solve. Because while GPT and trying to solve. Because while GPT and other SASbased models are an easy and other SASbased models are an easy and other SASbased models are an easy and fast way to begin prototyping, many fast way to begin prototyping, many fast way to begin prototyping, many organizations need the full control, organizations need the full",
      "control, organizations need the full control, customization, and flexibility that an customization, and flexibility that an customization, and flexibility that an open-source model like Llama or Mistl open-source model like Llama or Mistl open-source model like Llama or Mistl provides. But no matter what you choose, provides. But no matter what you choose, provides. But no matter what you choose, you'll need to consider the performance, you'll need to consider the performance, you'll need to",
      "consider the performance, speed, and price of the model. And speed, and price of the model. And speed, and price of the model. And there's a lot of tools to help out with there's a lot of tools to help out with there's a lot of tools to help out with this. So, let's get started here. I'm this. So, let's get started here. I'm this. So, let's get started here. I'm starting off at artificial analysis, starting off at artificial analysis, starting off at artificial analysis, comparing the entire",
      "landscape of comparing the entire landscape of comparing the entire landscape of models, both proprietary and open- models, both proprietary and open- models, both proprietary and open- source. And you're probably going to see source. And you're probably going to see source. And you're probably going to see some familiar names here. But something some familiar names here. But something some familiar names here. But something I do want to note is that there are some I do want to note is that there",
      "are some I do want to note is that there are some trends. For example, with higher trends. For example, with higher trends. For example, with higher intelligence typically results in a intelligence typically results in a intelligence typically results in a higher price or higher cost, while at higher price or higher cost, while at higher price or higher cost, while at the same time, smaller models might the same time, smaller models might the same time, smaller models might result in faster",
      "speeds and lower cost result in faster speeds and lower cost result in faster speeds and lower cost at the same time. Let's take at the same time. Let's take at the same time. Let's take intelligence as an example. So the intelligence as an example. So the intelligence as an example. So the numbers that they calculated actually numbers that they calculated actually numbers that they calculated actually result from a variety of benchmarks on result from a variety of benchmarks on result from a",
      "variety of benchmarks on MMLU Pro and similar evaluations. But MMLU Pro and similar evaluations. But MMLU Pro and similar evaluations. But let's say that you're scaling things up let's say that you're scaling things up let's say that you're scaling things up to millions of queries to your model. to millions of queries to your model. to millions of queries to your model. You probably don't need a PhD level AI You probably don't need a PhD level AI You probably don't need a PhD level AI for a",
      "simple task like that. But one of for a simple task like that. But one of for a simple task like that. But one of my favorite community-based platforms to my favorite community-based platforms to my favorite community-based platforms to valet models is the chatbot arena valet models is the chatbot arena valet models is the chatbot arena leaderboard by UC Berkeley and ALM leaderboard by UC Berkeley and ALM leaderboard by UC Berkeley and ALM Arena, which combines over a million Arena, which",
      "combines over a million Arena, which combines over a million blind user votes on models to rank them blind user votes on models to rank them blind user votes on models to rank them and essentially provide a vibe score. and essentially provide a vibe score. and essentially provide a vibe score. Because benchmarks sometimes can be Because benchmarks sometimes can be Because benchmarks sometimes can be reverse engineered by models, the reverse engineered by models, the reverse engineered by models,",
      "the chatbot arena is a great way to chatbot arena is a great way to chatbot arena is a great way to understand what the general AI community understand what the general AI community understand what the general AI community thinks is the best model. And this thinks is the best model. And this thinks is the best model. And this directly correlates to its abilities on directly correlates to its abilities on directly correlates to its abilities on reasoning, math, writing, and more. reasoning, math,",
      "writing, and more. reasoning, math, writing, and more. Plus, let's say, for example, that you Plus, let's say, for example, that you Plus, let's say, for example, that you want to compare two different models. want to compare two different models. want to compare two different models. You can actually do so in the interface. You can actually do so in the interface. You can actually do so in the interface. For example, I tried out this prompt to For example, I tried out this prompt to For example,",
      "I tried out this prompt to write an example customer response for a write an example customer response for a write an example customer response for a bank in JSON, and we're able to compare bank in JSON, and we're able to compare bank in JSON, and we're able to compare between granite 8 billion and Llama 8 between granite 8 billion and Llama 8 between granite 8 billion and Llama 8 billion. So, it's pretty cool, right? billion. So, it's pretty cool, right? billion. So, it's pretty cool, right? And",
      "finally, for simply open-source And finally, for simply open-source And finally, for simply open-source foundation and fine-tune models, the foundation and fine-tune models, the foundation and fine-tune models, the open LLM leaderboard has a wide variety open LLM leaderboard has a wide variety open LLM leaderboard has a wide variety of model metrics and filters in order to of model metrics and filters in order to of model metrics and filters in order to understand which model might be best for",
      "understand which model might be best for understand which model might be best for your specific use case. for example, if your specific use case. for example, if your specific use case. for example, if you have a GPU or you want to run it you have a GPU or you want to run it you have a GPU or you want to run it locally on your machine or even do locally on your machine or even do locally on your machine or even do real-time inferencing on a mobile or real-time inferencing on a mobile or real-time",
      "inferencing on a mobile or edge device. So, what's great is that edge device. So, what's great is that edge device. So, what's great is that you can easily select these filters and you can easily select these filters and you can easily select these filters and see the model directly on hugging face. see the model directly on hugging face. see the model directly on hugging face. For example, the number three result For example, the number three result For example, the number three result here is",
      "the granite model. And on here is the granite model. And on here is the granite model. And on hugging face, you can understand the hugging face, you can understand the hugging face, you can understand the millions of models and data sets that millions of models and data sets that millions of models and data sets that are on there and understand how you can are on there and understand how you can are on there and understand how you can use it on your machine. Now, we've taken use it on your",
      "machine. Now, we've taken use it on your machine. Now, we've taken a look at the general model landscape a look at the general model landscape a look at the general model landscape here, but let's start testing these out here, but let's start testing these out here, but let's start testing these out locally with our data. For example, this locally with our data. For example, this locally with our data. For example, this granite model that we have here on granite model that we have here on granite",
      "model that we have here on hugging face. In order to test out hugging face. In order to test out hugging face. In order to test out different models and their use cases, different models and their use cases, different models and their use cases, we're going to use Olama, which is a we're going to use Olama, which is a we're going to use Olama, which is a popular developer tool that enables popular developer tool that enables popular developer tool that enables everybody to run their own large",
      "everybody to run their own large everybody to run their own large language models on their own system. language models on their own system. language models on their own system. It's open source and it has a model It's open source and it has a model It's open source and it has a model repository, meaning that we can run repository, meaning that we can run repository, meaning that we can run chat, vision, tool calling, and even rag chat, vision, tool calling, and even rag chat, vision, tool",
      "calling, and even rag embedding models locally. So to start, embedding models locally. So to start, embedding models locally. So to start, we're going to run Granite, specifically we're going to run Granite, specifically we're going to run Granite, specifically that Granite 3.1 model that we took a that Granite 3.1 model that we took a that Granite 3.1 model that we took a look at earlier on HuggingFace. And here look at earlier on HuggingFace. And here look at earlier on HuggingFace. And here",
      "it's already quantized or optimized and it's already quantized or optimized and it's already quantized or optimized and compressed for our machine. And we're compressed for our machine. And we're compressed for our machine. And we're going to give it a quick question to going to give it a quick question to going to give it a quick question to make sure it's running. Talk like a make sure it's running. Talk like a make sure it's running. Talk like a pirate. Let's make sure. And there we pirate.",
      "Let's make sure. And there we pirate. Let's make sure. And there we go. We've got a funny response from our go. We've got a funny response from our go. We've got a funny response from our model. But now with the model running model. But now with the model running model. But now with the model running locally on our machine, I want to use it locally on our machine, I want to use it locally on our machine, I want to use it with my own data to understand what it with my own data to understand what",
      "it with my own data to understand what it can do and its possibilities. We're can do and its possibilities. We're can do and its possibilities. We're going to use rag or retrieval augmented going to use rag or retrieval augmented going to use rag or retrieval augmented generation in order to do this. Here's generation in order to do this. Here's generation in order to do this. Here's an open- source AI interface called open an open- source AI interface called open an open- source AI interface",
      "called open web UI and it's going to allow us to use web UI and it's going to allow us to use web UI and it's going to allow us to use a local model that we have running for a local model that we have running for a local model that we have running for example granite with Olama or maybe any example granite with Olama or maybe any example granite with Olama or maybe any open AI compatible API model remotely as open AI compatible API model remotely as open AI compatible API model remotely as well.",
      "But let's think about it as an AI well. But let's think about it as an AI well. But let's think about it as an AI application, right? The backend could be application, right? The backend could be application, right? The backend could be our model and model server and the front our model and model server and the front our model and model server and the front end could be a user interface like this end could be a user interface like this end could be a user interface like this that allows us to",
      "take in our own custom that allows us to take in our own custom that allows us to take in our own custom data to search the web or to build a data to search the web or to build a data to search the web or to build a gentic applications all with AI. So gentic applications all with AI. So gentic applications all with AI. So let's start off with rag by attaching a let's start off with rag by attaching a let's start off with rag by attaching a file of something the model file of something the model",
      "file of something the model traditionally wouldn't know. This is traditionally wouldn't know. This is traditionally wouldn't know. This is specific enterprise data, right? Stuff specific enterprise data, right? Stuff specific enterprise data, right? Stuff that the model wasn't trained on that the model wasn't trained on that the model wasn't trained on originally and specifically about Marty originally and specifically about Marty originally and specifically about Marty McFly. And we're going to",
      "provide this McFly. And we're going to provide this McFly. And we're going to provide this to the model and ask a specific to the model and ask a specific to the model and ask a specific question. What happened to Marty McFly question. What happened to Marty McFly question. What happened to Marty McFly in the 1955 accident from the claim? in the 1955 accident from the claim? in the 1955 accident from the claim? Now, traditionally, a model wouldn't Now, traditionally, a model wouldn't Now,",
      "traditionally, a model wouldn't know about this information, but by know about this information, but by know about this information, but by using an embedding model in the using an embedding model in the using an embedding model in the background as well as a vector database, background as well as a vector database, background as well as a vector database, we're able to pull certain information we're able to pull certain information we're able to pull certain information from that source document",
      "and even from that source document and even from that source document and even provide that in uh the citations here to provide that in uh the citations here to provide that in uh the citations here to have a clear source of truth for our have a clear source of truth for our have a clear source of truth for our model's answers. So, we're able to try model's answers. So, we're able to try model's answers. So, we're able to try out Rag here, but also different agentic out Rag here, but also",
      "different agentic out Rag here, but also different agentic functions as well, and it's a great functions as well, and it's a great functions as well, and it's a great place to get started with your own place to get started with your own place to get started with your own unique data. Now, let's say that you're unique data. Now, let's say that you're unique data. Now, let's say that you're building applications and you want to building applications and you want to building applications and you",
      "want to use a free coding assistant within your use a free coding assistant within your use a free coding assistant within your IDE. Well, traditionally, you need to IDE. Well, traditionally, you need to IDE. Well, traditionally, you need to use a SAS offering or a specifically use a SAS offering or a specifically use a SAS offering or a specifically fine-tuned coding model. But now, more fine-tuned coding model. But now, more fine-tuned coding model. But now, more recently, one model can now",
      "work with a recently, one model can now work with a recently, one model can now work with a variety of languages, including your variety of languages, including your variety of languages, including your code. So what I've set up here is code. So what I've set up here is code. So what I've set up here is continue and it's a open- source and continue and it's a open- source and continue and it's a open- source and free extension from the VS code free extension from the VS code free extension from",
      "the VS code marketplace or Intelligj and specified marketplace or Intelligj and specified marketplace or Intelligj and specified it to use a local model that I have it to use a local model that I have it to use a local model that I have running with Olama that granite model running with Olama that granite model running with Olama that granite model from earlier. So what we're able to do from earlier. So what we're able to do from earlier. So what we're able to do is to chat with our codebase",
      "explain is to chat with our codebase explain is to chat with our codebase explain entire files and make edits for us. So entire files and make edits for us. So entire files and make edits for us. So here I think we should add comments and here I think we should add comments and here I think we should add comments and some quick documentation on what this some quick documentation on what this some quick documentation on what this class is doing so that other developers class is doing so that other",
      "developers class is doing so that other developers can understand it as well. So I'm going can understand it as well. So I'm going can understand it as well. So I'm going to ask to add java.com comments to ask to add java.com comments to ask to add java.com comments describing the service and it's going to describing the service and it's going to describing the service and it's going to go in and add this necessary and go in and add this necessary and go in and add this necessary and important",
      "documentation to my project in important documentation to my project in important documentation to my project in line and be able to ask me to approve it line and be able to ask me to approve it line and be able to ask me to approve it or to deny it. So I think it's pretty or to deny it. So I think it's pretty or to deny it. So I think it's pretty cool and it's a great way to use an AI cool and it's a great way to use an AI cool and it's a great way to use an AI model with your codebase as well.",
      "Okay, model with your codebase as well. Okay, model with your codebase as well. Okay, great. So now you know the various ways great. So now you know the various ways great. So now you know the various ways to evaluate and test models both from to evaluate and test models both from to evaluate and test models both from online leaderboards and benchmarks as online leaderboards and benchmarks as online leaderboards and benchmarks as well as from your own machine. But well as from your own machine.",
      "But well as from your own machine. But remember, it all comes down to your use remember, it all comes down to your use remember, it all comes down to your use case. And there's even hybrid approaches case. And there's even hybrid approaches case. And there's even hybrid approaches of using a more powerful model in of using a more powerful model in of using a more powerful model in conjunction with a small model on conjunction with a small model on conjunction with a small model on device. But",
      "we're just getting started device. But we're just getting started device. But we're just getting started because after experimenting with models because after experimenting with models because after experimenting with models comes the stage of building something comes the stage of building something comes the stage of building something great with AI. Now, what are you working great with AI. Now, what are you working great with AI. Now, what are you working on these days? Please let us know in",
      "the on these days? Please let us know in the on these days? Please let us know in the comments below. But as always, thank you comments below. But as always, thank you comments below. But as always, thank you so much for watching. Be sure to leave a so much for watching. Be sure to leave a so much for watching. Be sure to leave a like if you learned something today."
    ],
    "chunk_count": 45,
    "content_id": "c05eb7d9-ad89-48ed-abf1-898e6ee82915",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554918"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=y9k-U9AuDeM": {
    "title": "Should You Use Open Source Large Language Models?",
    "url": "https://www.youtube.com/watch?v=y9k-U9AuDeM",
    "description": "Want to experiment with foundation models? Explore our interactive demo for watsonx.ai  → https://ibm.biz/Bdvu3f\n\nTo dive deeper get the guide to choosing the right AI foundation model → https://ibm.biz/Bdvu3H\n\nLarge Language Models (LLMs) can be proprietary to a given company, or open source and free for anyone to access and modify. While proprietary LLMs are often larger, the benefits of transparency, fine-tuning, and community contributions make open source an attractive alternative. Both proprietary and open source LLMs share risks, including inaccuracies, bias, and security concerns. In this video, Master Inventor Martin Keen covers the tradeoffs so you can make an informed decision of which option is best for you.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM. → https://ibm.biz/Bdvu3M",
    "duration": 400,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en There are over 325,000 models on Huggingface and thousands more are being added. And why might you choose to use AI models like these? Well, let's start by getting a few things straight. The models we're talking about in this video, they're specifically LLMs, and that's large language models, which are foundation models that use artificial intelligence, deep learning and massive datasets to generate text. We're talking generative AI. And there are two types of generative AI models: There's proprietary models, and there are open source models. Now, proprietary LLMs, those are owned by a company who can control its usage. A proprietary LLM may include a license that restricts how the LLM can be used. On the other hand, open source LLMs are free and available for anyone to access, and developers and researchers are free to use, improve or otherwise modify the model. Now look, it's not true in every instance, but generally many proprietary LLMs are far larger in size than open source models. And specifically in terms of parameter size. Some of the leading proprietary LLMs extend to thousands of billions of parameters. Probably? Actually, we don't necessarily know because, well, those LLMs and that parameter counts are proprietary. But bigger isn't necessarily better. And the open source model ecosystem is showing promise in challenging the proprietary LLM business model. So let's discuss the benefits of open source LLMs. Let's talk about the types of organizations that are using them. Let's talk about some of the leading open source models available today, and we should talk about the risks associated with using them. Now, clearly, one of the benefits of a open source large language model, that has to be transparency. Open source LLMs may offer better insight into how they work, their architecture, and the training data used to develop them. Another big one is pre-trained open source LLMs allow a process called fine tuning. That means you can add features to the LLM that benefit your specific use case and the LLMs can be trained on specific data sets. So I can fine tune an LLM with my own data. And community contributions are a big plus. Using a proprietary LLM means you're reliant on a single provider, whereas open source models benefit from community contributions and multiple service providers. You can experiment and use contributions from people with varying perspectives. And these benefits have led to all sorts of organizations to use open source LLMs. In another video, I addressed how NASA and IBM developed an open source LLM trained on geospatial data. Some healthcare organizations use open source LLMs for diagnostic tools and treatment optimization. There's an open source LLM called FinGPT [fin / financial]. It was developed for the financial industry. Which brings us onto the topic of talking about some specific open source LLMs that you might find of interest. Now Huggingface maintains an open LLM leaderboard, and that tracks , ranks, and evaluates open source LLMs on various benchmarks like which LLM is scoring highest on the Truthful AI Benchmark series, which measures whether a language model is truthful in generating answers to questions. So it gives those answers a score. And the top spots on this leaderboard, they change frequently. And it's quite fun to watch the progress these models are making. Many of the models on the leaderboard are variations on the Llama 2 open source LLM. That's the one provided by Meta AI. And Llama 2 encompasses pre-trained and fine tuned generative text models from 70 billion all the way down to 7 billion parameters. And it's licensed for commercial use. Vicuna was created on top of the Llama model and fine tuned to follow instructions. And then it's also Bloom by BigScience, which is a multilingual language model created by more than 1000 AI researchers. Now, one area that both proprietary and open source LLMs share is their associated risks. Although LLM outputs often sounds fluent and authoritative, they can be confidently wrong. Hallucinations, they can result from the LLM being trained on incomplete, contradictory, or inaccurate data from misunderstanding context. Bias happens when the source of data is not diverse or not representative. And security problems can include leaking PII, and cybercriminals using the LLMs for malicious tasks like phishing. Especially in these early days of large language models, we do need to mitigate risk. But open source LLMs are thriving in business. Here at IBM, the Watsonx.ai Studio makes available access to multiple Llama 2 models, and IBM has released a series of foundation models of its own called Granite. And this space is changing rapidly, making open source LLMs a field well-worth keeping a close eye on.",
    "chunks": [
      "Kind: captions Language: en There are over 325,000 models on Huggingface and thousands more are being added. And why might you choose to use AI models like these? Well, let's start by getting a few things straight. The models we're talking about in this video, they're specifically LLMs, and that's large language models, which are foundation models that use artificial intelligence, deep learning and massive datasets to generate text. We're talking generative AI. And there are two types of",
      "generative AI models: There's proprietary models, and there are open source models. Now, proprietary LLMs, those are owned by a company who can control its usage. A proprietary LLM may include a license that restricts how the LLM can be used. On the other hand, open source LLMs are free and available for anyone to access, and developers and researchers are free to use, improve or otherwise modify the model. Now look, it's not true in every instance, but generally many proprietary LLMs are far",
      "larger in size than open source models. And specifically in terms of parameter size. Some of the leading proprietary LLMs extend to thousands of billions of parameters. Probably? Actually, we don't necessarily know because, well, those LLMs and that parameter counts are proprietary. But bigger isn't necessarily better. And the open source model ecosystem is showing promise in challenging the proprietary LLM business model. So let's discuss the benefits of open source LLMs. Let's talk about the",
      "types of organizations that are using them. Let's talk about some of the leading open source models available today, and we should talk about the risks associated with using them. Now, clearly, one of the benefits of a open source large language model, that has to be transparency. Open source LLMs may offer better insight into how they work, their architecture, and the training data used to develop them. Another big one is pre-trained open source LLMs allow a process called fine tuning. That",
      "means you can add features to the LLM that benefit your specific use case and the LLMs can be trained on specific data sets. So I can fine tune an LLM with my own data. And community contributions are a big plus. Using a proprietary LLM means you're reliant on a single provider, whereas open source models benefit from community contributions and multiple service providers. You can experiment and use contributions from people with varying perspectives. And these benefits have led to all sorts of",
      "organizations to use open source LLMs. In another video, I addressed how NASA and IBM developed an open source LLM trained on geospatial data. Some healthcare organizations use open source LLMs for diagnostic tools and treatment optimization. There's an open source LLM called FinGPT [fin / financial]. It was developed for the financial industry. Which brings us onto the topic of talking about some specific open source LLMs that you might find of interest. Now Huggingface maintains an open LLM",
      "leaderboard, and that tracks , ranks, and evaluates open source LLMs on various benchmarks like which LLM is scoring highest on the Truthful AI Benchmark series, which measures whether a language model is truthful in generating answers to questions. So it gives those answers a score. And the top spots on this leaderboard, they change frequently. And it's quite fun to watch the progress these models are making. Many of the models on the leaderboard are variations on the Llama 2 open source LLM.",
      "That's the one provided by Meta AI. And Llama 2 encompasses pre-trained and fine tuned generative text models from 70 billion all the way down to 7 billion parameters. And it's licensed for commercial use. Vicuna was created on top of the Llama model and fine tuned to follow instructions. And then it's also Bloom by BigScience, which is a multilingual language model created by more than 1000 AI researchers. Now, one area that both proprietary and open source LLMs share is their associated risks.",
      "Although LLM outputs often sounds fluent and authoritative, they can be confidently wrong. Hallucinations, they can result from the LLM being trained on incomplete, contradictory, or inaccurate data from misunderstanding context. Bias happens when the source of data is not diverse or not representative. And security problems can include leaking PII, and cybercriminals using the LLMs for malicious tasks like phishing. Especially in these early days of large language models, we do need to mitigate",
      "risk. But open source LLMs are thriving in business. Here at IBM, the Watsonx.ai Studio makes available access to multiple Llama 2 models, and IBM has released a series of foundation models of its own called Granite. And this space is changing rapidly, making open source LLMs a field well-worth keeping a close eye on."
    ],
    "chunk_count": 10,
    "content_id": "c94900ac-60db-48a0-9c8a-02e43095ead3",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554922"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=KTonvXhsxpc": {
    "title": "What is DeepSeek? AI Model Basics Explained",
    "url": "https://www.youtube.com/watch?v=KTonvXhsxpc",
    "description": "Want to learn more about how to choose the right AI foundation model? Read the Ebook here → https://ibm.biz/BdGGqN\n\nLearn more about DeepSeek here → https://ibm.biz/BdGGqt\n\nWant to hear more about the facts and hype of DeepSeek from our Mixture of Experts? Watch here → https://ibm.biz/BdGGqk\n\nExplore the unique capabilities of DeepSeek, an innovative AI reasoning model. 🌟 Martin Keen and Aaron Baughman discuss the evolution of DeepSeek models, focusing on DeepSeek-R1. Learn how it uses chain of thought reasoning, reinforcement learning, and expert architectures to achieve top-tier performance efficiently. 🚀\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdGGq6\n\n#deepseek #ai #machinelearning",
    "duration": 622,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Chances are you've heard about the newest entrants to the very crowded and very competitive realm of AI models, DeepSeek. It's a startup based in China, and it caught everyone's attention by taking over OpenAI's coveted spot for most downloaded free app in the US on Apple's App Store. So how? Well, by releasing an open source model that it claims can match all surpass the performance of other industry leading models and at a fraction of the cost. Now, the specific model that's really making a splash from DeepSeek is called DeepSeek R1, and the R here that implies reasoning, because this is a reasoning model. DeepSeek R1 is their reasoning model. Now DeepSeek our one performs as well as some of the other models, including OpenAI's own reasoning model. That's called o1, and it can match or even outperform it across a number of AI benchmarks for math and coding tasks, which is all the more remarkable because according to DeepSeek, DeepSeek R1 is trained with far fewer chips and is approximately 96% cheaper to run than o1. Now, unlike previous AI models which produce an answer without explaining the why, a reasoning model solves complex problems by breaking them down into steps. So before answering a user query, the model spends time thinking, thinking in air quotes here, and that thinking time could be a few seconds or even minutes. Now, during this time, the model is performing step by step analysis through a process that is known as chain of thought. And unlike other reasoning models, R1 shows the use of that chain of thought process as it breaks the problem down, as it generates insights, as it backtrack and says it needs to, and as it ultimately arrives at an answer. Now I'm going to get into how this model works. But before that, let's talk about how it came to be a DeepSeek R1 seems to have come out of nowhere. But there are in fact many DeepSeek models that brought us to this point a model avalanche, if you like. And my colleague Aaron can help dig us out. Well, thanks, Martin. There is certainly a lot to dig out here. There's a lot of these models. But let's start from the very top in the beginning of all this. So we began and we go to, let's say, DeepSeek version one, which is a 67 billion model that was released in January of 2024. Now, this is a traditional transformer with a focus on the feedforward neural networks. This gets us down into DeepSeek version two, which really put this on the map. This is a very large 236 billion model that was released not that far away from the original, which is June 2024. But to put this into perspective, there are really two novel aspects around this model. The first one, was the multi-headed laden attention. And the second aspect was the DeepSeek mixture of experts. It just made the model really fast and performant. And it set us up for success for the DeepSeek version three, which was released December of 2024. Now, this one is even bigger. It's 671 billion parameters. But this is where we began to see the introduction of using reinforcement learning with that model, and some other contributions that this model had is it was able to balance load across many GPUs, because they used a lot of H800s within their infrastructure and that was also built around on top of DeepSeek v2. So all these models accumulate and build on top of each other, which gets us down into DeepSeek R1-Zero, which was released in January of 2025. So this is the first of the reasoning model is now, right? It is, yeah. And it's really neat how they began to train, you know, these types of models, right? So it's a type of fine tuning. But on this one, the exclusively use reinforcement learning, which is a way where you have policies and you want to reward or you want to penalize the model for some action that it has taken or output that it has taken in itself learns over time, and it was very performant. It did well, but it got even better with DeepSeek R1, right, which was again built on top of R1-Zero, and this one used a combination of reinforcement learning, and supervised fine tuning the best of both worlds so that it could even be better, and it's very close to performance on many standards and benchmarks as some of these OpenAI models we have now. And this gets us down into now distilled models, which is like a whole other paradigm. Distilled models. Okay, so tell me what that is all about. Yeah, great question and comment. So first of all, for a distilled model is where you have a student model, which is a very small model, and you have the teacher model, which is very big, and you want to distill or extract knowledge from the teacher model down into the student model, and some aspects you could think of it as model compression. But one interesting aspect around this is this is not just compression or transferring knowledge, but it's model translation because we're going from the R1-Zero, right? Which is one of those mixture of expert models down into, for example, a Llama series model, which is not a mixture of experts, but it's a traditional transformer, right? So, so you're going from one architecture type to another. And we do the same with Qwen, right? So there's different series of models that are the foundation that we then distill into from the R1-Zero. Well, thanks. It's really interesting to get the history behind all this. It didn't come from nowhere, but with all of these distilled models coming, I think you might need your shovel back to dig your way out of those. Thank you very much. There's going to be a lot of distilled models. So you're exactly right. I think you're going to go dig. Thanks. So our one didn't come from nowhere is an evolution of other models, but how does DeepSeek operate at such comparatively low cost? Well, by using a fraction of the highly specialized Invidia chips used by their American competitors to train their systems. If I can illustrate this in a graph. So if we consider different types of model and then the number of GPUs that they use. Well, DeepSeek engineers, for example, they said that they only need 2000 GPUs  that's graphical processing units to train the DeepSeek V3 Model, DeepSeek V3. Now in isolation, what does that mean? Is that good? Is that bad? Well, by contrast, meta said that the company was training that latest opensource model. That's Llama 4 and they are using a computer cluster with over 100,000 Nvidia GPUs. So that brings up the question of how is it so efficient? Well, DeepSeek R1 combines chain of thought reasoning with a process called reinforcement learning. This is a capability that Aaron mentioned just now which arrived at the V3 model of DeepSeek. And here an autonomous agent learns to perform a task through trial and error without any instructions from a human user. Now, traditionally, models will improve their ability to reason by being trained on labeled examples of correct or incorrect behavior. That's known as supervised learning, or by extracting information from hidden patterns that send us unsupervised learning. But the key hypothesis here with reinforcement learning is to reward the model for correctness, no matter how it arrived at the right answer and let the model discover the best way to think all on its own. Now DeepSeek R1 also uses a mixture of experts, architectural or MoE, and a mixture of experts architecture is considerably less resource intensive to train. Now the MoE architecture divides an AI model up into separate entities or sub networks, which we can think of as being individual experts. So in my little neural network here, I'm going to create three experts and a real MoE architecture probably have quite a bit more than that. But each one of these is specialized in a subset of the input data, and the model only activates the specific experts needed for a given task. So a request comes in. We activate the experts that we need and we only use those rather than activating the entire neural network. So consequently, the MoE architecture reduces computational costs during pre-training and achieves faster performance during inference time and look MoE, that architecture isn't unique to models from DeepSeek. There are models from the French AI company Mistral that also use this, and in fact the IBM Granite model that is also built on a mixture of experts architecture. So it's a commonly used architecture. So that is DeepSeek R1. It's an AI reasoning model that is matching other industry leading models on reasoning benchmarks, while being delivered at a fraction of the cost in both training and inference. All of which makes me think this is an exciting time for AI reasoning models.",
    "chunks": [
      "Kind: captions Language: en Chances are you've heard about the newest entrants to the very crowded and very competitive realm of AI models, DeepSeek. It's a startup based in China, and it caught everyone's attention by taking over OpenAI's coveted spot for most downloaded free app in the US on Apple's App Store. So how? Well, by releasing an open source model that it claims can match all surpass the performance of other industry leading models and at a fraction of the cost. Now, the specific",
      "model that's really making a splash from DeepSeek is called DeepSeek R1, and the R here that implies reasoning, because this is a reasoning model. DeepSeek R1 is their reasoning model. Now DeepSeek our one performs as well as some of the other models, including OpenAI's own reasoning model. That's called o1, and it can match or even outperform it across a number of AI benchmarks for math and coding tasks, which is all the more remarkable because according to DeepSeek, DeepSeek R1 is trained with",
      "far fewer chips and is approximately 96% cheaper to run than o1. Now, unlike previous AI models which produce an answer without explaining the why, a reasoning model solves complex problems by breaking them down into steps. So before answering a user query, the model spends time thinking, thinking in air quotes here, and that thinking time could be a few seconds or even minutes. Now, during this time, the model is performing step by step analysis through a process that is known as chain of",
      "thought. And unlike other reasoning models, R1 shows the use of that chain of thought process as it breaks the problem down, as it generates insights, as it backtrack and says it needs to, and as it ultimately arrives at an answer. Now I'm going to get into how this model works. But before that, let's talk about how it came to be a DeepSeek R1 seems to have come out of nowhere. But there are in fact many DeepSeek models that brought us to this point a model avalanche, if you like. And my",
      "colleague Aaron can help dig us out. Well, thanks, Martin. There is certainly a lot to dig out here. There's a lot of these models. But let's start from the very top in the beginning of all this. So we began and we go to, let's say, DeepSeek version one, which is a 67 billion model that was released in January of 2024. Now, this is a traditional transformer with a focus on the feedforward neural networks. This gets us down into DeepSeek version two, which really put this on the map. This is a",
      "very large 236 billion model that was released not that far away from the original, which is June 2024. But to put this into perspective, there are really two novel aspects around this model. The first one, was the multi-headed laden attention. And the second aspect was the DeepSeek mixture of experts. It just made the model really fast and performant. And it set us up for success for the DeepSeek version three, which was released December of 2024. Now, this one is even bigger. It's 671 billion",
      "parameters. But this is where we began to see the introduction of using reinforcement learning with that model, and some other contributions that this model had is it was able to balance load across many GPUs, because they used a lot of H800s within their infrastructure and that was also built around on top of DeepSeek v2. So all these models accumulate and build on top of each other, which gets us down into DeepSeek R1-Zero, which was released in January of 2025. So this is the first of the",
      "reasoning model is now, right? It is, yeah. And it's really neat how they began to train, you know, these types of models, right? So it's a type of fine tuning. But on this one, the exclusively use reinforcement learning, which is a way where you have policies and you want to reward or you want to penalize the model for some action that it has taken or output that it has taken in itself learns over time, and it was very performant. It did well, but it got even better with DeepSeek R1, right,",
      "which was again built on top of R1-Zero, and this one used a combination of reinforcement learning, and supervised fine tuning the best of both worlds so that it could even be better, and it's very close to performance on many standards and benchmarks as some of these OpenAI models we have now. And this gets us down into now distilled models, which is like a whole other paradigm. Distilled models. Okay, so tell me what that is all about. Yeah, great question and comment. So first of all, for a",
      "distilled model is where you have a student model, which is a very small model, and you have the teacher model, which is very big, and you want to distill or extract knowledge from the teacher model down into the student model, and some aspects you could think of it as model compression. But one interesting aspect around this is this is not just compression or transferring knowledge, but it's model translation because we're going from the R1-Zero, right? Which is one of those mixture of expert",
      "models down into, for example, a Llama series model, which is not a mixture of experts, but it's a traditional transformer, right? So, so you're going from one architecture type to another. And we do the same with Qwen, right? So there's different series of models that are the foundation that we then distill into from the R1-Zero. Well, thanks. It's really interesting to get the history behind all this. It didn't come from nowhere, but with all of these distilled models coming, I think you might",
      "need your shovel back to dig your way out of those. Thank you very much. There's going to be a lot of distilled models. So you're exactly right. I think you're going to go dig. Thanks. So our one didn't come from nowhere is an evolution of other models, but how does DeepSeek operate at such comparatively low cost? Well, by using a fraction of the highly specialized Invidia chips used by their American competitors to train their systems. If I can illustrate this in a graph. So if we consider",
      "different types of model and then the number of GPUs that they use. Well, DeepSeek engineers, for example, they said that they only need 2000 GPUs that's graphical processing units to train the DeepSeek V3 Model, DeepSeek V3. Now in isolation, what does that mean? Is that good? Is that bad? Well, by contrast, meta said that the company was training that latest opensource model. That's Llama 4 and they are using a computer cluster with over 100,000 Nvidia GPUs. So that brings up the question of",
      "how is it so efficient? Well, DeepSeek R1 combines chain of thought reasoning with a process called reinforcement learning. This is a capability that Aaron mentioned just now which arrived at the V3 model of DeepSeek. And here an autonomous agent learns to perform a task through trial and error without any instructions from a human user. Now, traditionally, models will improve their ability to reason by being trained on labeled examples of correct or incorrect behavior. That's known as supervised",
      "learning, or by extracting information from hidden patterns that send us unsupervised learning. But the key hypothesis here with reinforcement learning is to reward the model for correctness, no matter how it arrived at the right answer and let the model discover the best way to think all on its own. Now DeepSeek R1 also uses a mixture of experts, architectural or MoE, and a mixture of experts architecture is considerably less resource intensive to train. Now the MoE architecture divides an AI",
      "model up into separate entities or sub networks, which we can think of as being individual experts. So in my little neural network here, I'm going to create three experts and a real MoE architecture probably have quite a bit more than that. But each one of these is specialized in a subset of the input data, and the model only activates the specific experts needed for a given task. So a request comes in. We activate the experts that we need and we only use those rather than activating the entire",
      "neural network. So consequently, the MoE architecture reduces computational costs during pre-training and achieves faster performance during inference time and look MoE, that architecture isn't unique to models from DeepSeek. There are models from the French AI company Mistral that also use this, and in fact the IBM Granite model that is also built on a mixture of experts architecture. So it's a commonly used architecture. So that is DeepSeek R1. It's an AI reasoning model that is matching other",
      "industry leading models on reasoning benchmarks, while being delivered at a fraction of the cost in both training and inference. All of which makes me think this is an exciting time for AI reasoning models."
    ],
    "chunk_count": 18,
    "content_id": "17881bce-2b8c-4a95-b5a7-5270f9a512c5",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554925"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=iR2O2GPbB0E&t=1s": {
    "title": "What are Large Language Models (LLMs)?",
    "url": "https://www.youtube.com/watch?v=iR2O2GPbB0E&t=1s",
    "description": "Learn about Large Language Models (LLMs), a powerful neural network that enables computers to process and generate language better than ever before. Dale and Nikita share how LLMs work and how you can interact with them via prompts.\n\nResources:\nTry out Bard → https://goo.gle/420KQhw \nWhat is a Transformer? → https://goo.gle/3VABlDw \n\nSubscribe to Google Developers → https://goo.gle/developers \n\n#MachineLearning #MLmodels #LLMs #GenerativeAI",
    "duration": 330,
    "uploader": "Google for Developers",
    "transcript": "Kind: captions Language: en unless you've been living under a rock unless you've been living under a rock unless you've been living under a rock you've probably heard that AI is getting you've probably heard that AI is getting you've probably heard that AI is getting very good at conversation in fact maybe very good at conversation in fact maybe very good at conversation in fact maybe you even chatted with one of these AIS you even chatted with one of these AIS you even chatted with one of these AIS through a chatbot interface like Google through a chatbot interface like Google through a chatbot interface like Google bar this is all thanks to a powerful bar this is all thanks to a powerful bar this is all thanks to a powerful kind of neural network called a large kind of neural network called a large kind of neural network called a large language model or llm llms enable language model or llm llms enable language model or llm llms enable computers to understand and generate computers to understand and generate computers to understand and generate language better than ever before language better than ever before language better than ever before unlocking a whole host of new unlocking a whole host of new unlocking a whole host of new applications in this video we're going applications in this video we're going applications in this video we're going to talk about what LMS are and how to talk about what LMS are and how to talk about what LMS are and how anyone can get started building with anyone can get started building with anyone can get started building with them whether you're a developer or not them whether you're a developer or not them whether you're a developer or not ready let's Dive In LMS are machine learning models that are LMS are machine learning models that are LMS are machine learning models that are really good at understanding and really good at understanding and really good at understanding and generating human language they're based generating human language they're based generating human language they're based on Transformers a type of neural network on Transformers a type of neural network on Transformers a type of neural network architecture invented by Google Now what architecture invented by Google Now what architecture invented by Google Now what made the Transformer architecture so made the Transformer architecture so made the Transformer architecture so powerful was its ability to scale powerful was its ability to scale powerful was its ability to scale effectively allowing us to train these effectively allowing us to train these effectively allowing us to train these models on massive Text data sets that's models on massive Text data sets that's models on massive Text data sets that's where the large and large language where the large and large language where the large and large language models comes from both the size and models comes from both the size and models comes from both the size and complexity of the neural network itself complexity of the neural network itself complexity of the neural network itself as well as the size of the data set that as well as the size of the data set that as well as the size of the data set that it was trained on for some of these it was trained on for some of these it was trained on for some of these models we're talking about trillions of models we're talking about trillions of models we're talking about trillions of tokens from a bunch of publicly tokens from a bunch of publicly tokens from a bunch of publicly available sources available sources available sources and it wasn't until researchers started and it wasn't until researchers started and it wasn't until researchers started to make these models really large and to make these models really large and to make these models really large and train them on these huge data sets that train them on these huge data sets that train them on these huge data sets that they started showing these impressive they started showing these impressive they started showing these impressive results like understanding complex results like understanding complex results like understanding complex nuanced language and generating language nuanced language and generating language nuanced language and generating language more eloquently than ever more eloquently than ever more eloquently than ever if you're already familiar with machine if you're already familiar with machine if you're already familiar with machine learning you probably think about learning you probably think about learning you probably think about training a model for a specific task training a model for a specific task training a model for a specific task like is this tweet positive or negative like is this tweet positive or negative like is this tweet positive or negative or translate this text from French to or translate this text from French to or translate this text from French to English English English what makes llms especially powerful is what makes llms especially powerful is what makes llms especially powerful is that one model can be used for a whole that one model can be used for a whole that one model can be used for a whole variety of tasks like chat copywriting variety of tasks like chat copywriting variety of tasks like chat copywriting translation summarization brainstorming translation summarization brainstorming translation summarization brainstorming co-generation and a whole lot more best co-generation and a whole lot more best co-generation and a whole lot more best of all you can prototype language of all you can prototype language of all you can prototype language applications incredibly fast with llms applications incredibly fast with llms applications incredibly fast with llms in just minutes rather than months and in just minutes rather than months and in just minutes rather than months and you don't have to be a machine learning you don't have to be a machine learning you don't have to be a machine learning expert to do it all you really need to expert to do it all you really need to expert to do it all you really need to know is how to write so how do you know is how to write so how do you know is how to write so how do you actually use an llm well let's take a actually use an llm well let's take a actually use an llm well let's take a look look look llms learn about patterns and language llms learn about patterns and language llms learn about patterns and language from the massive amounts of text Data from the massive amounts of text Data from the massive amounts of text Data they're trained on then they take as they're trained on then they take as they're trained on then they take as input some text and produce some output input some text and produce some output input some text and produce some output text that's likely to follow another way text that's likely to follow another way text that's likely to follow another way to say this is that LMS are like really to say this is that LMS are like really to say this is that LMS are like really sophisticated autocomplete so for sophisticated autocomplete so for sophisticated autocomplete so for example if we give an LM the input it's example if we give an LM the input it's example if we give an LM the input it's raining cats and it'll probably predict raining cats and it'll probably predict raining cats and it'll probably predict that dogs is the most likely word to that dogs is the most likely word to that dogs is the most likely word to follow now this might not seem that follow now this might not seem that follow now this might not seem that exciting but we can actually use this exciting but we can actually use this exciting but we can actually use this autocomplete like functionality to solve autocomplete like functionality to solve autocomplete like functionality to solve tons of tasks just by writing strategic tons of tasks just by writing strategic tons of tasks just by writing strategic text input for example let's take text input for example let's take text input for example let's take Google's palm llm and input this Google's palm llm and input this Google's palm llm and input this sentence I have two apples and I eat one sentence I have two apples and I eat one sentence I have two apples and I eat one I'm left with the Palm model outputs the I'm left with the Palm model outputs the I'm left with the Palm model outputs the answer one in this way we get the llm to answer one in this way we get the llm to answer one in this way we get the llm to perform some simple math or take another perform some simple math or take another perform some simple math or take another example Paris is to France as Tokyo is example Paris is to France as Tokyo is example Paris is to France as Tokyo is the Palm model outputs Japan which tells the Palm model outputs Japan which tells the Palm model outputs Japan which tells us that the model can not only complete us that the model can not only complete us that the model can not only complete analogies but it also has some World analogies but it also has some World analogies but it also has some World Knowledge that it's learned from its Knowledge that it's learned from its Knowledge that it's learned from its training data training data training data so I should add the caveat that not all so I should add the caveat that not all so I should add the caveat that not all of the knowledge that the LM outputs is of the knowledge that the LM outputs is of the knowledge that the LM outputs is necessarily sexually accurate now all of necessarily sexually accurate now all of necessarily sexually accurate now all of the text that we feed into an llm as the text that we feed into an llm as the text that we feed into an llm as input is called a prompt and it turns input is called a prompt and it turns input is called a prompt and it turns out there's this whole art known as out there's this whole art known as out there's this whole art known as prompt design which is about figuring prompt design which is about figuring prompt design which is about figuring out how to write and format prompt text out how to write and format prompt text out how to write and format prompt text to get llms to do what you want to get llms to do what you want to get llms to do what you want for example one way to structure a for example one way to structure a for example one way to structure a prompt is as an instruction like write prompt is as an instruction like write prompt is as an instruction like write me a poem about Ada Lovelace and the me a poem about Ada Lovelace and the me a poem about Ada Lovelace and the style of Shakespeare style of Shakespeare style of Shakespeare or explain quantum physics to me like or explain quantum physics to me like or explain quantum physics to me like I'm five or generate a list of items I I'm five or generate a list of items I I'm five or generate a list of items I need for a camping trip to Yosemite need for a camping trip to Yosemite need for a camping trip to Yosemite National Park National Park National Park this approach using a single command to this approach using a single command to this approach using a single command to get an alarm to take on a behavior is get an alarm to take on a behavior is get an alarm to take on a behavior is called zero shot learning but in called zero shot learning but in called zero shot learning but in addition to just providing an addition to just providing an addition to just providing an instruction it can be helpful to the instruction it can be helpful to the instruction it can be helpful to the model what you want by adding examples model what you want by adding examples model what you want by adding examples this is called fuchsia learning because this is called fuchsia learning because this is called fuchsia learning because we show the model a few examples like we show the model a few examples like we show the model a few examples like here's a prompt for translating from here's a prompt for translating from here's a prompt for translating from English to French English to French English to French first we provide an instruction first we provide an instruction first we provide an instruction then we give some examples establishing then we give some examples establishing then we give some examples establishing the text pattern the text pattern the text pattern if we pass this prompt to an llm like if we pass this prompt to an llm like if we pass this prompt to an llm like Palm we get back something like the Palm we get back something like the Palm we get back something like the following following following the model did provide a French the model did provide a French the model did provide a French translation of lipstick but you might translation of lipstick but you might translation of lipstick but you might notice that it went on to generate all notice that it went on to generate all notice that it went on to generate all these additional English French these additional English French these additional English French translation pairs this might seem a translation pairs this might seem a translation pairs this might seem a little unexpected but the llm is just little unexpected but the llm is just little unexpected but the llm is just completing the pattern that we gave it completing the pattern that we gave it completing the pattern that we gave it in the prompt in the prompt in the prompt as another example here's a few shot as another example here's a few shot as another example here's a few shot prompt to convert python code Snippets prompt to convert python code Snippets prompt to convert python code Snippets to JavaScript our prompt starts with an to JavaScript our prompt starts with an to JavaScript our prompt starts with an instruction instruction instruction then we have some examples and finally then we have some examples and finally then we have some examples and finally the python code we actually want the python code we actually want the python code we actually want converted converted converted the very last part of this prompt is the very last part of this prompt is the very last part of this prompt is Javascript colon because we want to Javascript colon because we want to Javascript colon because we want to nudge the model to Output some nudge the model to Output some nudge the model to Output some JavaScript code just like this JavaScript code just like this JavaScript code just like this note that in a real application we note that in a real application we note that in a real application we probably want to parameterize the input probably want to parameterize the input probably want to parameterize the input instead of hard coding it into the instead of hard coding it into the instead of hard coding it into the prompt that way our users can provide prompt that way our users can provide prompt that way our users can provide the python code that they want converted the python code that they want converted the python code that they want converted and this is essentially how you would and this is essentially how you would and this is essentially how you would customize an LM for python to JavaScript customize an LM for python to JavaScript customize an LM for python to JavaScript now you might be wondering what the now you might be wondering what the now you might be wondering what the absolute best way to write a model absolute best way to write a model absolute best way to write a model prompt is and if so we've got some bad prompt is and if so we've got some bad prompt is and if so we've got some bad news for you there's currently no news for you there's currently no news for you there's currently no optimal way to write model prompts and optimal way to write model prompts and optimal way to write model prompts and that's because the results we get are so that's because the results we get are so that's because the results we get are so highly dependent on the underlying model highly dependent on the underlying model highly dependent on the underlying model sometimes small changes in wording or sometimes small changes in wording or sometimes small changes in wording or even word order can improve the lm's even word order can improve the lm's even word order can improve the lm's outputs in ways that are not always outputs in ways that are not always outputs in ways that are not always predictable that's why it's always worth predictable that's why it's always worth predictable that's why it's always worth trying out lots of different structures trying out lots of different structures trying out lots of different structures and examples and formats and seeing what and examples and formats and seeing what and examples and formats and seeing what works best for your use case there you works best for your use case there you works best for your use case there you have it that's the magic of LMS in a have it that's the magic of LMS in a have it that's the magic of LMS in a nutshell you can check out Bard at nutshell you can check out Bard at nutshell you can check out Bard at bard.google.com and definitely let us bard.google.com and definitely let us bard.google.com and definitely let us know in the comments below what you're know in the comments below what you're know in the comments below what you're building with llms",
    "chunks": [
      "Kind: captions Language: en unless you've been living under a rock unless you've been living under a rock unless you've been living under a rock you've probably heard that AI is getting you've probably heard that AI is getting you've probably heard that AI is getting very good at conversation in fact maybe very good at conversation in fact maybe very good at conversation in fact maybe you even chatted with one of these AIS you even chatted with one of these AIS you even chatted with one of these",
      "AIS through a chatbot interface like Google through a chatbot interface like Google through a chatbot interface like Google bar this is all thanks to a powerful bar this is all thanks to a powerful bar this is all thanks to a powerful kind of neural network called a large kind of neural network called a large kind of neural network called a large language model or llm llms enable language model or llm llms enable language model or llm llms enable computers to understand and generate computers to",
      "understand and generate computers to understand and generate language better than ever before language better than ever before language better than ever before unlocking a whole host of new unlocking a whole host of new unlocking a whole host of new applications in this video we're going applications in this video we're going applications in this video we're going to talk about what LMS are and how to talk about what LMS are and how to talk about what LMS are and how anyone can get started",
      "building with anyone can get started building with anyone can get started building with them whether you're a developer or not them whether you're a developer or not them whether you're a developer or not ready let's Dive In LMS are machine learning models that are LMS are machine learning models that are LMS are machine learning models that are really good at understanding and really good at understanding and really good at understanding and generating human language they're based generating",
      "human language they're based generating human language they're based on Transformers a type of neural network on Transformers a type of neural network on Transformers a type of neural network architecture invented by Google Now what architecture invented by Google Now what architecture invented by Google Now what made the Transformer architecture so made the Transformer architecture so made the Transformer architecture so powerful was its ability to scale powerful was its ability to scale",
      "powerful was its ability to scale effectively allowing us to train these effectively allowing us to train these effectively allowing us to train these models on massive Text data sets that's models on massive Text data sets that's models on massive Text data sets that's where the large and large language where the large and large language where the large and large language models comes from both the size and models comes from both the size and models comes from both the size and complexity of the",
      "neural network itself complexity of the neural network itself complexity of the neural network itself as well as the size of the data set that as well as the size of the data set that as well as the size of the data set that it was trained on for some of these it was trained on for some of these it was trained on for some of these models we're talking about trillions of models we're talking about trillions of models we're talking about trillions of tokens from a bunch of publicly tokens from a",
      "bunch of publicly tokens from a bunch of publicly available sources available sources available sources and it wasn't until researchers started and it wasn't until researchers started and it wasn't until researchers started to make these models really large and to make these models really large and to make these models really large and train them on these huge data sets that train them on these huge data sets that train them on these huge data sets that they started showing these impressive they",
      "started showing these impressive they started showing these impressive results like understanding complex results like understanding complex results like understanding complex nuanced language and generating language nuanced language and generating language nuanced language and generating language more eloquently than ever more eloquently than ever more eloquently than ever if you're already familiar with machine if you're already familiar with machine if you're already familiar with machine",
      "learning you probably think about learning you probably think about learning you probably think about training a model for a specific task training a model for a specific task training a model for a specific task like is this tweet positive or negative like is this tweet positive or negative like is this tweet positive or negative or translate this text from French to or translate this text from French to or translate this text from French to English English English what makes llms especially",
      "powerful is what makes llms especially powerful is what makes llms especially powerful is that one model can be used for a whole that one model can be used for a whole that one model can be used for a whole variety of tasks like chat copywriting variety of tasks like chat copywriting variety of tasks like chat copywriting translation summarization brainstorming translation summarization brainstorming translation summarization brainstorming co-generation and a whole lot more best co-generation and",
      "a whole lot more best co-generation and a whole lot more best of all you can prototype language of all you can prototype language of all you can prototype language applications incredibly fast with llms applications incredibly fast with llms applications incredibly fast with llms in just minutes rather than months and in just minutes rather than months and in just minutes rather than months and you don't have to be a machine learning you don't have to be a machine learning you don't have to be a",
      "machine learning expert to do it all you really need to expert to do it all you really need to expert to do it all you really need to know is how to write so how do you know is how to write so how do you know is how to write so how do you actually use an llm well let's take a actually use an llm well let's take a actually use an llm well let's take a look look look llms learn about patterns and language llms learn about patterns and language llms learn about patterns and language from the massive",
      "amounts of text Data from the massive amounts of text Data from the massive amounts of text Data they're trained on then they take as they're trained on then they take as they're trained on then they take as input some text and produce some output input some text and produce some output input some text and produce some output text that's likely to follow another way text that's likely to follow another way text that's likely to follow another way to say this is that LMS are like really to say",
      "this is that LMS are like really to say this is that LMS are like really sophisticated autocomplete so for sophisticated autocomplete so for sophisticated autocomplete so for example if we give an LM the input it's example if we give an LM the input it's example if we give an LM the input it's raining cats and it'll probably predict raining cats and it'll probably predict raining cats and it'll probably predict that dogs is the most likely word to that dogs is the most likely word to that dogs is",
      "the most likely word to follow now this might not seem that follow now this might not seem that follow now this might not seem that exciting but we can actually use this exciting but we can actually use this exciting but we can actually use this autocomplete like functionality to solve autocomplete like functionality to solve autocomplete like functionality to solve tons of tasks just by writing strategic tons of tasks just by writing strategic tons of tasks just by writing strategic text input",
      "for example let's take text input for example let's take text input for example let's take Google's palm llm and input this Google's palm llm and input this Google's palm llm and input this sentence I have two apples and I eat one sentence I have two apples and I eat one sentence I have two apples and I eat one I'm left with the Palm model outputs the I'm left with the Palm model outputs the I'm left with the Palm model outputs the answer one in this way we get the llm to answer one in this way",
      "we get the llm to answer one in this way we get the llm to perform some simple math or take another perform some simple math or take another perform some simple math or take another example Paris is to France as Tokyo is example Paris is to France as Tokyo is example Paris is to France as Tokyo is the Palm model outputs Japan which tells the Palm model outputs Japan which tells the Palm model outputs Japan which tells us that the model can not only complete us that the model can not only complete",
      "us that the model can not only complete analogies but it also has some World analogies but it also has some World analogies but it also has some World Knowledge that it's learned from its Knowledge that it's learned from its Knowledge that it's learned from its training data training data training data so I should add the caveat that not all so I should add the caveat that not all so I should add the caveat that not all of the knowledge that the LM outputs is of the knowledge that the LM outputs",
      "is of the knowledge that the LM outputs is necessarily sexually accurate now all of necessarily sexually accurate now all of necessarily sexually accurate now all of the text that we feed into an llm as the text that we feed into an llm as the text that we feed into an llm as input is called a prompt and it turns input is called a prompt and it turns input is called a prompt and it turns out there's this whole art known as out there's this whole art known as out there's this whole art known as",
      "prompt design which is about figuring prompt design which is about figuring prompt design which is about figuring out how to write and format prompt text out how to write and format prompt text out how to write and format prompt text to get llms to do what you want to get llms to do what you want to get llms to do what you want for example one way to structure a for example one way to structure a for example one way to structure a prompt is as an instruction like write prompt is as an instruction",
      "like write prompt is as an instruction like write me a poem about Ada Lovelace and the me a poem about Ada Lovelace and the me a poem about Ada Lovelace and the style of Shakespeare style of Shakespeare style of Shakespeare or explain quantum physics to me like or explain quantum physics to me like or explain quantum physics to me like I'm five or generate a list of items I I'm five or generate a list of items I I'm five or generate a list of items I need for a camping trip to Yosemite need for a",
      "camping trip to Yosemite need for a camping trip to Yosemite National Park National Park National Park this approach using a single command to this approach using a single command to this approach using a single command to get an alarm to take on a behavior is get an alarm to take on a behavior is get an alarm to take on a behavior is called zero shot learning but in called zero shot learning but in called zero shot learning but in addition to just providing an addition to just providing an",
      "addition to just providing an instruction it can be helpful to the instruction it can be helpful to the instruction it can be helpful to the model what you want by adding examples model what you want by adding examples model what you want by adding examples this is called fuchsia learning because this is called fuchsia learning because this is called fuchsia learning because we show the model a few examples like we show the model a few examples like we show the model a few examples like here's a",
      "prompt for translating from here's a prompt for translating from here's a prompt for translating from English to French English to French English to French first we provide an instruction first we provide an instruction first we provide an instruction then we give some examples establishing then we give some examples establishing then we give some examples establishing the text pattern the text pattern the text pattern if we pass this prompt to an llm like if we pass this prompt to an llm like if",
      "we pass this prompt to an llm like Palm we get back something like the Palm we get back something like the Palm we get back something like the following following following the model did provide a French the model did provide a French the model did provide a French translation of lipstick but you might translation of lipstick but you might translation of lipstick but you might notice that it went on to generate all notice that it went on to generate all notice that it went on to generate all",
      "these additional English French these additional English French these additional English French translation pairs this might seem a translation pairs this might seem a translation pairs this might seem a little unexpected but the llm is just little unexpected but the llm is just little unexpected but the llm is just completing the pattern that we gave it completing the pattern that we gave it completing the pattern that we gave it in the prompt in the prompt in the prompt as another example",
      "here's a few shot as another example here's a few shot as another example here's a few shot prompt to convert python code Snippets prompt to convert python code Snippets prompt to convert python code Snippets to JavaScript our prompt starts with an to JavaScript our prompt starts with an to JavaScript our prompt starts with an instruction instruction instruction then we have some examples and finally then we have some examples and finally then we have some examples and finally the python code we",
      "actually want the python code we actually want the python code we actually want converted converted converted the very last part of this prompt is the very last part of this prompt is the very last part of this prompt is Javascript colon because we want to Javascript colon because we want to Javascript colon because we want to nudge the model to Output some nudge the model to Output some nudge the model to Output some JavaScript code just like this JavaScript code just like this JavaScript code",
      "just like this note that in a real application we note that in a real application we note that in a real application we probably want to parameterize the input probably want to parameterize the input probably want to parameterize the input instead of hard coding it into the instead of hard coding it into the instead of hard coding it into the prompt that way our users can provide prompt that way our users can provide prompt that way our users can provide the python code that they want converted",
      "the python code that they want converted the python code that they want converted and this is essentially how you would and this is essentially how you would and this is essentially how you would customize an LM for python to JavaScript customize an LM for python to JavaScript customize an LM for python to JavaScript now you might be wondering what the now you might be wondering what the now you might be wondering what the absolute best way to write a model absolute best way to write a model",
      "absolute best way to write a model prompt is and if so we've got some bad prompt is and if so we've got some bad prompt is and if so we've got some bad news for you there's currently no news for you there's currently no news for you there's currently no optimal way to write model prompts and optimal way to write model prompts and optimal way to write model prompts and that's because the results we get are so that's because the results we get are so that's because the results we get are so highly",
      "dependent on the underlying model highly dependent on the underlying model highly dependent on the underlying model sometimes small changes in wording or sometimes small changes in wording or sometimes small changes in wording or even word order can improve the lm's even word order can improve the lm's even word order can improve the lm's outputs in ways that are not always outputs in ways that are not always outputs in ways that are not always predictable that's why it's always worth predictable",
      "that's why it's always worth predictable that's why it's always worth trying out lots of different structures trying out lots of different structures trying out lots of different structures and examples and formats and seeing what and examples and formats and seeing what and examples and formats and seeing what works best for your use case there you works best for your use case there you works best for your use case there you have it that's the magic of LMS in a have it that's the magic of LMS in",
      "a have it that's the magic of LMS in a nutshell you can check out Bard at nutshell you can check out Bard at nutshell you can check out Bard at bard.google.com and definitely let us bard.google.com and definitely let us bard.google.com and definitely let us know in the comments below what you're know in the comments below what you're know in the comments below what you're building with llms"
    ],
    "chunk_count": 35,
    "content_id": "8456d13b-e833-42f5-a035-67a0142464e1",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554928"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=S-n3hwpMap4": {
    "title": "LEARN AI: Module 3: Prompt Engineering - Listen",
    "url": "https://www.youtube.com/watch?v=S-n3hwpMap4",
    "description": "💡 LEARN AI – Module 3: Prompt Engineering\nDiscover how to talk to AI like a pro. In this module, you'll learn how to craft effective prompts that produce smarter, more useful responses. Then you’ll put that knowledge into action by building your own AI-powered travel itinerary—personalized, efficient, and surprisingly creative.\n\n🧠 LEARN Framework for This Module\n🎧 Listen to an introduction to Prompt Engineering\n🧭 Explore how prompt engineering works through guided practice\n✍️ Apply what you’ve learned to create your travel itinerary\n🤔 Reflect on the experience by discussing insights and lessons\n🤝 Network by sharing and discussing your itineraries\n\n📚 Learn more about the LEARN AI Certificate Program: https://cptc.libguides.com/TLC/LEARNAI\n🎓 Enroll now in the free Canvas course: https://cptc.instructure.com/enroll/M4AE8W\n📺 Watch the full video series: https://www.youtube.com/playlist?list=PLJVkPcvPGu-RhJcHw_MQNRaMUSLqar5MP\n📬 Get updates and bonus content: https://forms.gle/iNoZAZCX9EbA1DRs9\n💬 Join the conversation with other learners: https://www.linkedin.com/groups/10083722/\n🤖 Say hi to your course rep, Simon Robot: https://www.linkedin.com/in/simon-robot-163b83361/\n\n🌐 Connect the course creators:\n🟢 Geoff Cain: https://www.linkedin.com/in/geoffcain/\n🔵 Ronald Lethcoe: https://www.linkedin.com/in/rlethcoe/\n🟣 James Victor Shumate: https://www.linkedin.com/in/james-victor-shumate-65449bb0/\n\n🎶 Music: City Lights - Vens Adams",
    "duration": 459,
    "uploader": "Clover Park Technical College Teaching & Learning ",
    "transcript": "Kind: captions Language: en [Music] Imagine you're working on a group Imagine you're working on a group Imagine you're working on a group project related to climate change and project related to climate change and project related to climate change and you're asking your team members for you're asking your team members for you're asking your team members for help. If you say something vague like, help. If you say something vague like, help. If you say something vague like, \"Can you give me information on climate \"Can you give me information on climate \"Can you give me information on climate change?\" You probably won't get exactly change?\" You probably won't get exactly change?\" You probably won't get exactly what you need. Maybe they'll share what you need. Maybe they'll share what you need. Maybe they'll share irrelevant details or miss important irrelevant details or miss important irrelevant details or miss important points entirely. But if you're clear and points entirely. But if you're clear and points entirely. But if you're clear and specific, something like, \"Can you find specific, something like, \"Can you find specific, something like, \"Can you find three articles about climate change three articles about climate change three articles about climate change effects in coastal cities, you've given effects in coastal cities, you've given effects in coastal cities, you've given your teammate precise instructions, your teammate precise instructions, your teammate precise instructions, which makes it easier for them to which makes it easier for them to which makes it easier for them to deliver exactly what you're looking for. deliver exactly what you're looking for. deliver exactly what you're looking for. That's exactly how working with That's exactly how working with That's exactly how working with generative AI tools like chat GPT works. generative AI tools like chat GPT works. generative AI tools like chat GPT works. The quality of the AI's response depends The quality of the AI's response depends The quality of the AI's response depends heavily on how clearly and specifically heavily on how clearly and specifically heavily on how clearly and specifically you phrase your prompts, the you phrase your prompts, the you phrase your prompts, the instructions or questions that you give instructions or questions that you give instructions or questions that you give it. The better you craft your prompt, it. The better you craft your prompt, it. The better you craft your prompt, the better results you'll receive. This the better results you'll receive. This the better results you'll receive. This careful and intentional structuring of careful and intentional structuring of careful and intentional structuring of your request is called prompt your request is called prompt your request is called prompt engineering. How you phrase your prompt engineering. How you phrase your prompt engineering. How you phrase your prompt directly impacts the quality and directly impacts the quality and directly impacts the quality and accuracy of the AI's response. For accuracy of the AI's response. For accuracy of the AI's response. For example, if you ask Chai GPT something example, if you ask Chai GPT something example, if you ask Chai GPT something general like, \"Tell me about general like, \"Tell me about general like, \"Tell me about scientists.\" You'll likely get a broad scientists.\" You'll likely get a broad scientists.\" You'll likely get a broad generic answer. Probably not very generic answer. Probably not very generic answer. Probably not very helpful. Why? Because your prompt lacks helpful. Why? Because your prompt lacks helpful. Why? Because your prompt lacks specifics. Who are these scientists? specifics. Who are these scientists? specifics. Who are these scientists? What field? What exactly do you want to What field? What exactly do you want to What field? What exactly do you want to know about them? Now, imagine improving know about them? Now, imagine improving know about them? Now, imagine improving that prompt to something like, \"Tell me that prompt to something like, \"Tell me that prompt to something like, \"Tell me about three influential physicists and about three influential physicists and about three influential physicists and their most important discoveries.\" their most important discoveries.\" their most important discoveries.\" Instantly, the AI knows exactly what Instantly, the AI knows exactly what Instantly, the AI knows exactly what you're asking it. It understands the you're asking it. It understands the you're asking it. It understands the topic, the field, and the level of topic, the field, and the level of topic, the field, and the level of detail you're expecting. The result, a detail you're expecting. The result, a detail you're expecting. The result, a more accurate, detailed, and useful more accurate, detailed, and useful more accurate, detailed, and useful answer. Prompt engineering is about answer. Prompt engineering is about answer. Prompt engineering is about carefully crafting your questions and carefully crafting your questions and carefully crafting your questions and instructions to clearly communicate your instructions to clearly communicate your instructions to clearly communicate your goals to the AI. Unlike a traditional goals to the AI. Unlike a traditional goals to the AI. Unlike a traditional web search where you might type a web search where you might type a web search where you might type a general query and then sift through general query and then sift through general query and then sift through multiple websites hoping one of them multiple websites hoping one of them multiple websites hoping one of them contains the exact information you need, contains the exact information you need, contains the exact information you need, generative AI aims to directly create generative AI aims to directly create generative AI aims to directly create the exact response you're looking for. the exact response you're looking for. the exact response you're looking for. With generative AI, your prompt doesn't With generative AI, your prompt doesn't With generative AI, your prompt doesn't just guide you toward an answer. It just guide you toward an answer. It just guide you toward an answer. It generates the answer itself. generates the answer itself. generates the answer itself. The more you practice prompt The more you practice prompt The more you practice prompt engineering, the more natural it becomes engineering, the more natural it becomes engineering, the more natural it becomes and the more powerful your interactions and the more powerful your interactions and the more powerful your interactions with generative AI can be. Ready to see with generative AI can be. Ready to see with generative AI can be. Ready to see prompt engineering in action? Let's get prompt engineering in action? Let's get prompt engineering in action? Let's get started. So, what exactly is prompt started. So, what exactly is prompt started. So, what exactly is prompt engineering? Prompt engineering is the engineering? Prompt engineering is the engineering? Prompt engineering is the process of carefully crafting structured process of carefully crafting structured process of carefully crafting structured instructions or prompts that clearly instructions or prompts that clearly instructions or prompts that clearly communicate what you want from communicate what you want from communicate what you want from generative AI models like generative AI models like generative AI models like chatgpt. You can think of prompt chatgpt. You can think of prompt chatgpt. You can think of prompt engineering as having a conversation engineering as having a conversation engineering as having a conversation with a knowledgeable intern. The intern with a knowledgeable intern. The intern with a knowledgeable intern. The intern is eager to help, but depends on your is eager to help, but depends on your is eager to help, but depends on your clear instructions to succeed. A clear instructions to succeed. A clear instructions to succeed. A well-crafted prompt clearly states your well-crafted prompt clearly states your well-crafted prompt clearly states your goals and provides enough context to goals and provides enough context to goals and provides enough context to guide the AI toward the exact guide the AI toward the exact guide the AI toward the exact information you need. The approach is information you need. The approach is information you need. The approach is foundational for getting the best foundational for getting the best foundational for getting the best possible responses, ensuring the AI's possible responses, ensuring the AI's possible responses, ensuring the AI's output is accurate, relevant, and output is accurate, relevant, and output is accurate, relevant, and aligned with your expectations. To aligned with your expectations. To aligned with your expectations. To summarize summarize summarize clearly, prompt engineering involves clearly, prompt engineering involves clearly, prompt engineering involves crafting structured prompts specifically crafting structured prompts specifically crafting structured prompts specifically designed for interaction with large designed for interaction with large designed for interaction with large language models. The goal to provide language models. The goal to provide language models. The goal to provide clear, detailed information so the AI clear, detailed information so the AI clear, detailed information so the AI can deliver the most accurate and can deliver the most accurate and can deliver the most accurate and helpful responses. Ultimately, it's the helpful responses. Ultimately, it's the helpful responses. Ultimately, it's the foundation for using AI effectively. If foundation for using AI effectively. If foundation for using AI effectively. If you're interested in diving deeper into you're interested in diving deeper into you're interested in diving deeper into prompt engineering, I highly recommend prompt engineering, I highly recommend prompt engineering, I highly recommend checking out the resource from Google checking out the resource from Google checking out the resource from Google called Prompt Engineering for Generative called Prompt Engineering for Generative called Prompt Engineering for Generative AI. It provides detailed guidance, best AI. It provides detailed guidance, best AI. It provides detailed guidance, best practices, and practical examples for practices, and practical examples for practices, and practical examples for improving your prompt engineering improving your prompt engineering improving your prompt engineering skills. This resource is particularly skills. This resource is particularly skills. This resource is particularly valuable because it explains basic valuable because it explains basic valuable because it explains basic techniques, shares prompting best techniques, shares prompting best techniques, shares prompting best practices such as clearly communicating practices such as clearly communicating practices such as clearly communicating your goal, structuring prompts your goal, structuring prompts your goal, structuring prompts effectively, providing specific effectively, providing specific effectively, providing specific examples, and breaking down complex task examples, and breaking down complex task examples, and breaking down complex task into simpler steps. We'll actually go into simpler steps. We'll actually go into simpler steps. We'll actually go into this in more detail in the third into this in more detail in the third into this in more detail in the third course in our AI certificate program, course in our AI certificate program, course in our AI certificate program, but for now, it's a great way to expand but for now, it's a great way to expand but for now, it's a great way to expand your understanding and become proficient your understanding and become proficient your understanding and become proficient at getting the best results from at getting the best results from at getting the best results from generative AI. Now, let's discuss the generative AI. Now, let's discuss the generative AI. Now, let's discuss the basics of creating effective basics of creating effective basics of creating effective prompts. There are many elements you can prompts. There are many elements you can prompts. There are many elements you can include in a prompt, but three are include in a prompt, but three are include in a prompt, but three are especially important. Ro, context, and especially important. Ro, context, and especially important. Ro, context, and task. Although you'll usually write your task. Although you'll usually write your task. Although you'll usually write your prompt in the order of ro, context, and prompt in the order of ro, context, and prompt in the order of ro, context, and task, we'll discuss these elements task, we'll discuss these elements task, we'll discuss these elements starting with task because knowing what starting with task because knowing what starting with task because knowing what you want the AI to do is foundational to you want the AI to do is foundational to you want the AI to do is foundational to any effective prompt. Let's imagine any effective prompt. Let's imagine any effective prompt. Let's imagine you're creating a prompt to generate a you're creating a prompt to generate a you're creating a prompt to generate a travel itinerary for a dream vacation. travel itinerary for a dream vacation. travel itinerary for a dream vacation. The task explicitly states what you want The task explicitly states what you want The task explicitly states what you want from the AI. This could be requesting an from the AI. This could be requesting an from the AI. This could be requesting an itinerary, suggestions, comparisons, or itinerary, suggestions, comparisons, or itinerary, suggestions, comparisons, or any other action that clearly defines any other action that clearly defines any other action that clearly defines the outcome you're seeking. Next, the the outcome you're seeking. Next, the the outcome you're seeking. Next, the role identifies who would be the most role identifies who would be the most role identifies who would be the most knowledgeable about this task. In this knowledgeable about this task. In this knowledgeable about this task. In this scenario, the role could be a travel scenario, the role could be a travel scenario, the role could be a travel agent or perhaps a travel expert. agent or perhaps a travel expert. agent or perhaps a travel expert. Assigning a role helps the AI focus Assigning a role helps the AI focus Assigning a role helps the AI focus specifically on relevant data, filtering specifically on relevant data, filtering specifically on relevant data, filtering out the unrelated information from its out the unrelated information from its out the unrelated information from its vast knowledge base. Lastly, we add vast knowledge base. Lastly, we add vast knowledge base. Lastly, we add context, which provides additional context, which provides additional context, which provides additional background details for a travel background details for a travel background details for a travel itinerary. This could include budget, itinerary. This could include budget, itinerary. This could include budget, location, travel dates, preferences, or location, travel dates, preferences, or location, travel dates, preferences, or travel companions. The more context you travel companions. The more context you travel companions. The more context you provide, the better tailored the AI's provide, the better tailored the AI's provide, the better tailored the AI's response will be. Using ro, context, and response will be. Using ro, context, and response will be. Using ro, context, and task together creates a clear road map, task together creates a clear road map, task together creates a clear road map, helping the AI deliver accurate, helping the AI deliver accurate, helping the AI deliver accurate, relevant, and useful results. I've also relevant, and useful results. I've also relevant, and useful results. I've also included a link to Aman Eastwall's included a link to Aman Eastwall's included a link to Aman Eastwall's article mastering chat GPT prompting. He article mastering chat GPT prompting. He article mastering chat GPT prompting. He introduces the RTF framework role task introduces the RTF framework role task introduces the RTF framework role task format. While this is framework is also format. While this is framework is also format. While this is framework is also effective in this module we'll be effective in this module we'll be effective in this module we'll be prioritizing the role context and task prioritizing the role context and task prioritizing the role context and task because from my experience context because from my experience context because from my experience context significantly enhances the quality of significantly enhances the quality of significantly enhances the quality of your AI generated results. By mastering your AI generated results. By mastering your AI generated results. By mastering the elements of role, context, and task, the elements of role, context, and task, the elements of role, context, and task, you'll dramatically improve your you'll dramatically improve your you'll dramatically improve your interactions with generative AI, interactions with generative AI, interactions with generative AI, allowing you to unlock its full allowing you to unlock its full allowing you to unlock its full potential. Now that you've understood potential. Now that you've understood potential. Now that you've understood the basics, let's explore why prompt the basics, let's explore why prompt the basics, let's explore why prompt engineering matters and how it directly engineering matters and how it directly engineering matters and how it directly improves your experience with generative improves your experience with generative improves your experience with generative AI. Prompt engineering is powerful AI. Prompt engineering is powerful AI. Prompt engineering is powerful because it directly impacts the quality because it directly impacts the quality because it directly impacts the quality and effectiveness of the responses you and effectiveness of the responses you and effectiveness of the responses you get from large language models like get from large language models like get from large language models like chatbt. Here are three main reasons why chatbt. Here are three main reasons why chatbt. Here are three main reasons why good prompt engineering is so important. good prompt engineering is so important. good prompt engineering is so important. Number one, clarity. Clear prompts Number one, clarity. Clear prompts Number one, clarity. Clear prompts produce focused, accurate answers. When produce focused, accurate answers. When produce focused, accurate answers. When your instructions are specific, your instructions are specific, your instructions are specific, detailed, and unambiguous, the AI knows detailed, and unambiguous, the AI knows detailed, and unambiguous, the AI knows exactly what you're asking for, exactly what you're asking for, exactly what you're asking for, resulting in responses that directly resulting in responses that directly resulting in responses that directly align with your align with your align with your needs. Efficiency. Well ststructured needs. Efficiency. Well ststructured needs. Efficiency. Well ststructured prompts means less back and forth. prompts means less back and forth. prompts means less back and forth. Instead of continuously refining your Instead of continuously refining your Instead of continuously refining your question, a thoughtful initial prompt question, a thoughtful initial prompt question, a thoughtful initial prompt allows the AI to give you what you need allows the AI to give you what you need allows the AI to give you what you need right away, saving you time and effort. right away, saving you time and effort. right away, saving you time and effort. quality. Better prompts yield more quality. Better prompts yield more quality. Better prompts yield more creative, detailed, and useful results. creative, detailed, and useful results. creative, detailed, and useful results. The more context and precision you The more context and precision you The more context and precision you provide, the more tailored and valuable provide, the more tailored and valuable provide, the more tailored and valuable the AI generated content will become, the AI generated content will become, the AI generated content will become, leading to richer insights and more leading to richer insights and more leading to richer insights and more actionable information. Ultimately, actionable information. Ultimately, actionable information. Ultimately, mastering prompt engineering ensures mastering prompt engineering ensures mastering prompt engineering ensures that your interactions with generative that your interactions with generative that your interactions with generative AI are productive and satisfying, AI are productive and satisfying, AI are productive and satisfying, allowing you to unlock the full allowing you to unlock the full allowing you to unlock the full potential of AI tools like Chad GBT.",
    "chunks": [
      "Kind: captions Language: en [Music] Imagine you're working on a group Imagine you're working on a group Imagine you're working on a group project related to climate change and project related to climate change and project related to climate change and you're asking your team members for you're asking your team members for you're asking your team members for help. If you say something vague like, help. If you say something vague like, help. If you say something vague like, \"Can you give me",
      "information on climate \"Can you give me information on climate \"Can you give me information on climate change?\" You probably won't get exactly change?\" You probably won't get exactly change?\" You probably won't get exactly what you need. Maybe they'll share what you need. Maybe they'll share what you need. Maybe they'll share irrelevant details or miss important irrelevant details or miss important irrelevant details or miss important points entirely. But if you're clear and points entirely. But",
      "if you're clear and points entirely. But if you're clear and specific, something like, \"Can you find specific, something like, \"Can you find specific, something like, \"Can you find three articles about climate change three articles about climate change three articles about climate change effects in coastal cities, you've given effects in coastal cities, you've given effects in coastal cities, you've given your teammate precise instructions, your teammate precise instructions, your teammate",
      "precise instructions, which makes it easier for them to which makes it easier for them to which makes it easier for them to deliver exactly what you're looking for. deliver exactly what you're looking for. deliver exactly what you're looking for. That's exactly how working with That's exactly how working with That's exactly how working with generative AI tools like chat GPT works. generative AI tools like chat GPT works. generative AI tools like chat GPT works. The quality of the AI's response",
      "depends The quality of the AI's response depends The quality of the AI's response depends heavily on how clearly and specifically heavily on how clearly and specifically heavily on how clearly and specifically you phrase your prompts, the you phrase your prompts, the you phrase your prompts, the instructions or questions that you give instructions or questions that you give instructions or questions that you give it. The better you craft your prompt, it. The better you craft your prompt, it. The",
      "better you craft your prompt, the better results you'll receive. This the better results you'll receive. This the better results you'll receive. This careful and intentional structuring of careful and intentional structuring of careful and intentional structuring of your request is called prompt your request is called prompt your request is called prompt engineering. How you phrase your prompt engineering. How you phrase your prompt engineering. How you phrase your prompt directly impacts the",
      "quality and directly impacts the quality and directly impacts the quality and accuracy of the AI's response. For accuracy of the AI's response. For accuracy of the AI's response. For example, if you ask Chai GPT something example, if you ask Chai GPT something example, if you ask Chai GPT something general like, \"Tell me about general like, \"Tell me about general like, \"Tell me about scientists.\" You'll likely get a broad scientists.\" You'll likely get a broad scientists.\" You'll likely get a",
      "broad generic answer. Probably not very generic answer. Probably not very generic answer. Probably not very helpful. Why? Because your prompt lacks helpful. Why? Because your prompt lacks helpful. Why? Because your prompt lacks specifics. Who are these scientists? specifics. Who are these scientists? specifics. Who are these scientists? What field? What exactly do you want to What field? What exactly do you want to What field? What exactly do you want to know about them? Now, imagine improving",
      "know about them? Now, imagine improving know about them? Now, imagine improving that prompt to something like, \"Tell me that prompt to something like, \"Tell me that prompt to something like, \"Tell me about three influential physicists and about three influential physicists and about three influential physicists and their most important discoveries.\" their most important discoveries.\" their most important discoveries.\" Instantly, the AI knows exactly what Instantly, the AI knows exactly what",
      "Instantly, the AI knows exactly what you're asking it. It understands the you're asking it. It understands the you're asking it. It understands the topic, the field, and the level of topic, the field, and the level of topic, the field, and the level of detail you're expecting. The result, a detail you're expecting. The result, a detail you're expecting. The result, a more accurate, detailed, and useful more accurate, detailed, and useful more accurate, detailed, and useful answer. Prompt",
      "engineering is about answer. Prompt engineering is about answer. Prompt engineering is about carefully crafting your questions and carefully crafting your questions and carefully crafting your questions and instructions to clearly communicate your instructions to clearly communicate your instructions to clearly communicate your goals to the AI. Unlike a traditional goals to the AI. Unlike a traditional goals to the AI. Unlike a traditional web search where you might type a web search where you",
      "might type a web search where you might type a general query and then sift through general query and then sift through general query and then sift through multiple websites hoping one of them multiple websites hoping one of them multiple websites hoping one of them contains the exact information you need, contains the exact information you need, contains the exact information you need, generative AI aims to directly create generative AI aims to directly create generative AI aims to directly",
      "create the exact response you're looking for. the exact response you're looking for. the exact response you're looking for. With generative AI, your prompt doesn't With generative AI, your prompt doesn't With generative AI, your prompt doesn't just guide you toward an answer. It just guide you toward an answer. It just guide you toward an answer. It generates the answer itself. generates the answer itself. generates the answer itself. The more you practice prompt The more you practice prompt The",
      "more you practice prompt engineering, the more natural it becomes engineering, the more natural it becomes engineering, the more natural it becomes and the more powerful your interactions and the more powerful your interactions and the more powerful your interactions with generative AI can be. Ready to see with generative AI can be. Ready to see with generative AI can be. Ready to see prompt engineering in action? Let's get prompt engineering in action? Let's get prompt engineering in action?",
      "Let's get started. So, what exactly is prompt started. So, what exactly is prompt started. So, what exactly is prompt engineering? Prompt engineering is the engineering? Prompt engineering is the engineering? Prompt engineering is the process of carefully crafting structured process of carefully crafting structured process of carefully crafting structured instructions or prompts that clearly instructions or prompts that clearly instructions or prompts that clearly communicate what you want from",
      "communicate what you want from communicate what you want from generative AI models like generative AI models like generative AI models like chatgpt. You can think of prompt chatgpt. You can think of prompt chatgpt. You can think of prompt engineering as having a conversation engineering as having a conversation engineering as having a conversation with a knowledgeable intern. The intern with a knowledgeable intern. The intern with a knowledgeable intern. The intern is eager to help, but depends",
      "on your is eager to help, but depends on your is eager to help, but depends on your clear instructions to succeed. A clear instructions to succeed. A clear instructions to succeed. A well-crafted prompt clearly states your well-crafted prompt clearly states your well-crafted prompt clearly states your goals and provides enough context to goals and provides enough context to goals and provides enough context to guide the AI toward the exact guide the AI toward the exact guide the AI toward the",
      "exact information you need. The approach is information you need. The approach is information you need. The approach is foundational for getting the best foundational for getting the best foundational for getting the best possible responses, ensuring the AI's possible responses, ensuring the AI's possible responses, ensuring the AI's output is accurate, relevant, and output is accurate, relevant, and output is accurate, relevant, and aligned with your expectations. To aligned with your",
      "expectations. To aligned with your expectations. To summarize summarize summarize clearly, prompt engineering involves clearly, prompt engineering involves clearly, prompt engineering involves crafting structured prompts specifically crafting structured prompts specifically crafting structured prompts specifically designed for interaction with large designed for interaction with large designed for interaction with large language models. The goal to provide language models. The goal to provide",
      "language models. The goal to provide clear, detailed information so the AI clear, detailed information so the AI clear, detailed information so the AI can deliver the most accurate and can deliver the most accurate and can deliver the most accurate and helpful responses. Ultimately, it's the helpful responses. Ultimately, it's the helpful responses. Ultimately, it's the foundation for using AI effectively. If foundation for using AI effectively. If foundation for using AI effectively. If you're",
      "interested in diving deeper into you're interested in diving deeper into you're interested in diving deeper into prompt engineering, I highly recommend prompt engineering, I highly recommend prompt engineering, I highly recommend checking out the resource from Google checking out the resource from Google checking out the resource from Google called Prompt Engineering for Generative called Prompt Engineering for Generative called Prompt Engineering for Generative AI. It provides detailed guidance,",
      "best AI. It provides detailed guidance, best AI. It provides detailed guidance, best practices, and practical examples for practices, and practical examples for practices, and practical examples for improving your prompt engineering improving your prompt engineering improving your prompt engineering skills. This resource is particularly skills. This resource is particularly skills. This resource is particularly valuable because it explains basic valuable because it explains basic valuable because",
      "it explains basic techniques, shares prompting best techniques, shares prompting best techniques, shares prompting best practices such as clearly communicating practices such as clearly communicating practices such as clearly communicating your goal, structuring prompts your goal, structuring prompts your goal, structuring prompts effectively, providing specific effectively, providing specific effectively, providing specific examples, and breaking down complex task examples, and breaking down",
      "complex task examples, and breaking down complex task into simpler steps. We'll actually go into simpler steps. We'll actually go into simpler steps. We'll actually go into this in more detail in the third into this in more detail in the third into this in more detail in the third course in our AI certificate program, course in our AI certificate program, course in our AI certificate program, but for now, it's a great way to expand but for now, it's a great way to expand but for now, it's a great",
      "way to expand your understanding and become proficient your understanding and become proficient your understanding and become proficient at getting the best results from at getting the best results from at getting the best results from generative AI. Now, let's discuss the generative AI. Now, let's discuss the generative AI. Now, let's discuss the basics of creating effective basics of creating effective basics of creating effective prompts. There are many elements you can prompts. There are many",
      "elements you can prompts. There are many elements you can include in a prompt, but three are include in a prompt, but three are include in a prompt, but three are especially important. Ro, context, and especially important. Ro, context, and especially important. Ro, context, and task. Although you'll usually write your task. Although you'll usually write your task. Although you'll usually write your prompt in the order of ro, context, and prompt in the order of ro, context, and prompt in the",
      "order of ro, context, and task, we'll discuss these elements task, we'll discuss these elements task, we'll discuss these elements starting with task because knowing what starting with task because knowing what starting with task because knowing what you want the AI to do is foundational to you want the AI to do is foundational to you want the AI to do is foundational to any effective prompt. Let's imagine any effective prompt. Let's imagine any effective prompt. Let's imagine you're creating a",
      "prompt to generate a you're creating a prompt to generate a you're creating a prompt to generate a travel itinerary for a dream vacation. travel itinerary for a dream vacation. travel itinerary for a dream vacation. The task explicitly states what you want The task explicitly states what you want The task explicitly states what you want from the AI. This could be requesting an from the AI. This could be requesting an from the AI. This could be requesting an itinerary, suggestions, comparisons, or",
      "itinerary, suggestions, comparisons, or itinerary, suggestions, comparisons, or any other action that clearly defines any other action that clearly defines any other action that clearly defines the outcome you're seeking. Next, the the outcome you're seeking. Next, the the outcome you're seeking. Next, the role identifies who would be the most role identifies who would be the most role identifies who would be the most knowledgeable about this task. In this knowledgeable about this task. In this",
      "knowledgeable about this task. In this scenario, the role could be a travel scenario, the role could be a travel scenario, the role could be a travel agent or perhaps a travel expert. agent or perhaps a travel expert. agent or perhaps a travel expert. Assigning a role helps the AI focus Assigning a role helps the AI focus Assigning a role helps the AI focus specifically on relevant data, filtering specifically on relevant data, filtering specifically on relevant data, filtering out the unrelated",
      "information from its out the unrelated information from its out the unrelated information from its vast knowledge base. Lastly, we add vast knowledge base. Lastly, we add vast knowledge base. Lastly, we add context, which provides additional context, which provides additional context, which provides additional background details for a travel background details for a travel background details for a travel itinerary. This could include budget, itinerary. This could include budget, itinerary. This",
      "could include budget, location, travel dates, preferences, or location, travel dates, preferences, or location, travel dates, preferences, or travel companions. The more context you travel companions. The more context you travel companions. The more context you provide, the better tailored the AI's provide, the better tailored the AI's provide, the better tailored the AI's response will be. Using ro, context, and response will be. Using ro, context, and response will be. Using ro, context, and",
      "task together creates a clear road map, task together creates a clear road map, task together creates a clear road map, helping the AI deliver accurate, helping the AI deliver accurate, helping the AI deliver accurate, relevant, and useful results. I've also relevant, and useful results. I've also relevant, and useful results. I've also included a link to Aman Eastwall's included a link to Aman Eastwall's included a link to Aman Eastwall's article mastering chat GPT prompting. He article",
      "mastering chat GPT prompting. He article mastering chat GPT prompting. He introduces the RTF framework role task introduces the RTF framework role task introduces the RTF framework role task format. While this is framework is also format. While this is framework is also format. While this is framework is also effective in this module we'll be effective in this module we'll be effective in this module we'll be prioritizing the role context and task prioritizing the role context and task",
      "prioritizing the role context and task because from my experience context because from my experience context because from my experience context significantly enhances the quality of significantly enhances the quality of significantly enhances the quality of your AI generated results. By mastering your AI generated results. By mastering your AI generated results. By mastering the elements of role, context, and task, the elements of role, context, and task, the elements of role, context, and task,",
      "you'll dramatically improve your you'll dramatically improve your you'll dramatically improve your interactions with generative AI, interactions with generative AI, interactions with generative AI, allowing you to unlock its full allowing you to unlock its full allowing you to unlock its full potential. Now that you've understood potential. Now that you've understood potential. Now that you've understood the basics, let's explore why prompt the basics, let's explore why prompt the basics, let's",
      "explore why prompt engineering matters and how it directly engineering matters and how it directly engineering matters and how it directly improves your experience with generative improves your experience with generative improves your experience with generative AI. Prompt engineering is powerful AI. Prompt engineering is powerful AI. Prompt engineering is powerful because it directly impacts the quality because it directly impacts the quality because it directly impacts the quality and",
      "effectiveness of the responses you and effectiveness of the responses you and effectiveness of the responses you get from large language models like get from large language models like get from large language models like chatbt. Here are three main reasons why chatbt. Here are three main reasons why chatbt. Here are three main reasons why good prompt engineering is so important. good prompt engineering is so important. good prompt engineering is so important. Number one, clarity. Clear prompts",
      "Number one, clarity. Clear prompts Number one, clarity. Clear prompts produce focused, accurate answers. When produce focused, accurate answers. When produce focused, accurate answers. When your instructions are specific, your instructions are specific, your instructions are specific, detailed, and unambiguous, the AI knows detailed, and unambiguous, the AI knows detailed, and unambiguous, the AI knows exactly what you're asking for, exactly what you're asking for, exactly what you're asking for,",
      "resulting in responses that directly resulting in responses that directly resulting in responses that directly align with your align with your align with your needs. Efficiency. Well ststructured needs. Efficiency. Well ststructured needs. Efficiency. Well ststructured prompts means less back and forth. prompts means less back and forth. prompts means less back and forth. Instead of continuously refining your Instead of continuously refining your Instead of continuously refining your question, a",
      "thoughtful initial prompt question, a thoughtful initial prompt question, a thoughtful initial prompt allows the AI to give you what you need allows the AI to give you what you need allows the AI to give you what you need right away, saving you time and effort. right away, saving you time and effort. right away, saving you time and effort. quality. Better prompts yield more quality. Better prompts yield more quality. Better prompts yield more creative, detailed, and useful results. creative,",
      "detailed, and useful results. creative, detailed, and useful results. The more context and precision you The more context and precision you The more context and precision you provide, the more tailored and valuable provide, the more tailored and valuable provide, the more tailored and valuable the AI generated content will become, the AI generated content will become, the AI generated content will become, leading to richer insights and more leading to richer insights and more leading to richer",
      "insights and more actionable information. Ultimately, actionable information. Ultimately, actionable information. Ultimately, mastering prompt engineering ensures mastering prompt engineering ensures mastering prompt engineering ensures that your interactions with generative that your interactions with generative that your interactions with generative AI are productive and satisfying, AI are productive and satisfying, AI are productive and satisfying, allowing you to unlock the full allowing you",
      "to unlock the full allowing you to unlock the full potential of AI tools like Chad GBT."
    ],
    "chunk_count": 44,
    "content_id": "85a1d813-417d-434e-8485-ca0367f851d9",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554931"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=-QVoIxEpFkM": {
    "title": "What is a Context Window? Unlocking LLM Secrets",
    "url": "https://www.youtube.com/watch?v=-QVoIxEpFkM",
    "description": "Want to learn more about Generative AI? Read the Report Here → https://ibm.biz/BdGfdr\nLearn more about Context Window here → https://ibm.biz/BdGfds\n\nAre you tired of conversations getting lost in translation? Join Martin Keen as he explores the concept of context windows in large language models and how they can improve conversation flow and accuracy. Discover how to optimize your models with context windows and enhance your conversational AI capabilities.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdGfdj\n\n#largelanguagemodels #conversationalai #ai",
    "duration": 690,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en In the context of large language models. What is a context window? Well, it's the equivalent of its working memory. It determines how long of a conversation the LLM can carry out without forgetting details from earlier in the exchange. And allow me to illustrate this using the scientifically recognized IBU scale that's international blah units. So blah here, that represents me sending a prompt to an LLM chatbot. Now the chatbot that returns with a response blah. Right. And then we continue the conversation. So I say something else and then it responds back to me. Blah, blah, blah, blah. International blah units. Now, this box here represents the context window, and in this case, the entire conversation fits within it. Now, that means that when the LLM generated this response here, this blah, it had within its working memory my prompts to the model here and here. And it also had the other response that the model had returned to me in order to build this response. All good. Now let's consider a longer conversation. So more blahs. I send my prompt blah. It then sends me a response. And now we go back and forth with more conversations. I say something. It responds to that. I say one more thing and it responds to that. So now we have a longer conversation here to deal with. And it turns out that this conversation thread is actually longer than the context window of the model. Now, that means that the blahs from earlier in the conversation are no longer available to the model. It has no memory of them when generating new responses. Now the LLM can do its best to infer what came earlier by looking at the conversation that is within its context window. But now the LLM is making educated guesses and that can result in some wicked hallucinations. So understanding how the context window works is essential to getting the most out about a LLMs. Let's get into a bit more detail about that now. Now my producer is telling me that context window size is in fact not measured in IBUs and that I made that up. We actually measure context windows in something called tokens. So let's describe tokenization. Let's get into context, length, size, and we're going to talk about the challenges of long context windows. So the start, what is a token? Well, for us humans, the smallest unit of information that we use to represent language is a single. Character. So something like a letter or a number or a punctuation mark, something like that. But the smallest unit of language that AI models use is called a token. Now, a token can represent a character as well. But it might also be a part of a word or a whole word or even a short multi-word phrase. So, for example, let's consider the different roles played by the letter A. So I'm going to write some sentences and we're going to tokenize them. Let's start with Martin drove a car. Now A here is an entire word and it will be represented by a distinct token. Now, what if we try a different sentence? So, Martin amoral. Not sure why we would say that, but look, in this case, A is not a word, but it's an addition to moral that significantly changes the meaning of that word. So here a moral would be represented by two distinct tokens, a token for A and another token for moral. All right. one more. Martin loves his cat. Now the A in cat is simply a letter. In a word, it carries no semantic meaning by itself and would therefore not be a distinct token. The token here It's just cat. Now, the tool, the converts language, to tokens. It's got a name. It's called a tokenizer. And different tokenizer, as might tokenize the same passage of writing differently. But kind of a good rule of thumb is that a a regular word in English language is represented by something like 1.5 tokens by the tokenizer. So hundred words that might result in 150 tokens. So context windows consist of tokens, but how many tokens are we actually talking about? To answer that, we need to understand how LLM process tokens in a context window. Now, transformer models use something called the self attention mechanism. And the self attention mechanism is used to calculate the relationships and the dependencies between different parts of an input like words at the beginning and at the end of a paragraph. Now self attention mechanism computes vectors of weights in which each weight represents how relevant that token is to the other tokens in the sequence. So the size of the context window determines the maximum number of tokens that the model can pay attention to at any one time. Now, context window size has been rapidly increasing. So the first LLMs that I used, they had context windows of around 2000 tokens. The IBM Granite three model today has a context window of 128,000 tokens, and other models have larger context when they still. And but it almost seems like overkill, doesn't it? I would have to be conversing with a chat bot all day to fill a 128K token window. Well, actually, it's not necessarily true because there can be a lot of things taking up space within a model's context window. So let's take a look at what some of those things could be. Well, one of them is the the user input, the the blah that I sent into the model. And of course, we also have the model responses as well, the blahs that it was sending back, but a context window may also contain all sorts of other things as well. So most models provide what is called a system prompt. Into the context window. Now, this is often hidden from the user. But it conditions the behavior of the model, telling it what it can and cannot do. A user may also choose to attach some documents into their contacts window, or they might put in some source code as well. And that can be used by the LLM to refer to it and its responses. And then supplementary information drawn from external data sources for retrieval augmented generation or RAG, that might be stored within the context window during inference. So a few long documents, some snippets of source code, I can quickly fill up a context window. So the bigger the context window, the better, right? Well, larger context windows do present some challenges as well. What sort of challenges? Well, I think the most obvious one that would have to be compute. The compute requirements scale quadratically  with the length of a sequence. What does that mean? Well, essentially, as the number of input tokens doubles, that results in the model needing four times as much processing power to handle it. Now, remember, as the model predicts, the next token in a sequence. It computes the relationships between the token and every single preceding token in that sequence. So as context length increases, more and more computation is going to be required. Now, long context windows also can negatively affect performance, specifically the performance of the model. So like people and LLMs can be overwhelmed by an abundance of extra detail. They can also get lazy and take all sorts of cognitive shortcuts. A 2023 paper found that models perform best when relevant information is towards the beginning or towards the end of the input context. And they found that performance degrades when the model must carefully consider the information that is in the middle of long context. And then finally, we also have to be concerned with a number of safety challenges as well. Longer context window might have the unintended effect of presenting a longer attack surface for adversarial prompts, a long context length can increase a model's vulnerability to jailbreaking, where malicious content is embedded deep within the input, making it harder for the model safety mechanisms to detect and filter out harmful instructions. So no matter how you measure it with either with IBUs or more accurately, tokens, selecting the appropriate number of tokens for a context window involves balancing the need to supply ample information for the model's self attention mechanism. With the increasing demands and performance issues those additional tokens may bring.",
    "chunks": [
      "Kind: captions Language: en In the context of large language models. What is a context window? Well, it's the equivalent of its working memory. It determines how long of a conversation the LLM can carry out without forgetting details from earlier in the exchange. And allow me to illustrate this using the scientifically recognized IBU scale that's international blah units. So blah here, that represents me sending a prompt to an LLM chatbot. Now the chatbot that returns with a response blah.",
      "Right. And then we continue the conversation. So I say something else and then it responds back to me. Blah, blah, blah, blah. International blah units. Now, this box here represents the context window, and in this case, the entire conversation fits within it. Now, that means that when the LLM generated this response here, this blah, it had within its working memory my prompts to the model here and here. And it also had the other response that the model had returned to me in order to build this",
      "response. All good. Now let's consider a longer conversation. So more blahs. I send my prompt blah. It then sends me a response. And now we go back and forth with more conversations. I say something. It responds to that. I say one more thing and it responds to that. So now we have a longer conversation here to deal with. And it turns out that this conversation thread is actually longer than the context window of the model. Now, that means that the blahs from earlier in the conversation are no",
      "longer available to the model. It has no memory of them when generating new responses. Now the LLM can do its best to infer what came earlier by looking at the conversation that is within its context window. But now the LLM is making educated guesses and that can result in some wicked hallucinations. So understanding how the context window works is essential to getting the most out about a LLMs. Let's get into a bit more detail about that now. Now my producer is telling me that context window",
      "size is in fact not measured in IBUs and that I made that up. We actually measure context windows in something called tokens. So let's describe tokenization. Let's get into context, length, size, and we're going to talk about the challenges of long context windows. So the start, what is a token? Well, for us humans, the smallest unit of information that we use to represent language is a single. Character. So something like a letter or a number or a punctuation mark, something like that. But the",
      "smallest unit of language that AI models use is called a token. Now, a token can represent a character as well. But it might also be a part of a word or a whole word or even a short multi-word phrase. So, for example, let's consider the different roles played by the letter A. So I'm going to write some sentences and we're going to tokenize them. Let's start with Martin drove a car. Now A here is an entire word and it will be represented by a distinct token. Now, what if we try a different",
      "sentence? So, Martin amoral. Not sure why we would say that, but look, in this case, A is not a word, but it's an addition to moral that significantly changes the meaning of that word. So here a moral would be represented by two distinct tokens, a token for A and another token for moral. All right. one more. Martin loves his cat. Now the A in cat is simply a letter. In a word, it carries no semantic meaning by itself and would therefore not be a distinct token. The token here It's just cat. Now,",
      "the tool, the converts language, to tokens. It's got a name. It's called a tokenizer. And different tokenizer, as might tokenize the same passage of writing differently. But kind of a good rule of thumb is that a a regular word in English language is represented by something like 1.5 tokens by the tokenizer. So hundred words that might result in 150 tokens. So context windows consist of tokens, but how many tokens are we actually talking about? To answer that, we need to understand how LLM",
      "process tokens in a context window. Now, transformer models use something called the self attention mechanism. And the self attention mechanism is used to calculate the relationships and the dependencies between different parts of an input like words at the beginning and at the end of a paragraph. Now self attention mechanism computes vectors of weights in which each weight represents how relevant that token is to the other tokens in the sequence. So the size of the context window determines the",
      "maximum number of tokens that the model can pay attention to at any one time. Now, context window size has been rapidly increasing. So the first LLMs that I used, they had context windows of around 2000 tokens. The IBM Granite three model today has a context window of 128,000 tokens, and other models have larger context when they still. And but it almost seems like overkill, doesn't it? I would have to be conversing with a chat bot all day to fill a 128K token window. Well, actually, it's not",
      "necessarily true because there can be a lot of things taking up space within a model's context window. So let's take a look at what some of those things could be. Well, one of them is the the user input, the the blah that I sent into the model. And of course, we also have the model responses as well, the blahs that it was sending back, but a context window may also contain all sorts of other things as well. So most models provide what is called a system prompt. Into the context window. Now, this",
      "is often hidden from the user. But it conditions the behavior of the model, telling it what it can and cannot do. A user may also choose to attach some documents into their contacts window, or they might put in some source code as well. And that can be used by the LLM to refer to it and its responses. And then supplementary information drawn from external data sources for retrieval augmented generation or RAG, that might be stored within the context window during inference. So a few long",
      "documents, some snippets of source code, I can quickly fill up a context window. So the bigger the context window, the better, right? Well, larger context windows do present some challenges as well. What sort of challenges? Well, I think the most obvious one that would have to be compute. The compute requirements scale quadratically with the length of a sequence. What does that mean? Well, essentially, as the number of input tokens doubles, that results in the model needing four times as much",
      "processing power to handle it. Now, remember, as the model predicts, the next token in a sequence. It computes the relationships between the token and every single preceding token in that sequence. So as context length increases, more and more computation is going to be required. Now, long context windows also can negatively affect performance, specifically the performance of the model. So like people and LLMs can be overwhelmed by an abundance of extra detail. They can also get lazy and take all",
      "sorts of cognitive shortcuts. A 2023 paper found that models perform best when relevant information is towards the beginning or towards the end of the input context. And they found that performance degrades when the model must carefully consider the information that is in the middle of long context. And then finally, we also have to be concerned with a number of safety challenges as well. Longer context window might have the unintended effect of presenting a longer attack surface for adversarial",
      "prompts, a long context length can increase a model's vulnerability to jailbreaking, where malicious content is embedded deep within the input, making it harder for the model safety mechanisms to detect and filter out harmful instructions. So no matter how you measure it with either with IBUs or more accurately, tokens, selecting the appropriate number of tokens for a context window involves balancing the need to supply ample information for the model's self attention mechanism. With the",
      "increasing demands and performance issues those additional tokens may bring."
    ],
    "chunk_count": 17,
    "content_id": "d730751c-4a30-4cc5-a6fd-4a020d0b59e3",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554936"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=_6SlPLNEpzQ": {
    "title": "Ollama In 120 Seconds",
    "url": "https://www.youtube.com/watch?v=_6SlPLNEpzQ",
    "description": "Learn everything you need to know about Ollama in 2 minutes, in this video.\n\n🦙 Ollama isn't just another tool - it represents a paradigm shift in how we approach AI development. It integrates advanced techniques in machine learning locally to understand and generate human language. With Ollama, you can import powerful open source models and fine tune them to meet your specific needs, whether its for healthcare, finance, research, fraud detection, customer server automation, or just a fun hobby project. \n___________________________\n\nTry Warp now for FREE 👉 bit.ly/warpdotdev\n\nTwitter 🐦\nhttps://twitter.com/warpdotdev\n\nTikTok 📱\nhttps://www.tiktok.com/@warp.dev\n\nTIMESTAMPS\n0:00 Intro\n00:10 Pre-trained LLMS (& examples)\n00:33 Available on all platforms & works offline\n00:48 How to customize through CLI experience\n01:02 Creating your own custom model (Modelfile)\n01:21 REST API (with OpenAI compatibility)\n01:37 Run requests in parallel\n01:51 Web UI alternative (if you don't like the CLI experience)",
    "duration": 129,
    "uploader": "Warp",
    "transcript": "Kind: captions Language: en if you want to explore the world of open if you want to explore the world of open if you want to explore the world of open llms but don't want the annoying setup llms but don't want the annoying setup llms but don't want the annoying setup then olama is a must AMA helps you run then olama is a must AMA helps you run then olama is a must AMA helps you run large language models as efficiently as large language models as efficiently as large language models as efficiently as possible it's completely open source and possible it's completely open source and possible it's completely open source and free one of the biggest benefits olama free one of the biggest benefits olama free one of the biggest benefits olama provides is access to a diverse and provides is access to a diverse and provides is access to a diverse and expanding library of pre-trained llm expanding library of pre-trained llm expanding library of pre-trained llm models the most popular ones include models the most popular ones include models the most popular ones include llama 2 for question answering mistol llama 2 for question answering mistol llama 2 for question answering mistol for creative writing like poems or for creative writing like poems or for creative writing like poems or musical pieces code llama for code musical pieces code llama for code musical pieces code llama for code generation and lava for processing generation and lava for processing generation and lava for processing images here I'm using lava by entering images here I'm using lava by entering images here I'm using lava by entering my prompt and then entering the file my prompt and then entering the file my prompt and then entering the file location I want to use if you have location I want to use if you have location I want to use if you have hundreds of images this is a great way hundreds of images this is a great way hundreds of images this is a great way to go through them quickly AMA is to go through them quickly AMA is to go through them quickly AMA is currently available on all platforms you currently available on all platforms you currently available on all platforms you can install it as executable or through can install it as executable or through can install it as executable or through the command line on top of that it's the command line on top of that it's the command line on top of that it's able to function completely offline able to function completely offline able to function completely offline which makes it reliable when internet which makes it reliable when internet which makes it reliable when internet connectivity is limited and also ensures connectivity is limited and also ensures connectivity is limited and also ensures privacy as your data is kept within a privacy as your data is kept within a privacy as your data is kept within a local environment through ama's command local environment through ama's command local environment through ama's command line experience there are many ways to line experience there are many ways to line experience there are many ways to customize and interact with llms for customize and interact with llms for customize and interact with llms for example I can run temperature to control example I can run temperature to control example I can run temperature to control the randomness and creativity of the the randomness and creativity of the the randomness and creativity of the lm's responses or top K which determines lm's responses or top K which determines lm's responses or top K which determines the number of tokens the llm canist the number of tokens the llm canist the number of tokens the llm canist when generating responses if you want to when generating responses if you want to when generating responses if you want to create your own custom model you can create your own custom model you can create your own custom model you can create a model file there you can use an create a model file there you can use an create a model file there you can use an existing or downloaded model set the existing or downloaded model set the existing or downloaded model set the parameters update the system prompt and parameters update the system prompt and parameters update the system prompt and more for example here I've configured a more for example here I've configured a more for example here I've configured a model that will generate answers in the model that will generate answers in the model that will generate answers in the nerdiest way possible if you want some nerdiest way possible if you want some nerdiest way possible if you want some inspiration hugging face is a website inspiration hugging face is a website inspiration hugging face is a website that will have a preset version you can that will have a preset version you can that will have a preset version you can probably use but what makes AMA awesome probably use but what makes AMA awesome probably use but what makes AMA awesome is the rest API it automatically is the rest API it automatically is the rest API it automatically attaches so you can start integrating it attaches so you can start integrating it attaches so you can start integrating it within your applications it has open AI within your applications it has open AI within your applications it has open AI compatibility so so if you have an compatibility so so if you have an compatibility so so if you have an existing app all you have to do is existing app all you have to do is existing app all you have to do is change the url and the model and it will change the url and the model and it will change the url and the model and it will now work exactly the same and just now work exactly the same and just now work exactly the same and just recently they added the ability to run recently they added the ability to run recently they added the ability to run requests in parallel that means you can requests in parallel that means you can requests in parallel that means you can run multiple llms in parallel like if run multiple llms in parallel like if run multiple llms in parallel like if you wanted the speech and code to be you wanted the speech and code to be you wanted the speech and code to be generated by separate models in tandem generated by separate models in tandem generated by separate models in tandem if you don't like using the command line if you don't like using the command line if you don't like using the command line to interface with olama there's also a to interface with olama there's also a to interface with olama there's also a great featur web UI alternative called great featur web UI alternative called great featur web UI alternative called open web UI it's very similar to chat open web UI it's very similar to chat open web UI it's very similar to chat applications like chat GPT where you can applications like chat GPT where you can applications like chat GPT where you can select models adjust parameters and even select models adjust parameters and even select models adjust parameters and even review previous messages and responses review previous messages and responses review previous messages and responses even if your Hardware isn't that even if your Hardware isn't that even if your Hardware isn't that powerful you can still get a ton of powerful you can still get a ton of powerful you can still get a ton of value from running small models on your value from running small models on your value from running small models on your computer with Ama",
    "chunks": [
      "Kind: captions Language: en if you want to explore the world of open if you want to explore the world of open if you want to explore the world of open llms but don't want the annoying setup llms but don't want the annoying setup llms but don't want the annoying setup then olama is a must AMA helps you run then olama is a must AMA helps you run then olama is a must AMA helps you run large language models as efficiently as large language models as efficiently as large language models as",
      "efficiently as possible it's completely open source and possible it's completely open source and possible it's completely open source and free one of the biggest benefits olama free one of the biggest benefits olama free one of the biggest benefits olama provides is access to a diverse and provides is access to a diverse and provides is access to a diverse and expanding library of pre-trained llm expanding library of pre-trained llm expanding library of pre-trained llm models the most popular",
      "ones include models the most popular ones include models the most popular ones include llama 2 for question answering mistol llama 2 for question answering mistol llama 2 for question answering mistol for creative writing like poems or for creative writing like poems or for creative writing like poems or musical pieces code llama for code musical pieces code llama for code musical pieces code llama for code generation and lava for processing generation and lava for processing generation and lava",
      "for processing images here I'm using lava by entering images here I'm using lava by entering images here I'm using lava by entering my prompt and then entering the file my prompt and then entering the file my prompt and then entering the file location I want to use if you have location I want to use if you have location I want to use if you have hundreds of images this is a great way hundreds of images this is a great way hundreds of images this is a great way to go through them quickly AMA is to",
      "go through them quickly AMA is to go through them quickly AMA is currently available on all platforms you currently available on all platforms you currently available on all platforms you can install it as executable or through can install it as executable or through can install it as executable or through the command line on top of that it's the command line on top of that it's the command line on top of that it's able to function completely offline able to function completely offline able to",
      "function completely offline which makes it reliable when internet which makes it reliable when internet which makes it reliable when internet connectivity is limited and also ensures connectivity is limited and also ensures connectivity is limited and also ensures privacy as your data is kept within a privacy as your data is kept within a privacy as your data is kept within a local environment through ama's command local environment through ama's command local environment through ama's command",
      "line experience there are many ways to line experience there are many ways to line experience there are many ways to customize and interact with llms for customize and interact with llms for customize and interact with llms for example I can run temperature to control example I can run temperature to control example I can run temperature to control the randomness and creativity of the the randomness and creativity of the the randomness and creativity of the lm's responses or top K which",
      "determines lm's responses or top K which determines lm's responses or top K which determines the number of tokens the llm canist the number of tokens the llm canist the number of tokens the llm canist when generating responses if you want to when generating responses if you want to when generating responses if you want to create your own custom model you can create your own custom model you can create your own custom model you can create a model file there you can use an create a model file there",
      "you can use an create a model file there you can use an existing or downloaded model set the existing or downloaded model set the existing or downloaded model set the parameters update the system prompt and parameters update the system prompt and parameters update the system prompt and more for example here I've configured a more for example here I've configured a more for example here I've configured a model that will generate answers in the model that will generate answers in the model that",
      "will generate answers in the nerdiest way possible if you want some nerdiest way possible if you want some nerdiest way possible if you want some inspiration hugging face is a website inspiration hugging face is a website inspiration hugging face is a website that will have a preset version you can that will have a preset version you can that will have a preset version you can probably use but what makes AMA awesome probably use but what makes AMA awesome probably use but what makes AMA awesome",
      "is the rest API it automatically is the rest API it automatically is the rest API it automatically attaches so you can start integrating it attaches so you can start integrating it attaches so you can start integrating it within your applications it has open AI within your applications it has open AI within your applications it has open AI compatibility so so if you have an compatibility so so if you have an compatibility so so if you have an existing app all you have to do is existing app all",
      "you have to do is existing app all you have to do is change the url and the model and it will change the url and the model and it will change the url and the model and it will now work exactly the same and just now work exactly the same and just now work exactly the same and just recently they added the ability to run recently they added the ability to run recently they added the ability to run requests in parallel that means you can requests in parallel that means you can requests in parallel",
      "that means you can run multiple llms in parallel like if run multiple llms in parallel like if run multiple llms in parallel like if you wanted the speech and code to be you wanted the speech and code to be you wanted the speech and code to be generated by separate models in tandem generated by separate models in tandem generated by separate models in tandem if you don't like using the command line if you don't like using the command line if you don't like using the command line to interface with",
      "olama there's also a to interface with olama there's also a to interface with olama there's also a great featur web UI alternative called great featur web UI alternative called great featur web UI alternative called open web UI it's very similar to chat open web UI it's very similar to chat open web UI it's very similar to chat applications like chat GPT where you can applications like chat GPT where you can applications like chat GPT where you can select models adjust parameters and even select",
      "models adjust parameters and even select models adjust parameters and even review previous messages and responses review previous messages and responses review previous messages and responses even if your Hardware isn't that even if your Hardware isn't that even if your Hardware isn't that powerful you can still get a ton of powerful you can still get a ton of powerful you can still get a ton of value from running small models on your value from running small models on your value from running",
      "small models on your computer with Ama"
    ],
    "chunk_count": 16,
    "content_id": "70193214-f067-4746-9ae1-851593768ea3",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554939"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=X3XJeTApVMM": {
    "title": "What Are Orchestrator Agents? AI Tools Working Smarter Together",
    "url": "https://www.youtube.com/watch?v=X3XJeTApVMM",
    "description": "Ready to become a certified Solution Implementer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnXAG\n\nLearn more about AI Agent Orchestration here → https://ibm.biz/BdnXAn\n\n📋 What if AI could manage other AI? Melissa Hadley explains how Orchestrator Agents connect specialized tools like billing agents or scheduling assistants for seamless collaboration, improved UX, and multi-step workflow automation. 🚀 Discover how agent orchestration boosts efficiency, desilos tools, and scales automation with API integration and real-time execution strategies! 🌐\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnXAe\n\n#aiagents #workflowautomation #api",
    "duration": 264,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en In the past, we've talked about chat In the past, we've talked about chat In the past, we've talked about chat bots, AI assistants, and agents. These bots, AI assistants, and agents. These bots, AI assistants, and agents. These are helpful tools in the AI space are helpful tools in the AI space are helpful tools in the AI space focused around working smarter, not focused around working smarter, not focused around working smarter, not harder, and becoming more productive. AI harder, and becoming more productive. AI harder, and becoming more productive. AI assistants are the DIY, the assistants are the DIY, the assistants are the DIY, the do-it-yourself. Especially after a user do-it-yourself. Especially after a user do-it-yourself. Especially after a user asks a prompt, you follow the asks a prompt, you follow the asks a prompt, you follow the instructions and get work done. On the instructions and get work done. On the instructions and get work done. On the other hand, agents are more like the DI other hand, agents are more like the DI other hand, agents are more like the DI FY do it for you. FY do it for you. FY do it for you. Agents can make decisions, kick off Agents can make decisions, kick off Agents can make decisions, kick off workflows, and even use function calling workflows, and even use function calling workflows, and even use function calling to connect with external tools like APIs to connect with external tools like APIs to connect with external tools like APIs or data sources. They can also act or data sources. They can also act or data sources. They can also act autonomously without being prompted or autonomously without being prompted or autonomously without being prompted or supervised. Many AI agents are supervised. Many AI agents are supervised. Many AI agents are specialized too, meaning each one is specialized too, meaning each one is specialized too, meaning each one is designed to support a particular area. designed to support a particular area. designed to support a particular area. Some agents focus on business and Some agents focus on business and Some agents focus on business and customerf facing tasks like billing or customerf facing tasks like billing or customerf facing tasks like billing or scheduling while others handle more scheduling while others handle more scheduling while others handle more technical functions like data retrieval technical functions like data retrieval technical functions like data retrieval and process automation. I had someone and process automation. I had someone and process automation. I had someone recently ask me if agents are the future recently ask me if agents are the future recently ask me if agents are the future and they're already here. Melissa, what and they're already here. Melissa, what and they're already here. Melissa, what could possibly be next? My answer, agent could possibly be next? My answer, agent could possibly be next? My answer, agent orchestration. As AI adoption grows, so orchestration. As AI adoption grows, so orchestration. As AI adoption grows, so does the number of AI tools being does the number of AI tools being does the number of AI tools being developed, deployed, and integrated into developed, deployed, and integrated into developed, deployed, and integrated into various workflows. Different AI various workflows. Different AI various workflows. Different AI assistants and agents handle specialized assistants and agents handle specialized assistants and agents handle specialized tasks. Some focus on automating tasks. Some focus on automating tasks. Some focus on automating technical operations. Some of them handle operations. Some of them handle operations. Some of them handle different workflows. Some of them even connect to external Some of them even connect to external Some of them even connect to external data sources like APIs or cloud services. These tools can come from services. These tools can come from services. These tools can come from different vendors, operate on different different vendors, operate on different different vendors, operate on different architectures, and may even be architectures, and may even be architectures, and may even be distributed across multiple cloud distributed across multiple cloud distributed across multiple cloud environments or on premise. The result, environments or on premise. The result, environments or on premise. The result, a fragmented AI ecosystem where tools a fragmented AI ecosystem where tools a fragmented AI ecosystem where tools exist in silos, making coordination, exist in silos, making coordination, exist in silos, making coordination, interoperability, and governance interoperability, and governance interoperability, and governance increasingly complex. This brings us to increasingly complex. This brings us to increasingly complex. This brings us to the next key concept, the orchestrator the next key concept, the orchestrator the next key concept, the orchestrator agent, which I'll put up here. The orchestrator agent is a type here. The orchestrator agent is a type here. The orchestrator agent is a type of agent that specializes in overseeing of agent that specializes in overseeing of agent that specializes in overseeing how work gets done. They're like a how work gets done. They're like a how work gets done. They're like a supervisor at work. They supervise how supervisor at work. They supervise how supervisor at work. They supervise how work gets done, ensures everyone's being work gets done, ensures everyone's being work gets done, ensures everyone's being a team player, and routes work to those a team player, and routes work to those a team player, and routes work to those with the right skills for the job. with the right skills for the job. with the right skills for the job. In this diagram, they can route across In this diagram, they can route across In this diagram, they can route across multiple task oriented AI agents and multiple task oriented AI agents and multiple task oriented AI agents and assistants to get a job done. When the assistants to get a job done. When the assistants to get a job done. When the right specialized assistant and agents right specialized assistant and agents right specialized assistant and agents work together to complete complex work together to complete complex work together to complete complex workflows and tasks, the experience is workflows and tasks, the experience is workflows and tasks, the experience is coordinated and seamless. In order to coordinated and seamless. In order to coordinated and seamless. In order to facilitate smooth agentto agent or facilitate smooth agentto agent or facilitate smooth agentto agent or agentto assistant communication through agentto assistant communication through agentto assistant communication through agent orchestration, all we have to do agent orchestration, all we have to do agent orchestration, all we have to do is follow three steps. The first is is follow three steps. The first is is follow three steps. The first is we'll need to define the task execution we'll need to define the task execution we'll need to define the task execution sequence or the workflow. Next, we'll sequence or the workflow. Next, we'll sequence or the workflow. Next, we'll need to set up API integration so that need to set up API integration so that need to set up API integration so that the agent or assistant can access the agent or assistant can access the agent or assistant can access relevant data. And lastly, we'll need to relevant data. And lastly, we'll need to relevant data. And lastly, we'll need to implement some sort of implement some sort of implement some sort of opensource orchestration technology. opensource orchestration technology. opensource orchestration technology. Once this is complete, the orchestrator Once this is complete, the orchestrator Once this is complete, the orchestrator agent takes over realtime execution. agent takes over realtime execution. agent takes over realtime execution. Benefits to using orchestrator agents Benefits to using orchestrator agents Benefits to using orchestrator agents include things like enhanced include things like enhanced include things like enhanced efficiency. Think about the concept that efficiency. Think about the concept that efficiency. Think about the concept that it's it's it's dy, right? Do it for you. You can work a dy, right? Do it for you. You can work a dy, right? Do it for you. You can work a lot smarter if you have someone getting lot smarter if you have someone getting lot smarter if you have someone getting some of your to-do list all completed. some of your to-do list all completed. some of your to-do list all completed. The next is around improved experience. The next is around improved experience. The next is around improved experience. We're desiloing here. The more tools We're desiloing here. The more tools We're desiloing here. The more tools that you have access to, the better your that you have access to, the better your that you have access to, the better your experience might be. And lastly, experience might be. And lastly, experience might be. And lastly, scalability. scalability. scalability. Think about all of the workflows and Think about all of the workflows and Think about all of the workflows and decisions agents or assistants can help decisions agents or assistants can help decisions agents or assistants can help you make. This can only help your you make. This can only help your you make. This can only help your capabilities grow. So maybe you do have capabilities grow. So maybe you do have capabilities grow. So maybe you do have quite a few AI tools. By leveraging quite a few AI tools. By leveraging quite a few AI tools. By leveraging orchestrator agents, teams can ensure AI orchestrator agents, teams can ensure AI orchestrator agents, teams can ensure AI tools work together efficiently, tools work together efficiently, tools work together efficiently, reducing complexity and maximizing reducing complexity and maximizing reducing complexity and maximizing value.",
    "chunks": [
      "Kind: captions Language: en In the past, we've talked about chat In the past, we've talked about chat In the past, we've talked about chat bots, AI assistants, and agents. These bots, AI assistants, and agents. These bots, AI assistants, and agents. These are helpful tools in the AI space are helpful tools in the AI space are helpful tools in the AI space focused around working smarter, not focused around working smarter, not focused around working smarter, not harder, and becoming more",
      "productive. AI harder, and becoming more productive. AI harder, and becoming more productive. AI assistants are the DIY, the assistants are the DIY, the assistants are the DIY, the do-it-yourself. Especially after a user do-it-yourself. Especially after a user do-it-yourself. Especially after a user asks a prompt, you follow the asks a prompt, you follow the asks a prompt, you follow the instructions and get work done. On the instructions and get work done. On the instructions and get work done.",
      "On the other hand, agents are more like the DI other hand, agents are more like the DI other hand, agents are more like the DI FY do it for you. FY do it for you. FY do it for you. Agents can make decisions, kick off Agents can make decisions, kick off Agents can make decisions, kick off workflows, and even use function calling workflows, and even use function calling workflows, and even use function calling to connect with external tools like APIs to connect with external tools like APIs to",
      "connect with external tools like APIs or data sources. They can also act or data sources. They can also act or data sources. They can also act autonomously without being prompted or autonomously without being prompted or autonomously without being prompted or supervised. Many AI agents are supervised. Many AI agents are supervised. Many AI agents are specialized too, meaning each one is specialized too, meaning each one is specialized too, meaning each one is designed to support a particular",
      "area. designed to support a particular area. designed to support a particular area. Some agents focus on business and Some agents focus on business and Some agents focus on business and customerf facing tasks like billing or customerf facing tasks like billing or customerf facing tasks like billing or scheduling while others handle more scheduling while others handle more scheduling while others handle more technical functions like data retrieval technical functions like data retrieval technical",
      "functions like data retrieval and process automation. I had someone and process automation. I had someone and process automation. I had someone recently ask me if agents are the future recently ask me if agents are the future recently ask me if agents are the future and they're already here. Melissa, what and they're already here. Melissa, what and they're already here. Melissa, what could possibly be next? My answer, agent could possibly be next? My answer, agent could possibly be next? My",
      "answer, agent orchestration. As AI adoption grows, so orchestration. As AI adoption grows, so orchestration. As AI adoption grows, so does the number of AI tools being does the number of AI tools being does the number of AI tools being developed, deployed, and integrated into developed, deployed, and integrated into developed, deployed, and integrated into various workflows. Different AI various workflows. Different AI various workflows. Different AI assistants and agents handle specialized",
      "assistants and agents handle specialized assistants and agents handle specialized tasks. Some focus on automating tasks. Some focus on automating tasks. Some focus on automating technical operations. Some of them handle operations. Some of them handle operations. Some of them handle different workflows. Some of them even connect to external Some of them even connect to external Some of them even connect to external data sources like APIs or cloud services. These tools can come from services.",
      "These tools can come from services. These tools can come from different vendors, operate on different different vendors, operate on different different vendors, operate on different architectures, and may even be architectures, and may even be architectures, and may even be distributed across multiple cloud distributed across multiple cloud distributed across multiple cloud environments or on premise. The result, environments or on premise. The result, environments or on premise. The result, a",
      "fragmented AI ecosystem where tools a fragmented AI ecosystem where tools a fragmented AI ecosystem where tools exist in silos, making coordination, exist in silos, making coordination, exist in silos, making coordination, interoperability, and governance interoperability, and governance interoperability, and governance increasingly complex. This brings us to increasingly complex. This brings us to increasingly complex. This brings us to the next key concept, the orchestrator the next key",
      "concept, the orchestrator the next key concept, the orchestrator agent, which I'll put up here. The orchestrator agent is a type here. The orchestrator agent is a type here. The orchestrator agent is a type of agent that specializes in overseeing of agent that specializes in overseeing of agent that specializes in overseeing how work gets done. They're like a how work gets done. They're like a how work gets done. They're like a supervisor at work. They supervise how supervisor at work. They",
      "supervise how supervisor at work. They supervise how work gets done, ensures everyone's being work gets done, ensures everyone's being work gets done, ensures everyone's being a team player, and routes work to those a team player, and routes work to those a team player, and routes work to those with the right skills for the job. with the right skills for the job. with the right skills for the job. In this diagram, they can route across In this diagram, they can route across In this diagram, they",
      "can route across multiple task oriented AI agents and multiple task oriented AI agents and multiple task oriented AI agents and assistants to get a job done. When the assistants to get a job done. When the assistants to get a job done. When the right specialized assistant and agents right specialized assistant and agents right specialized assistant and agents work together to complete complex work together to complete complex work together to complete complex workflows and tasks, the experience",
      "is workflows and tasks, the experience is workflows and tasks, the experience is coordinated and seamless. In order to coordinated and seamless. In order to coordinated and seamless. In order to facilitate smooth agentto agent or facilitate smooth agentto agent or facilitate smooth agentto agent or agentto assistant communication through agentto assistant communication through agentto assistant communication through agent orchestration, all we have to do agent orchestration, all we have to do",
      "agent orchestration, all we have to do is follow three steps. The first is is follow three steps. The first is is follow three steps. The first is we'll need to define the task execution we'll need to define the task execution we'll need to define the task execution sequence or the workflow. Next, we'll sequence or the workflow. Next, we'll sequence or the workflow. Next, we'll need to set up API integration so that need to set up API integration so that need to set up API integration so that the",
      "agent or assistant can access the agent or assistant can access the agent or assistant can access relevant data. And lastly, we'll need to relevant data. And lastly, we'll need to relevant data. And lastly, we'll need to implement some sort of implement some sort of implement some sort of opensource orchestration technology. opensource orchestration technology. opensource orchestration technology. Once this is complete, the orchestrator Once this is complete, the orchestrator Once this is",
      "complete, the orchestrator agent takes over realtime execution. agent takes over realtime execution. agent takes over realtime execution. Benefits to using orchestrator agents Benefits to using orchestrator agents Benefits to using orchestrator agents include things like enhanced include things like enhanced include things like enhanced efficiency. Think about the concept that efficiency. Think about the concept that efficiency. Think about the concept that it's it's it's dy, right? Do it for",
      "you. You can work a dy, right? Do it for you. You can work a dy, right? Do it for you. You can work a lot smarter if you have someone getting lot smarter if you have someone getting lot smarter if you have someone getting some of your to-do list all completed. some of your to-do list all completed. some of your to-do list all completed. The next is around improved experience. The next is around improved experience. The next is around improved experience. We're desiloing here. The more tools We're",
      "desiloing here. The more tools We're desiloing here. The more tools that you have access to, the better your that you have access to, the better your that you have access to, the better your experience might be. And lastly, experience might be. And lastly, experience might be. And lastly, scalability. scalability. scalability. Think about all of the workflows and Think about all of the workflows and Think about all of the workflows and decisions agents or assistants can help decisions agents or",
      "assistants can help decisions agents or assistants can help you make. This can only help your you make. This can only help your you make. This can only help your capabilities grow. So maybe you do have capabilities grow. So maybe you do have capabilities grow. So maybe you do have quite a few AI tools. By leveraging quite a few AI tools. By leveraging quite a few AI tools. By leveraging orchestrator agents, teams can ensure AI orchestrator agents, teams can ensure AI orchestrator agents, teams",
      "can ensure AI tools work together efficiently, tools work together efficiently, tools work together efficiently, reducing complexity and maximizing reducing complexity and maximizing reducing complexity and maximizing value."
    ],
    "chunk_count": 21,
    "content_id": "1a8c3a6e-9947-4aa9-8da5-18844c4609ae",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554942"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=REVkXVxvMQE": {
    "title": "What is a graph database? (in 10 minutes)",
    "url": "https://www.youtube.com/watch?v=REVkXVxvMQE",
    "description": "🚨 2025 updated version here: https://youtu.be/F-bwn8p3Pes 🚨\n\nHigh-level overview of 'What is Neo4j'. https://bit.ly/3Z6f82h\n\n#Graphdatabases",
    "duration": 658,
    "uploader": "Neo4j",
    "transcript": "Kind: captions Language: en hello my name is Kareem Wallach I am the hello my name is Kareem Wallach I am the hello my name is Kareem Wallach I am the community manager at neo4j and over the community manager at neo4j and over the community manager at neo4j and over the last three plus years that I've been last three plus years that I've been last three plus years that I've been working at neo I realized there's still working at neo I realized there's still working at neo I realized there's still a lot of software developers and data a lot of software developers and data a lot of software developers and data scientists that do not understand what a scientists that do not understand what a scientists that do not understand what a graph database is they kind of think graph database is they kind of think graph database is they kind of think they know they might have heard of it they know they might have heard of it they know they might have heard of it but they really don't understand it so I but they really don't understand it so I but they really don't understand it so I just want to take a couple minutes and just want to take a couple minutes and just want to take a couple minutes and explain it to you because I think it's explain it to you because I think it's explain it to you because I think it's vitally important that anyone works with vitally important that anyone works with vitally important that anyone works with any kind of capacity with data should any kind of capacity with data should any kind of capacity with data should understand what it actually is because understand what it actually is because understand what it actually is because it really is a game changer so I'm gonna it really is a game changer so I'm gonna it really is a game changer so I'm gonna share my screen really quick I'm gonna share my screen really quick I'm gonna share my screen really quick I'm gonna try to make this as quick and easy as try to make this as quick and easy as try to make this as quick and easy as possible I all right sure my screen possible I all right sure my screen possible I all right sure my screen sharing and sharing it working on it I'm sharing and sharing it working on it I'm sharing and sharing it working on it I'm getting there hold on okay all right so getting there hold on okay all right so getting there hold on okay all right so I'm sharing my screen so so this is a I'm sharing my screen so so this is a I'm sharing my screen so so this is a example of like the new forge a browser example of like the new forge a browser example of like the new forge a browser right here so one thing that they think right here so one thing that they think right here so one thing that they think is really important to note very is really important to note very is really important to note very important to understand is that a great important to understand is that a great important to understand is that a great idea for J is a database okay it's not a idea for J is a database okay it's not a idea for J is a database okay it's not a visualization that sits on another visualization that sits on another visualization that sits on another database it is an acid compliant database it is an acid compliant database it is an acid compliant transactional database so that's transactional database so that's transactional database so that's something that's very important something that's very important something that's very important technically it's in the no sequel technically it's in the no sequel technically it's in the no sequel category but it's very very different category but it's very very different category but it's very very different for most normalized relational databases for most normalized relational databases for most normalized relational databases in the sense that most relational in the sense that most relational in the sense that most relational databases store data in the shape of databases store data in the shape of databases store data in the shape of tables and joins neo stores the data in tables and joins neo stores the data in tables and joins neo stores the data in the shape of a graph and when I say the shape of a graph and when I say the shape of a graph and when I say graph I do not mean a chart I mean a graph I do not mean a chart I mean a graph I do not mean a chart I mean a graph theory graph like a network so in graph theory graph like a network so in graph theory graph like a network so in the data model inside of neo4j is like the data model inside of neo4j is like the data model inside of neo4j is like you have your nodes here your nodes are you have your nodes here your nodes are you have your nodes here your nodes are your nouns that's person place thing your nouns that's person place thing your nouns that's person place thing location right these are your notes your location right these are your notes your location right these are your notes your nodes here and then and these notes nodes here and then and these notes nodes here and then and these notes could also have properties which is they could also have properties which is they could also have properties which is they could be like labeled right here so could be like labeled right here so could be like labeled right here so those your properties and then you could those your properties and then you could those your properties and then you could have relationships between those nodes have relationships between those nodes have relationships between those nodes and those relationships are and with an and those relationships are and with an and those relationships are and with an India for J relationships are actually India for J relationships are actually India for J relationships are actually first-class citizens meaning that first-class citizens meaning that first-class citizens meaning that they're just as important as the nodes they're just as important as the nodes they're just as important as the nodes themselves like how are things connected themselves like how are things connected themselves like how are things connected right so just like you could have right so just like you could have right so just like you could have different types of nodes you can also different types of nodes you can also different types of nodes you can also have different types of relationships have different types of relationships have different types of relationships you can put properties in those you can put properties in those you can put properties in those relationships you could put values so relationships you could put values so relationships you could put values so they could be weighted you can have they could be weighted you can have they could be weighted you can have geospatial information you could have geospatial information you could have geospatial information you could have date and time so basically whenever you date and time so basically whenever you date and time so basically whenever you have have have data that's complexly connected and you data that's complexly connected and you data that's complexly connected and you want to understand how these things are want to understand how these things are want to understand how these things are connected to each other or maybe you connected to each other or maybe you connected to each other or maybe you want to find a shortest path right let's want to find a shortest path right let's want to find a shortest path right let's show this path from here to here or show this path from here to here or show this path from here to here or maybe you're looking for patterns in maybe you're looking for patterns in maybe you're looking for patterns in your data or maybe you're looking for your data or maybe you're looking for your data or maybe you're looking for something that's like a combination of something that's like a combination of something that's like a combination of patterns right it might be like if patterns right it might be like if patterns right it might be like if you're doing fraud detection it might you're doing fraud detection it might you're doing fraud detection it might not be one transaction that sets off a not be one transaction that sets off a not be one transaction that sets off a flag but it's a transaction with these flag but it's a transaction with these flag but it's a transaction with these other patterns that in behavior that other patterns that in behavior that other patterns that in behavior that might be kind of you know raising the might be kind of you know raising the might be kind of you know raising the flag or if you're doing any kind of flag or if you're doing any kind of flag or if you're doing any kind of graph algorithm type of analysis whether graph algorithm type of analysis whether graph algorithm type of analysis whether you're like community detection or you're like community detection or you're like community detection or between the centrality and PageRank between the centrality and PageRank between the centrality and PageRank things like that like network related things like that like network related things like that like network related style queries like is the shape of your style queries like is the shape of your style queries like is the shape of your data network or is it a table so that's data network or is it a table so that's data network or is it a table so that's kind of like the big differences so kind of like the big differences so kind of like the big differences so there's a few things that I think that there's a few things that I think that there's a few things that I think that are really important to note about are really important to note about are really important to note about understanding a neo4j and what makes a understanding a neo4j and what makes a understanding a neo4j and what makes a difference so for one thing its new for difference so for one thing its new for difference so for one thing its new for J is a native graph database and what J is a native graph database and what J is a native graph database and what that means is the underlying that means is the underlying that means is the underlying architecture of how the data is actually architecture of how the data is actually architecture of how the data is actually stored is not built on top of tables stored is not built on top of tables stored is not built on top of tables okay everything is built to support this okay everything is built to support this okay everything is built to support this type of data model this highly connected type of data model this highly connected type of data model this highly connected data model so when you're doing a query data model so when you're doing a query data model so when you're doing a query with most like relational databases you with most like relational databases you with most like relational databases you know you're indexing and then you make know you're indexing and then you make know you're indexing and then you make another hop and you're indexing there another hop and you're indexing there another hop and you're indexing there and then you're another joins all these and then you're another joins all these and then you're another joins all these joints you know these joins are always joints you know these joins are always joints you know these joins are always indexing and that's very computationally indexing and that's very computationally indexing and that's very computationally expensive when you're doing a lot of expensive when you're doing a lot of expensive when you're doing a lot of hops hops hops I'm with neo4j when you do a query you I'm with neo4j when you do a query you I'm with neo4j when you do a query you index to find your initial starting index to find your initial starting index to find your initial starting point and then from there you're just point and then from there you're just point and then from there you're just basically chasing memory pointers which basically chasing memory pointers which basically chasing memory pointers which the computer happens to be pretty good the computer happens to be pretty good the computer happens to be pretty good at so the benefit of that not having to at so the benefit of that not having to at so the benefit of that not having to index every time you make a hop is index every time you make a hop is index every time you make a hop is pretty powerful the traversal time pretty powerful the traversal time pretty powerful the traversal time between doing one hop or twelve hops can between doing one hop or twelve hops can between doing one hop or twelve hops can be pretty consistent which is a pretty be pretty consistent which is a pretty be pretty consistent which is a pretty powerful thing when you're you know powerful thing when you're you know powerful thing when you're you know hopping through a very very highly hopping through a very very highly hopping through a very very highly connected so that's one thing that's connected so that's one thing that's connected so that's one thing that's very important to understand and then very important to understand and then very important to understand and then the other thing is also the query the other thing is also the query the other thing is also the query language so you're probably used to language so you're probably used to language so you're probably used to sequel because that's like a pretty sequel because that's like a pretty sequel because that's like a pretty standard you know query language problem standard you know query language problem standard you know query language problem is is that when you're working with is is that when you're working with is is that when you're working with graph databases graph databases graph databases and graph type of problems sequel isn't and graph type of problems sequel isn't and graph type of problems sequel isn't going to cut it because sequel is not going to cut it because sequel is not going to cut it because sequel is not built for highly connected data so near built for highly connected data so near built for highly connected data so near forge a actually developed language it's forge a actually developed language it's forge a actually developed language it's an open language a lot of other an open language a lot of other an open language a lot of other companies are using it it's called companies are using it it's called companies are using it it's called cipher and cipher is a it's basically cipher and cipher is a it's basically cipher and cipher is a it's basically sequel for graphs it's more like where sequel for graphs it's more like where sequel for graphs it's more like where sequel is kind of like give me this like sequel is kind of like give me this like sequel is kind of like give me this like cipher you could be a little bit more cipher you could be a little bit more cipher you could be a little bit more ambiguous it's based off of pattern ambiguous it's based off of pattern ambiguous it's based off of pattern matching like more network II kind of matching like more network II kind of matching like more network II kind of related queries which is really powerful related queries which is really powerful related queries which is really powerful so sequel is a decorative and it's also so sequel is a decorative and it's also so sequel is a decorative and it's also based off of ASCII art which it makes it based off of ASCII art which it makes it based off of ASCII art which it makes it really nice good to be able to see really nice good to be able to see really nice good to be able to see because it looks like what it actually because it looks like what it actually because it looks like what it actually represents so just a high-level overview represents so just a high-level overview represents so just a high-level overview of it your nodes here right in yellow of it your nodes here right in yellow of it your nodes here right in yellow blue is easier they're represented by blue is easier they're represented by blue is easier they're represented by parentheses see and then these parentheses see and then these parentheses see and then these relationships that are directional relationships that are directional relationships that are directional relationships between those nodes are an relationships between those nodes are an relationships between those nodes are an arrow literally an arrow and then arrow literally an arrow and then arrow literally an arrow and then brackets with it's almost funny to like brackets with it's almost funny to like brackets with it's almost funny to like look at it you're like oh yeah that look at it you're like oh yeah that look at it you're like oh yeah that makes sense so here's another node right makes sense so here's another node right makes sense so here's another node right so it's like we're looking for a company so it's like we're looking for a company so it's like we're looking for a company company that develops a game but we're company that develops a game but we're company that develops a game but we're also looking for there's another also looking for there's another also looking for there's another relationship here on this side where a relationship here on this side where a relationship here on this side where a company also publishes the game where company also publishes the game where company also publishes the game where the company is Electronic Arts Wow right the company is Electronic Arts Wow right the company is Electronic Arts Wow right that was crazy it's crazy so I really that was crazy it's crazy so I really that was crazy it's crazy so I really like this blog post cuz I think it kind like this blog post cuz I think it kind like this blog post cuz I think it kind of shows the power of the date the model of shows the power of the date the model of shows the power of the date the model in general like the data model like in general like the data model like in general like the data model like being able to do a lot of these hops but being able to do a lot of these hops but being able to do a lot of these hops but I think it also shows like the power of I think it also shows like the power of I think it also shows like the power of having a query language that can help having a query language that can help having a query language that can help you look into networks and graphs and you look into networks and graphs and you look into networks and graphs and patterns and pathways so I'll show you patterns and pathways so I'll show you patterns and pathways so I'll show you another example this one I also think is another example this one I also think is another example this one I also think is pretty powerful so here is an example of pretty powerful so here is an example of pretty powerful so here is an example of a video game recommendation right so you a video game recommendation right so you a video game recommendation right so you have here is a video game in the yellow have here is a video game in the yellow have here is a video game in the yellow node that's fallout 3 and here whoops node that's fallout 3 and here whoops node that's fallout 3 and here whoops too far yeah you have Borderlands 2 too far yeah you have Borderlands 2 too far yeah you have Borderlands 2 right and in the blue nodes they might right and in the blue nodes they might right and in the blue nodes they might be I don't know like consoles that the be I don't know like consoles that the be I don't know like consoles that the game is played on or whatever in the game is played on or whatever in the game is played on or whatever in the green you have different themes of the green you have different themes of the green you have different themes of the game is it zombie and pirates and pain game is it zombie and pirates and pain game is it zombie and pirates and pain and war whatever and and war whatever and and war whatever and remember to like these relationships remember to like these relationships remember to like these relationships because they're first-class citizens you because they're first-class citizens you because they're first-class citizens you could also have values on them so they could also have values on them so they could also have values on them so they could be weighted you could have a lot could be weighted you could have a lot could be weighted you could have a lot of pirates and a little bit of zombie or of pirates and a little bit of zombie or of pirates and a little bit of zombie or whatever so that part is also something whatever so that part is also something whatever so that part is also something that you could take an account which can that you could take an account which can that you could take an account which can be pretty powerful and you're like you be pretty powerful and you're like you be pretty powerful and you're like you know trying to make queries based on know trying to make queries based on know trying to make queries based on weights and then in bread you can have weights and then in bread you can have weights and then in bread you can have like how is the game plate is it played like how is the game plate is it played like how is the game plate is it played multiple player and you playing one multiple player and you playing one multiple player and you playing one player is a first-person or with a mouse player is a first-person or with a mouse player is a first-person or with a mouse or a joystick or a keyboard like how is or a joystick or a keyboard like how is or a joystick or a keyboard like how is the game actually played now here you if the game actually played now here you if the game actually played now here you if you have a user that likes both of these you have a user that likes both of these you have a user that likes both of these games here and you want to say okay I games here and you want to say okay I games here and you want to say okay I want to understand who my user is or want to understand who my user is or want to understand who my user is or maybe like find out what these two maybe like find out what these two maybe like find out what these two things have in common so I could find things have in common so I could find things have in common so I could find another game that has the most in common another game that has the most in common another game that has the most in common with these two games right just with these two games right just with these two games right just generally characteristics like what do generally characteristics like what do generally characteristics like what do they have in common if you were to do they have in common if you were to do they have in common if you were to do this in sequel it would be a very this in sequel it would be a very this in sequel it would be a very extensive query because of all the extensive query because of all the extensive query because of all the different types of nodes that you have different types of nodes that you have different types of nodes that you have in the different types of relationships in the different types of relationships in the different types of relationships so with cypher it's actually very very so with cypher it's actually very very so with cypher it's actually very very straightforward it might actually laugh straightforward it might actually laugh straightforward it might actually laugh at how amazing it is but um so this is at how amazing it is but um so this is at how amazing it is but um so this is an example of the cipher query for this an example of the cipher query for this an example of the cipher query for this query know it's crazy three lines so you query know it's crazy three lines so you query know it's crazy three lines so you have here is like your node in have here is like your node in have here is like your node in parenthesis and then here is where your parenthesis and then here is where your parenthesis and then here is where your relationship would be in this case the relationship would be in this case the relationship would be in this case the relationship is undefined any relationship is undefined any relationship is undefined any relationship any direction you could put relationship any direction you could put relationship any direction you could put a star six in there or something if you a star six in there or something if you a star six in there or something if you want to look six hops out or whatever want to look six hops out or whatever want to look six hops out or whatever there's all kinds of different things there's all kinds of different things there's all kinds of different things you could do a cipher but you're looking you could do a cipher but you're looking you could do a cipher but you're looking for characteristics and a game for characteristics and a game for characteristics and a game relationship between these two games relationship between these two games relationship between these two games damn right so this is like I think just damn right so this is like I think just damn right so this is like I think just like a really powerful example of like like a really powerful example of like like a really powerful example of like something you could do with not just the something you could do with not just the something you could do with not just the data model like being able to store the data model like being able to store the data model like being able to store the data and query it very quickly but also data and query it very quickly but also data and query it very quickly but also the ability to use cipher to kind of the ability to use cipher to kind of the ability to use cipher to kind of help you find the things that you know help you find the things that you know help you find the things that you know normally highly connected or distantly normally highly connected or distantly normally highly connected or distantly connected or you know those like connected or you know those like connected or you know those like graphene related kind of problems um so graphene related kind of problems um so graphene related kind of problems um so I know you're probably already thinking I know you're probably already thinking I know you're probably already thinking about like oh where can I use this about like oh where can I use this about like oh where can I use this because this does sound kind of because this does sound kind of because this does sound kind of interesting I will tell you there's a interesting I will tell you there's a interesting I will tell you there's a lot of really cool use cases for it the lot of really cool use cases for it the lot of really cool use cases for it the very standard ones like recommendations very standard ones like recommendations very standard ones like recommendations big one fraud detection like network and big one fraud detection like network and big one fraud detection like network and IT management those are like the really IT management those are like the really IT management those are like the really big ones that kind of uh big ones that kind of uh big ones that kind of uh are frequently used a lot of like NLP are frequently used a lot of like NLP are frequently used a lot of like NLP related stuff like even if you think related stuff like even if you think related stuff like even if you think about linguistics like how we speak about linguistics like how we speak about linguistics like how we speak right that is all a graph there's this right that is all a graph there's this right that is all a graph there's this thing that happens that we call it the thing that happens that we call it the thing that happens that we call it the graph epiphany basically when you start graph epiphany basically when you start graph epiphany basically when you start seeing in graphs seeing in graphs seeing in graphs everything you see graphs everywhere you everything you see graphs everywhere you everything you see graphs everywhere you can't get rid of it because everything can't get rid of it because everything can't get rid of it because everything is dependent on something else you know is dependent on something else you know is dependent on something else you know it's like all these intertwined it's like all these intertwined it's like all these intertwined connections of things but there are a connections of things but there are a connections of things but there are a lot of really really cool use cases lot of really really cool use cases lot of really really cool use cases actually probably one of my favorite actually probably one of my favorite actually probably one of my favorite things about my job is hearing about all things about my job is hearing about all things about my job is hearing about all the interesting use cases of how people the interesting use cases of how people the interesting use cases of how people use graphs in this case this one is use graphs in this case this one is use graphs in this case this one is Hetty oh if you go to head thought IO Hetty oh if you go to head thought IO Hetty oh if you go to head thought IO they have like they're the homepage here they have like they're the homepage here they have like they're the homepage here so like Hecht dot IO this was this was so like Hecht dot IO this was this was so like Hecht dot IO this was this was created by Daniel Himelstein who's a created by Daniel Himelstein who's a created by Daniel Himelstein who's a postdoc researcher at University of postdoc researcher at University of postdoc researcher at University of Pennsylvania but you can play around Pennsylvania but you can play around Pennsylvania but you can play around with this stuff you know that he's got with this stuff you know that he's got with this stuff you know that he's got the ability for you to explore there's a the ability for you to explore there's a the ability for you to explore there's a can you afford a browser thing and can you afford a browser thing and can you afford a browser thing and there's guides that kind of walk you there's guides that kind of walk you there's guides that kind of walk you through and tell you what you can do through and tell you what you can do through and tell you what you can do with it we also have neo4j sandbox I with it we also have neo4j sandbox I with it we also have neo4j sandbox I probably say it probably one of the best probably say it probably one of the best probably say it probably one of the best places to start just to kind of get you places to start just to kind of get you places to start just to kind of get you thinking in graphs so if you go to neo4j thinking in graphs so if you go to neo4j thinking in graphs so if you go to neo4j comm slash developer there's an online comm slash developer there's an online comm slash developer there's an online sandbox thing here you don't have to sandbox thing here you don't have to sandbox thing here you don't have to download anything there's pre-existing download anything there's pre-existing download anything there's pre-existing data sets you can just jump in you can data sets you can just jump in you can data sets you can just jump in you can follow the guide start playing around follow the guide start playing around follow the guide start playing around and then once you're ready um you know and then once you're ready um you know and then once you're ready um you know you can kind of dig into here a little you can kind of dig into here a little you can kind of dig into here a little bit there's like intro to graph bit there's like intro to graph bit there's like intro to graph databases YouTube series but we have all databases YouTube series but we have all databases YouTube series but we have all kinds of like you have graphic Adam E or kinds of like you have graphic Adam E or kinds of like you have graphic Adam E or you can go you know self-paced tutorial you can go you know self-paced tutorial you can go you know self-paced tutorial and stuff so yeah hopefully you're gonna and stuff so yeah hopefully you're gonna and stuff so yeah hopefully you're gonna thank me for this and not hate me for thank me for this and not hate me for thank me for this and not hate me for getting you addicted to graphs I will getting you addicted to graphs I will getting you addicted to graphs I will also make sure I mentioned because this also make sure I mentioned because this also make sure I mentioned because this does happen to people who are in the does happen to people who are in the does happen to people who are in the graph epiphany they try to put graphs graph epiphany they try to put graphs graph epiphany they try to put graphs everywhere they don't belong everywhere everywhere they don't belong everywhere everywhere they don't belong everywhere they are everywhere they don't belong they are everywhere they don't belong they are everywhere they don't belong everywhere they are highly connected everywhere they are highly connected everywhere they are highly connected data problems but that said once you're data problems but that said once you're data problems but that said once you're addicted to graphs and you already have addicted to graphs and you already have addicted to graphs and you already have these amazing awesome use cases that you these amazing awesome use cases that you these amazing awesome use cases that you want to share with the rest of the world want to share with the rest of the world want to share with the rest of the world then you can come to me and then you then you can come to me and then you then you can come to me and then you know we could do something with the know we could do something with the know we could do something with the community so yeah hopefully you enjoyed community so yeah hopefully you enjoyed community so yeah hopefully you enjoyed the session and hopefully this was the session and hopefully this was the session and hopefully this was helpful helpful helpful see you soon bye",
    "chunks": [
      "Kind: captions Language: en hello my name is Kareem Wallach I am the hello my name is Kareem Wallach I am the hello my name is Kareem Wallach I am the community manager at neo4j and over the community manager at neo4j and over the community manager at neo4j and over the last three plus years that I've been last three plus years that I've been last three plus years that I've been working at neo I realized there's still working at neo I realized there's still working at neo I realized there's",
      "still a lot of software developers and data a lot of software developers and data a lot of software developers and data scientists that do not understand what a scientists that do not understand what a scientists that do not understand what a graph database is they kind of think graph database is they kind of think graph database is they kind of think they know they might have heard of it they know they might have heard of it they know they might have heard of it but they really don't understand",
      "it so I but they really don't understand it so I but they really don't understand it so I just want to take a couple minutes and just want to take a couple minutes and just want to take a couple minutes and explain it to you because I think it's explain it to you because I think it's explain it to you because I think it's vitally important that anyone works with vitally important that anyone works with vitally important that anyone works with any kind of capacity with data should any kind of",
      "capacity with data should any kind of capacity with data should understand what it actually is because understand what it actually is because understand what it actually is because it really is a game changer so I'm gonna it really is a game changer so I'm gonna it really is a game changer so I'm gonna share my screen really quick I'm gonna share my screen really quick I'm gonna share my screen really quick I'm gonna try to make this as quick and easy as try to make this as quick and easy as try",
      "to make this as quick and easy as possible I all right sure my screen possible I all right sure my screen possible I all right sure my screen sharing and sharing it working on it I'm sharing and sharing it working on it I'm sharing and sharing it working on it I'm getting there hold on okay all right so getting there hold on okay all right so getting there hold on okay all right so I'm sharing my screen so so this is a I'm sharing my screen so so this is a I'm sharing my screen so so this is a",
      "example of like the new forge a browser example of like the new forge a browser example of like the new forge a browser right here so one thing that they think right here so one thing that they think right here so one thing that they think is really important to note very is really important to note very is really important to note very important to understand is that a great important to understand is that a great important to understand is that a great idea for J is a database okay it's not a",
      "idea for J is a database okay it's not a idea for J is a database okay it's not a visualization that sits on another visualization that sits on another visualization that sits on another database it is an acid compliant database it is an acid compliant database it is an acid compliant transactional database so that's transactional database so that's transactional database so that's something that's very important something that's very important something that's very important technically it's in",
      "the no sequel technically it's in the no sequel technically it's in the no sequel category but it's very very different category but it's very very different category but it's very very different for most normalized relational databases for most normalized relational databases for most normalized relational databases in the sense that most relational in the sense that most relational in the sense that most relational databases store data in the shape of databases store data in the shape of",
      "databases store data in the shape of tables and joins neo stores the data in tables and joins neo stores the data in tables and joins neo stores the data in the shape of a graph and when I say the shape of a graph and when I say the shape of a graph and when I say graph I do not mean a chart I mean a graph I do not mean a chart I mean a graph I do not mean a chart I mean a graph theory graph like a network so in graph theory graph like a network so in graph theory graph like a network so in the",
      "data model inside of neo4j is like the data model inside of neo4j is like the data model inside of neo4j is like you have your nodes here your nodes are you have your nodes here your nodes are you have your nodes here your nodes are your nouns that's person place thing your nouns that's person place thing your nouns that's person place thing location right these are your notes your location right these are your notes your location right these are your notes your nodes here and then and these",
      "notes nodes here and then and these notes nodes here and then and these notes could also have properties which is they could also have properties which is they could also have properties which is they could be like labeled right here so could be like labeled right here so could be like labeled right here so those your properties and then you could those your properties and then you could those your properties and then you could have relationships between those nodes have relationships between",
      "those nodes have relationships between those nodes and those relationships are and with an and those relationships are and with an and those relationships are and with an India for J relationships are actually India for J relationships are actually India for J relationships are actually first-class citizens meaning that first-class citizens meaning that first-class citizens meaning that they're just as important as the nodes they're just as important as the nodes they're just as important as the",
      "nodes themselves like how are things connected themselves like how are things connected themselves like how are things connected right so just like you could have right so just like you could have right so just like you could have different types of nodes you can also different types of nodes you can also different types of nodes you can also have different types of relationships have different types of relationships have different types of relationships you can put properties in those you can",
      "put properties in those you can put properties in those relationships you could put values so relationships you could put values so relationships you could put values so they could be weighted you can have they could be weighted you can have they could be weighted you can have geospatial information you could have geospatial information you could have geospatial information you could have date and time so basically whenever you date and time so basically whenever you date and time so basically",
      "whenever you have have have data that's complexly connected and you data that's complexly connected and you data that's complexly connected and you want to understand how these things are want to understand how these things are want to understand how these things are connected to each other or maybe you connected to each other or maybe you connected to each other or maybe you want to find a shortest path right let's want to find a shortest path right let's want to find a shortest path right let's",
      "show this path from here to here or show this path from here to here or show this path from here to here or maybe you're looking for patterns in maybe you're looking for patterns in maybe you're looking for patterns in your data or maybe you're looking for your data or maybe you're looking for your data or maybe you're looking for something that's like a combination of something that's like a combination of something that's like a combination of patterns right it might be like if patterns right",
      "it might be like if patterns right it might be like if you're doing fraud detection it might you're doing fraud detection it might you're doing fraud detection it might not be one transaction that sets off a not be one transaction that sets off a not be one transaction that sets off a flag but it's a transaction with these flag but it's a transaction with these flag but it's a transaction with these other patterns that in behavior that other patterns that in behavior that other patterns that in",
      "behavior that might be kind of you know raising the might be kind of you know raising the might be kind of you know raising the flag or if you're doing any kind of flag or if you're doing any kind of flag or if you're doing any kind of graph algorithm type of analysis whether graph algorithm type of analysis whether graph algorithm type of analysis whether you're like community detection or you're like community detection or you're like community detection or between the centrality and PageRank",
      "between the centrality and PageRank between the centrality and PageRank things like that like network related things like that like network related things like that like network related style queries like is the shape of your style queries like is the shape of your style queries like is the shape of your data network or is it a table so that's data network or is it a table so that's data network or is it a table so that's kind of like the big differences so kind of like the big differences so",
      "kind of like the big differences so there's a few things that I think that there's a few things that I think that there's a few things that I think that are really important to note about are really important to note about are really important to note about understanding a neo4j and what makes a understanding a neo4j and what makes a understanding a neo4j and what makes a difference so for one thing its new for difference so for one thing its new for difference so for one thing its new for J is a",
      "native graph database and what J is a native graph database and what J is a native graph database and what that means is the underlying that means is the underlying that means is the underlying architecture of how the data is actually architecture of how the data is actually architecture of how the data is actually stored is not built on top of tables stored is not built on top of tables stored is not built on top of tables okay everything is built to support this okay everything is built to",
      "support this okay everything is built to support this type of data model this highly connected type of data model this highly connected type of data model this highly connected data model so when you're doing a query data model so when you're doing a query data model so when you're doing a query with most like relational databases you with most like relational databases you with most like relational databases you know you're indexing and then you make know you're indexing and then you make know",
      "you're indexing and then you make another hop and you're indexing there another hop and you're indexing there another hop and you're indexing there and then you're another joins all these and then you're another joins all these and then you're another joins all these joints you know these joins are always joints you know these joins are always joints you know these joins are always indexing and that's very computationally indexing and that's very computationally indexing and that's very",
      "computationally expensive when you're doing a lot of expensive when you're doing a lot of expensive when you're doing a lot of hops hops hops I'm with neo4j when you do a query you I'm with neo4j when you do a query you I'm with neo4j when you do a query you index to find your initial starting index to find your initial starting index to find your initial starting point and then from there you're just point and then from there you're just point and then from there you're just basically chasing",
      "memory pointers which basically chasing memory pointers which basically chasing memory pointers which the computer happens to be pretty good the computer happens to be pretty good the computer happens to be pretty good at so the benefit of that not having to at so the benefit of that not having to at so the benefit of that not having to index every time you make a hop is index every time you make a hop is index every time you make a hop is pretty powerful the traversal time pretty powerful the",
      "traversal time pretty powerful the traversal time between doing one hop or twelve hops can between doing one hop or twelve hops can between doing one hop or twelve hops can be pretty consistent which is a pretty be pretty consistent which is a pretty be pretty consistent which is a pretty powerful thing when you're you know powerful thing when you're you know powerful thing when you're you know hopping through a very very highly hopping through a very very highly hopping through a very very",
      "highly connected so that's one thing that's connected so that's one thing that's connected so that's one thing that's very important to understand and then very important to understand and then very important to understand and then the other thing is also the query the other thing is also the query the other thing is also the query language so you're probably used to language so you're probably used to language so you're probably used to sequel because that's like a pretty sequel because that's",
      "like a pretty sequel because that's like a pretty standard you know query language problem standard you know query language problem standard you know query language problem is is that when you're working with is is that when you're working with is is that when you're working with graph databases graph databases graph databases and graph type of problems sequel isn't and graph type of problems sequel isn't and graph type of problems sequel isn't going to cut it because sequel is not going to cut",
      "it because sequel is not going to cut it because sequel is not built for highly connected data so near built for highly connected data so near built for highly connected data so near forge a actually developed language it's forge a actually developed language it's forge a actually developed language it's an open language a lot of other an open language a lot of other an open language a lot of other companies are using it it's called companies are using it it's called companies are using it it's",
      "called cipher and cipher is a it's basically cipher and cipher is a it's basically cipher and cipher is a it's basically sequel for graphs it's more like where sequel for graphs it's more like where sequel for graphs it's more like where sequel is kind of like give me this like sequel is kind of like give me this like sequel is kind of like give me this like cipher you could be a little bit more cipher you could be a little bit more cipher you could be a little bit more ambiguous it's based off",
      "of pattern ambiguous it's based off of pattern ambiguous it's based off of pattern matching like more network II kind of matching like more network II kind of matching like more network II kind of related queries which is really powerful related queries which is really powerful related queries which is really powerful so sequel is a decorative and it's also so sequel is a decorative and it's also so sequel is a decorative and it's also based off of ASCII art which it makes it based off of ASCII",
      "art which it makes it based off of ASCII art which it makes it really nice good to be able to see really nice good to be able to see really nice good to be able to see because it looks like what it actually because it looks like what it actually because it looks like what it actually represents so just a high-level overview represents so just a high-level overview represents so just a high-level overview of it your nodes here right in yellow of it your nodes here right in yellow of it your nodes",
      "here right in yellow blue is easier they're represented by blue is easier they're represented by blue is easier they're represented by parentheses see and then these parentheses see and then these parentheses see and then these relationships that are directional relationships that are directional relationships that are directional relationships between those nodes are an relationships between those nodes are an relationships between those nodes are an arrow literally an arrow and then arrow",
      "literally an arrow and then arrow literally an arrow and then brackets with it's almost funny to like brackets with it's almost funny to like brackets with it's almost funny to like look at it you're like oh yeah that look at it you're like oh yeah that look at it you're like oh yeah that makes sense so here's another node right makes sense so here's another node right makes sense so here's another node right so it's like we're looking for a company so it's like we're looking for a company so",
      "it's like we're looking for a company company that develops a game but we're company that develops a game but we're company that develops a game but we're also looking for there's another also looking for there's another also looking for there's another relationship here on this side where a relationship here on this side where a relationship here on this side where a company also publishes the game where company also publishes the game where company also publishes the game where the company is",
      "Electronic Arts Wow right the company is Electronic Arts Wow right the company is Electronic Arts Wow right that was crazy it's crazy so I really that was crazy it's crazy so I really that was crazy it's crazy so I really like this blog post cuz I think it kind like this blog post cuz I think it kind like this blog post cuz I think it kind of shows the power of the date the model of shows the power of the date the model of shows the power of the date the model in general like the data model like",
      "in general like the data model like in general like the data model like being able to do a lot of these hops but being able to do a lot of these hops but being able to do a lot of these hops but I think it also shows like the power of I think it also shows like the power of I think it also shows like the power of having a query language that can help having a query language that can help having a query language that can help you look into networks and graphs and you look into networks and graphs",
      "and you look into networks and graphs and patterns and pathways so I'll show you patterns and pathways so I'll show you patterns and pathways so I'll show you another example this one I also think is another example this one I also think is another example this one I also think is pretty powerful so here is an example of pretty powerful so here is an example of pretty powerful so here is an example of a video game recommendation right so you a video game recommendation right so you a video game",
      "recommendation right so you have here is a video game in the yellow have here is a video game in the yellow have here is a video game in the yellow node that's fallout 3 and here whoops node that's fallout 3 and here whoops node that's fallout 3 and here whoops too far yeah you have Borderlands 2 too far yeah you have Borderlands 2 too far yeah you have Borderlands 2 right and in the blue nodes they might right and in the blue nodes they might right and in the blue nodes they might be I don't",
      "know like consoles that the be I don't know like consoles that the be I don't know like consoles that the game is played on or whatever in the game is played on or whatever in the game is played on or whatever in the green you have different themes of the green you have different themes of the green you have different themes of the game is it zombie and pirates and pain game is it zombie and pirates and pain game is it zombie and pirates and pain and war whatever and and war whatever and and war",
      "whatever and remember to like these relationships remember to like these relationships remember to like these relationships because they're first-class citizens you because they're first-class citizens you because they're first-class citizens you could also have values on them so they could also have values on them so they could also have values on them so they could be weighted you could have a lot could be weighted you could have a lot could be weighted you could have a lot of pirates and a",
      "little bit of zombie or of pirates and a little bit of zombie or of pirates and a little bit of zombie or whatever so that part is also something whatever so that part is also something whatever so that part is also something that you could take an account which can that you could take an account which can that you could take an account which can be pretty powerful and you're like you be pretty powerful and you're like you be pretty powerful and you're like you know trying to make queries based",
      "on know trying to make queries based on know trying to make queries based on weights and then in bread you can have weights and then in bread you can have weights and then in bread you can have like how is the game plate is it played like how is the game plate is it played like how is the game plate is it played multiple player and you playing one multiple player and you playing one multiple player and you playing one player is a first-person or with a mouse player is a first-person or with a",
      "mouse player is a first-person or with a mouse or a joystick or a keyboard like how is or a joystick or a keyboard like how is or a joystick or a keyboard like how is the game actually played now here you if the game actually played now here you if the game actually played now here you if you have a user that likes both of these you have a user that likes both of these you have a user that likes both of these games here and you want to say okay I games here and you want to say okay I games here",
      "and you want to say okay I want to understand who my user is or want to understand who my user is or want to understand who my user is or maybe like find out what these two maybe like find out what these two maybe like find out what these two things have in common so I could find things have in common so I could find things have in common so I could find another game that has the most in common another game that has the most in common another game that has the most in common with these two games",
      "right just with these two games right just with these two games right just generally characteristics like what do generally characteristics like what do generally characteristics like what do they have in common if you were to do they have in common if you were to do they have in common if you were to do this in sequel it would be a very this in sequel it would be a very this in sequel it would be a very extensive query because of all the extensive query because of all the extensive query because",
      "of all the different types of nodes that you have different types of nodes that you have different types of nodes that you have in the different types of relationships in the different types of relationships in the different types of relationships so with cypher it's actually very very so with cypher it's actually very very so with cypher it's actually very very straightforward it might actually laugh straightforward it might actually laugh straightforward it might actually laugh at how amazing",
      "it is but um so this is at how amazing it is but um so this is at how amazing it is but um so this is an example of the cipher query for this an example of the cipher query for this an example of the cipher query for this query know it's crazy three lines so you query know it's crazy three lines so you query know it's crazy three lines so you have here is like your node in have here is like your node in have here is like your node in parenthesis and then here is where your parenthesis and then",
      "here is where your parenthesis and then here is where your relationship would be in this case the relationship would be in this case the relationship would be in this case the relationship is undefined any relationship is undefined any relationship is undefined any relationship any direction you could put relationship any direction you could put relationship any direction you could put a star six in there or something if you a star six in there or something if you a star six in there or something",
      "if you want to look six hops out or whatever want to look six hops out or whatever want to look six hops out or whatever there's all kinds of different things there's all kinds of different things there's all kinds of different things you could do a cipher but you're looking you could do a cipher but you're looking you could do a cipher but you're looking for characteristics and a game for characteristics and a game for characteristics and a game relationship between these two games relationship",
      "between these two games relationship between these two games damn right so this is like I think just damn right so this is like I think just damn right so this is like I think just like a really powerful example of like like a really powerful example of like like a really powerful example of like something you could do with not just the something you could do with not just the something you could do with not just the data model like being able to store the data model like being able to store the",
      "data model like being able to store the data and query it very quickly but also data and query it very quickly but also data and query it very quickly but also the ability to use cipher to kind of the ability to use cipher to kind of the ability to use cipher to kind of help you find the things that you know help you find the things that you know help you find the things that you know normally highly connected or distantly normally highly connected or distantly normally highly connected or",
      "distantly connected or you know those like connected or you know those like connected or you know those like graphene related kind of problems um so graphene related kind of problems um so graphene related kind of problems um so I know you're probably already thinking I know you're probably already thinking I know you're probably already thinking about like oh where can I use this about like oh where can I use this about like oh where can I use this because this does sound kind of because this",
      "does sound kind of because this does sound kind of interesting I will tell you there's a interesting I will tell you there's a interesting I will tell you there's a lot of really cool use cases for it the lot of really cool use cases for it the lot of really cool use cases for it the very standard ones like recommendations very standard ones like recommendations very standard ones like recommendations big one fraud detection like network and big one fraud detection like network and big one fraud",
      "detection like network and IT management those are like the really IT management those are like the really IT management those are like the really big ones that kind of uh big ones that kind of uh big ones that kind of uh are frequently used a lot of like NLP are frequently used a lot of like NLP are frequently used a lot of like NLP related stuff like even if you think related stuff like even if you think related stuff like even if you think about linguistics like how we speak about linguistics",
      "like how we speak about linguistics like how we speak right that is all a graph there's this right that is all a graph there's this right that is all a graph there's this thing that happens that we call it the thing that happens that we call it the thing that happens that we call it the graph epiphany basically when you start graph epiphany basically when you start graph epiphany basically when you start seeing in graphs seeing in graphs seeing in graphs everything you see graphs everywhere you",
      "everything you see graphs everywhere you everything you see graphs everywhere you can't get rid of it because everything can't get rid of it because everything can't get rid of it because everything is dependent on something else you know is dependent on something else you know is dependent on something else you know it's like all these intertwined it's like all these intertwined it's like all these intertwined connections of things but there are a connections of things but there are a",
      "connections of things but there are a lot of really really cool use cases lot of really really cool use cases lot of really really cool use cases actually probably one of my favorite actually probably one of my favorite actually probably one of my favorite things about my job is hearing about all things about my job is hearing about all things about my job is hearing about all the interesting use cases of how people the interesting use cases of how people the interesting use cases of how people",
      "use graphs in this case this one is use graphs in this case this one is use graphs in this case this one is Hetty oh if you go to head thought IO Hetty oh if you go to head thought IO Hetty oh if you go to head thought IO they have like they're the homepage here they have like they're the homepage here they have like they're the homepage here so like Hecht dot IO this was this was so like Hecht dot IO this was this was so like Hecht dot IO this was this was created by Daniel Himelstein who's a",
      "created by Daniel Himelstein who's a created by Daniel Himelstein who's a postdoc researcher at University of postdoc researcher at University of postdoc researcher at University of Pennsylvania but you can play around Pennsylvania but you can play around Pennsylvania but you can play around with this stuff you know that he's got with this stuff you know that he's got with this stuff you know that he's got the ability for you to explore there's a the ability for you to explore there's a the",
      "ability for you to explore there's a can you afford a browser thing and can you afford a browser thing and can you afford a browser thing and there's guides that kind of walk you there's guides that kind of walk you there's guides that kind of walk you through and tell you what you can do through and tell you what you can do through and tell you what you can do with it we also have neo4j sandbox I with it we also have neo4j sandbox I with it we also have neo4j sandbox I probably say it probably",
      "one of the best probably say it probably one of the best probably say it probably one of the best places to start just to kind of get you places to start just to kind of get you places to start just to kind of get you thinking in graphs so if you go to neo4j thinking in graphs so if you go to neo4j thinking in graphs so if you go to neo4j comm slash developer there's an online comm slash developer there's an online comm slash developer there's an online sandbox thing here you don't have to",
      "sandbox thing here you don't have to sandbox thing here you don't have to download anything there's pre-existing download anything there's pre-existing download anything there's pre-existing data sets you can just jump in you can data sets you can just jump in you can data sets you can just jump in you can follow the guide start playing around follow the guide start playing around follow the guide start playing around and then once you're ready um you know and then once you're ready um you know",
      "and then once you're ready um you know you can kind of dig into here a little you can kind of dig into here a little you can kind of dig into here a little bit there's like intro to graph bit there's like intro to graph bit there's like intro to graph databases YouTube series but we have all databases YouTube series but we have all databases YouTube series but we have all kinds of like you have graphic Adam E or kinds of like you have graphic Adam E or kinds of like you have graphic Adam E or you",
      "can go you know self-paced tutorial you can go you know self-paced tutorial you can go you know self-paced tutorial and stuff so yeah hopefully you're gonna and stuff so yeah hopefully you're gonna and stuff so yeah hopefully you're gonna thank me for this and not hate me for thank me for this and not hate me for thank me for this and not hate me for getting you addicted to graphs I will getting you addicted to graphs I will getting you addicted to graphs I will also make sure I mentioned because",
      "this also make sure I mentioned because this also make sure I mentioned because this does happen to people who are in the does happen to people who are in the does happen to people who are in the graph epiphany they try to put graphs graph epiphany they try to put graphs graph epiphany they try to put graphs everywhere they don't belong everywhere everywhere they don't belong everywhere everywhere they don't belong everywhere they are everywhere they don't belong they are everywhere they don't",
      "belong they are everywhere they don't belong everywhere they are highly connected everywhere they are highly connected everywhere they are highly connected data problems but that said once you're data problems but that said once you're data problems but that said once you're addicted to graphs and you already have addicted to graphs and you already have addicted to graphs and you already have these amazing awesome use cases that you these amazing awesome use cases that you these amazing awesome",
      "use cases that you want to share with the rest of the world want to share with the rest of the world want to share with the rest of the world then you can come to me and then you then you can come to me and then you then you can come to me and then you know we could do something with the know we could do something with the know we could do something with the community so yeah hopefully you enjoyed community so yeah hopefully you enjoyed community so yeah hopefully you enjoyed the session and",
      "hopefully this was the session and hopefully this was the session and hopefully this was helpful helpful helpful see you soon bye"
    ],
    "chunk_count": 69,
    "content_id": "de75e182-1ab2-481e-b99e-cb86b3fdd500",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554945"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=FZR0rG3HKIk": {
    "title": "Virtualization Explained",
    "url": "https://www.youtube.com/watch?v=FZR0rG3HKIk",
    "description": "Check out this Virtualization Guide → https://ibm.biz/BdPSf9\nCheck out this Hypervisor Guide → https://ibm.biz/BdPSfC\n\nLearn more about IBM Cloud for VMware Solutions → https://ibm.biz/BdPSfQ\n\nAlthough virtualization is a well-established technology, it is still a critical part of your modern cloud computing plan. Kaleigh Bovey is going to walk you through an overview of how virtualization works and detail how it is still an integral aspect of your cloud strategy.\n\nFrom hypervisors to cloud strategy benefits, she's got you covered on all things virtualization.\n\nGet started for free on IBM Cloud → https://ibm.biz/sign-up-today\n\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#Virtualization #VMware #IBMCloud",
    "duration": 320,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en hi my name is Kaylee Bovie with the IBM hi my name is Kaylee Bovie with the IBM hi my name is Kaylee Bovie with the IBM cloud team and today we're going to be cloud team and today we're going to be cloud team and today we're going to be talking about virtualization as you know talking about virtualization as you know talking about virtualization as you know virtualization is a fairly old virtualization is a fairly old virtualization is a fairly old technology but it's still super relevant technology but it's still super relevant technology but it's still super relevant to building our cloud computing strategy to building our cloud computing strategy to building our cloud computing strategy today so first off what is today so first off what is today so first off what is virtualization simply put virtualization virtualization simply put virtualization virtualization simply put virtualization is the process of creating a software is the process of creating a software is the process of creating a software based or virtual version of something based or virtual version of something based or virtual version of something whether that be compute storage whether that be compute storage whether that be compute storage networking servers or applications and networking servers or applications and networking servers or applications and what makes virtualization feasible is what makes virtualization feasible is what makes virtualization feasible is something called the hypervisor so we're something called the hypervisor so we're something called the hypervisor so we're gonna write that here and what a gonna write that here and what a gonna write that here and what a hypervisor is is it simply a piece of hypervisor is is it simply a piece of hypervisor is is it simply a piece of software that runs above the physical software that runs above the physical software that runs above the physical server or host and there are a couple server or host and there are a couple server or host and there are a couple different types of hypervisors out there different types of hypervisors out there different types of hypervisors out there and what they do is essentially pull the and what they do is essentially pull the and what they do is essentially pull the resources from the physical server and resources from the physical server and resources from the physical server and allocate them to your virtual allocate them to your virtual allocate them to your virtual environments there are two main types of environments there are two main types of environments there are two main types of hypervisors out there hypervisors out there hypervisors out there one being type 1 very simple to remember one being type 1 very simple to remember one being type 1 very simple to remember and 2 you guessed it type 2 so let's and 2 you guessed it type 2 so let's and 2 you guessed it type 2 so let's start with type 1 a type 1 hypervisor is start with type 1 a type 1 hypervisor is start with type 1 a type 1 hypervisor is a hypervisor that is installed directly a hypervisor that is installed directly a hypervisor that is installed directly on top of the physical server they're on top of the physical server they're on top of the physical server they're also called bare-metal hypervisor so also called bare-metal hypervisor so also called bare-metal hypervisor so we'll write that up here just so you can we'll write that up here just so you can we'll write that up here just so you can remember these are the most frequently remember these are the most frequently remember these are the most frequently typed of views hypervisors and their typed of views hypervisors and their typed of views hypervisors and their most secure they lower the latency and most secure they lower the latency and most secure they lower the latency and these are the ones that you'll see in these are the ones that you'll see in these are the ones that you'll see in the market the most some examples would the market the most some examples would the market the most some examples would be VMware ESXi or Microsoft hyper-v or be VMware ESXi or Microsoft hyper-v or be VMware ESXi or Microsoft hyper-v or even open-source KVM the other type of even open-source KVM the other type of even open-source KVM the other type of hypervisor is a type 2 hypervisor over hypervisor is a type 2 hypervisor over hypervisor is a type 2 hypervisor over here and what makes these different is here and what makes these different is here and what makes these different is that there is a layer of host OS that that there is a layer of host OS that that there is a layer of host OS that sits between the physical server and the sits between the physical server and the sits between the physical server and the hypervisor hypervisor hypervisor so by that nature they are also killed so by that nature they are also killed so by that nature they are also killed hosted these are a lot less frequent hosted these are a lot less frequent hosted these are a lot less frequent they're mostly used for end-user they're mostly used for end-user they're mostly used for end-user virtualization and you might see some in virtualization and you might see some in virtualization and you might see some in the market that are called like Oracle the market that are called like Oracle the market that are called like Oracle VirtualBox or VMware Workstation VirtualBox or VMware Workstation VirtualBox or VMware Workstation again there are a lot less frequent again there are a lot less frequent again there are a lot less frequent there a bit more they have a higher there a bit more they have a higher there a bit more they have a higher latency than a type 1 hypervisor so once latency than a type 1 hypervisor so once latency than a type 1 hypervisor so once you have your hypervisor installed you you have your hypervisor installed you you have your hypervisor installed you can build virtual environments or can build virtual environments or can build virtual environments or virtual machines or simply put VMs so virtual machines or simply put VMs so virtual machines or simply put VMs so let's spin up some environments so what let's spin up some environments so what let's spin up some environments so what makes a VM a VM a VM is simply a makes a VM a VM a VM is simply a makes a VM a VM a VM is simply a software based computer there run like a software based computer there run like a software based computer there run like a physical computer they have an operating physical computer they have an operating physical computer they have an operating system and applications and they're system and applications and they're system and applications and they're completely independent of one another completely independent of one another completely independent of one another but you can run multiple of them on a but you can run multiple of them on a but you can run multiple of them on a hypervisor and the hypervisor manages hypervisor and the hypervisor manages hypervisor and the hypervisor manages the resources that are allocated to the resources that are allocated to the resources that are allocated to these virtual environments from the these virtual environments from the these virtual environments from the physical server so because they're physical server so because they're physical server so because they're independent you can run different independent you can run different independent you can run different operating systems on different virtual operating systems on different virtual operating systems on different virtual machines so you could run Windows here machines so you could run Windows here machines so you could run Windows here or Linux here or UNIX here for example or Linux here or UNIX here for example or Linux here or UNIX here for example and because they're independent they're and because they're independent they're and because they're independent they're also extremely portable you can move a also extremely portable you can move a also extremely portable you can move a virtual machine from one hypervisor to virtual machine from one hypervisor to virtual machine from one hypervisor to another hypervisor on a completely another hypervisor on a completely another hypervisor on a completely different machine almost as different machine almost as different machine almost as instantaneously which gives you a lot of instantaneously which gives you a lot of instantaneously which gives you a lot of flexibility and a lot of portability flexibility and a lot of portability flexibility and a lot of portability within your environment so looking at within your environment so looking at within your environment so looking at all of this this is the core all of this this is the core all of this this is the core virtualization as a process so let's virtualization as a process so let's virtualization as a process so let's talk about a couple key benefits that talk about a couple key benefits that talk about a couple key benefits that you want to take away from this one cost you want to take away from this one cost you want to take away from this one cost savings when you think about this and savings when you think about this and savings when you think about this and the fact that you can run multiple the fact that you can run multiple the fact that you can run multiple virtual environments from one piece of virtual environments from one piece of virtual environments from one piece of infrastructure means that you can infrastructure means that you can infrastructure means that you can drastically reduce your physical drastically reduce your physical drastically reduce your physical infrastructure footprint this is infrastructure footprint this is infrastructure footprint this is consolidation at its core and the fact consolidation at its core and the fact consolidation at its core and the fact that you don't have to maintain nearly that you don't have to maintain nearly that you don't have to maintain nearly as many servers run as much electricity as many servers run as much electricity as many servers run as much electricity save a maintenance cost means that you save a maintenance cost means that you save a maintenance cost means that you save on your bottom line save on your bottom line save on your bottom line at the end of the day number two would at the end of the day number two would at the end of the day number two would be agility and speed so like I said be agility and speed so like I said be agility and speed so like I said spinning up a virtual machine is spinning up a virtual machine is spinning up a virtual machine is relatively easy and quick a lot more relatively easy and quick a lot more relatively easy and quick a lot more simple than provisioning an entire new simple than provisioning an entire new simple than provisioning an entire new environment for your developers if you environment for your developers if you environment for your developers if you if they say they want to spin up a new if they say they want to spin up a new if they say they want to spin up a new environment so that they can run a depth environment so that they can run a depth environment so that they can run a depth test scenario test scenario test scenario whatever it might be virtualization whatever it might be virtualization whatever it might be virtualization makes that process a lot simpler and makes that process a lot simpler and makes that process a lot simpler and quicker and three lowers your downtime so let's say that this host goes out so let's say that this host goes out so let's say that this host goes out unexpectedly the fact that you can move unexpectedly the fact that you can move unexpectedly the fact that you can move virtual machines from one hypervisor to virtual machines from one hypervisor to virtual machines from one hypervisor to another on a different physical server another on a different physical server another on a different physical server means that you have a great backup plan means that you have a great backup plan means that you have a great backup plan in place right so if this host goes down in place right so if this host goes down in place right so if this host goes down you can simply move your VMs very you can simply move your VMs very you can simply move your VMs very quickly to another hypervisor on a quickly to another hypervisor on a quickly to another hypervisor on a machine that is working so with this machine that is working so with this machine that is working so with this this is really a virtualization today this is really a virtualization today this is really a virtualization today and like I said at the beginning and like I said at the beginning and like I said at the beginning virtualization is a technology that's a virtualization is a technology that's a virtualization is a technology that's a few decades old at this point but it's few decades old at this point but it's few decades old at this point but it's still super critical to understand for still super critical to understand for still super critical to understand for your cloud computing strategy today your cloud computing strategy today your cloud computing strategy today thanks for watching as we discuss the thanks for watching as we discuss the thanks for watching as we discuss the basics of virtualization make sure to basics of virtualization make sure to basics of virtualization make sure to subscribe below and give us a big thumbs subscribe below and give us a big thumbs subscribe below and give us a big thumbs up if you liked this content",
    "chunks": [
      "Kind: captions Language: en hi my name is Kaylee Bovie with the IBM hi my name is Kaylee Bovie with the IBM hi my name is Kaylee Bovie with the IBM cloud team and today we're going to be cloud team and today we're going to be cloud team and today we're going to be talking about virtualization as you know talking about virtualization as you know talking about virtualization as you know virtualization is a fairly old virtualization is a fairly old virtualization is a fairly old technology but it's",
      "still super relevant technology but it's still super relevant technology but it's still super relevant to building our cloud computing strategy to building our cloud computing strategy to building our cloud computing strategy today so first off what is today so first off what is today so first off what is virtualization simply put virtualization virtualization simply put virtualization virtualization simply put virtualization is the process of creating a software is the process of creating a",
      "software is the process of creating a software based or virtual version of something based or virtual version of something based or virtual version of something whether that be compute storage whether that be compute storage whether that be compute storage networking servers or applications and networking servers or applications and networking servers or applications and what makes virtualization feasible is what makes virtualization feasible is what makes virtualization feasible is something",
      "called the hypervisor so we're something called the hypervisor so we're something called the hypervisor so we're gonna write that here and what a gonna write that here and what a gonna write that here and what a hypervisor is is it simply a piece of hypervisor is is it simply a piece of hypervisor is is it simply a piece of software that runs above the physical software that runs above the physical software that runs above the physical server or host and there are a couple server or host and",
      "there are a couple server or host and there are a couple different types of hypervisors out there different types of hypervisors out there different types of hypervisors out there and what they do is essentially pull the and what they do is essentially pull the and what they do is essentially pull the resources from the physical server and resources from the physical server and resources from the physical server and allocate them to your virtual allocate them to your virtual allocate them to your",
      "virtual environments there are two main types of environments there are two main types of environments there are two main types of hypervisors out there hypervisors out there hypervisors out there one being type 1 very simple to remember one being type 1 very simple to remember one being type 1 very simple to remember and 2 you guessed it type 2 so let's and 2 you guessed it type 2 so let's and 2 you guessed it type 2 so let's start with type 1 a type 1 hypervisor is start with type 1 a type 1",
      "hypervisor is start with type 1 a type 1 hypervisor is a hypervisor that is installed directly a hypervisor that is installed directly a hypervisor that is installed directly on top of the physical server they're on top of the physical server they're on top of the physical server they're also called bare-metal hypervisor so also called bare-metal hypervisor so also called bare-metal hypervisor so we'll write that up here just so you can we'll write that up here just so you can we'll write that up",
      "here just so you can remember these are the most frequently remember these are the most frequently remember these are the most frequently typed of views hypervisors and their typed of views hypervisors and their typed of views hypervisors and their most secure they lower the latency and most secure they lower the latency and most secure they lower the latency and these are the ones that you'll see in these are the ones that you'll see in these are the ones that you'll see in the market the most",
      "some examples would the market the most some examples would the market the most some examples would be VMware ESXi or Microsoft hyper-v or be VMware ESXi or Microsoft hyper-v or be VMware ESXi or Microsoft hyper-v or even open-source KVM the other type of even open-source KVM the other type of even open-source KVM the other type of hypervisor is a type 2 hypervisor over hypervisor is a type 2 hypervisor over hypervisor is a type 2 hypervisor over here and what makes these different is here and",
      "what makes these different is here and what makes these different is that there is a layer of host OS that that there is a layer of host OS that that there is a layer of host OS that sits between the physical server and the sits between the physical server and the sits between the physical server and the hypervisor hypervisor hypervisor so by that nature they are also killed so by that nature they are also killed so by that nature they are also killed hosted these are a lot less frequent hosted",
      "these are a lot less frequent hosted these are a lot less frequent they're mostly used for end-user they're mostly used for end-user they're mostly used for end-user virtualization and you might see some in virtualization and you might see some in virtualization and you might see some in the market that are called like Oracle the market that are called like Oracle the market that are called like Oracle VirtualBox or VMware Workstation VirtualBox or VMware Workstation VirtualBox or VMware",
      "Workstation again there are a lot less frequent again there are a lot less frequent again there are a lot less frequent there a bit more they have a higher there a bit more they have a higher there a bit more they have a higher latency than a type 1 hypervisor so once latency than a type 1 hypervisor so once latency than a type 1 hypervisor so once you have your hypervisor installed you you have your hypervisor installed you you have your hypervisor installed you can build virtual environments or",
      "can build virtual environments or can build virtual environments or virtual machines or simply put VMs so virtual machines or simply put VMs so virtual machines or simply put VMs so let's spin up some environments so what let's spin up some environments so what let's spin up some environments so what makes a VM a VM a VM is simply a makes a VM a VM a VM is simply a makes a VM a VM a VM is simply a software based computer there run like a software based computer there run like a software based",
      "computer there run like a physical computer they have an operating physical computer they have an operating physical computer they have an operating system and applications and they're system and applications and they're system and applications and they're completely independent of one another completely independent of one another completely independent of one another but you can run multiple of them on a but you can run multiple of them on a but you can run multiple of them on a hypervisor and",
      "the hypervisor manages hypervisor and the hypervisor manages hypervisor and the hypervisor manages the resources that are allocated to the resources that are allocated to the resources that are allocated to these virtual environments from the these virtual environments from the these virtual environments from the physical server so because they're physical server so because they're physical server so because they're independent you can run different independent you can run different independent",
      "you can run different operating systems on different virtual operating systems on different virtual operating systems on different virtual machines so you could run Windows here machines so you could run Windows here machines so you could run Windows here or Linux here or UNIX here for example or Linux here or UNIX here for example or Linux here or UNIX here for example and because they're independent they're and because they're independent they're and because they're independent they're also",
      "extremely portable you can move a also extremely portable you can move a also extremely portable you can move a virtual machine from one hypervisor to virtual machine from one hypervisor to virtual machine from one hypervisor to another hypervisor on a completely another hypervisor on a completely another hypervisor on a completely different machine almost as different machine almost as different machine almost as instantaneously which gives you a lot of instantaneously which gives you a lot of",
      "instantaneously which gives you a lot of flexibility and a lot of portability flexibility and a lot of portability flexibility and a lot of portability within your environment so looking at within your environment so looking at within your environment so looking at all of this this is the core all of this this is the core all of this this is the core virtualization as a process so let's virtualization as a process so let's virtualization as a process so let's talk about a couple key benefits that",
      "talk about a couple key benefits that talk about a couple key benefits that you want to take away from this one cost you want to take away from this one cost you want to take away from this one cost savings when you think about this and savings when you think about this and savings when you think about this and the fact that you can run multiple the fact that you can run multiple the fact that you can run multiple virtual environments from one piece of virtual environments from one piece of",
      "virtual environments from one piece of infrastructure means that you can infrastructure means that you can infrastructure means that you can drastically reduce your physical drastically reduce your physical drastically reduce your physical infrastructure footprint this is infrastructure footprint this is infrastructure footprint this is consolidation at its core and the fact consolidation at its core and the fact consolidation at its core and the fact that you don't have to maintain nearly that",
      "you don't have to maintain nearly that you don't have to maintain nearly as many servers run as much electricity as many servers run as much electricity as many servers run as much electricity save a maintenance cost means that you save a maintenance cost means that you save a maintenance cost means that you save on your bottom line save on your bottom line save on your bottom line at the end of the day number two would at the end of the day number two would at the end of the day number two would",
      "be agility and speed so like I said be agility and speed so like I said be agility and speed so like I said spinning up a virtual machine is spinning up a virtual machine is spinning up a virtual machine is relatively easy and quick a lot more relatively easy and quick a lot more relatively easy and quick a lot more simple than provisioning an entire new simple than provisioning an entire new simple than provisioning an entire new environment for your developers if you environment for your",
      "developers if you environment for your developers if you if they say they want to spin up a new if they say they want to spin up a new if they say they want to spin up a new environment so that they can run a depth environment so that they can run a depth environment so that they can run a depth test scenario test scenario test scenario whatever it might be virtualization whatever it might be virtualization whatever it might be virtualization makes that process a lot simpler and makes that",
      "process a lot simpler and makes that process a lot simpler and quicker and three lowers your downtime so let's say that this host goes out so let's say that this host goes out so let's say that this host goes out unexpectedly the fact that you can move unexpectedly the fact that you can move unexpectedly the fact that you can move virtual machines from one hypervisor to virtual machines from one hypervisor to virtual machines from one hypervisor to another on a different physical server another",
      "on a different physical server another on a different physical server means that you have a great backup plan means that you have a great backup plan means that you have a great backup plan in place right so if this host goes down in place right so if this host goes down in place right so if this host goes down you can simply move your VMs very you can simply move your VMs very you can simply move your VMs very quickly to another hypervisor on a quickly to another hypervisor on a quickly to",
      "another hypervisor on a machine that is working so with this machine that is working so with this machine that is working so with this this is really a virtualization today this is really a virtualization today this is really a virtualization today and like I said at the beginning and like I said at the beginning and like I said at the beginning virtualization is a technology that's a virtualization is a technology that's a virtualization is a technology that's a few decades old at this point but",
      "it's few decades old at this point but it's few decades old at this point but it's still super critical to understand for still super critical to understand for still super critical to understand for your cloud computing strategy today your cloud computing strategy today your cloud computing strategy today thanks for watching as we discuss the thanks for watching as we discuss the thanks for watching as we discuss the basics of virtualization make sure to basics of virtualization make sure to",
      "basics of virtualization make sure to subscribe below and give us a big thumbs subscribe below and give us a big thumbs subscribe below and give us a big thumbs up if you liked this content"
    ],
    "chunk_count": 28,
    "content_id": "4928cd62-7888-4933-9913-903ac8d46f14",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554948"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=cjXI-yxqGTI": {
    "title": "Containers vs VMs: What's the difference?",
    "url": "https://www.youtube.com/watch?v=cjXI-yxqGTI",
    "description": "Learn more about Containers: http://ibm.biz/guide-to-containers\nLearn more about VMs: http://ibm.biz/virtual-machines-guide\n\nWhat are virtual machines and containers, and how do they fit into our modern cloud-native way of building and architecting applications? \n\nIn this lightboard video, Nigel Brown with IBM Cloud, answers this question and much more in four parts. He also breaks down why users should not just look at virtual machines and containers as competing technologies because there can be a lot of benefits of having them work together for particular use cases.\n\nGet started on IBM Cloud at no cost: http://ibm.biz/start-ibm-cloud-lite-account\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#Containers #VirtualMachines #CloudComputing",
    "duration": 488,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en What's up y'all? I've got a question for you. What are virtual machines and containers, and how do they fit into our modern, cloud-native way of building and architecting applications? My name's Nigel and I'm going to try to answer that question for you in four parts. First, let's talk about some of the differences between virtual machines and containers. On one side we've got virtual machines, which I'm going to abbreviate as VMs, which you may have used because they've been popular for longer. And then containers over here on this other side, which may be a bit newer to you but you've probably seen them if you're working in the spaces of app modernization, or you're dealing with microservices, or anything that demands this new way of building and architecting applications. And I should say that it's not necessarily new, but it's been that we're dealing with them a lot more lately. So, the first thing that i want to bring up is the the level at which virtualization happens. So, these two technologies are different ways of achieving virtualization, and virtual machines are what's called \"hardware virtualization\", because it happens at the hardware level. So, we're going to start with our hardware down at the bottom, because these are computers after all. And what we have on top of our hardware is what's called a \"hypervisor\". And our hypervisor is what's responsible for creating these virtualized instances of each of the components that make up our machines. We're talking about processors, RAM, storage, network, cards, all of these things are being virtualized by our hypervisor. Whereas with containers, we start, again, with the hardware down at the bottom, but we build them up a little bit differently because we have, on top of our hardware, our kernel, which is what helps our software and hardware talk to each other. And, on top of our kernel, we have our operating system and we call it our \"Host OS\" because it's going to be what's hosting all of our containers. And then, on top of the operating system, we have each container that's running, C1, C2, containers ... we can run many dozens of containers in one instance of an operating system. And that's why this is called \"operating system level virtualization\", because it happens at the operating system level, whereas with our virtual machines we're working at the hardware level. And this model that I've drawn out is our Type 1 hypervisor, or we call it like ... \"full virtualization\", or \"full system virtualization\". The second thing that I want to bring up is about the type of isolation that we're achieving. With our virtual machines we're achieving isolation of machines. So, if we can imagine at our base layer we have one server that's in a rack somewhere, but we want to take our resources and split them up so that we're getting much more use out of what we have. So, we take our hypervisor and we make a \"Machine 1\", and we make a \"Machine 2\", and we make a \"Machine 3\". We're creating what looks like separate workstations, separate servers out of one, we're making our one server look like it's many different machines. And each machine is relatively independent of each other, like they don't really know about each other and we, interacting with them, wouldn't necessarily know that we're even in a virtual machine. It just appears as if it is another completely different server, machine somewhere else. Whereas with our containers, we're dealing with process isolation. So, when we're running applications on our laptops we have them running in usually the same environment. They can all see what's running at the same time and talk to each other and share resources and things like that, but when we're dealing with containers, perhaps for security, we want to make sure that our applications can only see what's absolutely necessary for them to run and nothing else. And that's what containers allow us to do, where they're sharing the same operating system, they're sharing the same kernel, but it's appearing to each container as if they have their own operating system and only what's installed in them is the the libraries that are needed, as well as the scripts, the code, everything that we need to run our applications and that's it. And we're able to run all these applications side-by-side and they don't necessarily know about each other. So, we're dealing with isolation of the process, as opposed to isolation of the machine. And the third thing that I want to bring up is about how these resources are accessed. So, again, our hypervisor is creating ... ... with our Type 1 hypervisor, we're creating different machines out of our server, and that's mainly where the interaction is happening. We're interacting with what we think is hardware, but that'sbeing managed for us by the hypervisor to what the actual hardware is. Whereas with containers, we're using a couple of features, there are more, but I just want to draw your attention to two features of the Linux kernel that are allowing this illusion of isolation for our processes. And the first one that I want to bring up are namespaces, and they're what is allowing the customization and the appearance that each instance of the container has its own operating system. And then we have our \"cgroups\" (control groups), which are responsible for monitoring and metering our resources to make sure that we never tax our system with containers, we're limiting the amount of resources they're accessing, we're monitoring what it is, and being able to control exactly what it is we're giving each container control of. So, we have a bit of granular control over the resources that are flowing to our containers. Whereas in our hypervisor, we're creating ... we have the the isolation of our different machines. And the last thing that I want to talk about is this difference in portability and flexibility. So, with our virtual machines we have what I like to think of as infinite flexibility of our hardware, because we're making a different machine out of our server, we're saying, \"this is how many processors that I want this machine to have\", \"this is how much RAM I would like it to have\", and we're able to be flexible about the kind of system that we're building. Whereas with containers, we have infinite portability, is how I like to think of it, because we have our container that's being defined in a single file and this is known to a lot of people as a Docker file. But we have essentially a few lines of text that are saying exactly how to build our container, how to run our container, what libraries are necessary, what steps to take to build our container up. And we take this one file, we run it on our machine, we can spin up our application. We store it in a repository somewhere else and then we're able to run that on different machines. We can take our one file and be able to make it run pretty much wherever, there's no hardware limitations, anything like that. Well, there are some hardware limitations if you built your containers for ARM or x86, but, aside from that, you have a lot of flexibility in how you're able to run your containers. I've been speaking a lot about Type 1 virtualization, about Type 1 hypervisors, but the truth is there's another type of virtualization called Type 2. So, in our Type 1, where we're dealing with our hypervisor right on top of our hardware, our Type 2 deals a little bit higher. And this is probably the kind of virtualization that you may be used to interacting with daily, where we're running something like a virtual box or parallels. So, we can take the flexibility that's offered by our lightweight Type 2 hypervisors, and the portability that's offered by containers, and run these together. We see the technologies around virtual machines maturing, like with KubeVirt, and we're seeing in newer versions of Kubernetes and OpenShift, that we can run virtual machines and containers not as competing technologies, but as technologies that can work together depending on our use-cases. Thank you so much for sticking with us through the video. If you have any questions please drop us a line below, and if you want to see more videos like this in the future please like and subscribe to our channel. And don't forget, you can grow your skills and earn a badge with IBM CloudLabs, which are free browser-based, interactive Kubernetes labs.",
    "chunks": [
      "Kind: captions Language: en What's up y'all? I've got a question for you. What are virtual machines and containers, and how do they fit into our modern, cloud-native way of building and architecting applications? My name's Nigel and I'm going to try to answer that question for you in four parts. First, let's talk about some of the differences between virtual machines and containers. On one side we've got virtual machines, which I'm going to abbreviate as VMs, which you may have used because",
      "they've been popular for longer. And then containers over here on this other side, which may be a bit newer to you but you've probably seen them if you're working in the spaces of app modernization, or you're dealing with microservices, or anything that demands this new way of building and architecting applications. And I should say that it's not necessarily new, but it's been that we're dealing with them a lot more lately. So, the first thing that i want to bring up is the the level at which",
      "virtualization happens. So, these two technologies are different ways of achieving virtualization, and virtual machines are what's called \"hardware virtualization\", because it happens at the hardware level. So, we're going to start with our hardware down at the bottom, because these are computers after all. And what we have on top of our hardware is what's called a \"hypervisor\". And our hypervisor is what's responsible for creating these virtualized instances of each of the components that make",
      "up our machines. We're talking about processors, RAM, storage, network, cards, all of these things are being virtualized by our hypervisor. Whereas with containers, we start, again, with the hardware down at the bottom, but we build them up a little bit differently because we have, on top of our hardware, our kernel, which is what helps our software and hardware talk to each other. And, on top of our kernel, we have our operating system and we call it our \"Host OS\" because it's going to be what's",
      "hosting all of our containers. And then, on top of the operating system, we have each container that's running, C1, C2, containers ... we can run many dozens of containers in one instance of an operating system. And that's why this is called \"operating system level virtualization\", because it happens at the operating system level, whereas with our virtual machines we're working at the hardware level. And this model that I've drawn out is our Type 1 hypervisor, or we call it like ... \"full",
      "virtualization\", or \"full system virtualization\". The second thing that I want to bring up is about the type of isolation that we're achieving. With our virtual machines we're achieving isolation of machines. So, if we can imagine at our base layer we have one server that's in a rack somewhere, but we want to take our resources and split them up so that we're getting much more use out of what we have. So, we take our hypervisor and we make a \"Machine 1\", and we make a \"Machine 2\", and we make a",
      "\"Machine 3\". We're creating what looks like separate workstations, separate servers out of one, we're making our one server look like it's many different machines. And each machine is relatively independent of each other, like they don't really know about each other and we, interacting with them, wouldn't necessarily know that we're even in a virtual machine. It just appears as if it is another completely different server, machine somewhere else. Whereas with our containers, we're dealing with",
      "process isolation. So, when we're running applications on our laptops we have them running in usually the same environment. They can all see what's running at the same time and talk to each other and share resources and things like that, but when we're dealing with containers, perhaps for security, we want to make sure that our applications can only see what's absolutely necessary for them to run and nothing else. And that's what containers allow us to do, where they're sharing the same operating",
      "system, they're sharing the same kernel, but it's appearing to each container as if they have their own operating system and only what's installed in them is the the libraries that are needed, as well as the scripts, the code, everything that we need to run our applications and that's it. And we're able to run all these applications side-by-side and they don't necessarily know about each other. So, we're dealing with isolation of the process, as opposed to isolation of the machine. And the third",
      "thing that I want to bring up is about how these resources are accessed. So, again, our hypervisor is creating ... ... with our Type 1 hypervisor, we're creating different machines out of our server, and that's mainly where the interaction is happening. We're interacting with what we think is hardware, but that'sbeing managed for us by the hypervisor to what the actual hardware is. Whereas with containers, we're using a couple of features, there are more, but I just want to draw your attention to",
      "two features of the Linux kernel that are allowing this illusion of isolation for our processes. And the first one that I want to bring up are namespaces, and they're what is allowing the customization and the appearance that each instance of the container has its own operating system. And then we have our \"cgroups\" (control groups), which are responsible for monitoring and metering our resources to make sure that we never tax our system with containers, we're limiting the amount of resources",
      "they're accessing, we're monitoring what it is, and being able to control exactly what it is we're giving each container control of. So, we have a bit of granular control over the resources that are flowing to our containers. Whereas in our hypervisor, we're creating ... we have the the isolation of our different machines. And the last thing that I want to talk about is this difference in portability and flexibility. So, with our virtual machines we have what I like to think of as infinite",
      "flexibility of our hardware, because we're making a different machine out of our server, we're saying, \"this is how many processors that I want this machine to have\", \"this is how much RAM I would like it to have\", and we're able to be flexible about the kind of system that we're building. Whereas with containers, we have infinite portability, is how I like to think of it, because we have our container that's being defined in a single file and this is known to a lot of people as a Docker file.",
      "But we have essentially a few lines of text that are saying exactly how to build our container, how to run our container, what libraries are necessary, what steps to take to build our container up. And we take this one file, we run it on our machine, we can spin up our application. We store it in a repository somewhere else and then we're able to run that on different machines. We can take our one file and be able to make it run pretty much wherever, there's no hardware limitations, anything like",
      "that. Well, there are some hardware limitations if you built your containers for ARM or x86, but, aside from that, you have a lot of flexibility in how you're able to run your containers. I've been speaking a lot about Type 1 virtualization, about Type 1 hypervisors, but the truth is there's another type of virtualization called Type 2. So, in our Type 1, where we're dealing with our hypervisor right on top of our hardware, our Type 2 deals a little bit higher. And this is probably the kind of",
      "virtualization that you may be used to interacting with daily, where we're running something like a virtual box or parallels. So, we can take the flexibility that's offered by our lightweight Type 2 hypervisors, and the portability that's offered by containers, and run these together. We see the technologies around virtual machines maturing, like with KubeVirt, and we're seeing in newer versions of Kubernetes and OpenShift, that we can run virtual machines and containers not as competing",
      "technologies, but as technologies that can work together depending on our use-cases. Thank you so much for sticking with us through the video. If you have any questions please drop us a line below, and if you want to see more videos like this in the future please like and subscribe to our channel. And don't forget, you can grow your skills and earn a badge with IBM CloudLabs, which are free browser-based, interactive Kubernetes labs."
    ],
    "chunk_count": 17,
    "content_id": "f2104897-2ab7-476a-b577-06ea5f6c4416",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554951"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=tzrwxLNHtRY": {
    "title": "Model Context Protocol Clearly Explained | MCP Beyond the Hype",
    "url": "https://www.youtube.com/watch?v=tzrwxLNHtRY",
    "description": "This video contains a very simple explanation of MCP, also known as Model Context Protocol. We will first understand what exactly is MCP and how it easing up the development of AI applications and then we will go into the technical details of how the actual protocol looks like. We will look into the code of MCP client and Server both.\n\nSample MCP client: https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-chatbot/mcp_simple_chatbot/main.py\nGoogle maps MCP server: https://github.com/modelcontextprotocol/servers/blob/main/src/google-maps/index.ts#L297\nMCP Schema: https://github.com/modelcontextprotocol/specification/blob/main/schema/2024-11-05/schema.ts\nSimple tool, resource, prompt servers: https://github.com/modelcontextprotocol/python-sdk/tree/main/examples/servers\nExample servers: https://modelcontextprotocol.io/examples\nMCP inspector (postman like tool): https://modelcontextprotocol.io/docs/tools/inspector\nBuild Your First MCP Server: https://youtu.be/jLM6n4mdRuA\n\nDo you want to learn technology from me? Check https://codebasics.io/?utm_source=description&utm_medium=yt&utm_campaign=description&utm_id=description for my affordable video courses.\n\nNeed help building software or data analytics/AI solutions? My company https://www.atliq.com/ can help. Click on the Contact button on that website.\n\n🎥 Codebasics Hindi channel: https://www.youtube.com/channel/UCTmFBhuhMibVoSfYom1uXEg\n\n#️⃣ Social Media #️⃣\n\n🧑‍🤝‍🧑 Discord for Community Support:  https://discord.gg/r42Kbuk\n📸 Codebasics' Instagram: https://www.instagram.com/codebasicshub/\n📝 Codebasics' Linkedin :  https://www.linkedin.com/company/codebasics/\n\n------\n\n📝 Dhaval's Linkedin : https://www.linkedin.com/in/dhavalsays/\n📝 Hem's Linkedin: https://www.linkedin.com/in/hemvad/\n\n📽️ Hem's Instagram for daily tips: https://www.instagram.com/hemvadivel/\n📸 Dhaval's Personal Instagram: https://www.instagram.com/dhavalsays/\n\n🔗 Patreon: https://www.patreon.com/codebasics?fan_landing=true",
    "duration": 904,
    "uploader": "codebasics",
    "transcript": "Kind: captions Language: en there has been a lot of hype around there has been a lot of hype around there has been a lot of hype around model context protocol and I'm going to model context protocol and I'm going to model context protocol and I'm going to make an attempt to provide an extremely make an attempt to provide an extremely make an attempt to provide an extremely simple explanation of mCP today I will simple explanation of mCP today I will simple explanation of mCP today I will also go into technical details so that also go into technical details so that also go into technical details so that if you are building an AI application it if you are building an AI application it if you are building an AI application it helps you if you think about a of helps you if you think about a of helps you if you think about a of building a applications we are seeing a building a applications we are seeing a building a applications we are seeing a clear Evolution first we started with clear Evolution first we started with clear Evolution first we started with llm without any tools Etc then we llm without any tools Etc then we llm without any tools Etc then we started building this agentic Frameworks started building this agentic Frameworks started building this agentic Frameworks where llm gets help from tools and where llm gets help from tools and where llm gets help from tools and knowledge now we are entering the realm knowledge now we are entering the realm knowledge now we are entering the realm of standardized ways of interacting with of standardized ways of interacting with of standardized ways of interacting with these tools and knowledge so that these tools and knowledge so that these tools and knowledge so that building AI applications becomes easier building AI applications becomes easier building AI applications becomes easier let me give you an example say you are a let me give you an example say you are a let me give you an example say you are a equity research analyst working at some equity research analyst working at some equity research analyst working at some company let's say jeffre you want to company let's say jeffre you want to company let's say jeffre you want to develop a report comparing Nvidia and develop a report comparing Nvidia and develop a report comparing Nvidia and Tesla stock that looks something like Tesla stock that looks something like Tesla stock that looks something like this where you have company description this where you have company description this where you have company description at the the beginning then you have some at the the beginning then you have some at the the beginning then you have some uh Financial metrics then you have a uh Financial metrics then you have a uh Financial metrics then you have a summary of those metrics and in the end summary of those metrics and in the end summary of those metrics and in the end you have recent news about those you have recent news about those you have recent news about those companies now you are a tax heavy person companies now you are a tax heavy person companies now you are a tax heavy person so you will talk to your AI engineer so you will talk to your AI engineer so you will talk to your AI engineer friend working at Jeff and you will ask friend working at Jeff and you will ask friend working at Jeff and you will ask him to uh build an a application that him to uh build an a application that him to uh build an a application that can automatically generate this report can automatically generate this report can automatically generate this report now your AI engineer friends understands now your AI engineer friends understands now your AI engineer friends understands the capability of llm see they know that the capability of llm see they know that the capability of llm see they know that llm can pull the description of Nvidia llm can pull the description of Nvidia llm can pull the description of Nvidia and Tesla because it is part of that and Tesla because it is part of that and Tesla because it is part of that training data set but it cannot pull the training data set but it cannot pull the training data set but it cannot pull the latest stock price for latest stock price for latest stock price for example once you have somehow retrieved example once you have somehow retrieved example once you have somehow retrieved let's say the latest stock price llm can let's say the latest stock price llm can let's say the latest stock price llm can summarize uh that particular information summarize uh that particular information summarize uh that particular information okay so if you think about pure llm it okay so if you think about pure llm it okay so if you think about pure llm it has all these capabilities now you'll has all these capabilities now you'll has all these capabilities now you'll ask me okay if I go go to chat GPT and ask me okay if I go go to chat GPT and ask me okay if I go go to chat GPT and if I type this question it is pulling if I type this question it is pulling if I type this question it is pulling the information but look at this it is the information but look at this it is the information but look at this it is searching the web actually so chat GPT searching the web actually so chat GPT searching the web actually so chat GPT is an agent I am referring to GPT 40 is an agent I am referring to GPT 40 is an agent I am referring to GPT 40 model okay so now you already know the model okay so now you already know the model okay so now you already know the answer that you can use web search or answer that you can use web search or answer that you can use web search or maybe you can uh call Yahoo finance API maybe you can uh call Yahoo finance API maybe you can uh call Yahoo finance API which is called a tool to retrieve the which is called a tool to retrieve the which is called a tool to retrieve the latest information to summarize AI latest information to summarize AI latest information to summarize AI engineer will build this application engineer will build this application engineer will build this application where the heart is llm and llm is where the heart is llm and llm is where the heart is llm and llm is interacting with some tools which can be interacting with some tools which can be interacting with some tools which can be Yahoo finance API or web search it is Yahoo finance API or web search it is Yahoo finance API or web search it is also interacting with some knowledge you also interacting with some knowledge you also interacting with some knowledge you might have some private database in Jeff might have some private database in Jeff might have some private database in Jeff some PDF files lmm might want to pull some PDF files lmm might want to pull some PDF files lmm might want to pull data from it and he will write all this data from it and he will write all this data from it and he will write all this glue code okay glue code is basically glue code okay glue code is basically glue code okay glue code is basically the code that you're writing the code that you're writing the code that you're writing for these interactions now this can be for these interactions now this can be for these interactions now this can be an agentic application in which glue an agentic application in which glue an agentic application in which glue code is written by the agentic framework code is written by the agentic framework code is written by the agentic framework such as crew AI egno Etc or it can be a such as crew AI egno Etc or it can be a such as crew AI egno Etc or it can be a workflow application where as part of workflow application where as part of workflow application where as part of your python code you writing all this your python code you writing all this your python code you writing all this glue code this is just one application glue code this is just one application glue code this is just one application imagine jeffre is building 20 such imagine jeffre is building 20 such imagine jeffre is building 20 such application and think about all the application and think about all the application and think about all the companies in the world building millions companies in the world building millions companies in the world building millions of applications that is a lot of glue of applications that is a lot of glue of applications that is a lot of glue code it sounds like you have this old code it sounds like you have this old code it sounds like you have this old computer and you're connecting with computer and you're connecting with computer and you're connecting with keyboard mouse Etc through all these keyboard mouse Etc through all these keyboard mouse Etc through all these different wires but you will be like no different wires but you will be like no different wires but you will be like no today things have changed today you can today things have changed today you can today things have changed today you can connect computer using this unified connect computer using this unified connect computer using this unified interface USB C Port you can have your interface USB C Port you can have your interface USB C Port you can have your USB hub and you can connect all this USB hub and you can connect all this USB hub and you can connect all this devices well that USBC moment has devices well that USBC moment has devices well that USBC moment has arrived for AI arrived for AI arrived for AI and that is your model context protocol and that is your model context protocol and that is your model context protocol in this your llm will interact through in this your llm will interact through in this your llm will interact through model context protocol with different model context protocol with different model context protocol with different mCP servers so for our Jeff example mCP servers so for our Jeff example mCP servers so for our Jeff example think that Yahoo finance is building an think that Yahoo finance is building an think that Yahoo finance is building an mCP server or let's say Google search is mCP server or let's say Google search is mCP server or let's say Google search is building another mCP server where building another mCP server where building another mCP server where they're exposing all these tools and they're exposing all these tools and they're exposing all these tools and resources and prompts Etc and that way resources and prompts Etc and that way resources and prompts Etc and that way these interactions becomes easier now these interactions becomes easier now these interactions becomes easier now you're a smart programmer and you'll be you're a smart programmer and you'll be you're a smart programmer and you'll be like hey we are still writing gluc code like hey we are still writing gluc code like hey we are still writing gluc code right because you need to do all these right because you need to do all these right because you need to do all these interactions the answer is yes you are interactions the answer is yes you are interactions the answer is yes you are writing some glue code here but the AE writing some glue code here but the AE writing some glue code here but the AE of writing that code is increasing going of writing that code is increasing going of writing that code is increasing going back to our old diagram the problem here back to our old diagram the problem here back to our old diagram the problem here was not just writing the code but was not just writing the code but was not just writing the code but maintaining it because tomorrow if Yahoo maintaining it because tomorrow if Yahoo maintaining it because tomorrow if Yahoo finance changes their API you have to finance changes their API you have to finance changes their API you have to maintain this code right so all this maintain this code right so all this maintain this code right so all this glue code becomes a nightmare to glue code becomes a nightmare to glue code becomes a nightmare to maintain whereas with this mCP due to maintain whereas with this mCP due to maintain whereas with this mCP due to the standard protocol writing and the standard protocol writing and the standard protocol writing and maintaining the code has become easier maintaining the code has become easier maintaining the code has become easier and also since writing this code is and also since writing this code is and also since writing this code is centralized let's say Yahoo finance centralized let's say Yahoo finance centralized let's say Yahoo finance folks will write their mCP server so now folks will write their mCP server so now folks will write their mCP server so now all these 10,000 programmers in the all these 10,000 programmers in the all these 10,000 programmers in the world they don't have to write the code world they don't have to write the code world they don't have to write the code okay so they are kind of getting this okay so they are kind of getting this okay so they are kind of getting this redimed thing and they are saving time redimed thing and they are saving time redimed thing and they are saving time on building their AI applications let's on building their AI applications let's on building their AI applications let's go deeper into technical details now say go deeper into technical details now say go deeper into technical details now say you are building this chatboard for your you are building this chatboard for your you are building this chatboard for your organization which requires interacting organization which requires interacting organization which requires interacting with Google Maps API so let's say you're with Google Maps API so let's say you're with Google Maps API so let's say you're getting some location and you are getting some location and you are getting some location and you are automatically creating the to-do task automatically creating the to-do task automatically creating the to-do task using this app todoist and the technical using this app todoist and the technical using this app todoist and the technical teams at Google Maps and todoist have teams at Google Maps and todoist have teams at Google Maps and todoist have already built these mCP already built these mCP already built these mCP servers in mCP client which is your servers in mCP client which is your servers in mCP client which is your chatboard you will have a configuration chatboard you will have a configuration chatboard you will have a configuration which will tell you what kind of servers which will tell you what kind of servers which will tell you what kind of servers are available to this client when this are available to this client when this are available to this client when this chatboard starts at the beginning let's chatboard starts at the beginning let's chatboard starts at the beginning let's say just think about Google Maps server say just think about Google Maps server say just think about Google Maps server it will make a call called list tools it will make a call called list tools it will make a call called list tools and this call will be made for Google and this call will be made for Google and this call will be made for Google Maps to doist all the servers which are Maps to doist all the servers which are Maps to doist all the servers which are listed Google Maps mCP server will listed Google Maps mCP server will listed Google Maps mCP server will return all its capabilities in form of return all its capabilities in form of return all its capabilities in form of this type of response so here you are this type of response so here you are this type of response so here you are saying that I can help you search places saying that I can help you search places saying that I can help you search places and you are providing this detailed and you are providing this detailed and you are providing this detailed description this description is very description this description is very description this description is very important because it will guide llm to important because it will guide llm to important because it will guide llm to uh call an appropriate tool llm has uh call an appropriate tool llm has uh call an appropriate tool llm has language intelligence so just by reading language intelligence so just by reading language intelligence so just by reading this description it can figure out that this description it can figure out that this description it can figure out that for whatever query if I have to search for whatever query if I have to search for whatever query if I have to search places I can use this map search places places I can use this map search places places I can use this map search places tool not only that from the query it tool not only that from the query it tool not only that from the query it will also extract the required will also extract the required will also extract the required parameters such as search query latitude parameters such as search query latitude parameters such as search query latitude longitude Etc let's say you are going longitude Etc let's say you are going longitude Etc let's say you are going for a hiking in lak and you'll say I'm for a hiking in lak and you'll say I'm for a hiking in lak and you'll say I'm going from lak to this place um you know going from lak to this place um you know going from lak to this place um you know show me the places is so from that show me the places is so from that show me the places is so from that natural question it will extract the natural question it will extract the natural question it will extract the parameter lak is a location and it will parameter lak is a location and it will parameter lak is a location and it will map it to Lang uh longitude and latitude map it to Lang uh longitude and latitude map it to Lang uh longitude and latitude and it will also uh determine that it and it will also uh determine that it and it will also uh determine that it needs to call this function now you will needs to call this function now you will needs to call this function now you will have all the tools you will have map have all the tools you will have map have all the tools you will have map place details and all the place details and all the place details and all the functionalities that Google Maps functionalities that Google Maps functionalities that Google Maps provides not only that you will have the provides not only that you will have the provides not only that you will have the tool description from other servers such tool description from other servers such tool description from other servers such as todoist as well so as todoist as well so as todoist as well so so once uh llm knows all these details so once uh llm knows all these details so once uh llm knows all these details now let's say you are asking this now let's say you are asking this now let's say you are asking this question that I'm going for hiking in question that I'm going for hiking in question that I'm going for hiking in lak and I need this place details what lak and I need this place details what lak and I need this place details what uh this application my chatboard will do uh this application my chatboard will do uh this application my chatboard will do is it will use this kind of a prompt so is it will use this kind of a prompt so is it will use this kind of a prompt so in this prompt this tool description is in this prompt this tool description is in this prompt this tool description is nothing but the combine tool description nothing but the combine tool description nothing but the combine tool description of all the tools you have available okay of all the tools you have available okay of all the tools you have available okay and it will say that choose the and it will say that choose the and it will say that choose the appropriate tool based on the user appropriate tool based on the user appropriate tool based on the user question so when you have tool question so when you have tool question so when you have tool description and this kind of nice prompt description and this kind of nice prompt description and this kind of nice prompt llm is smart enough to figure out which llm is smart enough to figure out which llm is smart enough to figure out which tool to call which parameter to extract tool to call which parameter to extract tool to call which parameter to extract from the user question and how to make a from the user question and how to make a from the user question and how to make a call get the response and how to read call get the response and how to read call get the response and how to read the response and serve to the end user the response and serve to the end user the response and serve to the end user here I have this uh mCP client from the here I have this uh mCP client from the here I have this uh mCP client from the python SDK that anthropic has provided python SDK that anthropic has provided python SDK that anthropic has provided when it starts it will go through all when it starts it will go through all when it starts it will go through all the servers remember that server the servers remember that server the servers remember that server configuration it will go through all the configuration it will go through all the configuration it will go through all the servers and each server it will ask list servers and each server it will ask list servers and each server it will ask list tools and whatever tools it is getting tools and whatever tools it is getting tools and whatever tools it is getting it will get the description of all those it will get the description of all those it will get the description of all those tools and it will put it here and look tools and it will put it here and look tools and it will put it here and look at this prompt okay so now you at this prompt okay so now you at this prompt okay so now you understand that llm is getting a understand that llm is getting a understand that llm is getting a question how it Maps or how it figure question how it Maps or how it figure question how it Maps or how it figure out an appropriate tool to call now let out an appropriate tool to call now let out an appropriate tool to call now let me show you the mCP server from Google me show you the mCP server from Google me show you the mCP server from Google Maps here it is listing the tools so Maps here it is listing the tools so Maps here it is listing the tools so when mCP client makes that request it when mCP client makes that request it when mCP client makes that request it will handle that request and it will will handle that request and it will will handle that request and it will list all the tools so you see this call list all the tools so you see this call list all the tools so you see this call okay and what are the tools so let's okay and what are the tools so let's okay and what are the tools so let's search for all the tools search for all the tools search for all the tools okay so search places tool you see okay so search places tool you see okay so search places tool you see search places tool geoc code tool okay search places tool geoc code tool okay search places tool geoc code tool okay so search places tool should be here So so search places tool should be here So so search places tool should be here So eventually it will come to this python eventually it will come to this python eventually it will come to this python function oh this is not python actually function oh this is not python actually function oh this is not python actually this is typescript so you can Implement this is typescript so you can Implement this is typescript so you can Implement your server in either typescript or your server in either typescript or your server in either typescript or Python and Python and Python and here from the user question it will here from the user question it will here from the user question it will derive the query location Etc and it derive the query location Etc and it derive the query location Etc and it will actually make an HTTP call to the will actually make an HTTP call to the will actually make an HTTP call to the Google Map API so it's not like you are Google Map API so it's not like you are Google Map API so it's not like you are replacing uh a rest protocol here or replacing uh a rest protocol here or replacing uh a rest protocol here or HTTP it's like a rapper HTTP it's like a rapper HTTP it's like a rapper and you are internally calling Google and you are internally calling Google and you are internally calling Google Map API and you are returning the Map API and you are returning the Map API and you are returning the response in a standardized format okay response in a standardized format okay response in a standardized format okay so there is a standard here so if you so there is a standard here so if you so there is a standard here so if you look at uh the input schema okay so let look at uh the input schema okay so let look at uh the input schema okay so let me search for input schema see input me search for input schema see input me search for input schema see input schema you see so for the search places schema you see so for the search places schema you see so for the search places tool you see here there is a standard tool you see here there is a standard tool you see here there is a standard way you will uh provide the description way you will uh provide the description way you will uh provide the description of the tool and also the query parameter of the tool and also the query parameter of the tool and also the query parameter Etc so see this input schema description Etc so see this input schema description Etc so see this input schema description Etc is part of this particular standard Etc is part of this particular standard Etc is part of this particular standard so you can find the this schema I'm so you can find the this schema I'm so you can find the this schema I'm going to provide all the links okay so going to provide all the links okay so going to provide all the links okay so this is the standard this is the schema this is the standard this is the schema this is the standard this is the schema that anybody who is building an mCP that anybody who is building an mCP that anybody who is building an mCP server will have to add her to so that server will have to add her to so that server will have to add her to so that way we have standard and some uh uniform way we have standard and some uh uniform way we have standard and some uh uniform or predictable way of communication okay or predictable way of communication okay or predictable way of communication okay so you look at this types script schema so you look at this types script schema so you look at this types script schema where you say input schema is this type where you say input schema is this type where you say input schema is this type required whatever just go through this required whatever just go through this required whatever just go through this schema and you will get an idea any mCP schema and you will get an idea any mCP schema and you will get an idea any mCP server will expose three capabilities server will expose three capabilities server will expose three capabilities tool resource and prompt their python tool resource and prompt their python tool resource and prompt their python SDK has simple examples for each of them SDK has simple examples for each of them SDK has simple examples for each of them so let's look at the tool this is a so let's look at the tool this is a so let's look at the tool this is a simple server with one single tool okay simple server with one single tool okay simple server with one single tool okay so if you look at the list tools so if you look at the list tools so if you look at the list tools function see list tool tools here it is function see list tool tools here it is function see list tool tools here it is exposing see this is an array okay so it exposing see this is an array okay so it exposing see this is an array okay so it is exposing a single tool called fatch is exposing a single tool called fatch is exposing a single tool called fatch and there is this standard description and there is this standard description and there is this standard description standard input schema and so on if you standard input schema and so on if you standard input schema and so on if you look at the implementation of fat it look at the implementation of fat it look at the implementation of fat it looks like this okay so you are fetching looks like this okay so you are fetching looks like this okay so you are fetching a website okay so a website okay so a website okay so here see you are fetching a website so here see you are fetching a website so here see you are fetching a website so whenever that fatch tool is called you whenever that fatch tool is called you whenever that fatch tool is called you call this call this call this function and you are just retrieving function and you are just retrieving function and you are just retrieving some information by making an HTTP call some information by making an HTTP call some information by making an HTTP call okay so this is pretty straightforward okay so this is pretty straightforward okay so this is pretty straightforward the second capability is a resource the second capability is a resource the second capability is a resource resource is um some kind of knowledge resource is um some kind of knowledge resource is um some kind of knowledge okay database files Etc and similar to okay database files Etc and similar to okay database files Etc and similar to list tool functions it will have list list tool functions it will have list list tool functions it will have list resources so when the mCP client starts resources so when the mCP client starts resources so when the mCP client starts it will call list tools list resources it will call list tools list resources it will call list tools list resources list prompts for each of the servers so list prompts for each of the servers so list prompts for each of the servers so it knows the full capabilities of all it knows the full capabilities of all it knows the full capabilities of all the servers that it has available or has the servers that it has available or has the servers that it has available or has access to so in the list resources you access to so in the list resources you access to so in the list resources you can have a file see this is a plain file can have a file see this is a plain file can have a file see this is a plain file very simple example you can have a file very simple example you can have a file very simple example you can have a file in your uh some drive or some you know in your uh some drive or some you know in your uh some drive or some you know like Amazon S3 Etc you can also have a like Amazon S3 Etc you can also have a like Amazon S3 Etc you can also have a prompt so just imagine you are building prompt so just imagine you are building prompt so just imagine you are building mCP server for Yahoo finance as a mCP server for Yahoo finance as a mCP server for Yahoo finance as a developer you know all the prompts that developer you know all the prompts that developer you know all the prompts that AI Engineers uh might need to interact AI Engineers uh might need to interact AI Engineers uh might need to interact with my API okay so you will provide all with my API okay so you will provide all with my API okay so you will provide all those prompts VI your server so writing those prompts VI your server so writing those prompts VI your server so writing prompts become very easy for the mCP prompts become very easy for the mCP prompts become very easy for the mCP client okay so here you are providing client okay so here you are providing client okay so here you are providing one single prompt so once again you have one single prompt so once again you have one single prompt so once again you have list prompts you are providing all the list prompts you are providing all the list prompts you are providing all the prompts see this is an array and this is prompts see this is an array and this is prompts see this is an array and this is the simple prompt that takes context and the simple prompt that takes context and the simple prompt that takes context and topic and if you look at the Imp topic and if you look at the Imp topic and if you look at the Imp mentation it's pretty simple you have mentation it's pretty simple you have mentation it's pretty simple you have context you have topic and you are context you have topic and you are context you have topic and you are creating the prompt using that context creating the prompt using that context creating the prompt using that context and topic folks that's it so that is and topic folks that's it so that is and topic folks that's it so that is what is model context protocol I'm going what is model context protocol I'm going what is model context protocol I'm going to provide documentation Etc so you can to provide documentation Etc so you can to provide documentation Etc so you can read through more details there has been read through more details there has been read through more details there has been a lot of hype but I believe we are in a lot of hype but I believe we are in a lot of hype but I believe we are in early days this has a lot of potential early days this has a lot of potential early days this has a lot of potential but how this is going to evolve and how but how this is going to evolve and how but how this is going to evolve and how this is going to help AI Engineers solve this is going to help AI Engineers solve this is going to help AI Engineers solve the real problems is something that we the real problems is something that we the real problems is something that we will know as time goes okay so some will know as time goes okay so some will know as time goes okay so some people are super excited well I'm people are super excited well I'm people are super excited well I'm excited too but just understand that we excited too but just understand that we excited too but just understand that we are in early days there has been lot of are in early days there has been lot of are in early days there has been lot of hype there is some reality we'll have to hype there is some reality we'll have to hype there is some reality we'll have to see how this thing evolves we are going see how this thing evolves we are going see how this thing evolves we are going to come up with few more technical to come up with few more technical to come up with few more technical tutorials uh on this so I'll be building tutorials uh on this so I'll be building tutorials uh on this so I'll be building some actual servers and clients using some actual servers and clients using some actual servers and clients using mCP if you have have any question please mCP if you have have any question please mCP if you have have any question please post in the comment box below thank you post in the comment box below thank you post in the comment box below thank you for watching",
    "chunks": [
      "Kind: captions Language: en there has been a lot of hype around there has been a lot of hype around there has been a lot of hype around model context protocol and I'm going to model context protocol and I'm going to model context protocol and I'm going to make an attempt to provide an extremely make an attempt to provide an extremely make an attempt to provide an extremely simple explanation of mCP today I will simple explanation of mCP today I will simple explanation of mCP today I will also go",
      "into technical details so that also go into technical details so that also go into technical details so that if you are building an AI application it if you are building an AI application it if you are building an AI application it helps you if you think about a of helps you if you think about a of helps you if you think about a of building a applications we are seeing a building a applications we are seeing a building a applications we are seeing a clear Evolution first we started with clear",
      "Evolution first we started with clear Evolution first we started with llm without any tools Etc then we llm without any tools Etc then we llm without any tools Etc then we started building this agentic Frameworks started building this agentic Frameworks started building this agentic Frameworks where llm gets help from tools and where llm gets help from tools and where llm gets help from tools and knowledge now we are entering the realm knowledge now we are entering the realm knowledge now we are",
      "entering the realm of standardized ways of interacting with of standardized ways of interacting with of standardized ways of interacting with these tools and knowledge so that these tools and knowledge so that these tools and knowledge so that building AI applications becomes easier building AI applications becomes easier building AI applications becomes easier let me give you an example say you are a let me give you an example say you are a let me give you an example say you are a equity",
      "research analyst working at some equity research analyst working at some equity research analyst working at some company let's say jeffre you want to company let's say jeffre you want to company let's say jeffre you want to develop a report comparing Nvidia and develop a report comparing Nvidia and develop a report comparing Nvidia and Tesla stock that looks something like Tesla stock that looks something like Tesla stock that looks something like this where you have company description this",
      "where you have company description this where you have company description at the the beginning then you have some at the the beginning then you have some at the the beginning then you have some uh Financial metrics then you have a uh Financial metrics then you have a uh Financial metrics then you have a summary of those metrics and in the end summary of those metrics and in the end summary of those metrics and in the end you have recent news about those you have recent news about those you have",
      "recent news about those companies now you are a tax heavy person companies now you are a tax heavy person companies now you are a tax heavy person so you will talk to your AI engineer so you will talk to your AI engineer so you will talk to your AI engineer friend working at Jeff and you will ask friend working at Jeff and you will ask friend working at Jeff and you will ask him to uh build an a application that him to uh build an a application that him to uh build an a application that can",
      "automatically generate this report can automatically generate this report can automatically generate this report now your AI engineer friends understands now your AI engineer friends understands now your AI engineer friends understands the capability of llm see they know that the capability of llm see they know that the capability of llm see they know that llm can pull the description of Nvidia llm can pull the description of Nvidia llm can pull the description of Nvidia and Tesla because it is",
      "part of that and Tesla because it is part of that and Tesla because it is part of that training data set but it cannot pull the training data set but it cannot pull the training data set but it cannot pull the latest stock price for latest stock price for latest stock price for example once you have somehow retrieved example once you have somehow retrieved example once you have somehow retrieved let's say the latest stock price llm can let's say the latest stock price llm can let's say the latest",
      "stock price llm can summarize uh that particular information summarize uh that particular information summarize uh that particular information okay so if you think about pure llm it okay so if you think about pure llm it okay so if you think about pure llm it has all these capabilities now you'll has all these capabilities now you'll has all these capabilities now you'll ask me okay if I go go to chat GPT and ask me okay if I go go to chat GPT and ask me okay if I go go to chat GPT and if I type",
      "this question it is pulling if I type this question it is pulling if I type this question it is pulling the information but look at this it is the information but look at this it is the information but look at this it is searching the web actually so chat GPT searching the web actually so chat GPT searching the web actually so chat GPT is an agent I am referring to GPT 40 is an agent I am referring to GPT 40 is an agent I am referring to GPT 40 model okay so now you already know the model okay so",
      "now you already know the model okay so now you already know the answer that you can use web search or answer that you can use web search or answer that you can use web search or maybe you can uh call Yahoo finance API maybe you can uh call Yahoo finance API maybe you can uh call Yahoo finance API which is called a tool to retrieve the which is called a tool to retrieve the which is called a tool to retrieve the latest information to summarize AI latest information to summarize AI latest",
      "information to summarize AI engineer will build this application engineer will build this application engineer will build this application where the heart is llm and llm is where the heart is llm and llm is where the heart is llm and llm is interacting with some tools which can be interacting with some tools which can be interacting with some tools which can be Yahoo finance API or web search it is Yahoo finance API or web search it is Yahoo finance API or web search it is also interacting with",
      "some knowledge you also interacting with some knowledge you also interacting with some knowledge you might have some private database in Jeff might have some private database in Jeff might have some private database in Jeff some PDF files lmm might want to pull some PDF files lmm might want to pull some PDF files lmm might want to pull data from it and he will write all this data from it and he will write all this data from it and he will write all this glue code okay glue code is basically glue",
      "code okay glue code is basically glue code okay glue code is basically the code that you're writing the code that you're writing the code that you're writing for these interactions now this can be for these interactions now this can be for these interactions now this can be an agentic application in which glue an agentic application in which glue an agentic application in which glue code is written by the agentic framework code is written by the agentic framework code is written by the agentic",
      "framework such as crew AI egno Etc or it can be a such as crew AI egno Etc or it can be a such as crew AI egno Etc or it can be a workflow application where as part of workflow application where as part of workflow application where as part of your python code you writing all this your python code you writing all this your python code you writing all this glue code this is just one application glue code this is just one application glue code this is just one application imagine jeffre is building",
      "20 such imagine jeffre is building 20 such imagine jeffre is building 20 such application and think about all the application and think about all the application and think about all the companies in the world building millions companies in the world building millions companies in the world building millions of applications that is a lot of glue of applications that is a lot of glue of applications that is a lot of glue code it sounds like you have this old code it sounds like you have this old",
      "code it sounds like you have this old computer and you're connecting with computer and you're connecting with computer and you're connecting with keyboard mouse Etc through all these keyboard mouse Etc through all these keyboard mouse Etc through all these different wires but you will be like no different wires but you will be like no different wires but you will be like no today things have changed today you can today things have changed today you can today things have changed today you can",
      "connect computer using this unified connect computer using this unified connect computer using this unified interface USB C Port you can have your interface USB C Port you can have your interface USB C Port you can have your USB hub and you can connect all this USB hub and you can connect all this USB hub and you can connect all this devices well that USBC moment has devices well that USBC moment has devices well that USBC moment has arrived for AI arrived for AI arrived for AI and that is your",
      "model context protocol and that is your model context protocol and that is your model context protocol in this your llm will interact through in this your llm will interact through in this your llm will interact through model context protocol with different model context protocol with different model context protocol with different mCP servers so for our Jeff example mCP servers so for our Jeff example mCP servers so for our Jeff example think that Yahoo finance is building an think that Yahoo",
      "finance is building an think that Yahoo finance is building an mCP server or let's say Google search is mCP server or let's say Google search is mCP server or let's say Google search is building another mCP server where building another mCP server where building another mCP server where they're exposing all these tools and they're exposing all these tools and they're exposing all these tools and resources and prompts Etc and that way resources and prompts Etc and that way resources and prompts",
      "Etc and that way these interactions becomes easier now these interactions becomes easier now these interactions becomes easier now you're a smart programmer and you'll be you're a smart programmer and you'll be you're a smart programmer and you'll be like hey we are still writing gluc code like hey we are still writing gluc code like hey we are still writing gluc code right because you need to do all these right because you need to do all these right because you need to do all these interactions",
      "the answer is yes you are interactions the answer is yes you are interactions the answer is yes you are writing some glue code here but the AE writing some glue code here but the AE writing some glue code here but the AE of writing that code is increasing going of writing that code is increasing going of writing that code is increasing going back to our old diagram the problem here back to our old diagram the problem here back to our old diagram the problem here was not just writing the code but",
      "was not just writing the code but was not just writing the code but maintaining it because tomorrow if Yahoo maintaining it because tomorrow if Yahoo maintaining it because tomorrow if Yahoo finance changes their API you have to finance changes their API you have to finance changes their API you have to maintain this code right so all this maintain this code right so all this maintain this code right so all this glue code becomes a nightmare to glue code becomes a nightmare to glue code becomes a",
      "nightmare to maintain whereas with this mCP due to maintain whereas with this mCP due to maintain whereas with this mCP due to the standard protocol writing and the standard protocol writing and the standard protocol writing and maintaining the code has become easier maintaining the code has become easier maintaining the code has become easier and also since writing this code is and also since writing this code is and also since writing this code is centralized let's say Yahoo finance centralized",
      "let's say Yahoo finance centralized let's say Yahoo finance folks will write their mCP server so now folks will write their mCP server so now folks will write their mCP server so now all these 10,000 programmers in the all these 10,000 programmers in the all these 10,000 programmers in the world they don't have to write the code world they don't have to write the code world they don't have to write the code okay so they are kind of getting this okay so they are kind of getting this okay so they",
      "are kind of getting this redimed thing and they are saving time redimed thing and they are saving time redimed thing and they are saving time on building their AI applications let's on building their AI applications let's on building their AI applications let's go deeper into technical details now say go deeper into technical details now say go deeper into technical details now say you are building this chatboard for your you are building this chatboard for your you are building this chatboard",
      "for your organization which requires interacting organization which requires interacting organization which requires interacting with Google Maps API so let's say you're with Google Maps API so let's say you're with Google Maps API so let's say you're getting some location and you are getting some location and you are getting some location and you are automatically creating the to-do task automatically creating the to-do task automatically creating the to-do task using this app todoist and the",
      "technical using this app todoist and the technical using this app todoist and the technical teams at Google Maps and todoist have teams at Google Maps and todoist have teams at Google Maps and todoist have already built these mCP already built these mCP already built these mCP servers in mCP client which is your servers in mCP client which is your servers in mCP client which is your chatboard you will have a configuration chatboard you will have a configuration chatboard you will have a",
      "configuration which will tell you what kind of servers which will tell you what kind of servers which will tell you what kind of servers are available to this client when this are available to this client when this are available to this client when this chatboard starts at the beginning let's chatboard starts at the beginning let's chatboard starts at the beginning let's say just think about Google Maps server say just think about Google Maps server say just think about Google Maps server it will",
      "make a call called list tools it will make a call called list tools it will make a call called list tools and this call will be made for Google and this call will be made for Google and this call will be made for Google Maps to doist all the servers which are Maps to doist all the servers which are Maps to doist all the servers which are listed Google Maps mCP server will listed Google Maps mCP server will listed Google Maps mCP server will return all its capabilities in form of return all its",
      "capabilities in form of return all its capabilities in form of this type of response so here you are this type of response so here you are this type of response so here you are saying that I can help you search places saying that I can help you search places saying that I can help you search places and you are providing this detailed and you are providing this detailed and you are providing this detailed description this description is very description this description is very description this",
      "description is very important because it will guide llm to important because it will guide llm to important because it will guide llm to uh call an appropriate tool llm has uh call an appropriate tool llm has uh call an appropriate tool llm has language intelligence so just by reading language intelligence so just by reading language intelligence so just by reading this description it can figure out that this description it can figure out that this description it can figure out that for whatever",
      "query if I have to search for whatever query if I have to search for whatever query if I have to search places I can use this map search places places I can use this map search places places I can use this map search places tool not only that from the query it tool not only that from the query it tool not only that from the query it will also extract the required will also extract the required will also extract the required parameters such as search query latitude parameters such as search query",
      "latitude parameters such as search query latitude longitude Etc let's say you are going longitude Etc let's say you are going longitude Etc let's say you are going for a hiking in lak and you'll say I'm for a hiking in lak and you'll say I'm for a hiking in lak and you'll say I'm going from lak to this place um you know going from lak to this place um you know going from lak to this place um you know show me the places is so from that show me the places is so from that show me the places is so",
      "from that natural question it will extract the natural question it will extract the natural question it will extract the parameter lak is a location and it will parameter lak is a location and it will parameter lak is a location and it will map it to Lang uh longitude and latitude map it to Lang uh longitude and latitude map it to Lang uh longitude and latitude and it will also uh determine that it and it will also uh determine that it and it will also uh determine that it needs to call this",
      "function now you will needs to call this function now you will needs to call this function now you will have all the tools you will have map have all the tools you will have map have all the tools you will have map place details and all the place details and all the place details and all the functionalities that Google Maps functionalities that Google Maps functionalities that Google Maps provides not only that you will have the provides not only that you will have the provides not only that you",
      "will have the tool description from other servers such tool description from other servers such tool description from other servers such as todoist as well so as todoist as well so as todoist as well so so once uh llm knows all these details so once uh llm knows all these details so once uh llm knows all these details now let's say you are asking this now let's say you are asking this now let's say you are asking this question that I'm going for hiking in question that I'm going for hiking in",
      "question that I'm going for hiking in lak and I need this place details what lak and I need this place details what lak and I need this place details what uh this application my chatboard will do uh this application my chatboard will do uh this application my chatboard will do is it will use this kind of a prompt so is it will use this kind of a prompt so is it will use this kind of a prompt so in this prompt this tool description is in this prompt this tool description is in this prompt this",
      "tool description is nothing but the combine tool description nothing but the combine tool description nothing but the combine tool description of all the tools you have available okay of all the tools you have available okay of all the tools you have available okay and it will say that choose the and it will say that choose the and it will say that choose the appropriate tool based on the user appropriate tool based on the user appropriate tool based on the user question so when you have tool",
      "question so when you have tool question so when you have tool description and this kind of nice prompt description and this kind of nice prompt description and this kind of nice prompt llm is smart enough to figure out which llm is smart enough to figure out which llm is smart enough to figure out which tool to call which parameter to extract tool to call which parameter to extract tool to call which parameter to extract from the user question and how to make a from the user question and how to",
      "make a from the user question and how to make a call get the response and how to read call get the response and how to read call get the response and how to read the response and serve to the end user the response and serve to the end user the response and serve to the end user here I have this uh mCP client from the here I have this uh mCP client from the here I have this uh mCP client from the python SDK that anthropic has provided python SDK that anthropic has provided python SDK that",
      "anthropic has provided when it starts it will go through all when it starts it will go through all when it starts it will go through all the servers remember that server the servers remember that server the servers remember that server configuration it will go through all the configuration it will go through all the configuration it will go through all the servers and each server it will ask list servers and each server it will ask list servers and each server it will ask list tools and whatever",
      "tools it is getting tools and whatever tools it is getting tools and whatever tools it is getting it will get the description of all those it will get the description of all those it will get the description of all those tools and it will put it here and look tools and it will put it here and look tools and it will put it here and look at this prompt okay so now you at this prompt okay so now you at this prompt okay so now you understand that llm is getting a understand that llm is getting a",
      "understand that llm is getting a question how it Maps or how it figure question how it Maps or how it figure question how it Maps or how it figure out an appropriate tool to call now let out an appropriate tool to call now let out an appropriate tool to call now let me show you the mCP server from Google me show you the mCP server from Google me show you the mCP server from Google Maps here it is listing the tools so Maps here it is listing the tools so Maps here it is listing the tools so when",
      "mCP client makes that request it when mCP client makes that request it when mCP client makes that request it will handle that request and it will will handle that request and it will will handle that request and it will list all the tools so you see this call list all the tools so you see this call list all the tools so you see this call okay and what are the tools so let's okay and what are the tools so let's okay and what are the tools so let's search for all the tools search for all the tools",
      "search for all the tools okay so search places tool you see okay so search places tool you see okay so search places tool you see search places tool geoc code tool okay search places tool geoc code tool okay search places tool geoc code tool okay so search places tool should be here So so search places tool should be here So so search places tool should be here So eventually it will come to this python eventually it will come to this python eventually it will come to this python function oh this",
      "is not python actually function oh this is not python actually function oh this is not python actually this is typescript so you can Implement this is typescript so you can Implement this is typescript so you can Implement your server in either typescript or your server in either typescript or your server in either typescript or Python and Python and Python and here from the user question it will here from the user question it will here from the user question it will derive the query location Etc",
      "and it derive the query location Etc and it derive the query location Etc and it will actually make an HTTP call to the will actually make an HTTP call to the will actually make an HTTP call to the Google Map API so it's not like you are Google Map API so it's not like you are Google Map API so it's not like you are replacing uh a rest protocol here or replacing uh a rest protocol here or replacing uh a rest protocol here or HTTP it's like a rapper HTTP it's like a rapper HTTP it's like a rapper",
      "and you are internally calling Google and you are internally calling Google and you are internally calling Google Map API and you are returning the Map API and you are returning the Map API and you are returning the response in a standardized format okay response in a standardized format okay response in a standardized format okay so there is a standard here so if you so there is a standard here so if you so there is a standard here so if you look at uh the input schema okay so let look at uh the",
      "input schema okay so let look at uh the input schema okay so let me search for input schema see input me search for input schema see input me search for input schema see input schema you see so for the search places schema you see so for the search places schema you see so for the search places tool you see here there is a standard tool you see here there is a standard tool you see here there is a standard way you will uh provide the description way you will uh provide the description way you",
      "will uh provide the description of the tool and also the query parameter of the tool and also the query parameter of the tool and also the query parameter Etc so see this input schema description Etc so see this input schema description Etc so see this input schema description Etc is part of this particular standard Etc is part of this particular standard Etc is part of this particular standard so you can find the this schema I'm so you can find the this schema I'm so you can find the this schema",
      "I'm going to provide all the links okay so going to provide all the links okay so going to provide all the links okay so this is the standard this is the schema this is the standard this is the schema this is the standard this is the schema that anybody who is building an mCP that anybody who is building an mCP that anybody who is building an mCP server will have to add her to so that server will have to add her to so that server will have to add her to so that way we have standard and some uh",
      "uniform way we have standard and some uh uniform way we have standard and some uh uniform or predictable way of communication okay or predictable way of communication okay or predictable way of communication okay so you look at this types script schema so you look at this types script schema so you look at this types script schema where you say input schema is this type where you say input schema is this type where you say input schema is this type required whatever just go through this required",
      "whatever just go through this required whatever just go through this schema and you will get an idea any mCP schema and you will get an idea any mCP schema and you will get an idea any mCP server will expose three capabilities server will expose three capabilities server will expose three capabilities tool resource and prompt their python tool resource and prompt their python tool resource and prompt their python SDK has simple examples for each of them SDK has simple examples for each of them",
      "SDK has simple examples for each of them so let's look at the tool this is a so let's look at the tool this is a so let's look at the tool this is a simple server with one single tool okay simple server with one single tool okay simple server with one single tool okay so if you look at the list tools so if you look at the list tools so if you look at the list tools function see list tool tools here it is function see list tool tools here it is function see list tool tools here it is exposing see",
      "this is an array okay so it exposing see this is an array okay so it exposing see this is an array okay so it is exposing a single tool called fatch is exposing a single tool called fatch is exposing a single tool called fatch and there is this standard description and there is this standard description and there is this standard description standard input schema and so on if you standard input schema and so on if you standard input schema and so on if you look at the implementation of fat it",
      "look at the implementation of fat it look at the implementation of fat it looks like this okay so you are fetching looks like this okay so you are fetching looks like this okay so you are fetching a website okay so a website okay so a website okay so here see you are fetching a website so here see you are fetching a website so here see you are fetching a website so whenever that fatch tool is called you whenever that fatch tool is called you whenever that fatch tool is called you call this call",
      "this call this function and you are just retrieving function and you are just retrieving function and you are just retrieving some information by making an HTTP call some information by making an HTTP call some information by making an HTTP call okay so this is pretty straightforward okay so this is pretty straightforward okay so this is pretty straightforward the second capability is a resource the second capability is a resource the second capability is a resource resource is um some kind of",
      "knowledge resource is um some kind of knowledge resource is um some kind of knowledge okay database files Etc and similar to okay database files Etc and similar to okay database files Etc and similar to list tool functions it will have list list tool functions it will have list list tool functions it will have list resources so when the mCP client starts resources so when the mCP client starts resources so when the mCP client starts it will call list tools list resources it will call list tools",
      "list resources it will call list tools list resources list prompts for each of the servers so list prompts for each of the servers so list prompts for each of the servers so it knows the full capabilities of all it knows the full capabilities of all it knows the full capabilities of all the servers that it has available or has the servers that it has available or has the servers that it has available or has access to so in the list resources you access to so in the list resources you access to so",
      "in the list resources you can have a file see this is a plain file can have a file see this is a plain file can have a file see this is a plain file very simple example you can have a file very simple example you can have a file very simple example you can have a file in your uh some drive or some you know in your uh some drive or some you know in your uh some drive or some you know like Amazon S3 Etc you can also have a like Amazon S3 Etc you can also have a like Amazon S3 Etc you can also have",
      "a prompt so just imagine you are building prompt so just imagine you are building prompt so just imagine you are building mCP server for Yahoo finance as a mCP server for Yahoo finance as a mCP server for Yahoo finance as a developer you know all the prompts that developer you know all the prompts that developer you know all the prompts that AI Engineers uh might need to interact AI Engineers uh might need to interact AI Engineers uh might need to interact with my API okay so you will provide all",
      "with my API okay so you will provide all with my API okay so you will provide all those prompts VI your server so writing those prompts VI your server so writing those prompts VI your server so writing prompts become very easy for the mCP prompts become very easy for the mCP prompts become very easy for the mCP client okay so here you are providing client okay so here you are providing client okay so here you are providing one single prompt so once again you have one single prompt so once again",
      "you have one single prompt so once again you have list prompts you are providing all the list prompts you are providing all the list prompts you are providing all the prompts see this is an array and this is prompts see this is an array and this is prompts see this is an array and this is the simple prompt that takes context and the simple prompt that takes context and the simple prompt that takes context and topic and if you look at the Imp topic and if you look at the Imp topic and if you look",
      "at the Imp mentation it's pretty simple you have mentation it's pretty simple you have mentation it's pretty simple you have context you have topic and you are context you have topic and you are context you have topic and you are creating the prompt using that context creating the prompt using that context creating the prompt using that context and topic folks that's it so that is and topic folks that's it so that is and topic folks that's it so that is what is model context protocol I'm going",
      "what is model context protocol I'm going what is model context protocol I'm going to provide documentation Etc so you can to provide documentation Etc so you can to provide documentation Etc so you can read through more details there has been read through more details there has been read through more details there has been a lot of hype but I believe we are in a lot of hype but I believe we are in a lot of hype but I believe we are in early days this has a lot of potential early days this has a",
      "lot of potential early days this has a lot of potential but how this is going to evolve and how but how this is going to evolve and how but how this is going to evolve and how this is going to help AI Engineers solve this is going to help AI Engineers solve this is going to help AI Engineers solve the real problems is something that we the real problems is something that we the real problems is something that we will know as time goes okay so some will know as time goes okay so some will know as",
      "time goes okay so some people are super excited well I'm people are super excited well I'm people are super excited well I'm excited too but just understand that we excited too but just understand that we excited too but just understand that we are in early days there has been lot of are in early days there has been lot of are in early days there has been lot of hype there is some reality we'll have to hype there is some reality we'll have to hype there is some reality we'll have to see how this",
      "thing evolves we are going see how this thing evolves we are going see how this thing evolves we are going to come up with few more technical to come up with few more technical to come up with few more technical tutorials uh on this so I'll be building tutorials uh on this so I'll be building tutorials uh on this so I'll be building some actual servers and clients using some actual servers and clients using some actual servers and clients using mCP if you have have any question please mCP if you",
      "have have any question please mCP if you have have any question please post in the comment box below thank you post in the comment box below thank you post in the comment box below thank you for watching"
    ],
    "chunk_count": 71,
    "content_id": "23528605-b459-49e4-866e-131b1a15a1a1",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554955"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=wgfSDrqYMJ4": {
    "title": "What are Word Embeddings?",
    "url": "https://www.youtube.com/watch?v=wgfSDrqYMJ4",
    "description": "Want to play with the technology yourself? Explore our interactive demo → https://ibm.biz/BdKet3\nLearn more about the technology → https://ibm.biz/BdKetT\n\nWord Embeddings the means of turning natural language into numerical vectors for machine learning tasks. Martin Keen explains how this process works, the varies means of creating vectors like GLOVE, word2vec, CBOW, and the impact of new transformer models are having on completing natural language processing (NLP) tasks.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdKetQ",
    "duration": 518,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en A word on word embeddings. Maybe a few words. Word embeddings represent words as numbers, specifically as numeric vectors, in a way that captures the semantic relationships and contextual information. So that means words with similar meanings are positioned close to each other, and the distance and direction between vectors encode the degree of similarity between words. But why do we need to transform words into numbers? The reason vectors are used to represent words is that most machine learning algorithms are just incapable of processing plain text in its raw form. They require numbers as input to perform any task, and that's where word embeddings come in. So let's take a look at how word embeddings are used and the model is used to create them. And let's start with a look at some applications. Now what embeddings have become a fundamental tool in the world of NLP. That's natural language processing. Natural language processing helps machines understand human language. Word embeddings are used in various NLP tasks, so for example, you'll find them used in text classification very frequently. Now in text classification, word embeddings are often used in tasks such as spam detection and topic categorization. Another common task is NER, that's an acronym for named entity recognition, and there is used to identify and classify entities in text. And an entity is like a name of a person or a place or an organization. Now, word embeddings can also help with tasks related to word similarity and word analogy tasks. So, for example, the king is the queen as man is to a woman, and then another example is in QA. So question and answering systems they can benefit from word embeddings for measuring semantic similarities between words or documents for tasks like clustering related articles, or finding similar documents, or recommending similar items. Now, word embeddings are created by trained models on a large corpus of text. So maybe, like all of Wikipedia, the process begins with preprocessing the text, including tokenization and removing stopwords and punctuation. A sliding context window identifies target and context words, allowing the model to learn word relationships. Then the model is trained to predict based on their context. Positioning semantically similar words close together in the vector space and throughout the training, the model parameters are adjusted to minimize prediction errors. So what does this look like? Well, let's start with a super small corpus of just six words. Here they are. Now we'll represent each word as a three dimensional vector. So each dimension has a numeric value creating a unique vector for each word. And these values represent the word's position in a continuous three dimensional vector space. And if you look closely, you can see that words with similar meanings or contexts have similar vector representations. So, for instance, the vectors for apple and for orange are close together, reflecting this semantic relationship. Likewise, the vectors for happy and sad have opposite directions, indicating their contrasting meanings. Now, of course, in real life, it's not this simple. A corpus of six words isn't going to be too helpful in practice, and actual word embeddings typically have hundreds of dimensions, not just three. To capture more intricate relationships and nuances in meaning. Now, there are two fundamental approaches to how word embedding methods generate effective representations for words. So let's take a look at some of these embedding methods. And we'll start with the first one which is frequency. So frequency based embeddings. Now frequency based embeddings are word representations that are derived from the frequency of words in a corpus. They're based on the idea that the importance or the significance of a word can be inferred from how frequently it occurs in the text. Now, one such embedding of frequency based is called TF-IDF that stands for Term Frequency Inverse Document Frequency. TF-IDF highlights words that are frequent within a specific document, but are rare across the entire corpus. So, for example, in a document about coffee, TF-IDF would emphasize words like espresso or cappuccino, which might appear often in that document, but rarely in others about different topics. Common words like the or and, which appear frequently across all documents, would receive low TF-IDF based scores. Now another embedding type is called prediction based embeddings and prediction based embeddings. They capture semantic relationships and contextual information between words. So, for example, in the sentences, \"the dog is barking loudly.\" and \"the dog is wagging its tail.\" A prediction based model would learn to associate dog with words like bark, wag, and tail. This allows these models to create a single fixed representation for dog that encompasses various, well, dog related concepts. Prediction based embeddings. They excel at separating words with close meanings, and can manage the various senses in which a word may be used. Now there are various models for generating word embeddings. One of the most popular is called word2vec that was developed by Google in 2013. Now word2vec has two main architectures. There's something called c b, o, w and there's something called skip-gram, and CBOW, that's an acronym for Continuous Bag of Words. Now CBOW predicts a target word based on its surrounding context words. Well, skip-gram does the opposite, predicting the context words given a target word. Now another popular method is called GLOVE. Also an acronym, that one stands for Global Vectors for Word Representation. That was created at Stanford University in 2014 and that uses co-occurrence statistics to create word vectors. Now, these models, they differ in their approach. What's a vec that focuses on learning from the immediate context around each word? While glove takes a broader view by analyzing how often words appear together across the entire corpus, then uses this information to create word vectors. Now, while these two word embedding models continue to be valuable tools in NLP, the field has seen some significant advances with the emergence of new tech, particularly transformers. While traditional word embeddings assign a fixed vector to each word, transformer models use a different type of embedding, and it's called a contextual based embedding. Now, contextual based embeddings are where the representation of a word changes based on its surrounding context. So, for example, in a transformer model, the word bank would have different representations in the sentence I'm going to the bank to deposit money and I'm sitting on the bank of a river. This context sensitivity allows these models to capture more nuanced meanings and relationships between words, which has led to all sorts of improvements in the various fields of NLP tasks. So that's word embeddings, from simple numeric vectors to complex representations. Word embeddings have revolutionized how machines understand and process human language. Proving that transforming words into numbers is indeed a powerful tool for making sense of our linguistic world.",
    "chunks": [
      "Kind: captions Language: en A word on word embeddings. Maybe a few words. Word embeddings represent words as numbers, specifically as numeric vectors, in a way that captures the semantic relationships and contextual information. So that means words with similar meanings are positioned close to each other, and the distance and direction between vectors encode the degree of similarity between words. But why do we need to transform words into numbers? The reason vectors are used to represent words",
      "is that most machine learning algorithms are just incapable of processing plain text in its raw form. They require numbers as input to perform any task, and that's where word embeddings come in. So let's take a look at how word embeddings are used and the model is used to create them. And let's start with a look at some applications. Now what embeddings have become a fundamental tool in the world of NLP. That's natural language processing. Natural language processing helps machines understand",
      "human language. Word embeddings are used in various NLP tasks, so for example, you'll find them used in text classification very frequently. Now in text classification, word embeddings are often used in tasks such as spam detection and topic categorization. Another common task is NER, that's an acronym for named entity recognition, and there is used to identify and classify entities in text. And an entity is like a name of a person or a place or an organization. Now, word embeddings can also help",
      "with tasks related to word similarity and word analogy tasks. So, for example, the king is the queen as man is to a woman, and then another example is in QA. So question and answering systems they can benefit from word embeddings for measuring semantic similarities between words or documents for tasks like clustering related articles, or finding similar documents, or recommending similar items. Now, word embeddings are created by trained models on a large corpus of text. So maybe, like all of",
      "Wikipedia, the process begins with preprocessing the text, including tokenization and removing stopwords and punctuation. A sliding context window identifies target and context words, allowing the model to learn word relationships. Then the model is trained to predict based on their context. Positioning semantically similar words close together in the vector space and throughout the training, the model parameters are adjusted to minimize prediction errors. So what does this look like? Well, let's",
      "start with a super small corpus of just six words. Here they are. Now we'll represent each word as a three dimensional vector. So each dimension has a numeric value creating a unique vector for each word. And these values represent the word's position in a continuous three dimensional vector space. And if you look closely, you can see that words with similar meanings or contexts have similar vector representations. So, for instance, the vectors for apple and for orange are close together,",
      "reflecting this semantic relationship. Likewise, the vectors for happy and sad have opposite directions, indicating their contrasting meanings. Now, of course, in real life, it's not this simple. A corpus of six words isn't going to be too helpful in practice, and actual word embeddings typically have hundreds of dimensions, not just three. To capture more intricate relationships and nuances in meaning. Now, there are two fundamental approaches to how word embedding methods generate effective",
      "representations for words. So let's take a look at some of these embedding methods. And we'll start with the first one which is frequency. So frequency based embeddings. Now frequency based embeddings are word representations that are derived from the frequency of words in a corpus. They're based on the idea that the importance or the significance of a word can be inferred from how frequently it occurs in the text. Now, one such embedding of frequency based is called TF-IDF that stands for Term",
      "Frequency Inverse Document Frequency. TF-IDF highlights words that are frequent within a specific document, but are rare across the entire corpus. So, for example, in a document about coffee, TF-IDF would emphasize words like espresso or cappuccino, which might appear often in that document, but rarely in others about different topics. Common words like the or and, which appear frequently across all documents, would receive low TF-IDF based scores. Now another embedding type is called prediction",
      "based embeddings and prediction based embeddings. They capture semantic relationships and contextual information between words. So, for example, in the sentences, \"the dog is barking loudly.\" and \"the dog is wagging its tail.\" A prediction based model would learn to associate dog with words like bark, wag, and tail. This allows these models to create a single fixed representation for dog that encompasses various, well, dog related concepts. Prediction based embeddings. They excel at separating",
      "words with close meanings, and can manage the various senses in which a word may be used. Now there are various models for generating word embeddings. One of the most popular is called word2vec that was developed by Google in 2013. Now word2vec has two main architectures. There's something called c b, o, w and there's something called skip-gram, and CBOW, that's an acronym for Continuous Bag of Words. Now CBOW predicts a target word based on its surrounding context words. Well, skip-gram does the",
      "opposite, predicting the context words given a target word. Now another popular method is called GLOVE. Also an acronym, that one stands for Global Vectors for Word Representation. That was created at Stanford University in 2014 and that uses co-occurrence statistics to create word vectors. Now, these models, they differ in their approach. What's a vec that focuses on learning from the immediate context around each word? While glove takes a broader view by analyzing how often words appear",
      "together across the entire corpus, then uses this information to create word vectors. Now, while these two word embedding models continue to be valuable tools in NLP, the field has seen some significant advances with the emergence of new tech, particularly transformers. While traditional word embeddings assign a fixed vector to each word, transformer models use a different type of embedding, and it's called a contextual based embedding. Now, contextual based embeddings are where the",
      "representation of a word changes based on its surrounding context. So, for example, in a transformer model, the word bank would have different representations in the sentence I'm going to the bank to deposit money and I'm sitting on the bank of a river. This context sensitivity allows these models to capture more nuanced meanings and relationships between words, which has led to all sorts of improvements in the various fields of NLP tasks. So that's word embeddings, from simple numeric vectors to",
      "complex representations. Word embeddings have revolutionized how machines understand and process human language. Proving that transforming words into numbers is indeed a powerful tool for making sense of our linguistic world."
    ],
    "chunk_count": 15,
    "content_id": "efb338dd-2c9d-4f58-bec3-aa64c0475e70",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554958"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=jmmW0F0biz0": {
    "title": "Neural Networks Explained in 5 minutes",
    "url": "https://www.youtube.com/watch?v=jmmW0F0biz0",
    "description": "Learn more about watsonx: https://ibm.biz/BdvxRs\n\nNeural networks reflect the behavior of the human brain, allowing computer programs to recognize patterns and solve common problems in the fields of AI, machine learning, and deep learning. Master Inventor, Martin Keen, makes some important points about neural networks and does it all in 5 minutes.\n\n#Software #ITModernization #NeuralNetworks #DataFabric #lightboard #IBM",
    "duration": 271,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Here are five things to know about neural networks in under five minutes. Number one: neural networks are composed of node layers. There is an input node layer, there is a hidden layer, and there is an output layer. And these neural networks reflect the behavior of the human brain, allowing computer programs to recognize patterns and solve common problems in the fields of AI and deep learning. In fact, we should be describing this as an artificial neural network, or anANN, to distinguish it from the very un-artificial neural network that's operating in our heads. Now, think of each node, or artificial neuron, as its own linear regression model. That's number two. Linear regression is a mathematical model that's used to predict future events. The weights of the connections between the nodes determines how much influence each input has on the output. So each node is composed of input data, weights, a bias, or a threshold, and then an output. Now data is passed from one layer in the neural network to the next in what is known as a feed forward network -- number three. To illustrate this, let's consider what a single node in our neural network might look like to decide -- should we go surfing. The decision to go or not is our predicted outcome or known as our yhat. Let's assume there are three factors influencing our decision. Are the wave's good, 1 for yes or 0 for no.  The waves are pumping, so x1 equals 1, 1 for yes.  Is the lineup empty, well unfortunately not, so that gets a 0. And then let's consider is it shark-free out there, that's x3 and yes, no shark attacks have been reported. Now to each decision we assign a weight based on its importance on a scale of 0 to 5. So let's say that the waves, we're going to score that one, eh, so this is important, let's give it a 5. And for the crowds, that's w2. Eh, not so important, we'll give that a 2. And sharks, well, we'll give that a score of a 4. Now we can plug in these values into the formula to get the desired output. So yhat equals (1 * 5) + (0 * 2) + (1 * 4), then minus 3, that's our threshold, and that gives us a value of 6. Six is greater than 0, so the output of this node is 1 -- we're going surfing. And if we adjust the weights or the threshold, we can achieve different outcomes. Number four. Well, yes, but but but number four, neural networks rely on training data to learn and improve their accuracy over time. We leverage supervised learning on labeled datasets to train the algorithm. As we train the model, we want to evaluate its accuracy using something called a cost function. Ultimately, the goal is to minimize our cost function to ensure the correctness of fit for any given observation, and that happens as the model adjusts its weights and biases to fit the training data set, through what's known as gradient descent, allowing the model to determine the direction to take to reduce errors, or more specifically, minimize the cost function. And then finally, number five: there are multiple types of neural networks beyond the feed forward neural network that we've described here. For example, there are convolutional neural networks, known as CNNs, which have a unique architecture that's well suited for identifying patterns like image recognition. And there are recurrent neural networks, or RNNs, which are identified by their feedback loops and RNNs are primarily leveraged using time series data to make predictions about future events like sales forecasting.  So, five things in five minutes. To learn more about neural networks, check out these videos. Thanks for watching. If you haveany questions, please drop us a line below. And if you want to see more videos like this in the future, please Like and Subscribe.",
    "chunks": [
      "Kind: captions Language: en Here are five things to know about neural networks in under five minutes. Number one: neural networks are composed of node layers. There is an input node layer, there is a hidden layer, and there is an output layer. And these neural networks reflect the behavior of the human brain, allowing computer programs to recognize patterns and solve common problems in the fields of AI and deep learning. In fact, we should be describing this as an artificial neural network, or",
      "anANN, to distinguish it from the very un-artificial neural network that's operating in our heads. Now, think of each node, or artificial neuron, as its own linear regression model. That's number two. Linear regression is a mathematical model that's used to predict future events. The weights of the connections between the nodes determines how much influence each input has on the output. So each node is composed of input data, weights, a bias, or a threshold, and then an output. Now data is passed",
      "from one layer in the neural network to the next in what is known as a feed forward network -- number three. To illustrate this, let's consider what a single node in our neural network might look like to decide -- should we go surfing. The decision to go or not is our predicted outcome or known as our yhat. Let's assume there are three factors influencing our decision. Are the wave's good, 1 for yes or 0 for no. The waves are pumping, so x1 equals 1, 1 for yes. Is the lineup empty, well",
      "unfortunately not, so that gets a 0. And then let's consider is it shark-free out there, that's x3 and yes, no shark attacks have been reported. Now to each decision we assign a weight based on its importance on a scale of 0 to 5. So let's say that the waves, we're going to score that one, eh, so this is important, let's give it a 5. And for the crowds, that's w2. Eh, not so important, we'll give that a 2. And sharks, well, we'll give that a score of a 4. Now we can plug in these values into the",
      "formula to get the desired output. So yhat equals (1 * 5) + (0 * 2) + (1 * 4), then minus 3, that's our threshold, and that gives us a value of 6. Six is greater than 0, so the output of this node is 1 -- we're going surfing. And if we adjust the weights or the threshold, we can achieve different outcomes. Number four. Well, yes, but but but number four, neural networks rely on training data to learn and improve their accuracy over time. We leverage supervised learning on labeled datasets to",
      "train the algorithm. As we train the model, we want to evaluate its accuracy using something called a cost function. Ultimately, the goal is to minimize our cost function to ensure the correctness of fit for any given observation, and that happens as the model adjusts its weights and biases to fit the training data set, through what's known as gradient descent, allowing the model to determine the direction to take to reduce errors, or more specifically, minimize the cost function. And then",
      "finally, number five: there are multiple types of neural networks beyond the feed forward neural network that we've described here. For example, there are convolutional neural networks, known as CNNs, which have a unique architecture that's well suited for identifying patterns like image recognition. And there are recurrent neural networks, or RNNs, which are identified by their feedback loops and RNNs are primarily leveraged using time series data to make predictions about future events like",
      "sales forecasting. So, five things in five minutes. To learn more about neural networks, check out these videos. Thanks for watching. If you haveany questions, please drop us a line below. And if you want to see more videos like this in the future, please Like and Subscribe."
    ],
    "chunk_count": 8,
    "content_id": "79a43cb5-eddf-47d3-8ef6-5c29d0eda708",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554961"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=QzY57FaENXg": {
    "title": "What are Convolutional Neural Networks (CNNs)?",
    "url": "https://www.youtube.com/watch?v=QzY57FaENXg",
    "description": "Ready to start your career in AI? Begin with this certificate → https://ibm.biz/BdKU7G\nLearn more about watsonx → https://ibm.biz/BdvxDe\n\nConvolutional neural networks, or CNNs, are distinguished from other neural networks by their superior performance with image, speech, or audio signal inputs. But how exactly do they work?\n\nIn this lightboard video, Martin Keen with IBM, explains how this deep learning algorithm operates to enable machines to view the world as humans do.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdKU7g\n\n#ConvolutionalNeuralNetworks #NeuralNetworks #AI",
    "duration": 380,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en okay pop quiz okay pop quiz okay pop quiz what what what am i drawing am i drawing am i drawing i'm gonna make i'm gonna make i'm gonna make three predictions here three predictions here three predictions here firstly firstly firstly you can go to a house you'd be right you can go to a house you'd be right you can go to a house you'd be right secondly secondly secondly that it just came really easily to you that it just came really easily to you that it just came really easily to you as effortless and thirdly you're as effortless and thirdly you're as effortless and thirdly you're thinking that i'm not much of an artist thinking that i'm not much of an artist thinking that i'm not much of an artist and you'd be right on all counts there and you'd be right on all counts there and you'd be right on all counts there but how can we look at this set of but how can we look at this set of but how can we look at this set of geometric shapes and think oh geometric shapes and think oh geometric shapes and think oh a house a house a house if you live in a house i bet it looks if you live in a house i bet it looks if you live in a house i bet it looks nothing like this well that ability to nothing like this well that ability to nothing like this well that ability to perform object identification that comes perform object identification that comes perform object identification that comes so easily to us so easily to us so easily to us does not come so easily to a computer does not come so easily to a computer does not come so easily to a computer but that is where we can apply something but that is where we can apply something but that is where we can apply something called convolutional called convolutional called convolutional neural networks neural networks neural networks to the problem to the problem to the problem now a convolutional neural network or a now a convolutional neural network or a now a convolutional neural network or a is a area of deep learning that is a area of deep learning that is a area of deep learning that specializes in pattern recognition specializes in pattern recognition specializes in pattern recognition my name is martin keane and i work in my name is martin keane and i work in my name is martin keane and i work in the ibm garage at ibm the ibm garage at ibm the ibm garage at ibm now let's take a look now let's take a look now let's take a look at how cnn works at a high level at how cnn works at a high level at how cnn works at a high level well let's break it down cnn well let's break it down cnn well let's break it down cnn convolutional neural network well let's convolutional neural network well let's convolutional neural network well let's start with the start with the start with the artificial neural network part artificial neural network part artificial neural network part this is a standard network that consists this is a standard network that consists this is a standard network that consists of multiple layers that are of multiple layers that are of multiple layers that are interconnected interconnected interconnected and each layer receives some input and each layer receives some input and each layer receives some input transforms that input to something else transforms that input to something else transforms that input to something else and passes it as output to the next and passes it as output to the next and passes it as output to the next layer that's how neural networks work layer that's how neural networks work layer that's how neural networks work and cnn is a particular part of the and cnn is a particular part of the and cnn is a particular part of the neural network or a section of layers neural network or a section of layers neural network or a section of layers let's say it's these three layers here let's say it's these three layers here let's say it's these three layers here and within these layers we have and within these layers we have and within these layers we have something called filters something called filters something called filters and it's the filters that perform and it's the filters that perform and it's the filters that perform pattern recognition that cnn is so good pattern recognition that cnn is so good pattern recognition that cnn is so good let's apply this to our house example let's apply this to our house example let's apply this to our house example now if this house were an actual image now if this house were an actual image now if this house were an actual image it would be it would be it would be a series of pixels just like any image and if we zoom in on a particular part and if we zoom in on a particular part and if we zoom in on a particular part of this house let's say we zoom in of this house let's say we zoom in of this house let's say we zoom in around here then we would get well around here then we would get well around here then we would get well the window and what we're saying here is that a and what we're saying here is that a and what we're saying here is that a window consists of some perfectly window consists of some perfectly window consists of some perfectly straight lines straight lines straight lines almost perfectly straight lines but you almost perfectly straight lines but you almost perfectly straight lines but you know a window doesn't need to look like know a window doesn't need to look like know a window doesn't need to look like that a window could equally kind of look that a window could equally kind of look that a window could equally kind of look like this and we would still say it was like this and we would still say it was like this and we would still say it was a window a window a window the cool thing about cnn is that using the cool thing about cnn is that using the cool thing about cnn is that using filters cnn could also say that these filters cnn could also say that these filters cnn could also say that these two objects represent the same thing two objects represent the same thing two objects represent the same thing the way they do that then is through the the way they do that then is through the the way they do that then is through the application of these filters so let's application of these filters so let's application of these filters so let's take a look at how that works take a look at how that works take a look at how that works now a filter is basically now a filter is basically now a filter is basically just a three by three block and within just a three by three block and within just a three by three block and within that block we can specify a pattern to that block we can specify a pattern to that block we can specify a pattern to look for look for look for so we could say let's look for so we could say let's look for so we could say let's look for pattern like this a right angle pattern like this a right angle pattern like this a right angle in our image in our image in our image so we do is we take this filter and it's so we do is we take this filter and it's so we do is we take this filter and it's a three by three block here we will a three by three block here we will a three by three block here we will analyze the equivalent three by three analyze the equivalent three by three analyze the equivalent three by three block up here as well block up here as well block up here as well we'll look at first of all these first we'll look at first of all these first we'll look at first of all these first group of three by three pixels and we group of three by three pixels and we group of three by three pixels and we will see how close are they to the will see how close are they to the will see how close are they to the filter shape filter shape filter shape and we'll give that a numeric score and we'll give that a numeric score and we'll give that a numeric score then we will move across one column to then we will move across one column to then we will move across one column to the right and look at the next three by the right and look at the next three by the right and look at the next three by three block of pixels and score how three block of pixels and score how three block of pixels and score how close they are to the filter shape and close they are to the filter shape and close they are to the filter shape and we will continue to slide over or we will continue to slide over or we will continue to slide over or convolve convolve convolve over all of these pixel layers until we over all of these pixel layers until we over all of these pixel layers until we have mapped every three by three block have mapped every three by three block have mapped every three by three block now that's just for one filter but what now that's just for one filter but what now that's just for one filter but what that will give us is an array of numbers that will give us is an array of numbers that will give us is an array of numbers that say how closely an image that say how closely an image that say how closely an image matches our filter matches our filter matches our filter but we can add more filters so we could but we can add more filters so we could but we can add more filters so we could add another three by three filter here add another three by three filter here add another three by three filter here and perhaps this one looks for a shape and perhaps this one looks for a shape and perhaps this one looks for a shape like this like this like this and we could add a third filter here and we could add a third filter here and we could add a third filter here and perhaps this looks for a different and perhaps this looks for a different and perhaps this looks for a different kind of right angle shape if we take the numeric arrays from all if we take the numeric arrays from all if we take the numeric arrays from all of these filters and combine them of these filters and combine them of these filters and combine them together in a process called pooling together in a process called pooling together in a process called pooling then we have a much better understanding then we have a much better understanding then we have a much better understanding what is contained within this series of what is contained within this series of what is contained within this series of pixels pixels pixels now that's just the first layer of the now that's just the first layer of the now that's just the first layer of the cnn and as we go deeper into the neural cnn and as we go deeper into the neural cnn and as we go deeper into the neural network the filters become more abstract network the filters become more abstract network the filters become more abstract or they can do more or they can do more or they can do more the second layer of filters perhaps can the second layer of filters perhaps can the second layer of filters perhaps can perform tasks like basic object perform tasks like basic object perform tasks like basic object recognition recognition recognition so we could have filters here that might so we could have filters here that might so we could have filters here that might be able to recognize the presence of a be able to recognize the presence of a be able to recognize the presence of a window window window or the presence of a door or the or the presence of a door or the or the presence of a door or the presence presence presence of a roof of a roof of a roof and as we go deeper into the cnn to the and as we go deeper into the cnn to the and as we go deeper into the cnn to the next layer well maybe these filters can next layer well maybe these filters can next layer well maybe these filters can perform even more abstract tasks like perform even more abstract tasks like perform even more abstract tasks like being able to determine whether we're being able to determine whether we're being able to determine whether we're looking at a house looking at a house looking at a house or we're looking at an apartment or we're looking at an apartment or we're looking at an apartment or whether we're looking at a skyscraper so you can see the application of these so you can see the application of these so you can see the application of these filters filters filters increases as we go through the network increases as we go through the network increases as we go through the network and can perform more and more tasks and and can perform more and more tasks and and can perform more and more tasks and that's a very high level basic overview that's a very high level basic overview that's a very high level basic overview of what cnn is it has a ton of business of what cnn is it has a ton of business of what cnn is it has a ton of business applications think of ocr for example applications think of ocr for example applications think of ocr for example for understanding handwritten documents for understanding handwritten documents for understanding handwritten documents think of visual recognition and facial think of visual recognition and facial think of visual recognition and facial detection and visual search detection and visual search detection and visual search think of medical imagery and looking at think of medical imagery and looking at think of medical imagery and looking at that and determining what is being shown that and determining what is being shown that and determining what is being shown in an imaging scan and even think of the in an imaging scan and even think of the in an imaging scan and even think of the fact that we can apply a cnn to perform fact that we can apply a cnn to perform fact that we can apply a cnn to perform object identification for object identification for object identification for badly drawn houses badly drawn houses badly drawn houses if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching",
    "chunks": [
      "Kind: captions Language: en okay pop quiz okay pop quiz okay pop quiz what what what am i drawing am i drawing am i drawing i'm gonna make i'm gonna make i'm gonna make three predictions here three predictions here three predictions here firstly firstly firstly you can go to a house you'd be right you can go to a house you'd be right you can go to a house you'd be right secondly secondly secondly that it just came really easily to you that it just came really easily to you that it just came",
      "really easily to you as effortless and thirdly you're as effortless and thirdly you're as effortless and thirdly you're thinking that i'm not much of an artist thinking that i'm not much of an artist thinking that i'm not much of an artist and you'd be right on all counts there and you'd be right on all counts there and you'd be right on all counts there but how can we look at this set of but how can we look at this set of but how can we look at this set of geometric shapes and think oh geometric",
      "shapes and think oh geometric shapes and think oh a house a house a house if you live in a house i bet it looks if you live in a house i bet it looks if you live in a house i bet it looks nothing like this well that ability to nothing like this well that ability to nothing like this well that ability to perform object identification that comes perform object identification that comes perform object identification that comes so easily to us so easily to us so easily to us does not come so easily",
      "to a computer does not come so easily to a computer does not come so easily to a computer but that is where we can apply something but that is where we can apply something but that is where we can apply something called convolutional called convolutional called convolutional neural networks neural networks neural networks to the problem to the problem to the problem now a convolutional neural network or a now a convolutional neural network or a now a convolutional neural network or a is a area of",
      "deep learning that is a area of deep learning that is a area of deep learning that specializes in pattern recognition specializes in pattern recognition specializes in pattern recognition my name is martin keane and i work in my name is martin keane and i work in my name is martin keane and i work in the ibm garage at ibm the ibm garage at ibm the ibm garage at ibm now let's take a look now let's take a look now let's take a look at how cnn works at a high level at how cnn works at a high level",
      "at how cnn works at a high level well let's break it down cnn well let's break it down cnn well let's break it down cnn convolutional neural network well let's convolutional neural network well let's convolutional neural network well let's start with the start with the start with the artificial neural network part artificial neural network part artificial neural network part this is a standard network that consists this is a standard network that consists this is a standard network that consists",
      "of multiple layers that are of multiple layers that are of multiple layers that are interconnected interconnected interconnected and each layer receives some input and each layer receives some input and each layer receives some input transforms that input to something else transforms that input to something else transforms that input to something else and passes it as output to the next and passes it as output to the next and passes it as output to the next layer that's how neural networks work",
      "layer that's how neural networks work layer that's how neural networks work and cnn is a particular part of the and cnn is a particular part of the and cnn is a particular part of the neural network or a section of layers neural network or a section of layers neural network or a section of layers let's say it's these three layers here let's say it's these three layers here let's say it's these three layers here and within these layers we have and within these layers we have and within these",
      "layers we have something called filters something called filters something called filters and it's the filters that perform and it's the filters that perform and it's the filters that perform pattern recognition that cnn is so good pattern recognition that cnn is so good pattern recognition that cnn is so good let's apply this to our house example let's apply this to our house example let's apply this to our house example now if this house were an actual image now if this house were an actual",
      "image now if this house were an actual image it would be it would be it would be a series of pixels just like any image and if we zoom in on a particular part and if we zoom in on a particular part and if we zoom in on a particular part of this house let's say we zoom in of this house let's say we zoom in of this house let's say we zoom in around here then we would get well around here then we would get well around here then we would get well the window and what we're saying here is that a and",
      "what we're saying here is that a and what we're saying here is that a window consists of some perfectly window consists of some perfectly window consists of some perfectly straight lines straight lines straight lines almost perfectly straight lines but you almost perfectly straight lines but you almost perfectly straight lines but you know a window doesn't need to look like know a window doesn't need to look like know a window doesn't need to look like that a window could equally kind of look",
      "that a window could equally kind of look that a window could equally kind of look like this and we would still say it was like this and we would still say it was like this and we would still say it was a window a window a window the cool thing about cnn is that using the cool thing about cnn is that using the cool thing about cnn is that using filters cnn could also say that these filters cnn could also say that these filters cnn could also say that these two objects represent the same thing two",
      "objects represent the same thing two objects represent the same thing the way they do that then is through the the way they do that then is through the the way they do that then is through the application of these filters so let's application of these filters so let's application of these filters so let's take a look at how that works take a look at how that works take a look at how that works now a filter is basically now a filter is basically now a filter is basically just a three by three",
      "block and within just a three by three block and within just a three by three block and within that block we can specify a pattern to that block we can specify a pattern to that block we can specify a pattern to look for look for look for so we could say let's look for so we could say let's look for so we could say let's look for pattern like this a right angle pattern like this a right angle pattern like this a right angle in our image in our image in our image so we do is we take this filter",
      "and it's so we do is we take this filter and it's so we do is we take this filter and it's a three by three block here we will a three by three block here we will a three by three block here we will analyze the equivalent three by three analyze the equivalent three by three analyze the equivalent three by three block up here as well block up here as well block up here as well we'll look at first of all these first we'll look at first of all these first we'll look at first of all these first group",
      "of three by three pixels and we group of three by three pixels and we group of three by three pixels and we will see how close are they to the will see how close are they to the will see how close are they to the filter shape filter shape filter shape and we'll give that a numeric score and we'll give that a numeric score and we'll give that a numeric score then we will move across one column to then we will move across one column to then we will move across one column to the right and look at",
      "the next three by the right and look at the next three by the right and look at the next three by three block of pixels and score how three block of pixels and score how three block of pixels and score how close they are to the filter shape and close they are to the filter shape and close they are to the filter shape and we will continue to slide over or we will continue to slide over or we will continue to slide over or convolve convolve convolve over all of these pixel layers until we over all",
      "of these pixel layers until we over all of these pixel layers until we have mapped every three by three block have mapped every three by three block have mapped every three by three block now that's just for one filter but what now that's just for one filter but what now that's just for one filter but what that will give us is an array of numbers that will give us is an array of numbers that will give us is an array of numbers that say how closely an image that say how closely an image that say",
      "how closely an image matches our filter matches our filter matches our filter but we can add more filters so we could but we can add more filters so we could but we can add more filters so we could add another three by three filter here add another three by three filter here add another three by three filter here and perhaps this one looks for a shape and perhaps this one looks for a shape and perhaps this one looks for a shape like this like this like this and we could add a third filter here",
      "and we could add a third filter here and we could add a third filter here and perhaps this looks for a different and perhaps this looks for a different and perhaps this looks for a different kind of right angle shape if we take the numeric arrays from all if we take the numeric arrays from all if we take the numeric arrays from all of these filters and combine them of these filters and combine them of these filters and combine them together in a process called pooling together in a process called",
      "pooling together in a process called pooling then we have a much better understanding then we have a much better understanding then we have a much better understanding what is contained within this series of what is contained within this series of what is contained within this series of pixels pixels pixels now that's just the first layer of the now that's just the first layer of the now that's just the first layer of the cnn and as we go deeper into the neural cnn and as we go deeper into the",
      "neural cnn and as we go deeper into the neural network the filters become more abstract network the filters become more abstract network the filters become more abstract or they can do more or they can do more or they can do more the second layer of filters perhaps can the second layer of filters perhaps can the second layer of filters perhaps can perform tasks like basic object perform tasks like basic object perform tasks like basic object recognition recognition recognition so we could have",
      "filters here that might so we could have filters here that might so we could have filters here that might be able to recognize the presence of a be able to recognize the presence of a be able to recognize the presence of a window window window or the presence of a door or the or the presence of a door or the or the presence of a door or the presence presence presence of a roof of a roof of a roof and as we go deeper into the cnn to the and as we go deeper into the cnn to the and as we go deeper",
      "into the cnn to the next layer well maybe these filters can next layer well maybe these filters can next layer well maybe these filters can perform even more abstract tasks like perform even more abstract tasks like perform even more abstract tasks like being able to determine whether we're being able to determine whether we're being able to determine whether we're looking at a house looking at a house looking at a house or we're looking at an apartment or we're looking at an apartment or we're",
      "looking at an apartment or whether we're looking at a skyscraper so you can see the application of these so you can see the application of these so you can see the application of these filters filters filters increases as we go through the network increases as we go through the network increases as we go through the network and can perform more and more tasks and and can perform more and more tasks and and can perform more and more tasks and that's a very high level basic overview that's a very",
      "high level basic overview that's a very high level basic overview of what cnn is it has a ton of business of what cnn is it has a ton of business of what cnn is it has a ton of business applications think of ocr for example applications think of ocr for example applications think of ocr for example for understanding handwritten documents for understanding handwritten documents for understanding handwritten documents think of visual recognition and facial think of visual recognition and facial",
      "think of visual recognition and facial detection and visual search detection and visual search detection and visual search think of medical imagery and looking at think of medical imagery and looking at think of medical imagery and looking at that and determining what is being shown that and determining what is being shown that and determining what is being shown in an imaging scan and even think of the in an imaging scan and even think of the in an imaging scan and even think of the fact that we",
      "can apply a cnn to perform fact that we can apply a cnn to perform fact that we can apply a cnn to perform object identification for object identification for object identification for badly drawn houses badly drawn houses badly drawn houses if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like",
      "this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching"
    ],
    "chunk_count": 29,
    "content_id": "899a0706-963d-4434-bc0d-5c17faa7439e",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554964"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=TpMIssRdhco": {
    "title": "What are GANs (Generative Adversarial Networks)?",
    "url": "https://www.youtube.com/watch?v=TpMIssRdhco",
    "description": "Learn more about watsonx: https://ibm.biz/BdvxDJ\n\nGenerative Adversarial Networks (GANs) pit two different deep learning models against each other in a game. In this lightboard video, Martin Keen with IBM, explains how this competition between the generator and discriminator can be utilized to both create and detect how you can benefit from the competition.\n\n#GAN #GenerativeAdversarialNetworks #AI #watsonX",
    "duration": 502,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en one of my favorite machine learning one of my favorite machine learning one of my favorite machine learning algorithms is generative adversarial algorithms is generative adversarial algorithms is generative adversarial networks or Gan it pits two AI models networks or Gan it pits two AI models networks or Gan it pits two AI models off against each other hence the off against each other hence the off against each other hence the adversarial part now most machine adversarial part now most machine adversarial part now most machine learning models are used to generate a learning models are used to generate a learning models are used to generate a prediction so we start with some input prediction so we start with some input prediction so we start with some input training training training data and we feed that into our mod model data and we feed that into our mod model data and we feed that into our mod model our model then makes a prediction in the our model then makes a prediction in the our model then makes a prediction in the form of form of form of output and we can compare the predicted output and we can compare the predicted output and we can compare the predicted output with the expected output from the output with the expected output from the output with the expected output from the training data set and then based upon training data set and then based upon training data set and then based upon that expected output and the actual that expected output and the actual that expected output and the actual predicted output we can figure out how predicted output we can figure out how predicted output we can figure out how we should update our model to create we should update our model to create we should update our model to create better outputs that is an example of supervised that is an example of supervised that is an example of supervised learning a gan is an example of learning a gan is an example of learning a gan is an example of unsupervised learning it effectively unsupervised learning it effectively unsupervised learning it effectively supervises itself and it consists of two supervises itself and it consists of two supervises itself and it consists of two sub models so we have a generator sub model and we have a discriminator sub model now the generator's job is to model now the generator's job is to model now the generator's job is to create fake input or fake samples and the discriminator job is to samples and the discriminator job is to samples and the discriminator job is to take a given sample and figure out if it take a given sample and figure out if it take a given sample and figure out if it is a fake sample or if it's a real is a fake sample or if it's a real is a fake sample or if it's a real sample from the domain and therein lies the adversarial domain and therein lies the adversarial domain and therein lies the adversarial nature of this we have a a generator nature of this we have a a generator nature of this we have a a generator creating fake samples and sending them creating fake samples and sending them creating fake samples and sending them to a discriminator the discriminator is to a discriminator the discriminator is to a discriminator the discriminator is taking a look at a given sample and taking a look at a given sample and taking a look at a given sample and figuring out is this a fake sample from figuring out is this a fake sample from figuring out is this a fake sample from the generator or is this a real sample the generator or is this a real sample the generator or is this a real sample from the domain set now this sort of from the domain set now this sort of from the domain set now this sort of scenario is often applied in image scenario is often applied in image scenario is often applied in image generation there are images all over the generation there are images all over the generation there are images all over the Internet of generators that have been Internet of generators that have been Internet of generators that have been used to create fake 3D models fake faces used to create fake 3D models fake faces used to create fake 3D models fake faces fake cats and so forth so this really fake cats and so forth so this really fake cats and so forth so this really works by the generator iterating through works by the generator iterating through works by the generator iterating through a number of different cycles of creating a number of different cycles of creating a number of different cycles of creating samples updating its model and so forth samples updating its model and so forth samples updating its model and so forth until it can create a sample that is so until it can create a sample that is so until it can create a sample that is so convincing that it can full a convincing that it can full a convincing that it can full a discriminator and also fool as humans as discriminator and also fool as humans as discriminator and also fool as humans as well so let's let's take a an example of well so let's let's take a an example of well so let's let's take a an example of how this works with let's say a flower how this works with let's say a flower how this works with let's say a flower so we are going to train our generator so we are going to train our generator so we are going to train our generator to create really convincing fake flowers to create really convincing fake flowers to create really convincing fake flowers and the way that we start by doing this and the way that we start by doing this and the way that we start by doing this is we need to first of all train our is we need to first of all train our is we need to first of all train our discriminator model to recognize what a discriminator model to recognize what a discriminator model to recognize what a picture of flower looks like so our picture of flower looks like so our picture of flower looks like so our domain is lots of pictures of flowers domain is lots of pictures of flowers domain is lots of pictures of flowers and we will be feeding this into the and we will be feeding this into the and we will be feeding this into the discriminator model and telling it to discriminator model and telling it to discriminator model and telling it to look at all of the attributes that make look at all of the attributes that make look at all of the attributes that make up those flower images take a look at up those flower images take a look at up those flower images take a look at the colors the shading the shapes and so the colors the shading the shapes and so the colors the shading the shapes and so forth and when our discriminator gets forth and when our discriminator gets forth and when our discriminator gets good at recognizing real flowers then good at recognizing real flowers then good at recognizing real flowers then we'll feed in some shapes that are not we'll feed in some shapes that are not we'll feed in some shapes that are not flowers at all flowers at all flowers at all and make sure that it can discriminate and make sure that it can discriminate and make sure that it can discriminate those as being not flowers now this those as being not flowers now this those as being not flowers now this whole time our generator here was frozen whole time our generator here was frozen whole time our generator here was frozen it wasn't doing anything but when our it wasn't doing anything but when our it wasn't doing anything but when our discriminator gets good enough at discriminator gets good enough at discriminator gets good enough at recognizing things from our domain then recognizing things from our domain then recognizing things from our domain then we apply our generator to create start we apply our generator to create start we apply our generator to create start creating fake versions of those things creating fake versions of those things creating fake versions of those things so our generator is going to take a so our generator is going to take a so our generator is going to take a random input vector and it is going to random input vector and it is going to random input vector and it is going to use that to create use that to create use that to create its own fake flower now this fake flower its own fake flower now this fake flower its own fake flower now this fake flower image is sent to the discriminator and image is sent to the discriminator and image is sent to the discriminator and now the discriminator has a decision to now the discriminator has a decision to now the discriminator has a decision to make is that image of a flower the real make is that image of a flower the real make is that image of a flower the real thing from the domain or is it a fake thing from the domain or is it a fake thing from the domain or is it a fake from the from the from the generator now the answer is revealed to generator now the answer is revealed to generator now the answer is revealed to both the generator and the discriminator both the generator and the discriminator both the generator and the discriminator the flower was fake and based upon that the flower was fake and based upon that the flower was fake and based upon that the generator and discriminator will the generator and discriminator will the generator and discriminator will change their behavior this is a zero sum change their behavior this is a zero sum change their behavior this is a zero sum game there's always a winner and a loser game there's always a winner and a loser game there's always a winner and a loser the winner gets to remain blissfully the winner gets to remain blissfully the winner gets to remain blissfully unchanged their model doesn't change at unchanged their model doesn't change at unchanged their model doesn't change at all whereas the loser has to update all whereas the loser has to update all whereas the loser has to update their model so if the discriminator their model so if the discriminator their model so if the discriminator successfully spotted that this flower successfully spotted that this flower successfully spotted that this flower was a fake image then the discriminator was a fake image then the discriminator was a fake image then the discriminator remains unchanged but the generator will remains unchanged but the generator will remains unchanged but the generator will need to change its model to generate need to change its model to generate need to change its model to generate better fakes whereas if the reverse is better fakes whereas if the reverse is better fakes whereas if the reverse is true and the generator is creating true and the generator is creating true and the generator is creating something that fooled the discriminator something that fooled the discriminator something that fooled the discriminator the discriminator model will need to be the discriminator model will need to be the discriminator model will need to be updated itself in order updated itself in order updated itself in order to better be able to tell where we have to better be able to tell where we have to better be able to tell where we have a a fake sample coming in so it's fooled a a fake sample coming in so it's fooled a a fake sample coming in so it's fooled less easily and that's basically uh how less easily and that's basically uh how less easily and that's basically uh how these things work and we go through many these things work and we go through many these things work and we go through many many iterations of this until the many iterations of this until the many iterations of this until the generator gets so good that the generator gets so good that the generator gets so good that the discriminator can no longer pick out its discriminator can no longer pick out its discriminator can no longer pick out its fakes and there we have built a very fakes and there we have built a very fakes and there we have built a very successful generator to do whatever it successful generator to do whatever it successful generator to do whatever it is we wanted it to is we wanted it to is we wanted it to do now often in terms of images the do now often in terms of images the do now often in terms of images the generator and the discriminator are generator and the discriminator are generator and the discriminator are implemented as cnns these are implemented as cnns these are implemented as cnns these are convolutional neural networks cnns are convolutional neural networks cnns are convolutional neural networks cnns are great way of recognizing patterns in great way of recognizing patterns in great way of recognizing patterns in image data and entering into sort of the image data and entering into sort of the image data and entering into sort of the area of object identification we have a area of object identification we have a area of object identification we have a whole separate video on cnns but they're whole separate video on cnns but they're whole separate video on cnns but they're a great way to to Really implement the a great way to to Really implement the a great way to to Really implement the generator and discriminator in this generator and discriminator in this generator and discriminator in this scenario but the the whole process of a scenario but the the whole process of a scenario but the the whole process of a gan isn't just to create really good gan isn't just to create really good gan isn't just to create really good fake flowers or fake cat images for the fake flowers or fake cat images for the fake flowers or fake cat images for the internet you can apply it to all sorts internet you can apply it to all sorts internet you can apply it to all sorts of use cases so take for example video of use cases so take for example video of use cases so take for example video frame frame frame prediction if we feed in a particular prediction if we feed in a particular prediction if we feed in a particular frame of video from a camera we can use frame of video from a camera we can use frame of video from a camera we can use a gan to predict what the next frame in a gan to predict what the next frame in a gan to predict what the next frame in the sequence will look like this is a the sequence will look like this is a the sequence will look like this is a great way to be able to predict what's great way to be able to predict what's great way to be able to predict what's going to happen in the immediate future going to happen in the immediate future going to happen in the immediate future and might be used for example in a and might be used for example in a and might be used for example in a surveillance system if we can figure out surveillance system if we can figure out surveillance system if we can figure out what is likely to happen next we can what is likely to happen next we can what is likely to happen next we can take some action based upon that there's take some action based upon that there's take some action based upon that there's also other things you can do like image also other things you can do like image also other things you can do like image enhancement so if we have a kind of a enhancement so if we have a kind of a enhancement so if we have a kind of a low resolution image we can use a began low resolution image we can use a began low resolution image we can use a began to create a much higher resolution to create a much higher resolution to create a much higher resolution version of that image by figuring out version of that image by figuring out version of that image by figuring out what each individual pixel is and then what each individual pixel is and then what each individual pixel is and then creating a higher resolution version of creating a higher resolution version of creating a higher resolution version of that and we can even go as far as using that and we can even go as far as using that and we can even go as far as using this for things that are not related to this for things that are not related to this for things that are not related to images at all like encryption where we images at all like encryption where we images at all like encryption where we can create a secure encryption algorithm can create a secure encryption algorithm can create a secure encryption algorithm that can be decrypted and encrypted by that can be decrypted and encrypted by that can be decrypted and encrypted by the sender and receiver but cannot be the sender and receiver but cannot be the sender and receiver but cannot be easily intercepted Again by going easily intercepted Again by going easily intercepted Again by going through these Gan iterations to create a through these Gan iterations to create a through these Gan iterations to create a really good really good really good generator so that's Gan it's the battle generator so that's Gan it's the battle generator so that's Gan it's the battle of the Bots where you can take your of the Bots where you can take your of the Bots where you can take your young impressionable and unchanged young impressionable and unchanged young impressionable and unchanged generator and turn it into a master of generator and turn it into a master of generator and turn it into a master of forgery if you have any questions please forgery if you have any questions please forgery if you have any questions please drop us a line below and if you want to drop us a line below and if you want to drop us a line below and if you want to see more videos like this in the future see more videos like this in the future see more videos like this in the future please like And subscribe",
    "chunks": [
      "Kind: captions Language: en one of my favorite machine learning one of my favorite machine learning one of my favorite machine learning algorithms is generative adversarial algorithms is generative adversarial algorithms is generative adversarial networks or Gan it pits two AI models networks or Gan it pits two AI models networks or Gan it pits two AI models off against each other hence the off against each other hence the off against each other hence the adversarial part now most machine",
      "adversarial part now most machine adversarial part now most machine learning models are used to generate a learning models are used to generate a learning models are used to generate a prediction so we start with some input prediction so we start with some input prediction so we start with some input training training training data and we feed that into our mod model data and we feed that into our mod model data and we feed that into our mod model our model then makes a prediction in the our",
      "model then makes a prediction in the our model then makes a prediction in the form of form of form of output and we can compare the predicted output and we can compare the predicted output and we can compare the predicted output with the expected output from the output with the expected output from the output with the expected output from the training data set and then based upon training data set and then based upon training data set and then based upon that expected output and the actual that",
      "expected output and the actual that expected output and the actual predicted output we can figure out how predicted output we can figure out how predicted output we can figure out how we should update our model to create we should update our model to create we should update our model to create better outputs that is an example of supervised that is an example of supervised that is an example of supervised learning a gan is an example of learning a gan is an example of learning a gan is an example",
      "of unsupervised learning it effectively unsupervised learning it effectively unsupervised learning it effectively supervises itself and it consists of two supervises itself and it consists of two supervises itself and it consists of two sub models so we have a generator sub model and we have a discriminator sub model now the generator's job is to model now the generator's job is to model now the generator's job is to create fake input or fake samples and the discriminator job is to samples and",
      "the discriminator job is to samples and the discriminator job is to take a given sample and figure out if it take a given sample and figure out if it take a given sample and figure out if it is a fake sample or if it's a real is a fake sample or if it's a real is a fake sample or if it's a real sample from the domain and therein lies the adversarial domain and therein lies the adversarial domain and therein lies the adversarial nature of this we have a a generator nature of this we have a a",
      "generator nature of this we have a a generator creating fake samples and sending them creating fake samples and sending them creating fake samples and sending them to a discriminator the discriminator is to a discriminator the discriminator is to a discriminator the discriminator is taking a look at a given sample and taking a look at a given sample and taking a look at a given sample and figuring out is this a fake sample from figuring out is this a fake sample from figuring out is this a fake",
      "sample from the generator or is this a real sample the generator or is this a real sample the generator or is this a real sample from the domain set now this sort of from the domain set now this sort of from the domain set now this sort of scenario is often applied in image scenario is often applied in image scenario is often applied in image generation there are images all over the generation there are images all over the generation there are images all over the Internet of generators that have",
      "been Internet of generators that have been Internet of generators that have been used to create fake 3D models fake faces used to create fake 3D models fake faces used to create fake 3D models fake faces fake cats and so forth so this really fake cats and so forth so this really fake cats and so forth so this really works by the generator iterating through works by the generator iterating through works by the generator iterating through a number of different cycles of creating a number of",
      "different cycles of creating a number of different cycles of creating samples updating its model and so forth samples updating its model and so forth samples updating its model and so forth until it can create a sample that is so until it can create a sample that is so until it can create a sample that is so convincing that it can full a convincing that it can full a convincing that it can full a discriminator and also fool as humans as discriminator and also fool as humans as discriminator and",
      "also fool as humans as well so let's let's take a an example of well so let's let's take a an example of well so let's let's take a an example of how this works with let's say a flower how this works with let's say a flower how this works with let's say a flower so we are going to train our generator so we are going to train our generator so we are going to train our generator to create really convincing fake flowers to create really convincing fake flowers to create really convincing fake",
      "flowers and the way that we start by doing this and the way that we start by doing this and the way that we start by doing this is we need to first of all train our is we need to first of all train our is we need to first of all train our discriminator model to recognize what a discriminator model to recognize what a discriminator model to recognize what a picture of flower looks like so our picture of flower looks like so our picture of flower looks like so our domain is lots of pictures of",
      "flowers domain is lots of pictures of flowers domain is lots of pictures of flowers and we will be feeding this into the and we will be feeding this into the and we will be feeding this into the discriminator model and telling it to discriminator model and telling it to discriminator model and telling it to look at all of the attributes that make look at all of the attributes that make look at all of the attributes that make up those flower images take a look at up those flower images take a look",
      "at up those flower images take a look at the colors the shading the shapes and so the colors the shading the shapes and so the colors the shading the shapes and so forth and when our discriminator gets forth and when our discriminator gets forth and when our discriminator gets good at recognizing real flowers then good at recognizing real flowers then good at recognizing real flowers then we'll feed in some shapes that are not we'll feed in some shapes that are not we'll feed in some shapes that",
      "are not flowers at all flowers at all flowers at all and make sure that it can discriminate and make sure that it can discriminate and make sure that it can discriminate those as being not flowers now this those as being not flowers now this those as being not flowers now this whole time our generator here was frozen whole time our generator here was frozen whole time our generator here was frozen it wasn't doing anything but when our it wasn't doing anything but when our it wasn't doing anything",
      "but when our discriminator gets good enough at discriminator gets good enough at discriminator gets good enough at recognizing things from our domain then recognizing things from our domain then recognizing things from our domain then we apply our generator to create start we apply our generator to create start we apply our generator to create start creating fake versions of those things creating fake versions of those things creating fake versions of those things so our generator is going to",
      "take a so our generator is going to take a so our generator is going to take a random input vector and it is going to random input vector and it is going to random input vector and it is going to use that to create use that to create use that to create its own fake flower now this fake flower its own fake flower now this fake flower its own fake flower now this fake flower image is sent to the discriminator and image is sent to the discriminator and image is sent to the discriminator and now the",
      "discriminator has a decision to now the discriminator has a decision to now the discriminator has a decision to make is that image of a flower the real make is that image of a flower the real make is that image of a flower the real thing from the domain or is it a fake thing from the domain or is it a fake thing from the domain or is it a fake from the from the from the generator now the answer is revealed to generator now the answer is revealed to generator now the answer is revealed to both the",
      "generator and the discriminator both the generator and the discriminator both the generator and the discriminator the flower was fake and based upon that the flower was fake and based upon that the flower was fake and based upon that the generator and discriminator will the generator and discriminator will the generator and discriminator will change their behavior this is a zero sum change their behavior this is a zero sum change their behavior this is a zero sum game there's always a winner and",
      "a loser game there's always a winner and a loser game there's always a winner and a loser the winner gets to remain blissfully the winner gets to remain blissfully the winner gets to remain blissfully unchanged their model doesn't change at unchanged their model doesn't change at unchanged their model doesn't change at all whereas the loser has to update all whereas the loser has to update all whereas the loser has to update their model so if the discriminator their model so if the discriminator",
      "their model so if the discriminator successfully spotted that this flower successfully spotted that this flower successfully spotted that this flower was a fake image then the discriminator was a fake image then the discriminator was a fake image then the discriminator remains unchanged but the generator will remains unchanged but the generator will remains unchanged but the generator will need to change its model to generate need to change its model to generate need to change its model to",
      "generate better fakes whereas if the reverse is better fakes whereas if the reverse is better fakes whereas if the reverse is true and the generator is creating true and the generator is creating true and the generator is creating something that fooled the discriminator something that fooled the discriminator something that fooled the discriminator the discriminator model will need to be the discriminator model will need to be the discriminator model will need to be updated itself in order",
      "updated itself in order updated itself in order to better be able to tell where we have to better be able to tell where we have to better be able to tell where we have a a fake sample coming in so it's fooled a a fake sample coming in so it's fooled a a fake sample coming in so it's fooled less easily and that's basically uh how less easily and that's basically uh how less easily and that's basically uh how these things work and we go through many these things work and we go through many these",
      "things work and we go through many many iterations of this until the many iterations of this until the many iterations of this until the generator gets so good that the generator gets so good that the generator gets so good that the discriminator can no longer pick out its discriminator can no longer pick out its discriminator can no longer pick out its fakes and there we have built a very fakes and there we have built a very fakes and there we have built a very successful generator to do",
      "whatever it successful generator to do whatever it successful generator to do whatever it is we wanted it to is we wanted it to is we wanted it to do now often in terms of images the do now often in terms of images the do now often in terms of images the generator and the discriminator are generator and the discriminator are generator and the discriminator are implemented as cnns these are implemented as cnns these are implemented as cnns these are convolutional neural networks cnns are",
      "convolutional neural networks cnns are convolutional neural networks cnns are great way of recognizing patterns in great way of recognizing patterns in great way of recognizing patterns in image data and entering into sort of the image data and entering into sort of the image data and entering into sort of the area of object identification we have a area of object identification we have a area of object identification we have a whole separate video on cnns but they're whole separate video on cnns",
      "but they're whole separate video on cnns but they're a great way to to Really implement the a great way to to Really implement the a great way to to Really implement the generator and discriminator in this generator and discriminator in this generator and discriminator in this scenario but the the whole process of a scenario but the the whole process of a scenario but the the whole process of a gan isn't just to create really good gan isn't just to create really good gan isn't just to create",
      "really good fake flowers or fake cat images for the fake flowers or fake cat images for the fake flowers or fake cat images for the internet you can apply it to all sorts internet you can apply it to all sorts internet you can apply it to all sorts of use cases so take for example video of use cases so take for example video of use cases so take for example video frame frame frame prediction if we feed in a particular prediction if we feed in a particular prediction if we feed in a particular",
      "frame of video from a camera we can use frame of video from a camera we can use frame of video from a camera we can use a gan to predict what the next frame in a gan to predict what the next frame in a gan to predict what the next frame in the sequence will look like this is a the sequence will look like this is a the sequence will look like this is a great way to be able to predict what's great way to be able to predict what's great way to be able to predict what's going to happen in the",
      "immediate future going to happen in the immediate future going to happen in the immediate future and might be used for example in a and might be used for example in a and might be used for example in a surveillance system if we can figure out surveillance system if we can figure out surveillance system if we can figure out what is likely to happen next we can what is likely to happen next we can what is likely to happen next we can take some action based upon that there's take some action based",
      "upon that there's take some action based upon that there's also other things you can do like image also other things you can do like image also other things you can do like image enhancement so if we have a kind of a enhancement so if we have a kind of a enhancement so if we have a kind of a low resolution image we can use a began low resolution image we can use a began low resolution image we can use a began to create a much higher resolution to create a much higher resolution to create a much",
      "higher resolution version of that image by figuring out version of that image by figuring out version of that image by figuring out what each individual pixel is and then what each individual pixel is and then what each individual pixel is and then creating a higher resolution version of creating a higher resolution version of creating a higher resolution version of that and we can even go as far as using that and we can even go as far as using that and we can even go as far as using this for",
      "things that are not related to this for things that are not related to this for things that are not related to images at all like encryption where we images at all like encryption where we images at all like encryption where we can create a secure encryption algorithm can create a secure encryption algorithm can create a secure encryption algorithm that can be decrypted and encrypted by that can be decrypted and encrypted by that can be decrypted and encrypted by the sender and receiver but",
      "cannot be the sender and receiver but cannot be the sender and receiver but cannot be easily intercepted Again by going easily intercepted Again by going easily intercepted Again by going through these Gan iterations to create a through these Gan iterations to create a through these Gan iterations to create a really good really good really good generator so that's Gan it's the battle generator so that's Gan it's the battle generator so that's Gan it's the battle of the Bots where you can take",
      "your of the Bots where you can take your of the Bots where you can take your young impressionable and unchanged young impressionable and unchanged young impressionable and unchanged generator and turn it into a master of generator and turn it into a master of generator and turn it into a master of forgery if you have any questions please forgery if you have any questions please forgery if you have any questions please drop us a line below and if you want to drop us a line below and if you want to",
      "drop us a line below and if you want to see more videos like this in the future see more videos like this in the future see more videos like this in the future please like And subscribe"
    ],
    "chunk_count": 36,
    "content_id": "da10ab54-610f-4de2-a09b-f6557afc1535",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554967"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=b61DPVFX03I": {
    "title": "What is LSTM (Long Short Term Memory)?",
    "url": "https://www.youtube.com/watch?v=b61DPVFX03I",
    "description": "Learn about watsonx → https://ibm.biz/BdvxRB\n\nLong Short Term Memory, also known as LSTMs, are a special kind of Recurrent Neural Network, or RNN, architecture capable of learning long-term dependencies as well as a solution to the vanishing gradient problem that can occur when training traditional RNNs.\n\nIn this lightboard video, Martin Keen with IBM, breaks down why we need LSTMs to address the problem of long-term dependencies, how the cell state and its various gates help transfer relative information in a sequence chain, and a few key LSTM use cases. \n\n#LSTM #RNN #AI",
    "duration": 499,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en imagine you're at a murder mystery imagine you're at a murder mystery imagine you're at a murder mystery dinner dinner dinner right at the start the lord of the manor right at the start the lord of the manor right at the start the lord of the manor abruptly kills over and your task is to abruptly kills over and your task is to abruptly kills over and your task is to figure out figure out figure out who done it who done it who done it it could be the maid it could be the maid it could be the maid it could be the butler it could be the butler it could be the butler but you've got a problem your short-term but you've got a problem your short-term but you've got a problem your short-term memory isn't working so well you can't memory isn't working so well you can't memory isn't working so well you can't remember any of the clues past the last remember any of the clues past the last remember any of the clues past the last 10 minutes well in that sort of 10 minutes well in that sort of 10 minutes well in that sort of situation your prediction is going to be situation your prediction is going to be situation your prediction is going to be well nothing better than just a random well nothing better than just a random well nothing better than just a random guess guess guess or imagine you have the opposite problem or imagine you have the opposite problem or imagine you have the opposite problem where you can remember where you can remember where you can remember every word of every conversation that every word of every conversation that every word of every conversation that you've ever had if somebody asked you to you've ever had if somebody asked you to you've ever had if somebody asked you to outline your partner's wedding vows well outline your partner's wedding vows well outline your partner's wedding vows well you might have some trouble doing that you might have some trouble doing that you might have some trouble doing that there's just so many words that you'd there's just so many words that you'd there's just so many words that you'd need to process be much better than if need to process be much better than if need to process be much better than if you could just remember you could just remember you could just remember well the well the well the memorable stuff memorable stuff memorable stuff and that's where something called and that's where something called and that's where something called long long long short short short term term term memory memory memory comes into play comes into play comes into play also abbreviated as lstm also abbreviated as lstm also abbreviated as lstm it allows a neural network to remember it allows a neural network to remember it allows a neural network to remember the stuff that it needs to keep hold of the stuff that it needs to keep hold of the stuff that it needs to keep hold of context but also to forget the stuff context but also to forget the stuff context but also to forget the stuff that well is no longer applicable that well is no longer applicable that well is no longer applicable so take for example this sequence of so take for example this sequence of so take for example this sequence of letters we need to predict what the next letter we need to predict what the next letter we need to predict what the next letter in the sequence is going to be in the sequence is going to be in the sequence is going to be well just by looking at the letters well just by looking at the letters well just by looking at the letters individually it's not obvious what the individually it's not obvious what the individually it's not obvious what the next sequence is like we have two m's next sequence is like we have two m's next sequence is like we have two m's and they both have a different letter and they both have a different letter and they both have a different letter following them following them following them so how do we predict the sequence well so how do we predict the sequence well so how do we predict the sequence well if we have gone back through the time if we have gone back through the time if we have gone back through the time series to look at all of the letters in series to look at all of the letters in series to look at all of the letters in the sequence we can establish context the sequence we can establish context the sequence we can establish context and we can clearly see oh yes it's my and we can clearly see oh yes it's my and we can clearly see oh yes it's my name is name is name is and if we instead of looking at letters and if we instead of looking at letters and if we instead of looking at letters looked at words we can establish that looked at words we can establish that looked at words we can establish that the whole sentence here says my name is the whole sentence here says my name is the whole sentence here says my name is oh yes martin now a now a now a recurrent neural network is really where recurrent neural network is really where recurrent neural network is really where an lstm lives so effectively in lstm is an lstm lives so effectively in lstm is an lstm lives so effectively in lstm is a type of recurrent neural network a type of recurrent neural network a type of recurrent neural network recurrent recurrent recurrent neural net neural net neural net and recurrent neural networks and recurrent neural networks and recurrent neural networks work work work in the sense that they have a node so in the sense that they have a node so in the sense that they have a node so there's a node there's a node there's a node here and this node receives some input here and this node receives some input here and this node receives some input so we've got some input so we've got some input so we've got some input coming in that input is then processed in some way that input is then processed in some way that input is then processed in some way so there's some kind of computation and so there's some kind of computation and so there's some kind of computation and that results in an output that's pretty that results in an output that's pretty that results in an output that's pretty standard stuff but what makes an rnn standard stuff but what makes an rnn standard stuff but what makes an rnn node a little bit different is the fact node a little bit different is the fact node a little bit different is the fact that it is that it is that it is recurrent recurrent recurrent and that means that it loops around so and that means that it loops around so and that means that it loops around so the output the output the output of a given step of a given step of a given step is provided alongside the input in the is provided alongside the input in the is provided alongside the input in the next step next step next step so step one has some input it's so step one has some input it's so step one has some input it's processed and that results in some processed and that results in some processed and that results in some output then step two has some new input output then step two has some new input output then step two has some new input but it also receives the output of the but it also receives the output of the but it also receives the output of the prior step as well that is what makes an prior step as well that is what makes an prior step as well that is what makes an rnn a little bit different and it allows rnn a little bit different and it allows rnn a little bit different and it allows it to remember previous steps in a it to remember previous steps in a it to remember previous steps in a sequence so when we're looking at a sequence so when we're looking at a sequence so when we're looking at a sentence like my name i we don't have to sentence like my name i we don't have to sentence like my name i we don't have to go back too far through those steps to go back too far through those steps to go back too far through those steps to figure out what the context is figure out what the context is figure out what the context is but rnn does suffer from what's known as but rnn does suffer from what's known as but rnn does suffer from what's known as the long term dependency problem which the long term dependency problem which the long term dependency problem which is to say that over time as more and is to say that over time as more and is to say that over time as more and more information piles up more information piles up more information piles up then rnn's become less effective at then rnn's become less effective at then rnn's become less effective at learning new things learning new things learning new things so while we didn't have to go too far so while we didn't have to go too far so while we didn't have to go too far back for my name i if we were going back back for my name i if we were going back back for my name i if we were going back through an hour's worth of clues at our through an hour's worth of clues at our through an hour's worth of clues at our murder mystery dinner well that's a lot murder mystery dinner well that's a lot murder mystery dinner well that's a lot more information that needs to be more information that needs to be more information that needs to be processed processed processed lstm lstm lstm provides a solution to this long term provides a solution to this long term provides a solution to this long term dependency problem and that is to add dependency problem and that is to add dependency problem and that is to add something called an internal state something called an internal state something called an internal state to the rnn node to the rnn node to the rnn node now when an rnn input comes in now when an rnn input comes in now when an rnn input comes in it is receiving the state information as it is receiving the state information as it is receiving the state information as well well well so a step receives the output from the so a step receives the output from the so a step receives the output from the previous step previous step previous step the input of the new step and also the input of the new step and also the input of the new step and also some state information some state information some state information from the lstm state from the lstm state from the lstm state now what is this state well it's now what is this state well it's now what is this state well it's actually a cell let's take a look at actually a cell let's take a look at actually a cell let's take a look at what's in there what's in there what's in there so this is an lstm cell so this is an lstm cell so this is an lstm cell and it consists of three parts and it consists of three parts and it consists of three parts each part is a gate there is a forget each part is a gate there is a forget each part is a gate there is a forget gate gate gate there's an input gate there's an input gate there's an input gate and there's an output gate and there's an output gate and there's an output gate now the now the now the forget gate forget gate forget gate says what sort of state information says what sort of state information says what sort of state information that's stored in this internal state that's stored in this internal state that's stored in this internal state here here here can be forgotten it's no longer can be forgotten it's no longer can be forgotten it's no longer contextually relevant contextually relevant contextually relevant the input gate says what new information the input gate says what new information the input gate says what new information should we add or update into this should we add or update into this should we add or update into this working storage state information and working storage state information and working storage state information and the output gate says of all the the output gate says of all the the output gate says of all the information that's stored in that state information that's stored in that state information that's stored in that state which part of it should be output in which part of it should be output in which part of it should be output in this particular instance this particular instance this particular instance and these gates can be assigned numbers and these gates can be assigned numbers and these gates can be assigned numbers between zero and one between zero and one between zero and one where zero where zero where zero means that the gate is effectively means that the gate is effectively means that the gate is effectively closed and nothing gets through and one closed and nothing gets through and one closed and nothing gets through and one means the gate is wide open and means the gate is wide open and means the gate is wide open and everything gets through everything gets through everything gets through so we can say forget everything or just so we can say forget everything or just so we can say forget everything or just forget a little bit we can say add forget a little bit we can say add forget a little bit we can say add everything to the input state or add everything to the input state or add everything to the input state or add just a little bit and we can say output just a little bit and we can say output just a little bit and we can say output everything or just output a little bit everything or just output a little bit everything or just output a little bit or output nothing at all or output nothing at all or output nothing at all so now when we're processing so now when we're processing so now when we're processing in our rnn cell we have this additional in our rnn cell we have this additional in our rnn cell we have this additional state information that can provide us state information that can provide us state information that can provide us with some additional context with some additional context with some additional context so if we take an example of another so if we take an example of another so if we take an example of another sentence like sentence like sentence like martin martin martin [Music] [Music] [Music] buying apples buying apples buying apples there's some information that we might there's some information that we might there's some information that we might want to store want to store want to store in this state in this state in this state martin is most likely to derive to the martin is most likely to derive to the martin is most likely to derive to the gender of males so we might want to gender of males so we might want to gender of males so we might want to store that because that might be useful store that because that might be useful store that because that might be useful apples is a plural so maybe we're going apples is a plural so maybe we're going apples is a plural so maybe we're going to store that it is a plural for later to store that it is a plural for later to store that it is a plural for later now as this sentence continues to now as this sentence continues to now as this sentence continues to develop it now starts to talk about develop it now starts to talk about develop it now starts to talk about jennifer jennifer jennifer jennifer is at this point we can make some changes at this point we can make some changes at this point we can make some changes to our state data so we've changed to our state data so we've changed to our state data so we've changed subjects from martin to jennifer so we subjects from martin to jennifer so we subjects from martin to jennifer so we don't care about the gender of martin don't care about the gender of martin don't care about the gender of martin anymore so we can forget that part anymore so we can forget that part anymore so we can forget that part and we can say the most likely gender and we can say the most likely gender and we can say the most likely gender for jennifer is female and store that for jennifer is female and store that for jennifer is female and store that instead instead instead and really that is and really that is and really that is how we can apply this lstm to any sort how we can apply this lstm to any sort how we can apply this lstm to any sort of series where we have a sequence of series where we have a sequence of series where we have a sequence prediction that's required and some long prediction that's required and some long prediction that's required and some long term dependency data to go alongside of term dependency data to go alongside of term dependency data to go alongside of now some some typical use cases for now some some typical use cases for now some some typical use cases for using lstm machine translation is a good using lstm machine translation is a good using lstm machine translation is a good another one are chat bots so q a chat another one are chat bots so q a chat another one are chat bots so q a chat bots bots bots where we might need to retrieve some where we might need to retrieve some where we might need to retrieve some information that was in a previous step information that was in a previous step information that was in a previous step in that chat bot and recall it later on in that chat bot and recall it later on in that chat bot and recall it later on yeah all good examples of where we have yeah all good examples of where we have yeah all good examples of where we have a time sequence of things and some long a time sequence of things and some long a time sequence of things and some long term dependencies and term dependencies and term dependencies and had we also applied lstm to our murder had we also applied lstm to our murder had we also applied lstm to our murder mystery dinner we probably could have mystery dinner we probably could have mystery dinner we probably could have won first prize by having it forecast to won first prize by having it forecast to won first prize by having it forecast to us that whodunit was the butler us that whodunit was the butler us that whodunit was the butler so is the butler so is the butler so is the butler if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please consider liking and subscribing",
    "chunks": [
      "Kind: captions Language: en imagine you're at a murder mystery imagine you're at a murder mystery imagine you're at a murder mystery dinner dinner dinner right at the start the lord of the manor right at the start the lord of the manor right at the start the lord of the manor abruptly kills over and your task is to abruptly kills over and your task is to abruptly kills over and your task is to figure out figure out figure out who done it who done it who done it it could be the maid it could be",
      "the maid it could be the maid it could be the butler it could be the butler it could be the butler but you've got a problem your short-term but you've got a problem your short-term but you've got a problem your short-term memory isn't working so well you can't memory isn't working so well you can't memory isn't working so well you can't remember any of the clues past the last remember any of the clues past the last remember any of the clues past the last 10 minutes well in that sort of 10 minutes",
      "well in that sort of 10 minutes well in that sort of situation your prediction is going to be situation your prediction is going to be situation your prediction is going to be well nothing better than just a random well nothing better than just a random well nothing better than just a random guess guess guess or imagine you have the opposite problem or imagine you have the opposite problem or imagine you have the opposite problem where you can remember where you can remember where you can",
      "remember every word of every conversation that every word of every conversation that every word of every conversation that you've ever had if somebody asked you to you've ever had if somebody asked you to you've ever had if somebody asked you to outline your partner's wedding vows well outline your partner's wedding vows well outline your partner's wedding vows well you might have some trouble doing that you might have some trouble doing that you might have some trouble doing that there's just so",
      "many words that you'd there's just so many words that you'd there's just so many words that you'd need to process be much better than if need to process be much better than if need to process be much better than if you could just remember you could just remember you could just remember well the well the well the memorable stuff memorable stuff memorable stuff and that's where something called and that's where something called and that's where something called long long long short short short term",
      "term term memory memory memory comes into play comes into play comes into play also abbreviated as lstm also abbreviated as lstm also abbreviated as lstm it allows a neural network to remember it allows a neural network to remember it allows a neural network to remember the stuff that it needs to keep hold of the stuff that it needs to keep hold of the stuff that it needs to keep hold of context but also to forget the stuff context but also to forget the stuff context but also to forget the stuff",
      "that well is no longer applicable that well is no longer applicable that well is no longer applicable so take for example this sequence of so take for example this sequence of so take for example this sequence of letters we need to predict what the next letter we need to predict what the next letter we need to predict what the next letter in the sequence is going to be in the sequence is going to be in the sequence is going to be well just by looking at the letters well just by looking at the",
      "letters well just by looking at the letters individually it's not obvious what the individually it's not obvious what the individually it's not obvious what the next sequence is like we have two m's next sequence is like we have two m's next sequence is like we have two m's and they both have a different letter and they both have a different letter and they both have a different letter following them following them following them so how do we predict the sequence well so how do we predict the",
      "sequence well so how do we predict the sequence well if we have gone back through the time if we have gone back through the time if we have gone back through the time series to look at all of the letters in series to look at all of the letters in series to look at all of the letters in the sequence we can establish context the sequence we can establish context the sequence we can establish context and we can clearly see oh yes it's my and we can clearly see oh yes it's my and we can clearly see",
      "oh yes it's my name is name is name is and if we instead of looking at letters and if we instead of looking at letters and if we instead of looking at letters looked at words we can establish that looked at words we can establish that looked at words we can establish that the whole sentence here says my name is the whole sentence here says my name is the whole sentence here says my name is oh yes martin now a now a now a recurrent neural network is really where recurrent neural network is really",
      "where recurrent neural network is really where an lstm lives so effectively in lstm is an lstm lives so effectively in lstm is an lstm lives so effectively in lstm is a type of recurrent neural network a type of recurrent neural network a type of recurrent neural network recurrent recurrent recurrent neural net neural net neural net and recurrent neural networks and recurrent neural networks and recurrent neural networks work work work in the sense that they have a node so in the sense that they",
      "have a node so in the sense that they have a node so there's a node there's a node there's a node here and this node receives some input here and this node receives some input here and this node receives some input so we've got some input so we've got some input so we've got some input coming in that input is then processed in some way that input is then processed in some way that input is then processed in some way so there's some kind of computation and so there's some kind of computation and",
      "so there's some kind of computation and that results in an output that's pretty that results in an output that's pretty that results in an output that's pretty standard stuff but what makes an rnn standard stuff but what makes an rnn standard stuff but what makes an rnn node a little bit different is the fact node a little bit different is the fact node a little bit different is the fact that it is that it is that it is recurrent recurrent recurrent and that means that it loops around so and that",
      "means that it loops around so and that means that it loops around so the output the output the output of a given step of a given step of a given step is provided alongside the input in the is provided alongside the input in the is provided alongside the input in the next step next step next step so step one has some input it's so step one has some input it's so step one has some input it's processed and that results in some processed and that results in some processed and that results in some",
      "output then step two has some new input output then step two has some new input output then step two has some new input but it also receives the output of the but it also receives the output of the but it also receives the output of the prior step as well that is what makes an prior step as well that is what makes an prior step as well that is what makes an rnn a little bit different and it allows rnn a little bit different and it allows rnn a little bit different and it allows it to remember",
      "previous steps in a it to remember previous steps in a it to remember previous steps in a sequence so when we're looking at a sequence so when we're looking at a sequence so when we're looking at a sentence like my name i we don't have to sentence like my name i we don't have to sentence like my name i we don't have to go back too far through those steps to go back too far through those steps to go back too far through those steps to figure out what the context is figure out what the context is",
      "figure out what the context is but rnn does suffer from what's known as but rnn does suffer from what's known as but rnn does suffer from what's known as the long term dependency problem which the long term dependency problem which the long term dependency problem which is to say that over time as more and is to say that over time as more and is to say that over time as more and more information piles up more information piles up more information piles up then rnn's become less effective at then",
      "rnn's become less effective at then rnn's become less effective at learning new things learning new things learning new things so while we didn't have to go too far so while we didn't have to go too far so while we didn't have to go too far back for my name i if we were going back back for my name i if we were going back back for my name i if we were going back through an hour's worth of clues at our through an hour's worth of clues at our through an hour's worth of clues at our murder mystery",
      "dinner well that's a lot murder mystery dinner well that's a lot murder mystery dinner well that's a lot more information that needs to be more information that needs to be more information that needs to be processed processed processed lstm lstm lstm provides a solution to this long term provides a solution to this long term provides a solution to this long term dependency problem and that is to add dependency problem and that is to add dependency problem and that is to add something called an",
      "internal state something called an internal state something called an internal state to the rnn node to the rnn node to the rnn node now when an rnn input comes in now when an rnn input comes in now when an rnn input comes in it is receiving the state information as it is receiving the state information as it is receiving the state information as well well well so a step receives the output from the so a step receives the output from the so a step receives the output from the previous step",
      "previous step previous step the input of the new step and also the input of the new step and also the input of the new step and also some state information some state information some state information from the lstm state from the lstm state from the lstm state now what is this state well it's now what is this state well it's now what is this state well it's actually a cell let's take a look at actually a cell let's take a look at actually a cell let's take a look at what's in there what's in",
      "there what's in there so this is an lstm cell so this is an lstm cell so this is an lstm cell and it consists of three parts and it consists of three parts and it consists of three parts each part is a gate there is a forget each part is a gate there is a forget each part is a gate there is a forget gate gate gate there's an input gate there's an input gate there's an input gate and there's an output gate and there's an output gate and there's an output gate now the now the now the forget gate",
      "forget gate forget gate says what sort of state information says what sort of state information says what sort of state information that's stored in this internal state that's stored in this internal state that's stored in this internal state here here here can be forgotten it's no longer can be forgotten it's no longer can be forgotten it's no longer contextually relevant contextually relevant contextually relevant the input gate says what new information the input gate says what new information",
      "the input gate says what new information should we add or update into this should we add or update into this should we add or update into this working storage state information and working storage state information and working storage state information and the output gate says of all the the output gate says of all the the output gate says of all the information that's stored in that state information that's stored in that state information that's stored in that state which part of it should be",
      "output in which part of it should be output in which part of it should be output in this particular instance this particular instance this particular instance and these gates can be assigned numbers and these gates can be assigned numbers and these gates can be assigned numbers between zero and one between zero and one between zero and one where zero where zero where zero means that the gate is effectively means that the gate is effectively means that the gate is effectively closed and nothing",
      "gets through and one closed and nothing gets through and one closed and nothing gets through and one means the gate is wide open and means the gate is wide open and means the gate is wide open and everything gets through everything gets through everything gets through so we can say forget everything or just so we can say forget everything or just so we can say forget everything or just forget a little bit we can say add forget a little bit we can say add forget a little bit we can say add",
      "everything to the input state or add everything to the input state or add everything to the input state or add just a little bit and we can say output just a little bit and we can say output just a little bit and we can say output everything or just output a little bit everything or just output a little bit everything or just output a little bit or output nothing at all or output nothing at all or output nothing at all so now when we're processing so now when we're processing so now when we're",
      "processing in our rnn cell we have this additional in our rnn cell we have this additional in our rnn cell we have this additional state information that can provide us state information that can provide us state information that can provide us with some additional context with some additional context with some additional context so if we take an example of another so if we take an example of another so if we take an example of another sentence like sentence like sentence like martin martin",
      "martin [Music] [Music] [Music] buying apples buying apples buying apples there's some information that we might there's some information that we might there's some information that we might want to store want to store want to store in this state in this state in this state martin is most likely to derive to the martin is most likely to derive to the martin is most likely to derive to the gender of males so we might want to gender of males so we might want to gender of males so we might want to",
      "store that because that might be useful store that because that might be useful store that because that might be useful apples is a plural so maybe we're going apples is a plural so maybe we're going apples is a plural so maybe we're going to store that it is a plural for later to store that it is a plural for later to store that it is a plural for later now as this sentence continues to now as this sentence continues to now as this sentence continues to develop it now starts to talk about",
      "develop it now starts to talk about develop it now starts to talk about jennifer jennifer jennifer jennifer is at this point we can make some changes at this point we can make some changes at this point we can make some changes to our state data so we've changed to our state data so we've changed to our state data so we've changed subjects from martin to jennifer so we subjects from martin to jennifer so we subjects from martin to jennifer so we don't care about the gender of martin don't care",
      "about the gender of martin don't care about the gender of martin anymore so we can forget that part anymore so we can forget that part anymore so we can forget that part and we can say the most likely gender and we can say the most likely gender and we can say the most likely gender for jennifer is female and store that for jennifer is female and store that for jennifer is female and store that instead instead instead and really that is and really that is and really that is how we can apply this",
      "lstm to any sort how we can apply this lstm to any sort how we can apply this lstm to any sort of series where we have a sequence of series where we have a sequence of series where we have a sequence prediction that's required and some long prediction that's required and some long prediction that's required and some long term dependency data to go alongside of term dependency data to go alongside of term dependency data to go alongside of now some some typical use cases for now some some typical",
      "use cases for now some some typical use cases for using lstm machine translation is a good using lstm machine translation is a good using lstm machine translation is a good another one are chat bots so q a chat another one are chat bots so q a chat another one are chat bots so q a chat bots bots bots where we might need to retrieve some where we might need to retrieve some where we might need to retrieve some information that was in a previous step information that was in a previous step",
      "information that was in a previous step in that chat bot and recall it later on in that chat bot and recall it later on in that chat bot and recall it later on yeah all good examples of where we have yeah all good examples of where we have yeah all good examples of where we have a time sequence of things and some long a time sequence of things and some long a time sequence of things and some long term dependencies and term dependencies and term dependencies and had we also applied lstm to our",
      "murder had we also applied lstm to our murder had we also applied lstm to our murder mystery dinner we probably could have mystery dinner we probably could have mystery dinner we probably could have won first prize by having it forecast to won first prize by having it forecast to won first prize by having it forecast to us that whodunit was the butler us that whodunit was the butler us that whodunit was the butler so is the butler so is the butler so is the butler if you have any questions please",
      "drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please consider liking and subscribing"
    ],
    "chunk_count": 37,
    "content_id": "59df6e24-d73b-4b30-ac55-a21683c99592",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554970"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=fLvJ8VdHLA0": {
    "title": "What is NLP (Natural Language Processing)?",
    "url": "https://www.youtube.com/watch?v=fLvJ8VdHLA0",
    "description": "Learn more about watsonx: https://ibm.biz/BdvxDL\n\nEvery time you surf the internet you encounter a Natural Language Processing, or NLP, application. But what exactly is NLP and how does it work?\n\nIn this lightboard video, Master Inventor with IBM, Martin Keen, visually explains what NLP is and why we need it, as well as how NLP takes unstructured human speech and converts it to structured data that a computer can understand.\n\nChapters\n0:00 - Intro\n0:38 - Unstructured data\n1:12 - Structured data\n2:03 - Natural Language Understanding (NLU) & Natural Language Generation (NLG)\n2:36 - Machine Translation use case\n3:40 - Virtual Assistance / Chat Bots use case\n4:14 - Sentiment Analysis use case\n4:44 - Spam Detection use case\n5:44 - Tokenization\n6:18 - Stemming & Lemmatization\n7:42 - Part of Speech Tagging\n8:22 - Named Entity Recognition (NER)\n9:08 - Summary\n\n#NLP #NaturalLangueProcessing #AI",
    "duration": 577,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en what is natural language processing what is natural language processing what is natural language processing well you're doing it right now you're well you're doing it right now you're well you're doing it right now you're listening to the words and the sentences listening to the words and the sentences listening to the words and the sentences that i'm forming that i'm forming that i'm forming and you are forming some sort of and you are forming some sort of and you are forming some sort of comprehension comprehension comprehension from it and when we ask a computer to do from it and when we ask a computer to do from it and when we ask a computer to do that that that that is nlp on natural language that is nlp on natural language that is nlp on natural language processing processing processing my name is martin keane i'm a master my name is martin keane i'm a master my name is martin keane i'm a master inventor at ibm inventor at ibm inventor at ibm and i've utilized nlp in a good number and i've utilized nlp in a good number and i've utilized nlp in a good number of my invention of my invention of my invention disclosures nlp really has a disclosures nlp really has a disclosures nlp really has a really high utility value in all sorts really high utility value in all sorts really high utility value in all sorts of ai of ai of ai applications now nlp applications now nlp applications now nlp starts with something called starts with something called starts with something called unstructured text what is that unstructured text what is that unstructured text what is that well that's just what you and i say well that's just what you and i say well that's just what you and i say that's how we speak that's how we speak that's how we speak so for example some unstructured text is so for example some unstructured text is so for example some unstructured text is add eggs and milk add eggs and milk add eggs and milk to my shopping list to my shopping list to my shopping list now you and i understand exactly now you and i understand exactly now you and i understand exactly what that means but it is unstructured what that means but it is unstructured what that means but it is unstructured at least at least at least to a computer so what we need to do is to have a so what we need to do is to have a so what we need to do is to have a structured representation of that same structured representation of that same structured representation of that same information that a computer can process information that a computer can process information that a computer can process now that might look something a bit more now that might look something a bit more now that might look something a bit more like this where we have a like this where we have a like this where we have a shopping list element shopping list element shopping list element and then it has sub elements within it and then it has sub elements within it and then it has sub elements within it like an like an like an item for eggs and an item for milk that is an example of something that is that is an example of something that is that is an example of something that is structured now the job of natural language now the job of natural language now the job of natural language processing processing processing is to translate between these two things is to translate between these two things is to translate between these two things so nlp sits right in so nlp sits right in so nlp sits right in the middle here translating between the middle here translating between the middle here translating between unstructured and structured unstructured and structured unstructured and structured data and when we go from structure from data and when we go from structure from data and when we go from structure from unstructured here to structured this way unstructured here to structured this way unstructured here to structured this way that's called n l u or that's called n l u or that's called n l u or natural language understanding and when natural language understanding and when natural language understanding and when we go this way we go this way we go this way from structured to unstructured that's from structured to unstructured that's from structured to unstructured that's called natural called natural called natural language generation or nl language generation or nl language generation or nl we're going to focus today primarily on we're going to focus today primarily on we're going to focus today primarily on going from unstructured to going from unstructured to going from unstructured to structured in natural language structured in natural language structured in natural language processing processing processing now let's think of some use cases where now let's think of some use cases where now let's think of some use cases where might be quite handy first of all might be quite handy first of all might be quite handy first of all we've got machine translation now when we translate from now when we translate from now when we translate from one language to another we need one language to another we need one language to another we need to understand the context of to understand the context of to understand the context of that sentence it's not just a case of that sentence it's not just a case of that sentence it's not just a case of taking each individual word taking each individual word taking each individual word uh from say english and then translating uh from say english and then translating uh from say english and then translating it into another language we need to it into another language we need to it into another language we need to understand the understand the understand the the overall structure and and context of the overall structure and and context of the overall structure and and context of what's being said what's being said what's being said and my favorite example of this going and my favorite example of this going and my favorite example of this going horribly wrong horribly wrong horribly wrong is if you take the phrase the spirit is if you take the phrase the spirit is if you take the phrase the spirit is willing but the flesh is weak and you is willing but the flesh is weak and you is willing but the flesh is weak and you translate that from english to russian translate that from english to russian translate that from english to russian and then you translate that russian and then you translate that russian and then you translate that russian translation back into english translation back into english translation back into english you're going to go from the spirit is you're going to go from the spirit is you're going to go from the spirit is willing but the flesh willing but the flesh willing but the flesh is weak to something a bit more like the is weak to something a bit more like the is weak to something a bit more like the vodka is good vodka is good vodka is good but the meat is rotten which is really but the meat is rotten which is really but the meat is rotten which is really not the intended not the intended not the intended context of that sentence whatsoever so context of that sentence whatsoever so context of that sentence whatsoever so nlp can help nlp can help nlp can help with situations like that now with situations like that now with situations like that now the the second kind of use case that i the the second kind of use case that i the the second kind of use case that i like to mention like to mention like to mention relates to virtual assistants relates to virtual assistants relates to virtual assistants and also to things like chat bots now and also to things like chat bots now and also to things like chat bots now a virtual assistant that's something a virtual assistant that's something a virtual assistant that's something like siri or alexa like siri or alexa like siri or alexa on your phone that is taking human on your phone that is taking human on your phone that is taking human utterances utterances utterances and deriving a command to execute based and deriving a command to execute based and deriving a command to execute based upon that upon that upon that and a chatbot is something similar and a chatbot is something similar and a chatbot is something similar except in written except in written except in written language and that's taking written language and that's taking written language and that's taking written language and then language and then language and then using it to traverse a decision tree in using it to traverse a decision tree in using it to traverse a decision tree in order to take order to take order to take an action nlp is very helpful there an action nlp is very helpful there an action nlp is very helpful there another use case is for sentiment another use case is for sentiment another use case is for sentiment analysis now this analysis now this analysis now this is taking some text perhaps an email is taking some text perhaps an email is taking some text perhaps an email message or a product message or a product message or a product review and trying to derive the review and trying to derive the review and trying to derive the sentiment that's expressed within it sentiment that's expressed within it sentiment that's expressed within it so for example is this so for example is this so for example is this product review a positive sentiment or a product review a positive sentiment or a product review a positive sentiment or a negative sentiment negative sentiment negative sentiment is it written as a serious statement is it written as a serious statement is it written as a serious statement or is it being sarcastic we can use nlp or is it being sarcastic we can use nlp or is it being sarcastic we can use nlp to tell us and then finally to tell us and then finally to tell us and then finally another good example is spam detection another good example is spam detection another good example is spam detection so this is a case of looking at a given so this is a case of looking at a given so this is a case of looking at a given email message and trying to drive email message and trying to drive email message and trying to drive is this a real email message or is it is this a real email message or is it is this a real email message or is it spam and we can look for pointers within spam and we can look for pointers within spam and we can look for pointers within the content of the message the content of the message the content of the message so things like overused words or poor so things like overused words or poor so things like overused words or poor grammar grammar grammar or an inappropriate claim of urgency can or an inappropriate claim of urgency can or an inappropriate claim of urgency can indicate that this is actually perhaps indicate that this is actually perhaps indicate that this is actually perhaps spam spam spam so those are some of the things that nlp so those are some of the things that nlp so those are some of the things that nlp can provide but can provide but can provide but how does it work well the thing with nlp how does it work well the thing with nlp how does it work well the thing with nlp is it's it's not like one algorithm is it's it's not like one algorithm is it's it's not like one algorithm it's actually more like a bag of tools it's actually more like a bag of tools it's actually more like a bag of tools and you can apply these bag of tools and you can apply these bag of tools and you can apply these bag of tools to be able to resolve some of these use to be able to resolve some of these use to be able to resolve some of these use cases cases cases now the input to nlp is some now the input to nlp is some now the input to nlp is some unstructured text so either some written unstructured text so either some written unstructured text so either some written text or spoken text that has been text or spoken text that has been text or spoken text that has been converted to written text converted to written text converted to written text through a speech to text algorithm once through a speech to text algorithm once through a speech to text algorithm once we've got that we've got that we've got that the first stage the first stage the first stage of nlp is called tokenization this is about taking a string and this is about taking a string and this is about taking a string and breaking it down into breaking it down into breaking it down into chunks so if we consider the chunks so if we consider the chunks so if we consider the unstructured text we've got here unstructured text we've got here unstructured text we've got here add eggs and milk to my shopping list add eggs and milk to my shopping list add eggs and milk to my shopping list that's eight words that can be eight that's eight words that can be eight that's eight words that can be eight tokens tokens tokens and from here on in we are going to work and from here on in we are going to work and from here on in we are going to work one token at a time one token at a time one token at a time as we traverse through this now the as we traverse through this now the as we traverse through this now the first first first stage once we've got things down into stage once we've got things down into stage once we've got things down into tokens that we can perform tokens that we can perform tokens that we can perform is called stemming and this is all about deriving the word and this is all about deriving the word and this is all about deriving the word stem stem stem for a given token so for example running for a given token so for example running for a given token so for example running runs and ran the word stem for all three runs and ran the word stem for all three runs and ran the word stem for all three of those of those of those is run we're just kind of removing the is run we're just kind of removing the is run we're just kind of removing the prefix and the suffixes and normalizing prefix and the suffixes and normalizing prefix and the suffixes and normalizing the tense and we're getting to the word the tense and we're getting to the word the tense and we're getting to the word stem stem stem but stemming doesn't work well for every but stemming doesn't work well for every but stemming doesn't work well for every token token token for example universal and university for example universal and university for example universal and university will well they don't really stem down to will well they don't really stem down to will well they don't really stem down to universe universe universe for situations like that there is for situations like that there is for situations like that there is another tool that we have available another tool that we have available another tool that we have available and that is called limitization and that is called limitization and that is called limitization and limitization takes a given token and and limitization takes a given token and and limitization takes a given token and learns its meaning through a dictionary learns its meaning through a dictionary learns its meaning through a dictionary definition definition definition and from there it can derive its root or and from there it can derive its root or and from there it can derive its root or lem so take better for example lem so take better for example lem so take better for example better is derived from good so the root better is derived from good so the root better is derived from good so the root or the or the or the lem of beta is good the lem of beta is good the lem of beta is good the stem of better would be bet stem of better would be bet stem of better would be bet so you can see that it is significant so you can see that it is significant so you can see that it is significant whether we use whether we use whether we use stemming or we use limitization stemming or we use limitization stemming or we use limitization for a given token for a given token for a given token now next thing we can do is we can do now next thing we can do is we can do now next thing we can do is we can do a process called part of speech tagging and what this is doing is for a given and what this is doing is for a given and what this is doing is for a given token token token it's looking where that token is used it's looking where that token is used it's looking where that token is used within the context of within the context of within the context of a sentence so take a sentence so take a sentence so take the word make for example if i the word make for example if i the word make for example if i say i'm going to make dinner make say i'm going to make dinner make say i'm going to make dinner make is a verb but if i ask you what make is is a verb but if i ask you what make is is a verb but if i ask you what make is your laptop your laptop your laptop well make is now a noun so where that well make is now a noun so where that well make is now a noun so where that token is used in the sentence token is used in the sentence token is used in the sentence matters part of speech tagging can help matters part of speech tagging can help matters part of speech tagging can help us derive that context us derive that context us derive that context and then finally another stage is and then finally another stage is and then finally another stage is named entity recognition named entity recognition named entity recognition and what this is asking is for a given and what this is asking is for a given and what this is asking is for a given token token token is there an entity associated with it so is there an entity associated with it so is there an entity associated with it so for example a token of arizona has an for example a token of arizona has an for example a token of arizona has an entity of a u.s state whereas entity of a u.s state whereas entity of a u.s state whereas a token of ralph has an entity of a token of ralph has an entity of a token of ralph has an entity of a person's name and these are some of a person's name and these are some of a person's name and these are some of the tools the tools the tools that we can apply in this big bag of that we can apply in this big bag of that we can apply in this big bag of tools that we have for nlp tools that we have for nlp tools that we have for nlp in order to get from this unstructured in order to get from this unstructured in order to get from this unstructured human speech through to something human speech through to something human speech through to something structured that a computer can structured that a computer can structured that a computer can understand understand understand and once we've done that then we can and once we've done that then we can and once we've done that then we can apply that structured data to apply that structured data to apply that structured data to all sorts of ai applications now there's all sorts of ai applications now there's all sorts of ai applications now there's obviously obviously obviously a lot more to it than this and i've a lot more to it than this and i've a lot more to it than this and i've included some links in the description included some links in the description included some links in the description if you'd like to know more if you'd like to know more if you'd like to know more but hopefully this made some sense but hopefully this made some sense but hopefully this made some sense and that you were able to process and that you were able to process and that you were able to process some of the natural language that i've some of the natural language that i've some of the natural language that i've shared today shared today shared today thanks for watching if you have thanks for watching if you have thanks for watching if you have questions please drop us a line below questions please drop us a line below questions please drop us a line below if you want to see more videos like this if you want to see more videos like this if you want to see more videos like this in the future please like and subscribe",
    "chunks": [
      "Kind: captions Language: en what is natural language processing what is natural language processing what is natural language processing well you're doing it right now you're well you're doing it right now you're well you're doing it right now you're listening to the words and the sentences listening to the words and the sentences listening to the words and the sentences that i'm forming that i'm forming that i'm forming and you are forming some sort of and you are forming some sort of and you",
      "are forming some sort of comprehension comprehension comprehension from it and when we ask a computer to do from it and when we ask a computer to do from it and when we ask a computer to do that that that that is nlp on natural language that is nlp on natural language that is nlp on natural language processing processing processing my name is martin keane i'm a master my name is martin keane i'm a master my name is martin keane i'm a master inventor at ibm inventor at ibm inventor at ibm and i've",
      "utilized nlp in a good number and i've utilized nlp in a good number and i've utilized nlp in a good number of my invention of my invention of my invention disclosures nlp really has a disclosures nlp really has a disclosures nlp really has a really high utility value in all sorts really high utility value in all sorts really high utility value in all sorts of ai of ai of ai applications now nlp applications now nlp applications now nlp starts with something called starts with something called",
      "starts with something called unstructured text what is that unstructured text what is that unstructured text what is that well that's just what you and i say well that's just what you and i say well that's just what you and i say that's how we speak that's how we speak that's how we speak so for example some unstructured text is so for example some unstructured text is so for example some unstructured text is add eggs and milk add eggs and milk add eggs and milk to my shopping list to my shopping",
      "list to my shopping list now you and i understand exactly now you and i understand exactly now you and i understand exactly what that means but it is unstructured what that means but it is unstructured what that means but it is unstructured at least at least at least to a computer so what we need to do is to have a so what we need to do is to have a so what we need to do is to have a structured representation of that same structured representation of that same structured representation of that",
      "same information that a computer can process information that a computer can process information that a computer can process now that might look something a bit more now that might look something a bit more now that might look something a bit more like this where we have a like this where we have a like this where we have a shopping list element shopping list element shopping list element and then it has sub elements within it and then it has sub elements within it and then it has sub elements",
      "within it like an like an like an item for eggs and an item for milk that is an example of something that is that is an example of something that is that is an example of something that is structured now the job of natural language now the job of natural language now the job of natural language processing processing processing is to translate between these two things is to translate between these two things is to translate between these two things so nlp sits right in so nlp sits right in so nlp",
      "sits right in the middle here translating between the middle here translating between the middle here translating between unstructured and structured unstructured and structured unstructured and structured data and when we go from structure from data and when we go from structure from data and when we go from structure from unstructured here to structured this way unstructured here to structured this way unstructured here to structured this way that's called n l u or that's called n l u or that's",
      "called n l u or natural language understanding and when natural language understanding and when natural language understanding and when we go this way we go this way we go this way from structured to unstructured that's from structured to unstructured that's from structured to unstructured that's called natural called natural called natural language generation or nl language generation or nl language generation or nl we're going to focus today primarily on we're going to focus today primarily on",
      "we're going to focus today primarily on going from unstructured to going from unstructured to going from unstructured to structured in natural language structured in natural language structured in natural language processing processing processing now let's think of some use cases where now let's think of some use cases where now let's think of some use cases where might be quite handy first of all might be quite handy first of all might be quite handy first of all we've got machine translation",
      "now when we translate from now when we translate from now when we translate from one language to another we need one language to another we need one language to another we need to understand the context of to understand the context of to understand the context of that sentence it's not just a case of that sentence it's not just a case of that sentence it's not just a case of taking each individual word taking each individual word taking each individual word uh from say english and then",
      "translating uh from say english and then translating uh from say english and then translating it into another language we need to it into another language we need to it into another language we need to understand the understand the understand the the overall structure and and context of the overall structure and and context of the overall structure and and context of what's being said what's being said what's being said and my favorite example of this going and my favorite example of this going",
      "and my favorite example of this going horribly wrong horribly wrong horribly wrong is if you take the phrase the spirit is if you take the phrase the spirit is if you take the phrase the spirit is willing but the flesh is weak and you is willing but the flesh is weak and you is willing but the flesh is weak and you translate that from english to russian translate that from english to russian translate that from english to russian and then you translate that russian and then you translate that",
      "russian and then you translate that russian translation back into english translation back into english translation back into english you're going to go from the spirit is you're going to go from the spirit is you're going to go from the spirit is willing but the flesh willing but the flesh willing but the flesh is weak to something a bit more like the is weak to something a bit more like the is weak to something a bit more like the vodka is good vodka is good vodka is good but the meat is rotten",
      "which is really but the meat is rotten which is really but the meat is rotten which is really not the intended not the intended not the intended context of that sentence whatsoever so context of that sentence whatsoever so context of that sentence whatsoever so nlp can help nlp can help nlp can help with situations like that now with situations like that now with situations like that now the the second kind of use case that i the the second kind of use case that i the the second kind of use case",
      "that i like to mention like to mention like to mention relates to virtual assistants relates to virtual assistants relates to virtual assistants and also to things like chat bots now and also to things like chat bots now and also to things like chat bots now a virtual assistant that's something a virtual assistant that's something a virtual assistant that's something like siri or alexa like siri or alexa like siri or alexa on your phone that is taking human on your phone that is taking human on",
      "your phone that is taking human utterances utterances utterances and deriving a command to execute based and deriving a command to execute based and deriving a command to execute based upon that upon that upon that and a chatbot is something similar and a chatbot is something similar and a chatbot is something similar except in written except in written except in written language and that's taking written language and that's taking written language and that's taking written language and then",
      "language and then language and then using it to traverse a decision tree in using it to traverse a decision tree in using it to traverse a decision tree in order to take order to take order to take an action nlp is very helpful there an action nlp is very helpful there an action nlp is very helpful there another use case is for sentiment another use case is for sentiment another use case is for sentiment analysis now this analysis now this analysis now this is taking some text perhaps an email is",
      "taking some text perhaps an email is taking some text perhaps an email message or a product message or a product message or a product review and trying to derive the review and trying to derive the review and trying to derive the sentiment that's expressed within it sentiment that's expressed within it sentiment that's expressed within it so for example is this so for example is this so for example is this product review a positive sentiment or a product review a positive sentiment or a product",
      "review a positive sentiment or a negative sentiment negative sentiment negative sentiment is it written as a serious statement is it written as a serious statement is it written as a serious statement or is it being sarcastic we can use nlp or is it being sarcastic we can use nlp or is it being sarcastic we can use nlp to tell us and then finally to tell us and then finally to tell us and then finally another good example is spam detection another good example is spam detection another good",
      "example is spam detection so this is a case of looking at a given so this is a case of looking at a given so this is a case of looking at a given email message and trying to drive email message and trying to drive email message and trying to drive is this a real email message or is it is this a real email message or is it is this a real email message or is it spam and we can look for pointers within spam and we can look for pointers within spam and we can look for pointers within the content of",
      "the message the content of the message the content of the message so things like overused words or poor so things like overused words or poor so things like overused words or poor grammar grammar grammar or an inappropriate claim of urgency can or an inappropriate claim of urgency can or an inappropriate claim of urgency can indicate that this is actually perhaps indicate that this is actually perhaps indicate that this is actually perhaps spam spam spam so those are some of the things that nlp",
      "so those are some of the things that nlp so those are some of the things that nlp can provide but can provide but can provide but how does it work well the thing with nlp how does it work well the thing with nlp how does it work well the thing with nlp is it's it's not like one algorithm is it's it's not like one algorithm is it's it's not like one algorithm it's actually more like a bag of tools it's actually more like a bag of tools it's actually more like a bag of tools and you can apply these",
      "bag of tools and you can apply these bag of tools and you can apply these bag of tools to be able to resolve some of these use to be able to resolve some of these use to be able to resolve some of these use cases cases cases now the input to nlp is some now the input to nlp is some now the input to nlp is some unstructured text so either some written unstructured text so either some written unstructured text so either some written text or spoken text that has been text or spoken text that has",
      "been text or spoken text that has been converted to written text converted to written text converted to written text through a speech to text algorithm once through a speech to text algorithm once through a speech to text algorithm once we've got that we've got that we've got that the first stage the first stage the first stage of nlp is called tokenization this is about taking a string and this is about taking a string and this is about taking a string and breaking it down into breaking it down",
      "into breaking it down into chunks so if we consider the chunks so if we consider the chunks so if we consider the unstructured text we've got here unstructured text we've got here unstructured text we've got here add eggs and milk to my shopping list add eggs and milk to my shopping list add eggs and milk to my shopping list that's eight words that can be eight that's eight words that can be eight that's eight words that can be eight tokens tokens tokens and from here on in we are going to work",
      "and from here on in we are going to work and from here on in we are going to work one token at a time one token at a time one token at a time as we traverse through this now the as we traverse through this now the as we traverse through this now the first first first stage once we've got things down into stage once we've got things down into stage once we've got things down into tokens that we can perform tokens that we can perform tokens that we can perform is called stemming and this is all",
      "about deriving the word and this is all about deriving the word and this is all about deriving the word stem stem stem for a given token so for example running for a given token so for example running for a given token so for example running runs and ran the word stem for all three runs and ran the word stem for all three runs and ran the word stem for all three of those of those of those is run we're just kind of removing the is run we're just kind of removing the is run we're just kind of",
      "removing the prefix and the suffixes and normalizing prefix and the suffixes and normalizing prefix and the suffixes and normalizing the tense and we're getting to the word the tense and we're getting to the word the tense and we're getting to the word stem stem stem but stemming doesn't work well for every but stemming doesn't work well for every but stemming doesn't work well for every token token token for example universal and university for example universal and university for example",
      "universal and university will well they don't really stem down to will well they don't really stem down to will well they don't really stem down to universe universe universe for situations like that there is for situations like that there is for situations like that there is another tool that we have available another tool that we have available another tool that we have available and that is called limitization and that is called limitization and that is called limitization and limitization",
      "takes a given token and and limitization takes a given token and and limitization takes a given token and learns its meaning through a dictionary learns its meaning through a dictionary learns its meaning through a dictionary definition definition definition and from there it can derive its root or and from there it can derive its root or and from there it can derive its root or lem so take better for example lem so take better for example lem so take better for example better is derived from",
      "good so the root better is derived from good so the root better is derived from good so the root or the or the or the lem of beta is good the lem of beta is good the lem of beta is good the stem of better would be bet stem of better would be bet stem of better would be bet so you can see that it is significant so you can see that it is significant so you can see that it is significant whether we use whether we use whether we use stemming or we use limitization stemming or we use limitization",
      "stemming or we use limitization for a given token for a given token for a given token now next thing we can do is we can do now next thing we can do is we can do now next thing we can do is we can do a process called part of speech tagging and what this is doing is for a given and what this is doing is for a given and what this is doing is for a given token token token it's looking where that token is used it's looking where that token is used it's looking where that token is used within the",
      "context of within the context of within the context of a sentence so take a sentence so take a sentence so take the word make for example if i the word make for example if i the word make for example if i say i'm going to make dinner make say i'm going to make dinner make say i'm going to make dinner make is a verb but if i ask you what make is is a verb but if i ask you what make is is a verb but if i ask you what make is your laptop your laptop your laptop well make is now a noun so where that",
      "well make is now a noun so where that well make is now a noun so where that token is used in the sentence token is used in the sentence token is used in the sentence matters part of speech tagging can help matters part of speech tagging can help matters part of speech tagging can help us derive that context us derive that context us derive that context and then finally another stage is and then finally another stage is and then finally another stage is named entity recognition named entity",
      "recognition named entity recognition and what this is asking is for a given and what this is asking is for a given and what this is asking is for a given token token token is there an entity associated with it so is there an entity associated with it so is there an entity associated with it so for example a token of arizona has an for example a token of arizona has an for example a token of arizona has an entity of a u.s state whereas entity of a u.s state whereas entity of a u.s state whereas a",
      "token of ralph has an entity of a token of ralph has an entity of a token of ralph has an entity of a person's name and these are some of a person's name and these are some of a person's name and these are some of the tools the tools the tools that we can apply in this big bag of that we can apply in this big bag of that we can apply in this big bag of tools that we have for nlp tools that we have for nlp tools that we have for nlp in order to get from this unstructured in order to get from this",
      "unstructured in order to get from this unstructured human speech through to something human speech through to something human speech through to something structured that a computer can structured that a computer can structured that a computer can understand understand understand and once we've done that then we can and once we've done that then we can and once we've done that then we can apply that structured data to apply that structured data to apply that structured data to all sorts of ai",
      "applications now there's all sorts of ai applications now there's all sorts of ai applications now there's obviously obviously obviously a lot more to it than this and i've a lot more to it than this and i've a lot more to it than this and i've included some links in the description included some links in the description included some links in the description if you'd like to know more if you'd like to know more if you'd like to know more but hopefully this made some sense but hopefully this made",
      "some sense but hopefully this made some sense and that you were able to process and that you were able to process and that you were able to process some of the natural language that i've some of the natural language that i've some of the natural language that i've shared today shared today shared today thanks for watching if you have thanks for watching if you have thanks for watching if you have questions please drop us a line below questions please drop us a line below questions please drop us",
      "a line below if you want to see more videos like this if you want to see more videos like this if you want to see more videos like this in the future please like and subscribe"
    ],
    "chunk_count": 41,
    "content_id": "e8fab700-94c1-4601-967e-175e013a1fde",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554973"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=1I6bQ12VxV0": {
    "title": "NLP vs NLU vs NLG",
    "url": "https://www.youtube.com/watch?v=1I6bQ12VxV0",
    "description": "Learn more about watsonx → https://ibm.biz/BdvDak\n\nNatural language is complicated. So are the computer systems that process natural language. In this video Martin Keen explains Natural Language Processing (NLP), Natural Language Understanding (NLU), and Natural Language Generation (NLG) and their relationship to each other.",
    "duration": 408,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en natural language natural language natural language processing natural language natural language natural language understanding and natural language and natural language and natural language generation generation generation n l p n l p n l p n l u n l u n l u n l g n l g n l g what's the difference what's the difference what's the difference allow me to demonstrate now look what i just did there is well now look what i just did there is well now look what i just did there is well by writing a story by writing a story by writing a story i performed an example of i performed an example of i performed an example of natural language generation and if natural language generation and if natural language generation and if you're now peering at the screen reading you're now peering at the screen reading you're now peering at the screen reading it or even just and trying to understand it or even just and trying to understand it or even just and trying to understand and make sense of what i'm saying right and make sense of what i'm saying right and make sense of what i'm saying right you're participating in natural language you're participating in natural language you're participating in natural language understanding and together you and i are understanding and together you and i are understanding and together you and i are both performing subsets of the overall both performing subsets of the overall both performing subsets of the overall collective of collective of collective of natural language natural language natural language processing so processing so processing so and n l and n l and n l they're both subsets of n they're both subsets of n they're both subsets of n but we're missing one quite important but we're missing one quite important but we're missing one quite important point here the natural language point here the natural language point here the natural language processing stuff that we're interested processing stuff that we're interested processing stuff that we're interested in today is performed by in today is performed by in today is performed by computers not humans so when we use computers not humans so when we use computers not humans so when we use these terms what do we really mean and these terms what do we really mean and these terms what do we really mean and how can these models be put to work well how can these models be put to work well how can these models be put to work well nlp enables computers to understand nlp enables computers to understand nlp enables computers to understand human language in both written and human language in both written and human language in both written and verbal forms using deep learning verbal forms using deep learning verbal forms using deep learning techniques to complete tasks typical techniques to complete tasks typical techniques to complete tasks typical examples for that are things like examples for that are things like examples for that are things like language translation or conducting a language translation or conducting a language translation or conducting a conversation in a chat bot conversation in a chat bot conversation in a chat bot now it does this through the now it does this through the now it does this through the identification of named entities which identification of named entities which identification of named entities which is a process called named entity is a process called named entity is a process called named entity recognition recognition recognition and identification of word patterns and identification of word patterns and identification of word patterns using methods like tokenization stemming using methods like tokenization stemming using methods like tokenization stemming and lemmatization and i've covered some and lemmatization and i've covered some and lemmatization and i've covered some of this in a previous video about nlp so of this in a previous video about nlp so of this in a previous video about nlp so we won't go over that in detail here we won't go over that in detail here we won't go over that in detail here let's focus instead on these two things let's focus instead on these two things let's focus instead on these two things natural language understanding and natural language understanding and natural language understanding and natural language generation natural language generation natural language generation so natural language understanding so natural language understanding so natural language understanding uses syntactic and semantic analysis of uses syntactic and semantic analysis of uses syntactic and semantic analysis of text and speech to determine the meaning text and speech to determine the meaning text and speech to determine the meaning of a sentence of a sentence of a sentence unlike structured computer code our unlike structured computer code our unlike structured computer code our unstructured messy human language has unstructured messy human language has unstructured messy human language has all sorts of nuances that nlu needs to all sorts of nuances that nlu needs to all sorts of nuances that nlu needs to account for so let's take a look at a account for so let's take a look at a account for so let's take a look at a couple of examples i'm going to cover couple of examples i'm going to cover couple of examples i'm going to cover some sentences here so like alice some sentences here so like alice some sentences here so like alice is swimming against the current this is a sentence that we could feed this is a sentence that we could feed this is a sentence that we could feed into an nlu algorithm and ask it to into an nlu algorithm and ask it to into an nlu algorithm and ask it to really make sense of it really make sense of it really make sense of it another example another example another example the current the current the current version of the file is in the cloud so that's two sentence examples let's so that's two sentence examples let's so that's two sentence examples let's take a closer look at trying to make take a closer look at trying to make take a closer look at trying to make some sense of these so we've got the some sense of these so we've got the some sense of these so we've got the word current here in this first sentence word current here in this first sentence word current here in this first sentence the word current is a noun and that's the word current is a noun and that's the word current is a noun and that's preceded by a verb the verb here is preceded by a verb the verb here is preceded by a verb the verb here is swimming together that provides swimming together that provides swimming together that provides additional context to the reader additional context to the reader additional context to the reader allowing us to conclude that we are allowing us to conclude that we are allowing us to conclude that we are referring to the flow of water in the referring to the flow of water in the referring to the flow of water in the ocean when we talk about current in this ocean when we talk about current in this ocean when we talk about current in this situation in the second example here's situation in the second example here's situation in the second example here's the word current the word current the word current and this time it's an adjective and the and this time it's an adjective and the and this time it's an adjective and the noun it describes noun it describes noun it describes is version is version is version so that denotes that we've got multiple so that denotes that we've got multiple so that denotes that we've got multiple iterations of a report and here current iterations of a report and here current iterations of a report and here current is implying that we have the most is implying that we have the most is implying that we have the most up-to-date status of the file up-to-date status of the file up-to-date status of the file so two completely different meanings for so two completely different meanings for so two completely different meanings for current current current and understanding the relationships and understanding the relationships and understanding the relationships between words and phrases is what nlu is between words and phrases is what nlu is between words and phrases is what nlu is really all about and enables us to really all about and enables us to really all about and enables us to derive the intended meaning of a derive the intended meaning of a derive the intended meaning of a sentence sentence sentence now while nlu is all about improving a now while nlu is all about improving a now while nlu is all about improving a computer's reading comprehension computer's reading comprehension computer's reading comprehension nlg or natural language generation nlg or natural language generation nlg or natural language generation focuses on enabling computers to write focuses on enabling computers to write focuses on enabling computers to write it's the process of producing a human it's the process of producing a human it's the process of producing a human language text response based on some language text response based on some language text response based on some data input data input data input nlg applications need to consider nlg applications need to consider nlg applications need to consider language rules based on morphology language rules based on morphology language rules based on morphology lexicons syntax and semantics to make lexicons syntax and semantics to make lexicons syntax and semantics to make choices on how to choices on how to choices on how to phrase responses appropriately phrase responses appropriately phrase responses appropriately now nlg typically consists of three now nlg typically consists of three now nlg typically consists of three stages so if we look at nlg stages so if we look at nlg stages so if we look at nlg the first stage is text the first stage is text the first stage is text planning and text planning formulates the orders and text planning formulates the orders and text planning formulates the orders and the content in a logical manner and the content in a logical manner and the content in a logical manner similarly we have sentence similarly we have sentence similarly we have sentence planning and sentence planning considers things and sentence planning considers things and sentence planning considers things like punctuation and text flow and like punctuation and text flow and like punctuation and text flow and breaks out the content into paragraphs breaks out the content into paragraphs breaks out the content into paragraphs and sentences and then the third stage and sentences and then the third stage and sentences and then the third stage is called is called is called realization realization realization and realization ensures we're playing and realization ensures we're playing and realization ensures we're playing correctly by the rules of grammar that correctly by the rules of grammar that correctly by the rules of grammar that for example we know that the past tense for example we know that the past tense for example we know that the past tense of the verb run of the verb run of the verb run actually ran actually ran actually ran and not and not and not runned runned runned yeah that's that's not right yeah that's that's not right yeah that's that's not right so nlg is enabled by a variety of so nlg is enabled by a variety of so nlg is enabled by a variety of machine learning models to perform this machine learning models to perform this machine learning models to perform this stuff and that includes things like stuff and that includes things like stuff and that includes things like hidden markov chains recurrent neural hidden markov chains recurrent neural hidden markov chains recurrent neural networks and transformers networks and transformers networks and transformers look natural language processing and its look natural language processing and its look natural language processing and its subsets nlu and nlg have numerous subsets nlu and nlg have numerous subsets nlu and nlg have numerous practical applications from healthcare practical applications from healthcare practical applications from healthcare diagnosis to online customer service diagnosis to online customer service diagnosis to online customer service oh and another oh and another oh and another way you can use these is in way you can use these is in way you can use these is in hey lightboard videos in fact i asked an hey lightboard videos in fact i asked an hey lightboard videos in fact i asked an nlg algorithm to write me a sentence to nlg algorithm to write me a sentence to nlg algorithm to write me a sentence to conclude this talk conclude this talk conclude this talk and it said and it said and it said natural language processing is amazing natural language processing is amazing natural language processing is amazing and has many practical applications and has many practical applications and has many practical applications like me like me like me thanks nlp algorithm thanks nlp algorithm thanks nlp algorithm if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching",
    "chunks": [
      "Kind: captions Language: en natural language natural language natural language processing natural language natural language natural language understanding and natural language and natural language and natural language generation generation generation n l p n l p n l p n l u n l u n l u n l g n l g n l g what's the difference what's the difference what's the difference allow me to demonstrate now look what i just did there is well now look what i just did there is well now look what i just did",
      "there is well by writing a story by writing a story by writing a story i performed an example of i performed an example of i performed an example of natural language generation and if natural language generation and if natural language generation and if you're now peering at the screen reading you're now peering at the screen reading you're now peering at the screen reading it or even just and trying to understand it or even just and trying to understand it or even just and trying to understand",
      "and make sense of what i'm saying right and make sense of what i'm saying right and make sense of what i'm saying right you're participating in natural language you're participating in natural language you're participating in natural language understanding and together you and i are understanding and together you and i are understanding and together you and i are both performing subsets of the overall both performing subsets of the overall both performing subsets of the overall collective of",
      "collective of collective of natural language natural language natural language processing so processing so processing so and n l and n l and n l they're both subsets of n they're both subsets of n they're both subsets of n but we're missing one quite important but we're missing one quite important but we're missing one quite important point here the natural language point here the natural language point here the natural language processing stuff that we're interested processing stuff that we're",
      "interested processing stuff that we're interested in today is performed by in today is performed by in today is performed by computers not humans so when we use computers not humans so when we use computers not humans so when we use these terms what do we really mean and these terms what do we really mean and these terms what do we really mean and how can these models be put to work well how can these models be put to work well how can these models be put to work well nlp enables computers to",
      "understand nlp enables computers to understand nlp enables computers to understand human language in both written and human language in both written and human language in both written and verbal forms using deep learning verbal forms using deep learning verbal forms using deep learning techniques to complete tasks typical techniques to complete tasks typical techniques to complete tasks typical examples for that are things like examples for that are things like examples for that are things like",
      "language translation or conducting a language translation or conducting a language translation or conducting a conversation in a chat bot conversation in a chat bot conversation in a chat bot now it does this through the now it does this through the now it does this through the identification of named entities which identification of named entities which identification of named entities which is a process called named entity is a process called named entity is a process called named entity",
      "recognition recognition recognition and identification of word patterns and identification of word patterns and identification of word patterns using methods like tokenization stemming using methods like tokenization stemming using methods like tokenization stemming and lemmatization and i've covered some and lemmatization and i've covered some and lemmatization and i've covered some of this in a previous video about nlp so of this in a previous video about nlp so of this in a previous video",
      "about nlp so we won't go over that in detail here we won't go over that in detail here we won't go over that in detail here let's focus instead on these two things let's focus instead on these two things let's focus instead on these two things natural language understanding and natural language understanding and natural language understanding and natural language generation natural language generation natural language generation so natural language understanding so natural language understanding",
      "so natural language understanding uses syntactic and semantic analysis of uses syntactic and semantic analysis of uses syntactic and semantic analysis of text and speech to determine the meaning text and speech to determine the meaning text and speech to determine the meaning of a sentence of a sentence of a sentence unlike structured computer code our unlike structured computer code our unlike structured computer code our unstructured messy human language has unstructured messy human language",
      "has unstructured messy human language has all sorts of nuances that nlu needs to all sorts of nuances that nlu needs to all sorts of nuances that nlu needs to account for so let's take a look at a account for so let's take a look at a account for so let's take a look at a couple of examples i'm going to cover couple of examples i'm going to cover couple of examples i'm going to cover some sentences here so like alice some sentences here so like alice some sentences here so like alice is swimming",
      "against the current this is a sentence that we could feed this is a sentence that we could feed this is a sentence that we could feed into an nlu algorithm and ask it to into an nlu algorithm and ask it to into an nlu algorithm and ask it to really make sense of it really make sense of it really make sense of it another example another example another example the current the current the current version of the file is in the cloud so that's two sentence examples let's so that's two sentence",
      "examples let's so that's two sentence examples let's take a closer look at trying to make take a closer look at trying to make take a closer look at trying to make some sense of these so we've got the some sense of these so we've got the some sense of these so we've got the word current here in this first sentence word current here in this first sentence word current here in this first sentence the word current is a noun and that's the word current is a noun and that's the word current is a noun",
      "and that's preceded by a verb the verb here is preceded by a verb the verb here is preceded by a verb the verb here is swimming together that provides swimming together that provides swimming together that provides additional context to the reader additional context to the reader additional context to the reader allowing us to conclude that we are allowing us to conclude that we are allowing us to conclude that we are referring to the flow of water in the referring to the flow of water in the",
      "referring to the flow of water in the ocean when we talk about current in this ocean when we talk about current in this ocean when we talk about current in this situation in the second example here's situation in the second example here's situation in the second example here's the word current the word current the word current and this time it's an adjective and the and this time it's an adjective and the and this time it's an adjective and the noun it describes noun it describes noun it",
      "describes is version is version is version so that denotes that we've got multiple so that denotes that we've got multiple so that denotes that we've got multiple iterations of a report and here current iterations of a report and here current iterations of a report and here current is implying that we have the most is implying that we have the most is implying that we have the most up-to-date status of the file up-to-date status of the file up-to-date status of the file so two completely",
      "different meanings for so two completely different meanings for so two completely different meanings for current current current and understanding the relationships and understanding the relationships and understanding the relationships between words and phrases is what nlu is between words and phrases is what nlu is between words and phrases is what nlu is really all about and enables us to really all about and enables us to really all about and enables us to derive the intended meaning of a",
      "derive the intended meaning of a derive the intended meaning of a sentence sentence sentence now while nlu is all about improving a now while nlu is all about improving a now while nlu is all about improving a computer's reading comprehension computer's reading comprehension computer's reading comprehension nlg or natural language generation nlg or natural language generation nlg or natural language generation focuses on enabling computers to write focuses on enabling computers to write focuses",
      "on enabling computers to write it's the process of producing a human it's the process of producing a human it's the process of producing a human language text response based on some language text response based on some language text response based on some data input data input data input nlg applications need to consider nlg applications need to consider nlg applications need to consider language rules based on morphology language rules based on morphology language rules based on morphology",
      "lexicons syntax and semantics to make lexicons syntax and semantics to make lexicons syntax and semantics to make choices on how to choices on how to choices on how to phrase responses appropriately phrase responses appropriately phrase responses appropriately now nlg typically consists of three now nlg typically consists of three now nlg typically consists of three stages so if we look at nlg stages so if we look at nlg stages so if we look at nlg the first stage is text the first stage is text",
      "the first stage is text planning and text planning formulates the orders and text planning formulates the orders and text planning formulates the orders and the content in a logical manner and the content in a logical manner and the content in a logical manner similarly we have sentence similarly we have sentence similarly we have sentence planning and sentence planning considers things and sentence planning considers things and sentence planning considers things like punctuation and text flow",
      "and like punctuation and text flow and like punctuation and text flow and breaks out the content into paragraphs breaks out the content into paragraphs breaks out the content into paragraphs and sentences and then the third stage and sentences and then the third stage and sentences and then the third stage is called is called is called realization realization realization and realization ensures we're playing and realization ensures we're playing and realization ensures we're playing correctly by",
      "the rules of grammar that correctly by the rules of grammar that correctly by the rules of grammar that for example we know that the past tense for example we know that the past tense for example we know that the past tense of the verb run of the verb run of the verb run actually ran actually ran actually ran and not and not and not runned runned runned yeah that's that's not right yeah that's that's not right yeah that's that's not right so nlg is enabled by a variety of so nlg is enabled by a",
      "variety of so nlg is enabled by a variety of machine learning models to perform this machine learning models to perform this machine learning models to perform this stuff and that includes things like stuff and that includes things like stuff and that includes things like hidden markov chains recurrent neural hidden markov chains recurrent neural hidden markov chains recurrent neural networks and transformers networks and transformers networks and transformers look natural language processing and",
      "its look natural language processing and its look natural language processing and its subsets nlu and nlg have numerous subsets nlu and nlg have numerous subsets nlu and nlg have numerous practical applications from healthcare practical applications from healthcare practical applications from healthcare diagnosis to online customer service diagnosis to online customer service diagnosis to online customer service oh and another oh and another oh and another way you can use these is in way you can",
      "use these is in way you can use these is in hey lightboard videos in fact i asked an hey lightboard videos in fact i asked an hey lightboard videos in fact i asked an nlg algorithm to write me a sentence to nlg algorithm to write me a sentence to nlg algorithm to write me a sentence to conclude this talk conclude this talk conclude this talk and it said and it said and it said natural language processing is amazing natural language processing is amazing natural language processing is amazing and",
      "has many practical applications and has many practical applications and has many practical applications like me like me like me thanks nlp algorithm thanks nlp algorithm thanks nlp algorithm if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the",
      "future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching"
    ],
    "chunk_count": 28,
    "content_id": "3c7e192b-d2f3-4c3a-af90-02b615146186",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554976"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=L3ynnRgpZwg": {
    "title": "What is an RBM (Restricted Boltzmann Machine)?",
    "url": "https://www.youtube.com/watch?v=L3ynnRgpZwg",
    "description": "Learn more about WatsonX: https://ibm.biz/BdPuC6\n\nLearn more about AI → http://ibm.biz/what-is-ai\nCheck out IBM Watson → http://ibm.biz/Check-Out-Watson\n\nHow do those \"you may also like\" lists get generated?  Well, a great way to do that is by using a restricted Boltzmann machine (RBM). But what actually is an RBM?\nIn this video, Martin Keen will answer that question and explain more about how they work and what else an RBM is good for. \n\nDownload a free AI ebook → http://ibm.biz/free-ai-book\nRead about the Journey to AI → http://ibm.biz/blog-journey-to-ai\n\nGet started for free on IBM Cloud → http://ibm.biz/start-free-cloud\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#AI #Software #ITModernization #watsonx",
    "duration": 365,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en at this very moment you made a decision at this very moment you made a decision at this very moment you made a decision to watch this video thank you but when to watch this video thank you but when to watch this video thank you but when we're done you'll have another decision we're done you'll have another decision we're done you'll have another decision to make do you want to watch another one to make do you want to watch another one to make do you want to watch another one well to assist you with that you'll be well to assist you with that you'll be well to assist you with that you'll be presented with a personalized list of presented with a personalized list of presented with a personalized list of videos that might interest you and videos that might interest you and videos that might interest you and that's a great use case for something that's a great use case for something that's a great use case for something called a restricted boltzman machine or RBM in fact rbms became increasingly RBM in fact rbms became increasingly RBM in fact rbms became increasingly popular after a Netflix competition when popular after a Netflix competition when popular after a Netflix competition when it was used as a collaborative filtering it was used as a collaborative filtering it was used as a collaborative filtering strategy to forecast user ratings for strategy to forecast user ratings for strategy to forecast user ratings for movies and it outperformed most of its movies and it outperformed most of its movies and it outperformed most of its rivals a restricted boltzman machine is rivals a restricted boltzman machine is rivals a restricted boltzman machine is a probabilistic graphical model for a probabilistic graphical model for a probabilistic graphical model for unsupervised learning that is used to unsupervised learning that is used to unsupervised learning that is used to discover hidden structures in data and a discover hidden structures in data and a discover hidden structures in data and a video recommendation system is just a video recommendation system is just a video recommendation system is just a perfect application of that rbms are perfect application of that rbms are perfect application of that rbms are made up of two parts so there's the made up of two parts so there's the made up of two parts so there's the visible layer that contains some nodes and then there is the hidden layer now every node in the visible layer now every node in the visible layer now every node in the visible layer is connected to every node in the layer is connected to every node in the layer is connected to every node in the hidden layer so it's a one to many so hidden layer so it's a one to many so hidden layer so it's a one to many so each node here goes to every node in the each node here goes to every node in the each node here goes to every node in the hidden layer and so is the case for all hidden layer and so is the case for all hidden layer and so is the case for all of the other nodes in the visible layer of the other nodes in the visible layer of the other nodes in the visible layer the restricted part here that comes the restricted part here that comes the restricted part here that comes about because no node is connected to about because no node is connected to about because no node is connected to any other node in the same layer so you any other node in the same layer so you any other node in the same layer so you can see here the visible nodes are not can see here the visible nodes are not can see here the visible nodes are not connected to each other and nor are the connected to each other and nor are the connected to each other and nor are the hidden now all of these nodes are hidden now all of these nodes are hidden now all of these nodes are connected by edges that have something connected by edges that have something connected by edges that have something called called called weights associated with weights associated with weights associated with them and the weights represent the them and the weights represent the them and the weights represent the probability of being active now this is probability of being active now this is probability of being active now this is a very efficient structure for a neuron a very efficient structure for a neuron a very efficient structure for a neuron Network because one input layer can be Network because one input layer can be Network because one input layer can be used for many hidden layers for training used for many hidden layers for training used for many hidden layers for training now to train the network we need to now to train the network we need to now to train the network we need to provide multiple inputs the nodes in the visible layer inputs the nodes in the visible layer inputs the nodes in the visible layer they'll receive the training data this they'll receive the training data this they'll receive the training data this is multiplied by the weights and added is multiplied by the weights and added is multiplied by the weights and added to a bias value at the hidden to a bias value at the hidden to a bias value at the hidden layer this is the first first phase of layer this is the first first phase of layer this is the first first phase of an RBM and it's called the an RBM and it's called the an RBM and it's called the feed Ford pass here we're basically identifying pass here we're basically identifying pass here we're basically identifying the positive associations meaning the the positive associations meaning the the positive associations meaning the link between the visible unit and the link between the visible unit and the link between the visible unit and the hidden unit is a match so maybe this one hidden unit is a match so maybe this one hidden unit is a match so maybe this one is a match and we're looking for is a match and we're looking for is a match and we're looking for negative association when the link negative association when the link negative association when the link between the two nodes is actually between the two nodes is actually between the two nodes is actually negative the second phase is the feed negative the second phase is the feed negative the second phase is the feed backward pass and this pass and this pass and this PA is really used to determine how PA is really used to determine how PA is really used to determine how weightings should be weightings should be weightings should be adjusted and that pass us three things adjusted and that pass us three things adjusted and that pass us three things basically it adjusts the weights it basically it adjusts the weights it basically it adjusts the weights it adjusts the biases and it logs adjusts the biases and it logs adjusts the biases and it logs probability for every Edge between the probability for every Edge between the probability for every Edge between the layers putting enough training data layers putting enough training data layers putting enough training data through these two phases teaches us the through these two phases teaches us the through these two phases teaches us the pattern that is responsible to activate pattern that is responsible to activate pattern that is responsible to activate the hidden nodes we're basically the hidden nodes we're basically the hidden nodes we're basically learning the probability distribution learning the probability distribution learning the probability distribution across a data set now in our video across a data set now in our video across a data set now in our video recommendation example our visible layer recommendation example our visible layer recommendation example our visible layer could consists of videos that a person could consists of videos that a person could consists of videos that a person has watched and then our hidden layer has watched and then our hidden layer has watched and then our hidden layer well that could consist of a well that could consist of a well that could consist of a classification for each video such as classification for each video such as classification for each video such as what is the video about machine learning what is the video about machine learning what is the video about machine learning Python Programming Python Programming Python Programming cats or the hidden layer could be cats or the hidden layer could be cats or the hidden layer could be something else like the style of video something else like the style of video something else like the style of video so like a demo video a vlog and a so like a demo video a vlog and a so like a demo video a vlog and a talking head video by observing the talking head video by observing the talking head video by observing the videos a person is watching our RBM can videos a person is watching our RBM can videos a person is watching our RBM can adjust the waiting and bias to determine adjust the waiting and bias to determine adjust the waiting and bias to determine things such as How likely a person who things such as How likely a person who things such as How likely a person who is interested in machine learning videos is interested in machine learning videos is interested in machine learning videos is also interested in python is also interested in python is also interested in python videos now Beyond recommendation engines videos now Beyond recommendation engines videos now Beyond recommendation engines which are an example of collaborative which are an example of collaborative which are an example of collaborative filtering there are many other use cases filtering there are many other use cases filtering there are many other use cases for RBM for example feature extraction for RBM for example feature extraction for RBM for example feature extraction pattern recognition and that can be used to recognition and that can be used to recognition and that can be used to understand things like handwritten text understand things like handwritten text understand things like handwritten text or we can identify structures in data or we can identify structures in data or we can identify structures in data sets like the hierarchy of what causes sets like the hierarchy of what causes sets like the hierarchy of what causes events to happen using an RBM can be a very very happen using an RBM can be a very very happen using an RBM can be a very very powerful way to learn about your data powerful way to learn about your data powerful way to learn about your data without having to write code around without having to write code around without having to write code around iterating over every node and adjusting iterating over every node and adjusting iterating over every node and adjusting those weights those weights those weights manually and if you do have a bit more manually and if you do have a bit more manually and if you do have a bit more time perhaps the recommendation system time perhaps the recommendation system time perhaps the recommendation system can find you another video that suits can find you another video that suits can find you another video that suits your interests hopefully one from the your interests hopefully one from the your interests hopefully one from the IBM technology Channel if you have any questions please Channel if you have any questions please Channel if you have any questions please drop us a line below and if you want to drop us a line below and if you want to drop us a line below and if you want to see more videos like this in the future see more videos like this in the future see more videos like this in the future please like and subscribe thanks for please like and subscribe thanks for please like and subscribe thanks for watching",
    "chunks": [
      "Kind: captions Language: en at this very moment you made a decision at this very moment you made a decision at this very moment you made a decision to watch this video thank you but when to watch this video thank you but when to watch this video thank you but when we're done you'll have another decision we're done you'll have another decision we're done you'll have another decision to make do you want to watch another one to make do you want to watch another one to make do you want to watch",
      "another one well to assist you with that you'll be well to assist you with that you'll be well to assist you with that you'll be presented with a personalized list of presented with a personalized list of presented with a personalized list of videos that might interest you and videos that might interest you and videos that might interest you and that's a great use case for something that's a great use case for something that's a great use case for something called a restricted boltzman machine or",
      "RBM in fact rbms became increasingly RBM in fact rbms became increasingly RBM in fact rbms became increasingly popular after a Netflix competition when popular after a Netflix competition when popular after a Netflix competition when it was used as a collaborative filtering it was used as a collaborative filtering it was used as a collaborative filtering strategy to forecast user ratings for strategy to forecast user ratings for strategy to forecast user ratings for movies and it outperformed",
      "most of its movies and it outperformed most of its movies and it outperformed most of its rivals a restricted boltzman machine is rivals a restricted boltzman machine is rivals a restricted boltzman machine is a probabilistic graphical model for a probabilistic graphical model for a probabilistic graphical model for unsupervised learning that is used to unsupervised learning that is used to unsupervised learning that is used to discover hidden structures in data and a discover hidden structures",
      "in data and a discover hidden structures in data and a video recommendation system is just a video recommendation system is just a video recommendation system is just a perfect application of that rbms are perfect application of that rbms are perfect application of that rbms are made up of two parts so there's the made up of two parts so there's the made up of two parts so there's the visible layer that contains some nodes and then there is the hidden layer now every node in the visible layer now",
      "every node in the visible layer now every node in the visible layer is connected to every node in the layer is connected to every node in the layer is connected to every node in the hidden layer so it's a one to many so hidden layer so it's a one to many so hidden layer so it's a one to many so each node here goes to every node in the each node here goes to every node in the each node here goes to every node in the hidden layer and so is the case for all hidden layer and so is the case for all",
      "hidden layer and so is the case for all of the other nodes in the visible layer of the other nodes in the visible layer of the other nodes in the visible layer the restricted part here that comes the restricted part here that comes the restricted part here that comes about because no node is connected to about because no node is connected to about because no node is connected to any other node in the same layer so you any other node in the same layer so you any other node in the same layer so you",
      "can see here the visible nodes are not can see here the visible nodes are not can see here the visible nodes are not connected to each other and nor are the connected to each other and nor are the connected to each other and nor are the hidden now all of these nodes are hidden now all of these nodes are hidden now all of these nodes are connected by edges that have something connected by edges that have something connected by edges that have something called called called weights associated with",
      "weights associated with weights associated with them and the weights represent the them and the weights represent the them and the weights represent the probability of being active now this is probability of being active now this is probability of being active now this is a very efficient structure for a neuron a very efficient structure for a neuron a very efficient structure for a neuron Network because one input layer can be Network because one input layer can be Network because one input",
      "layer can be used for many hidden layers for training used for many hidden layers for training used for many hidden layers for training now to train the network we need to now to train the network we need to now to train the network we need to provide multiple inputs the nodes in the visible layer inputs the nodes in the visible layer inputs the nodes in the visible layer they'll receive the training data this they'll receive the training data this they'll receive the training data this is",
      "multiplied by the weights and added is multiplied by the weights and added is multiplied by the weights and added to a bias value at the hidden to a bias value at the hidden to a bias value at the hidden layer this is the first first phase of layer this is the first first phase of layer this is the first first phase of an RBM and it's called the an RBM and it's called the an RBM and it's called the feed Ford pass here we're basically identifying pass here we're basically identifying pass here",
      "we're basically identifying the positive associations meaning the the positive associations meaning the the positive associations meaning the link between the visible unit and the link between the visible unit and the link between the visible unit and the hidden unit is a match so maybe this one hidden unit is a match so maybe this one hidden unit is a match so maybe this one is a match and we're looking for is a match and we're looking for is a match and we're looking for negative association",
      "when the link negative association when the link negative association when the link between the two nodes is actually between the two nodes is actually between the two nodes is actually negative the second phase is the feed negative the second phase is the feed negative the second phase is the feed backward pass and this pass and this pass and this PA is really used to determine how PA is really used to determine how PA is really used to determine how weightings should be weightings should be",
      "weightings should be adjusted and that pass us three things adjusted and that pass us three things adjusted and that pass us three things basically it adjusts the weights it basically it adjusts the weights it basically it adjusts the weights it adjusts the biases and it logs adjusts the biases and it logs adjusts the biases and it logs probability for every Edge between the probability for every Edge between the probability for every Edge between the layers putting enough training data layers",
      "putting enough training data layers putting enough training data through these two phases teaches us the through these two phases teaches us the through these two phases teaches us the pattern that is responsible to activate pattern that is responsible to activate pattern that is responsible to activate the hidden nodes we're basically the hidden nodes we're basically the hidden nodes we're basically learning the probability distribution learning the probability distribution learning the",
      "probability distribution across a data set now in our video across a data set now in our video across a data set now in our video recommendation example our visible layer recommendation example our visible layer recommendation example our visible layer could consists of videos that a person could consists of videos that a person could consists of videos that a person has watched and then our hidden layer has watched and then our hidden layer has watched and then our hidden layer well that could",
      "consist of a well that could consist of a well that could consist of a classification for each video such as classification for each video such as classification for each video such as what is the video about machine learning what is the video about machine learning what is the video about machine learning Python Programming Python Programming Python Programming cats or the hidden layer could be cats or the hidden layer could be cats or the hidden layer could be something else like the style of",
      "video something else like the style of video something else like the style of video so like a demo video a vlog and a so like a demo video a vlog and a so like a demo video a vlog and a talking head video by observing the talking head video by observing the talking head video by observing the videos a person is watching our RBM can videos a person is watching our RBM can videos a person is watching our RBM can adjust the waiting and bias to determine adjust the waiting and bias to determine",
      "adjust the waiting and bias to determine things such as How likely a person who things such as How likely a person who things such as How likely a person who is interested in machine learning videos is interested in machine learning videos is interested in machine learning videos is also interested in python is also interested in python is also interested in python videos now Beyond recommendation engines videos now Beyond recommendation engines videos now Beyond recommendation engines which are",
      "an example of collaborative which are an example of collaborative which are an example of collaborative filtering there are many other use cases filtering there are many other use cases filtering there are many other use cases for RBM for example feature extraction for RBM for example feature extraction for RBM for example feature extraction pattern recognition and that can be used to recognition and that can be used to recognition and that can be used to understand things like handwritten text",
      "understand things like handwritten text understand things like handwritten text or we can identify structures in data or we can identify structures in data or we can identify structures in data sets like the hierarchy of what causes sets like the hierarchy of what causes sets like the hierarchy of what causes events to happen using an RBM can be a very very happen using an RBM can be a very very happen using an RBM can be a very very powerful way to learn about your data powerful way to learn",
      "about your data powerful way to learn about your data without having to write code around without having to write code around without having to write code around iterating over every node and adjusting iterating over every node and adjusting iterating over every node and adjusting those weights those weights those weights manually and if you do have a bit more manually and if you do have a bit more manually and if you do have a bit more time perhaps the recommendation system time perhaps the",
      "recommendation system time perhaps the recommendation system can find you another video that suits can find you another video that suits can find you another video that suits your interests hopefully one from the your interests hopefully one from the your interests hopefully one from the IBM technology Channel if you have any questions please Channel if you have any questions please Channel if you have any questions please drop us a line below and if you want to drop us a line below and if you",
      "want to drop us a line below and if you want to see more videos like this in the future see more videos like this in the future see more videos like this in the future please like and subscribe thanks for please like and subscribe thanks for please like and subscribe thanks for watching"
    ],
    "chunk_count": 24,
    "content_id": "52d53387-1696-42e2-bdd0-b2f80236b429",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554979"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=q6kJ71tEYqM": {
    "title": "Machine Learning vs Deep Learning",
    "url": "https://www.youtube.com/watch?v=q6kJ71tEYqM",
    "description": "Learn about watsonx  → https://ibm.biz/BdvxDm\n\nGet a unique perspective on what the difference is between Machine Learning and Deep Learning - explained and illustrated in a delicious analogy of ordering pizza by IBMer and Master Inventor, Martin Keen.\n\n#AI #Software #ITModernization #DeepLearning #MachineLearning",
    "duration": 469,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en look fair warning if you're feeling a look fair warning if you're feeling a look fair warning if you're feeling a little hungry right now you might want little hungry right now you might want little hungry right now you might want to pause this video and grab a snack to pause this video and grab a snack to pause this video and grab a snack before continuing because before continuing because before continuing because i'm going to explain the difference i'm going to explain the difference i'm going to explain the difference between machine learning between machine learning between machine learning and deep learning and deep learning and deep learning talking about pizza talking about pizza talking about pizza delicious delicious delicious tasty tasty tasty pizza pizza pizza now before we get to that let's let's now before we get to that let's let's now before we get to that let's let's address the fundamental question here address the fundamental question here address the fundamental question here what is the difference between these two what is the difference between these two what is the difference between these two terms terms terms well put simply deep learning is a well put simply deep learning is a well put simply deep learning is a subset of machine learning actually the subset of machine learning actually the subset of machine learning actually the the hierarchy goes like this at the top the hierarchy goes like this at the top the hierarchy goes like this at the top we have a we have a we have a or artificial intelligence now a or artificial intelligence now a or artificial intelligence now a subfield of a i subfield of a i subfield of a i is ml is ml is ml or machine learning or machine learning or machine learning beneath that then we have n n or beneath that then we have n n or beneath that then we have n n or neural networks neural networks neural networks and they make up the backbone of and they make up the backbone of and they make up the backbone of deep deep deep learning algorithms dl here on the ibm technology channel we here on the ibm technology channel we here on the ibm technology channel we have a whole bunch of videos on these have a whole bunch of videos on these have a whole bunch of videos on these topics you might want to consider topics you might want to consider topics you might want to consider subscribing subscribing subscribing now machine learning algorithms leverage now machine learning algorithms leverage now machine learning algorithms leverage structured labeled data to make structured labeled data to make structured labeled data to make predictions predictions predictions let's build one a model to determine let's build one a model to determine let's build one a model to determine whether whether whether we should order pizza for dinner we should order pizza for dinner we should order pizza for dinner there are three main factors that there are three main factors that there are three main factors that influence that decision so let's map influence that decision so let's map influence that decision so let's map those out as inputs the first of those those out as inputs the first of those those out as inputs the first of those inputs we'll call inputs we'll call inputs we'll call and x1 asks will it save time by and x1 asks will it save time by and x1 asks will it save time by ordering out ordering out ordering out we can say yes with a one or no with a we can say yes with a one or no with a we can say yes with a one or no with a zero zero zero yes it will so x that equals one yes it will so x that equals one yes it will so x that equals one now x two that input says will i lose weight by that input says will i lose weight by that input says will i lose weight by ordering pizza ordering pizza ordering pizza that's a zero i'm i'm ordering all the that's a zero i'm i'm ordering all the that's a zero i'm i'm ordering all the toppings toppings toppings and x3 and x3 and x3 will it save me will it save me will it save me money money money actually i have a coupon for a free actually i have a coupon for a free actually i have a coupon for a free pizza today pizza today pizza today so that's a one so that's a one so that's a one now look these binary responses ones and now look these binary responses ones and now look these binary responses ones and zeros i'm using them for simplicity but zeros i'm using them for simplicity but zeros i'm using them for simplicity but neurons in a network can represent neurons in a network can represent neurons in a network can represent values from well everything to values from well everything to values from well everything to everything negative infinity to positive everything negative infinity to positive everything negative infinity to positive infinity infinity infinity with our inputs defined we can assign with our inputs defined we can assign with our inputs defined we can assign weights to determine importance weights to determine importance weights to determine importance larger weights make a single inputs larger weights make a single inputs larger weights make a single inputs contribution to the output more contribution to the output more contribution to the output more significant compared to other inputs significant compared to other inputs significant compared to other inputs now my threshold here is five so let's now my threshold here is five so let's now my threshold here is five so let's weight each one of these w1 weight each one of these w1 weight each one of these w1 well i'm going to give this a full well i'm going to give this a full well i'm going to give this a full five because i value my time five because i value my time five because i value my time and w2 and w2 and w2 this was the will i lose weight 1 i'm this was the will i lose weight 1 i'm this was the will i lose weight 1 i'm going to rate this a 3 because i have going to rate this a 3 because i have going to rate this a 3 because i have some interest in keeping in shape some interest in keeping in shape some interest in keeping in shape and for w3 and for w3 and for w3 i'm going to give this a 2 because like i'm going to give this a 2 because like i'm going to give this a 2 because like either way this isn't going to break the either way this isn't going to break the either way this isn't going to break the bank to order dinner bank to order dinner bank to order dinner now we plug these weights into our model now we plug these weights into our model now we plug these weights into our model and using an activation function we can and using an activation function we can and using an activation function we can calculate the output calculate the output calculate the output which in this case is the decision to which in this case is the decision to which in this case is the decision to order pizza or not order pizza or not order pizza or not so to calculate that we're going to so to calculate that we're going to so to calculate that we're going to calculate the y hat calculate the y hat calculate the y hat and we're going to use these weights and and we're going to use these weights and and we're going to use these weights and these inputs so here we've got 1 times 5 these inputs so here we've got 1 times 5 these inputs so here we've got 1 times 5 we've got we've got we've got 0 times 3 0 times 3 0 times 3 and we've got 1 times and we've got 1 times and we've got 1 times and we need to consider as well our and we need to consider as well our and we need to consider as well our threshold which was threshold which was threshold which was so that gives us if we just add these up so that gives us if we just add these up so that gives us if we just add these up 1 times 5 that's 5 plus 1 times 5 that's 5 plus 1 times 5 that's 5 plus 0 times 3 that's 0 plus 1 times 2 that's 0 times 3 that's 0 plus 1 times 2 that's 0 times 3 that's 0 plus 1 times 2 that's minus 5. well that gives us a total of minus 5. well that gives us a total of minus 5. well that gives us a total of positive 2. positive 2. positive 2. and because the output is a positive and because the output is a positive and because the output is a positive number this correlates to number this correlates to number this correlates to pizza night pizza night pizza night okay so that's machine learning but what okay so that's machine learning but what okay so that's machine learning but what differentiates differentiates differentiates deep learning deep learning deep learning well the answer to that is well the answer to that is well the answer to that is more than three more than three more than three as in a neural network is considered a as in a neural network is considered a as in a neural network is considered a deep neural network deep neural network deep neural network if it consists of more than three layers if it consists of more than three layers if it consists of more than three layers that includes the input and the output that includes the input and the output that includes the input and the output layer so we've got our input and output layer so we've got our input and output layer so we've got our input and output we have multiple layers in the middle we have multiple layers in the middle we have multiple layers in the middle and this would be considered and this would be considered and this would be considered a deep a deep a deep learning learning learning network network network classical machine learning is more classical machine learning is more classical machine learning is more dependent on human intervention to learn dependent on human intervention to learn dependent on human intervention to learn human experts well they determine a human experts well they determine a human experts well they determine a hierarchy of features to understand the hierarchy of features to understand the hierarchy of features to understand the differences between data inputs so if i differences between data inputs so if i differences between data inputs so if i showed you a series of images of showed you a series of images of showed you a series of images of different types of fast food like pizza different types of fast food like pizza different types of fast food like pizza burger and taco burger and taco burger and taco you could label these in a data set for you could label these in a data set for you could label these in a data set for processing by the neural network a human processing by the neural network a human processing by the neural network a human expert here has determined the expert here has determined the expert here has determined the characteristics which distinguish each characteristics which distinguish each characteristics which distinguish each picture as the specific fast food type picture as the specific fast food type picture as the specific fast food type so for example it might be the bread of so for example it might be the bread of so for example it might be the bread of each food type might be a distinguishing each food type might be a distinguishing each food type might be a distinguishing feature across each picture feature across each picture feature across each picture now this is known as supervised learning now this is known as supervised learning now this is known as supervised learning because the process incorporates human because the process incorporates human because the process incorporates human intervention or human supervision intervention or human supervision intervention or human supervision deep machine learning doesn't deep machine learning doesn't deep machine learning doesn't necessarily require a labeled data set necessarily require a labeled data set necessarily require a labeled data set it can ingest unstructured data in its it can ingest unstructured data in its it can ingest unstructured data in its raw form like text and images and it can raw form like text and images and it can raw form like text and images and it can automatically determine the set of automatically determine the set of automatically determine the set of features which distinguish pizza features which distinguish pizza features which distinguish pizza burger and taco from one another burger and taco from one another burger and taco from one another by observing patterns in the data a deep by observing patterns in the data a deep by observing patterns in the data a deep learning model can cluster inputs learning model can cluster inputs learning model can cluster inputs appropriately appropriately appropriately these algorithms discover hidden these algorithms discover hidden these algorithms discover hidden patterns of data groupings without the patterns of data groupings without the patterns of data groupings without the need for human intervention and they're need for human intervention and they're need for human intervention and they're known as unsupervised learning known as unsupervised learning known as unsupervised learning most deep neural networks are feed most deep neural networks are feed most deep neural networks are feed forward that means that they go in one forward that means that they go in one forward that means that they go in one direction from the input to the output direction from the input to the output direction from the input to the output however you can also train your model however you can also train your model however you can also train your model through something called a back through something called a back through something called a back propagation propagation propagation that is it moves in the opposite that is it moves in the opposite that is it moves in the opposite direction from output to input direction from output to input direction from output to input back propagation allows us to calculate back propagation allows us to calculate back propagation allows us to calculate and attribute the error associated with and attribute the error associated with and attribute the error associated with each neuron and allows us to adjust and each neuron and allows us to adjust and each neuron and allows us to adjust and fit the algorithm appropriately fit the algorithm appropriately fit the algorithm appropriately so when we talk about machine learning so when we talk about machine learning so when we talk about machine learning and deep learning and deep learning and deep learning we're essentially talking about the same we're essentially talking about the same we're essentially talking about the same field of study neural networks they're field of study neural networks they're field of study neural networks they're the foundation of both types of learning the foundation of both types of learning the foundation of both types of learning and both are considered subfields of a i and both are considered subfields of a i and both are considered subfields of a i the main distinction between the two are the main distinction between the two are the main distinction between the two are that number of layers in a neural that number of layers in a neural that number of layers in a neural network network network more than three more than three more than three and whether or not human intervention is and whether or not human intervention is and whether or not human intervention is required to label data required to label data required to label data pizza burgers tacos pizza burgers tacos pizza burgers tacos yeah that's uh that's enough for today yeah that's uh that's enough for today yeah that's uh that's enough for today it's time for lunch it's time for lunch it's time for lunch oh oh and before i go if you did enjoy oh oh and before i go if you did enjoy oh oh and before i go if you did enjoy this video here are some others you this video here are some others you this video here are some others you might also like might also like might also like if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching",
    "chunks": [
      "Kind: captions Language: en look fair warning if you're feeling a look fair warning if you're feeling a look fair warning if you're feeling a little hungry right now you might want little hungry right now you might want little hungry right now you might want to pause this video and grab a snack to pause this video and grab a snack to pause this video and grab a snack before continuing because before continuing because before continuing because i'm going to explain the difference i'm going to",
      "explain the difference i'm going to explain the difference between machine learning between machine learning between machine learning and deep learning and deep learning and deep learning talking about pizza talking about pizza talking about pizza delicious delicious delicious tasty tasty tasty pizza pizza pizza now before we get to that let's let's now before we get to that let's let's now before we get to that let's let's address the fundamental question here address the fundamental question",
      "here address the fundamental question here what is the difference between these two what is the difference between these two what is the difference between these two terms terms terms well put simply deep learning is a well put simply deep learning is a well put simply deep learning is a subset of machine learning actually the subset of machine learning actually the subset of machine learning actually the the hierarchy goes like this at the top the hierarchy goes like this at the top the",
      "hierarchy goes like this at the top we have a we have a we have a or artificial intelligence now a or artificial intelligence now a or artificial intelligence now a subfield of a i subfield of a i subfield of a i is ml is ml is ml or machine learning or machine learning or machine learning beneath that then we have n n or beneath that then we have n n or beneath that then we have n n or neural networks neural networks neural networks and they make up the backbone of and they make up the backbone",
      "of and they make up the backbone of deep deep deep learning algorithms dl here on the ibm technology channel we here on the ibm technology channel we here on the ibm technology channel we have a whole bunch of videos on these have a whole bunch of videos on these have a whole bunch of videos on these topics you might want to consider topics you might want to consider topics you might want to consider subscribing subscribing subscribing now machine learning algorithms leverage now machine learning",
      "algorithms leverage now machine learning algorithms leverage structured labeled data to make structured labeled data to make structured labeled data to make predictions predictions predictions let's build one a model to determine let's build one a model to determine let's build one a model to determine whether whether whether we should order pizza for dinner we should order pizza for dinner we should order pizza for dinner there are three main factors that there are three main factors that there",
      "are three main factors that influence that decision so let's map influence that decision so let's map influence that decision so let's map those out as inputs the first of those those out as inputs the first of those those out as inputs the first of those inputs we'll call inputs we'll call inputs we'll call and x1 asks will it save time by and x1 asks will it save time by and x1 asks will it save time by ordering out ordering out ordering out we can say yes with a one or no with a we can say yes",
      "with a one or no with a we can say yes with a one or no with a zero zero zero yes it will so x that equals one yes it will so x that equals one yes it will so x that equals one now x two that input says will i lose weight by that input says will i lose weight by that input says will i lose weight by ordering pizza ordering pizza ordering pizza that's a zero i'm i'm ordering all the that's a zero i'm i'm ordering all the that's a zero i'm i'm ordering all the toppings toppings toppings and x3 and",
      "x3 and x3 will it save me will it save me will it save me money money money actually i have a coupon for a free actually i have a coupon for a free actually i have a coupon for a free pizza today pizza today pizza today so that's a one so that's a one so that's a one now look these binary responses ones and now look these binary responses ones and now look these binary responses ones and zeros i'm using them for simplicity but zeros i'm using them for simplicity but zeros i'm using them for",
      "simplicity but neurons in a network can represent neurons in a network can represent neurons in a network can represent values from well everything to values from well everything to values from well everything to everything negative infinity to positive everything negative infinity to positive everything negative infinity to positive infinity infinity infinity with our inputs defined we can assign with our inputs defined we can assign with our inputs defined we can assign weights to determine",
      "importance weights to determine importance weights to determine importance larger weights make a single inputs larger weights make a single inputs larger weights make a single inputs contribution to the output more contribution to the output more contribution to the output more significant compared to other inputs significant compared to other inputs significant compared to other inputs now my threshold here is five so let's now my threshold here is five so let's now my threshold here is five so",
      "let's weight each one of these w1 weight each one of these w1 weight each one of these w1 well i'm going to give this a full well i'm going to give this a full well i'm going to give this a full five because i value my time five because i value my time five because i value my time and w2 and w2 and w2 this was the will i lose weight 1 i'm this was the will i lose weight 1 i'm this was the will i lose weight 1 i'm going to rate this a 3 because i have going to rate this a 3 because i have going to",
      "rate this a 3 because i have some interest in keeping in shape some interest in keeping in shape some interest in keeping in shape and for w3 and for w3 and for w3 i'm going to give this a 2 because like i'm going to give this a 2 because like i'm going to give this a 2 because like either way this isn't going to break the either way this isn't going to break the either way this isn't going to break the bank to order dinner bank to order dinner bank to order dinner now we plug these weights into",
      "our model now we plug these weights into our model now we plug these weights into our model and using an activation function we can and using an activation function we can and using an activation function we can calculate the output calculate the output calculate the output which in this case is the decision to which in this case is the decision to which in this case is the decision to order pizza or not order pizza or not order pizza or not so to calculate that we're going to so to calculate",
      "that we're going to so to calculate that we're going to calculate the y hat calculate the y hat calculate the y hat and we're going to use these weights and and we're going to use these weights and and we're going to use these weights and these inputs so here we've got 1 times 5 these inputs so here we've got 1 times 5 these inputs so here we've got 1 times 5 we've got we've got we've got 0 times 3 0 times 3 0 times 3 and we've got 1 times and we've got 1 times and we've got 1 times and we need",
      "to consider as well our and we need to consider as well our and we need to consider as well our threshold which was threshold which was threshold which was so that gives us if we just add these up so that gives us if we just add these up so that gives us if we just add these up 1 times 5 that's 5 plus 1 times 5 that's 5 plus 1 times 5 that's 5 plus 0 times 3 that's 0 plus 1 times 2 that's 0 times 3 that's 0 plus 1 times 2 that's 0 times 3 that's 0 plus 1 times 2 that's minus 5. well that gives us",
      "a total of minus 5. well that gives us a total of minus 5. well that gives us a total of positive 2. positive 2. positive 2. and because the output is a positive and because the output is a positive and because the output is a positive number this correlates to number this correlates to number this correlates to pizza night pizza night pizza night okay so that's machine learning but what okay so that's machine learning but what okay so that's machine learning but what differentiates",
      "differentiates differentiates deep learning deep learning deep learning well the answer to that is well the answer to that is well the answer to that is more than three more than three more than three as in a neural network is considered a as in a neural network is considered a as in a neural network is considered a deep neural network deep neural network deep neural network if it consists of more than three layers if it consists of more than three layers if it consists of more than three layers",
      "that includes the input and the output that includes the input and the output that includes the input and the output layer so we've got our input and output layer so we've got our input and output layer so we've got our input and output we have multiple layers in the middle we have multiple layers in the middle we have multiple layers in the middle and this would be considered and this would be considered and this would be considered a deep a deep a deep learning learning learning network network",
      "network classical machine learning is more classical machine learning is more classical machine learning is more dependent on human intervention to learn dependent on human intervention to learn dependent on human intervention to learn human experts well they determine a human experts well they determine a human experts well they determine a hierarchy of features to understand the hierarchy of features to understand the hierarchy of features to understand the differences between data inputs so if",
      "i differences between data inputs so if i differences between data inputs so if i showed you a series of images of showed you a series of images of showed you a series of images of different types of fast food like pizza different types of fast food like pizza different types of fast food like pizza burger and taco burger and taco burger and taco you could label these in a data set for you could label these in a data set for you could label these in a data set for processing by the neural network",
      "a human processing by the neural network a human processing by the neural network a human expert here has determined the expert here has determined the expert here has determined the characteristics which distinguish each characteristics which distinguish each characteristics which distinguish each picture as the specific fast food type picture as the specific fast food type picture as the specific fast food type so for example it might be the bread of so for example it might be the bread of so",
      "for example it might be the bread of each food type might be a distinguishing each food type might be a distinguishing each food type might be a distinguishing feature across each picture feature across each picture feature across each picture now this is known as supervised learning now this is known as supervised learning now this is known as supervised learning because the process incorporates human because the process incorporates human because the process incorporates human intervention or",
      "human supervision intervention or human supervision intervention or human supervision deep machine learning doesn't deep machine learning doesn't deep machine learning doesn't necessarily require a labeled data set necessarily require a labeled data set necessarily require a labeled data set it can ingest unstructured data in its it can ingest unstructured data in its it can ingest unstructured data in its raw form like text and images and it can raw form like text and images and it can raw form",
      "like text and images and it can automatically determine the set of automatically determine the set of automatically determine the set of features which distinguish pizza features which distinguish pizza features which distinguish pizza burger and taco from one another burger and taco from one another burger and taco from one another by observing patterns in the data a deep by observing patterns in the data a deep by observing patterns in the data a deep learning model can cluster inputs learning",
      "model can cluster inputs learning model can cluster inputs appropriately appropriately appropriately these algorithms discover hidden these algorithms discover hidden these algorithms discover hidden patterns of data groupings without the patterns of data groupings without the patterns of data groupings without the need for human intervention and they're need for human intervention and they're need for human intervention and they're known as unsupervised learning known as unsupervised learning",
      "known as unsupervised learning most deep neural networks are feed most deep neural networks are feed most deep neural networks are feed forward that means that they go in one forward that means that they go in one forward that means that they go in one direction from the input to the output direction from the input to the output direction from the input to the output however you can also train your model however you can also train your model however you can also train your model through something",
      "called a back through something called a back through something called a back propagation propagation propagation that is it moves in the opposite that is it moves in the opposite that is it moves in the opposite direction from output to input direction from output to input direction from output to input back propagation allows us to calculate back propagation allows us to calculate back propagation allows us to calculate and attribute the error associated with and attribute the error associated",
      "with and attribute the error associated with each neuron and allows us to adjust and each neuron and allows us to adjust and each neuron and allows us to adjust and fit the algorithm appropriately fit the algorithm appropriately fit the algorithm appropriately so when we talk about machine learning so when we talk about machine learning so when we talk about machine learning and deep learning and deep learning and deep learning we're essentially talking about the same we're essentially talking",
      "about the same we're essentially talking about the same field of study neural networks they're field of study neural networks they're field of study neural networks they're the foundation of both types of learning the foundation of both types of learning the foundation of both types of learning and both are considered subfields of a i and both are considered subfields of a i and both are considered subfields of a i the main distinction between the two are the main distinction between the two are",
      "the main distinction between the two are that number of layers in a neural that number of layers in a neural that number of layers in a neural network network network more than three more than three more than three and whether or not human intervention is and whether or not human intervention is and whether or not human intervention is required to label data required to label data required to label data pizza burgers tacos pizza burgers tacos pizza burgers tacos yeah that's uh that's enough for",
      "today yeah that's uh that's enough for today yeah that's uh that's enough for today it's time for lunch it's time for lunch it's time for lunch oh oh and before i go if you did enjoy oh oh and before i go if you did enjoy oh oh and before i go if you did enjoy this video here are some others you this video here are some others you this video here are some others you might also like might also like might also like if you have any questions please drop us if you have any questions please drop us if",
      "you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching"
    ],
    "chunk_count": 33,
    "content_id": "a7858d7a-57e8-4248-9899-bc14105107ff",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554982"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=gkXX4h3qYm4": {
    "title": "What is Random Forest?",
    "url": "https://www.youtube.com/watch?v=gkXX4h3qYm4",
    "description": "Learn about watsonx: https://ibm.biz/BdvxRb\n\nCan't see the random forest for the search trees? What IS a \"random forest\" anyway?\nIBM Master Inventor Martin Keen explains what random forests are, how to set them up, and how they can help you with all sorts of classification problems in a wide variety of applications such as finance, medicine, and economics.\n\n#DataMigration #DataScience #DataMining #AI",
    "duration": 321,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en I just can't decide, should I play a round of golf today? Well, let's use this decision tree to make the decision. So first off, do I have the time? If I don't, well, then that's an easy decision. No golf. But let's say I do. Second decision point, is it sunny today? If there's sun, then I don't care about any other factor. I'm playing golf. If there's no sun, let's go down to the next level. Well, do I have my clubs with me? Do I have them handy? If I do not, then I'm not going to bother playing if it's not sunny. If I do, then I absolutely will. The decision tree here is an example of a classification problem where the class labels are \"golf yes\" and \"golf no\". And, while they're helpful, decision trees they can though be prone to problems. Things like bias and overfitting. But that is where something called \"random forest\" comes in to play. Random forest is a type of machine learning model that uses an ensemble of decision trees to make its predictions. And why do we call it random forest? Well, the reason is because it's actually built by taking a random sample of my data and then building an ongoing series of decision trees on the subsets. So we're essentially creating a whole bunch of decision trees together. And those give us a larger model or group. Look, the chances are that other people have built different and maybe better decision trees to answer the same question. Maybe those trees consider things like the time of day, which I didn't consider, or the difficulty of the course. The more decision trees that I use with different criteria, the better my random forest will perform because it's essentially increasing my prediction accuracy. And if one or two of these smaller decision trees are not relevant on a certain day, well, we just ignore them. One of the primary benefits of random forest is that it can help reduce overfitting. And this occurs when your model starts to memorize the data rather than trying to generalize from making predictions on future data. Essentially, it helps me get around the limitations of my data, which might not be fully representative of all golfers or all the best features in my model. It can also help reduce something else, and that's bias. Bias can occur when there is a certain degree of error introduced into the model. Bias occurs when you're not evenly splitting your instance space during training. So instead of seeing all of the data points, you might see only half because of how you set your model up. Now to set up a random forest, you will set some parameters. We have parameters for node size. We have parameters for number of trees. And we also have parameters for a number of features. And it can be challenging at first because you'll want to use a lot of trees, like as many as you can, to get the best predictive accuracy, but you don't want so many trees that it'll take you a long time to train the model and use a lot of memory space. But once you've set up these parameters, you'll use a random forest model to make predictions on your test data. And you can even segment or slice your results by different criteria. Maybe you want to know how your random forest does on certain types of golf courses or how it performs during different times of day. Random forest is pretty popular among data science professionals and with good reason. It can be extremely helpful in all sorts of classification problems. In finance, for example, it can be used to predict the likelihood of a default. In a medical diagnosis, it can be used to predict prognosis or survival rates depending on treatment options and in economics. It can be used to sort of help understand whether a policy is effective or ineffective. So, what do you think? Should I play golf today? Well, the sum of all my random forest decision trees say yes. I'll see you out on the course. If you have any questions, please drop us a line below, and if you want to see more videos like this in the future, please like and subscribe. Thanks for watching.",
    "chunks": [
      "Kind: captions Language: en I just can't decide, should I play a round of golf today? Well, let's use this decision tree to make the decision. So first off, do I have the time? If I don't, well, then that's an easy decision. No golf. But let's say I do. Second decision point, is it sunny today? If there's sun, then I don't care about any other factor. I'm playing golf. If there's no sun, let's go down to the next level. Well, do I have my clubs with me? Do I have them handy? If I do not, then",
      "I'm not going to bother playing if it's not sunny. If I do, then I absolutely will. The decision tree here is an example of a classification problem where the class labels are \"golf yes\" and \"golf no\". And, while they're helpful, decision trees they can though be prone to problems. Things like bias and overfitting. But that is where something called \"random forest\" comes in to play. Random forest is a type of machine learning model that uses an ensemble of decision trees to make its predictions.",
      "And why do we call it random forest? Well, the reason is because it's actually built by taking a random sample of my data and then building an ongoing series of decision trees on the subsets. So we're essentially creating a whole bunch of decision trees together. And those give us a larger model or group. Look, the chances are that other people have built different and maybe better decision trees to answer the same question. Maybe those trees consider things like the time of day, which I didn't",
      "consider, or the difficulty of the course. The more decision trees that I use with different criteria, the better my random forest will perform because it's essentially increasing my prediction accuracy. And if one or two of these smaller decision trees are not relevant on a certain day, well, we just ignore them. One of the primary benefits of random forest is that it can help reduce overfitting. And this occurs when your model starts to memorize the data rather than trying to generalize from",
      "making predictions on future data. Essentially, it helps me get around the limitations of my data, which might not be fully representative of all golfers or all the best features in my model. It can also help reduce something else, and that's bias. Bias can occur when there is a certain degree of error introduced into the model. Bias occurs when you're not evenly splitting your instance space during training. So instead of seeing all of the data points, you might see only half because of how you",
      "set your model up. Now to set up a random forest, you will set some parameters. We have parameters for node size. We have parameters for number of trees. And we also have parameters for a number of features. And it can be challenging at first because you'll want to use a lot of trees, like as many as you can, to get the best predictive accuracy, but you don't want so many trees that it'll take you a long time to train the model and use a lot of memory space. But once you've set up these",
      "parameters, you'll use a random forest model to make predictions on your test data. And you can even segment or slice your results by different criteria. Maybe you want to know how your random forest does on certain types of golf courses or how it performs during different times of day. Random forest is pretty popular among data science professionals and with good reason. It can be extremely helpful in all sorts of classification problems. In finance, for example, it can be used to predict the",
      "likelihood of a default. In a medical diagnosis, it can be used to predict prognosis or survival rates depending on treatment options and in economics. It can be used to sort of help understand whether a policy is effective or ineffective. So, what do you think? Should I play golf today? Well, the sum of all my random forest decision trees say yes. I'll see you out on the course. If you have any questions, please drop us a line below, and if you want to see more videos like this in the future,",
      "please like and subscribe. Thanks for watching."
    ],
    "chunk_count": 9,
    "content_id": "fe28171b-af28-4d52-b09b-1a2ab1dafc7d",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554985"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=7TqhmX92P6U": {
    "title": "What is Monte Carlo Simulation?",
    "url": "https://www.youtube.com/watch?v=7TqhmX92P6U",
    "description": "Learn more about watsonx: https://ibm.biz/BdvxDh\n\nMonte Carlo Simulation, also known as the Monte Carlo Method or a multiple probability simulation, is a mathematical technique, which is used to estimate the possible outcomes of an uncertain event. Watch master inventor Martin Keen explain how Monte Carlo Simulation may be your closest thing to a time machine - or more simply - how it may be a way to look into the future for making business decisions.\n\n#AI #Software #ITModernization #MonteCarloSimulation #lightboard #IBM",
    "duration": 274,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en monte carlo simulation is a mathematical monte carlo simulation is a mathematical monte carlo simulation is a mathematical technique which is used to estimate the technique which is used to estimate the technique which is used to estimate the possible outcomes of an uncertain event possible outcomes of an uncertain event possible outcomes of an uncertain event it's a chance to see into the future and it's a chance to see into the future and it's a chance to see into the future and while actual time travel is still beyond while actual time travel is still beyond while actual time travel is still beyond us let's address three questions about us let's address three questions about us let's address three questions about monte carlo simulations to get you on monte carlo simulations to get you on monte carlo simulations to get you on your way to making better decisions your way to making better decisions your way to making better decisions come on in guys so number one come on in guys so number one come on in guys so number one how do they work how do they work how do they work multi-color simulation works by modeling multi-color simulation works by modeling multi-color simulation works by modeling the probability of different outcomes in the probability of different outcomes in the probability of different outcomes in a process or a system that cannot easily a process or a system that cannot easily a process or a system that cannot easily be predicted due to the intervention of be predicted due to the intervention of be predicted due to the intervention of random variables and it uses something random variables and it uses something random variables and it uses something called random called random called random sampling sampling sampling and rambling random sampling is used to and rambling random sampling is used to and rambling random sampling is used to generate multiple possible outcomes and generate multiple possible outcomes and generate multiple possible outcomes and calculate the average result calculate the average result calculate the average result so take for example the calculation of so take for example the calculation of so take for example the calculation of the probability the probability the probability of rolling two standard dice well if you wanted to calculate this well if you wanted to calculate this well if you wanted to calculate this probability the brute force way you probability the brute force way you probability the brute force way you would have to roll the dice a whole would have to roll the dice a whole would have to roll the dice a whole bunch say bunch say bunch say 000 times if we consider that there are 000 times if we consider that there are 000 times if we consider that there are six sides to a dice we have two of them six sides to a dice we have two of them six sides to a dice we have two of them and we want to run this a thousand times and we want to run this a thousand times and we want to run this a thousand times to get a good sample size but with a to get a good sample size but with a to get a good sample size but with a monte carlo simulation we can reduce the monte carlo simulation we can reduce the monte carlo simulation we can reduce the number of roles by randomly sampling the number of roles by randomly sampling the number of roles by randomly sampling the possible outcomes knowing there are 36 possible outcomes knowing there are 36 possible outcomes knowing there are 36 combination of dice rolls and combination of dice rolls and combination of dice rolls and calculating the percentage of times that calculating the percentage of times that calculating the percentage of times that we get say a seven we get say a seven we get say a seven now number two now number two now number two who uses them there are a number of who uses them there are a number of who uses them there are a number of common applications for monte carlo common applications for monte carlo common applications for monte carlo simulations and perhaps the most well simulations and perhaps the most well simulations and perhaps the most well known of those is in the area known of those is in the area known of those is in the area of just portfolio management of just portfolio management of just portfolio management and also in the area of investment and also in the area of investment and also in the area of investment planning planning planning by running thousands or even millions of by running thousands or even millions of by running thousands or even millions of simulations investors can get a better simulations investors can get a better simulations investors can get a better idea of how their portfolio might idea of how their portfolio might idea of how their portfolio might perform under different market perform under different market perform under different market conditions and other common applications conditions and other common applications conditions and other common applications are things like risk analysis option are things like risk analysis option are things like risk analysis option pricing and planning for spare capacity pricing and planning for spare capacity pricing and planning for spare capacity but a monte carlo simulation is applied but a monte carlo simulation is applied but a monte carlo simulation is applied in all sorts of fields from medicine all in all sorts of fields from medicine all in all sorts of fields from medicine all the way through to astrophysics all the the way through to astrophysics all the the way through to astrophysics all the way through to figuring out way through to figuring out way through to figuring out what today's what today's what today's wordle might actually be wordle might actually be wordle might actually be okay number three how okay number three how okay number three how to run one to run one to run one monte carlo techniques involve three monte carlo techniques involve three monte carlo techniques involve three basic steps basic steps basic steps first you set up the first you set up the first you set up the predictive predictive predictive model model model and this is identifying both the and this is identifying both the and this is identifying both the dependent variable to be predicted and dependent variable to be predicted and dependent variable to be predicted and the independent variables also known as the independent variables also known as the independent variables also known as the input risk of predictive variables the input risk of predictive variables the input risk of predictive variables that will drive the predictions that will drive the predictions that will drive the predictions secondly you specify the probability secondly you specify the probability secondly you specify the probability distribution distribution distribution and that's the probability distribution and that's the probability distribution and that's the probability distribution of the independent variables you can use of the independent variables you can use of the independent variables you can use historical data or an analyst's historical data or an analyst's historical data or an analyst's subjective judgment to define a range of subjective judgment to define a range of subjective judgment to define a range of likely values and assign probability likely values and assign probability likely values and assign probability weights for each weights for each weights for each and then number three and then number three and then number three we can run we can run we can run simulations repeatedly generating random simulations repeatedly generating random simulations repeatedly generating random values of the independent variables values of the independent variables values of the independent variables do this until enough results are do this until enough results are do this until enough results are gathered to make up a representative gathered to make up a representative gathered to make up a representative sample of the near infinite number of sample of the near infinite number of sample of the near infinite number of possible combinations possible combinations possible combinations you can run as many monte carlo you can run as many monte carlo you can run as many monte carlo simulations as you wish by modifying the simulations as you wish by modifying the simulations as you wish by modifying the underlying parameters you use to underlying parameters you use to underlying parameters you use to simulate the data however you'll also simulate the data however you'll also simulate the data however you'll also want to compute the range of variation want to compute the range of variation want to compute the range of variation within a sample by calculating the within a sample by calculating the within a sample by calculating the variance and the standard deviation variance and the standard deviation variance and the standard deviation which are commonly used measures of which are commonly used measures of which are commonly used measures of spread spread spread the more you sample the more accurate the more you sample the more accurate the more you sample the more accurate your sampling range and then the better your sampling range and then the better your sampling range and then the better your estimation your estimation your estimation and while you may not be able to travel and while you may not be able to travel and while you may not be able to travel into the future with monte carlo into the future with monte carlo into the future with monte carlo simulation you'll have a much better simulation you'll have a much better simulation you'll have a much better idea about the possibilities idea about the possibilities idea about the possibilities that the future holds that the future holds that the future holds if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe thanks for watching",
    "chunks": [
      "Kind: captions Language: en monte carlo simulation is a mathematical monte carlo simulation is a mathematical monte carlo simulation is a mathematical technique which is used to estimate the technique which is used to estimate the technique which is used to estimate the possible outcomes of an uncertain event possible outcomes of an uncertain event possible outcomes of an uncertain event it's a chance to see into the future and it's a chance to see into the future and it's a chance to see into",
      "the future and while actual time travel is still beyond while actual time travel is still beyond while actual time travel is still beyond us let's address three questions about us let's address three questions about us let's address three questions about monte carlo simulations to get you on monte carlo simulations to get you on monte carlo simulations to get you on your way to making better decisions your way to making better decisions your way to making better decisions come on in guys so",
      "number one come on in guys so number one come on in guys so number one how do they work how do they work how do they work multi-color simulation works by modeling multi-color simulation works by modeling multi-color simulation works by modeling the probability of different outcomes in the probability of different outcomes in the probability of different outcomes in a process or a system that cannot easily a process or a system that cannot easily a process or a system that cannot easily be",
      "predicted due to the intervention of be predicted due to the intervention of be predicted due to the intervention of random variables and it uses something random variables and it uses something random variables and it uses something called random called random called random sampling sampling sampling and rambling random sampling is used to and rambling random sampling is used to and rambling random sampling is used to generate multiple possible outcomes and generate multiple possible outcomes",
      "and generate multiple possible outcomes and calculate the average result calculate the average result calculate the average result so take for example the calculation of so take for example the calculation of so take for example the calculation of the probability the probability the probability of rolling two standard dice well if you wanted to calculate this well if you wanted to calculate this well if you wanted to calculate this probability the brute force way you probability the brute force",
      "way you probability the brute force way you would have to roll the dice a whole would have to roll the dice a whole would have to roll the dice a whole bunch say bunch say bunch say 000 times if we consider that there are 000 times if we consider that there are 000 times if we consider that there are six sides to a dice we have two of them six sides to a dice we have two of them six sides to a dice we have two of them and we want to run this a thousand times and we want to run this a thousand",
      "times and we want to run this a thousand times to get a good sample size but with a to get a good sample size but with a to get a good sample size but with a monte carlo simulation we can reduce the monte carlo simulation we can reduce the monte carlo simulation we can reduce the number of roles by randomly sampling the number of roles by randomly sampling the number of roles by randomly sampling the possible outcomes knowing there are 36 possible outcomes knowing there are 36 possible outcomes",
      "knowing there are 36 combination of dice rolls and combination of dice rolls and combination of dice rolls and calculating the percentage of times that calculating the percentage of times that calculating the percentage of times that we get say a seven we get say a seven we get say a seven now number two now number two now number two who uses them there are a number of who uses them there are a number of who uses them there are a number of common applications for monte carlo common applications",
      "for monte carlo common applications for monte carlo simulations and perhaps the most well simulations and perhaps the most well simulations and perhaps the most well known of those is in the area known of those is in the area known of those is in the area of just portfolio management of just portfolio management of just portfolio management and also in the area of investment and also in the area of investment and also in the area of investment planning planning planning by running thousands or",
      "even millions of by running thousands or even millions of by running thousands or even millions of simulations investors can get a better simulations investors can get a better simulations investors can get a better idea of how their portfolio might idea of how their portfolio might idea of how their portfolio might perform under different market perform under different market perform under different market conditions and other common applications conditions and other common applications",
      "conditions and other common applications are things like risk analysis option are things like risk analysis option are things like risk analysis option pricing and planning for spare capacity pricing and planning for spare capacity pricing and planning for spare capacity but a monte carlo simulation is applied but a monte carlo simulation is applied but a monte carlo simulation is applied in all sorts of fields from medicine all in all sorts of fields from medicine all in all sorts of fields from",
      "medicine all the way through to astrophysics all the the way through to astrophysics all the the way through to astrophysics all the way through to figuring out way through to figuring out way through to figuring out what today's what today's what today's wordle might actually be wordle might actually be wordle might actually be okay number three how okay number three how okay number three how to run one to run one to run one monte carlo techniques involve three monte carlo techniques involve",
      "three monte carlo techniques involve three basic steps basic steps basic steps first you set up the first you set up the first you set up the predictive predictive predictive model model model and this is identifying both the and this is identifying both the and this is identifying both the dependent variable to be predicted and dependent variable to be predicted and dependent variable to be predicted and the independent variables also known as the independent variables also known as the",
      "independent variables also known as the input risk of predictive variables the input risk of predictive variables the input risk of predictive variables that will drive the predictions that will drive the predictions that will drive the predictions secondly you specify the probability secondly you specify the probability secondly you specify the probability distribution distribution distribution and that's the probability distribution and that's the probability distribution and that's the",
      "probability distribution of the independent variables you can use of the independent variables you can use of the independent variables you can use historical data or an analyst's historical data or an analyst's historical data or an analyst's subjective judgment to define a range of subjective judgment to define a range of subjective judgment to define a range of likely values and assign probability likely values and assign probability likely values and assign probability weights for each",
      "weights for each weights for each and then number three and then number three and then number three we can run we can run we can run simulations repeatedly generating random simulations repeatedly generating random simulations repeatedly generating random values of the independent variables values of the independent variables values of the independent variables do this until enough results are do this until enough results are do this until enough results are gathered to make up a representative",
      "gathered to make up a representative gathered to make up a representative sample of the near infinite number of sample of the near infinite number of sample of the near infinite number of possible combinations possible combinations possible combinations you can run as many monte carlo you can run as many monte carlo you can run as many monte carlo simulations as you wish by modifying the simulations as you wish by modifying the simulations as you wish by modifying the underlying parameters you",
      "use to underlying parameters you use to underlying parameters you use to simulate the data however you'll also simulate the data however you'll also simulate the data however you'll also want to compute the range of variation want to compute the range of variation want to compute the range of variation within a sample by calculating the within a sample by calculating the within a sample by calculating the variance and the standard deviation variance and the standard deviation variance and the",
      "standard deviation which are commonly used measures of which are commonly used measures of which are commonly used measures of spread spread spread the more you sample the more accurate the more you sample the more accurate the more you sample the more accurate your sampling range and then the better your sampling range and then the better your sampling range and then the better your estimation your estimation your estimation and while you may not be able to travel and while you may not be able",
      "to travel and while you may not be able to travel into the future with monte carlo into the future with monte carlo into the future with monte carlo simulation you'll have a much better simulation you'll have a much better simulation you'll have a much better idea about the possibilities idea about the possibilities idea about the possibilities that the future holds that the future holds that the future holds if you have any questions please drop us if you have any questions please drop us if you",
      "have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe thanks for watching"
    ],
    "chunk_count": 21,
    "content_id": "04e184bc-2f9e-4574-a146-2f58bfa43c98",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554989"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=0RT2Q0qwXSA": {
    "title": "Overfitting, Underfitting, and Bad Data Are Ruining Your Predictive Models",
    "url": "https://www.youtube.com/watch?v=0RT2Q0qwXSA",
    "description": "Check out watsonx: https://ibm.biz/BdvyLp\n\nData modeling is the process of creating a visual representation of either a whole information system or parts of it to communicate connections between data points and structures. Watch master inventor Martin Keen explain 3 Data Modeling Forecasting Pitfalls to avoid in relation to your business.\n\n#AI #Software #ITModernization #DataModeling #lightboard #IBM #watsonx",
    "duration": 408,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en if your forecasting model looked like if your forecasting model looked like if your forecasting model looked like this this this but in reality the numbers came out but in reality the numbers came out but in reality the numbers came out looking more like this looking more like this looking more like this well you might be asking yourself what well you might be asking yourself what well you might be asking yourself what went wrong went wrong went wrong we're going to look at three common data we're going to look at three common data we're going to look at three common data model forecasting pitfalls to understand model forecasting pitfalls to understand model forecasting pitfalls to understand what they are what they are what they are why they happen why they happen why they happen and how to avoid them so first up number and how to avoid them so first up number and how to avoid them so first up number one that is called one that is called one that is called under under under fitting now this is a scenario in data science now this is a scenario in data science now this is a scenario in data science where a data model is unable to capture where a data model is unable to capture where a data model is unable to capture the relationship between the input and the relationship between the input and the relationship between the input and the output variables accurately and the output variables accurately and the output variables accurately and under fitting usually occurs when a under fitting usually occurs when a under fitting usually occurs when a model is too simple it just cannot model is too simple it just cannot model is too simple it just cannot establish the dominant trend within the establish the dominant trend within the establish the dominant trend within the data and if a model can't generalize data and if a model can't generalize data and if a model can't generalize well to new data well to new data well to new data then it's not going to do a very good then it's not going to do a very good then it's not going to do a very good job with prediction tasks you'll get bad job with prediction tasks you'll get bad job with prediction tasks you'll get bad forecasting models now an optimal data forecasting models now an optimal data forecasting models now an optimal data model in this training data set it might model in this training data set it might model in this training data set it might look something look something look something roughly like that roughly like that roughly like that an underfit model well that will look an underfit model well that will look an underfit model well that will look more like more like more like this this this there's high bias here and low variance there's high bias here and low variance there's high bias here and low variance see how straight that line is and that see how straight that line is and that see how straight that line is and that is a good indicator that you have under is a good indicator that you have under is a good indicator that you have under fitting now fortunately under fitting is fitting now fortunately under fitting is fitting now fortunately under fitting is usually quite easy to spot it shows up usually quite easy to spot it shows up usually quite easy to spot it shows up even when modeling the training data set even when modeling the training data set even when modeling the training data set to fix it we need to better establish to fix it we need to better establish to fix it we need to better establish the dominant relationship between the the dominant relationship between the the dominant relationship between the input and the output variables at the input and the output variables at the input and the output variables at the onset to build a better fitting and onset to build a better fitting and onset to build a better fitting and likely a little bit more complex model likely a little bit more complex model likely a little bit more complex model now three ways that you can do that one now three ways that you can do that one now three ways that you can do that one of those of those of those is to decrease is to decrease is to decrease something called something called something called regular regular regular regularization now this really means that we're going now this really means that we're going now this really means that we're going to let the model be a little more free to let the model be a little more free to let the model be a little more free and how it draws its relationships and how it draws its relationships and how it draws its relationships between inputs and outputs and there are between inputs and outputs and there are between inputs and outputs and there are a number of methods like l1 a number of methods like l1 a number of methods like l1 regularization and lasso regularization regularization and lasso regularization regularization and lasso regularization which help to reduce the noise and which help to reduce the noise and which help to reduce the noise and outliers within a model outliers within a model outliers within a model second thing you could do is to increase second thing you could do is to increase second thing you could do is to increase your training your training your training data data data stopping training too soon is a common stopping training too soon is a common stopping training too soon is a common cause of underfitting cause of underfitting cause of underfitting more data can lead to a better fitting more data can lead to a better fitting more data can lead to a better fitting model and finally the other thing you model and finally the other thing you model and finally the other thing you might want to consider might want to consider might want to consider is called feature is called feature is called feature selection now this is used in any model where we now this is used in any model where we now this is used in any model where we want to take specific features to want to take specific features to want to take specific features to determine a given outcome and if there determine a given outcome and if there determine a given outcome and if there are not enough predictive features are not enough predictive features are not enough predictive features present then more features or features present then more features or features present then more features or features just of greater importance should be just of greater importance should be just of greater importance should be introduced that's what feature selection introduced that's what feature selection introduced that's what feature selection okay okay okay number two we've had underfitting number number two we've had underfitting number number two we've had underfitting number two is over two is over two is over fitting thing with overfitting is it occurs when thing with overfitting is it occurs when thing with overfitting is it occurs when a statistical model fits exactly exactly a statistical model fits exactly exactly a statistical model fits exactly exactly against its training data and when this against its training data and when this against its training data and when this happens the algorithm can't perform happens the algorithm can't perform happens the algorithm can't perform accurately against unseen data which accurately against unseen data which accurately against unseen data which well rather defeats its purpose oh and well rather defeats its purpose oh and well rather defeats its purpose oh and here's the rub with overfitting it can here's the rub with overfitting it can here's the rub with overfitting it can be caused by addressing be caused by addressing be caused by addressing underfitting a little too aggressively underfitting a little too aggressively underfitting a little too aggressively now an overfit model it might look a bit now an overfit model it might look a bit now an overfit model it might look a bit more more more like like like this it has a low error rate and a very high it has a low error rate and a very high it has a low error rate and a very high variance this is anything but straight variance this is anything but straight variance this is anything but straight now here the model is just so well tuned now here the model is just so well tuned now here the model is just so well tuned to the training data it's mistaken the to the training data it's mistaken the to the training data it's mistaken the noise or some of the irrelevant noise or some of the irrelevant noise or some of the irrelevant information from the training data set information from the training data set information from the training data set as the signal now unlike under fitting as the signal now unlike under fitting as the signal now unlike under fitting overfitting isn't always so easy to overfitting isn't always so easy to overfitting isn't always so easy to initially detect and to find it we need initially detect and to find it we need initially detect and to find it we need to test we can test for model fitness to test we can test for model fitness to test we can test for model fitness using techniques like k-fold using techniques like k-fold using techniques like k-fold cross-validation and that splits cross-validation and that splits cross-validation and that splits training data into equally sized subsets training data into equally sized subsets training data into equally sized subsets they're called folds and that results in they're called folds and that results in they're called folds and that results in an evaluation score for your model an evaluation score for your model an evaluation score for your model now to prevent overfitting you might now to prevent overfitting you might now to prevent overfitting you might want to consider some of the following want to consider some of the following want to consider some of the following techniques techniques techniques is data is data is data augmentation augmentation augmentation while it is better generally to inject while it is better generally to inject while it is better generally to inject clean relevant information into your clean relevant information into your clean relevant information into your training data set sometimes a little bit training data set sometimes a little bit training data set sometimes a little bit of noisy data is added to make the model of noisy data is added to make the model of noisy data is added to make the model a little bit more stable a little bit more stable a little bit more stable another method is another method is another method is ensembl methods well some more methods are made up of a well some more methods are made up of a well some more methods are made up of a set of classifiers and their predictors set of classifiers and their predictors set of classifiers and their predictors are aggregated to identify the most are aggregated to identify the most are aggregated to identify the most popular result bagging is one such popular result bagging is one such popular result bagging is one such method where multiple models are trained method where multiple models are trained method where multiple models are trained in parallel on different subsets of data in parallel on different subsets of data in parallel on different subsets of data and then and then and then option number three something called option number three something called option number three something called early early early stopping stopping stopping can be used and this method seeks to can be used and this method seeks to can be used and this method seeks to pause training before the model starts pause training before the model starts pause training before the model starts learning the noise within that model of learning the noise within that model of learning the noise within that model of training data training data training data of course you do need to be careful not of course you do need to be careful not of course you do need to be careful not to stop too soon or you'll be dealing to stop too soon or you'll be dealing to stop too soon or you'll be dealing with a bad case of with a bad case of with a bad case of underfitting underfitting underfitting okay so finally in addition to okay so finally in addition to okay so finally in addition to underfitting and overfitting another underfitting and overfitting another underfitting and overfitting another common problem is number three common problem is number three common problem is number three data now this is data that is incorrect or now this is data that is incorrect or now this is data that is incorrect or irrelevant or incomplete and bad irrelevant or incomplete and bad irrelevant or incomplete and bad training data can lead to higher error training data can lead to higher error training data can lead to higher error rates and biased decision-making even rates and biased decision-making even rates and biased decision-making even when the underlying model is sound when the underlying model is sound when the underlying model is sound data forecasting models are only as data forecasting models are only as data forecasting models are only as effective as the data they're trained on effective as the data they're trained on effective as the data they're trained on now some tips to avoid bad data now some tips to avoid bad data now some tips to avoid bad data firstly you want to ensure that your firstly you want to ensure that your firstly you want to ensure that your data is accurate and complete by data is accurate and complete by data is accurate and complete by performing cross performing cross performing cross checking checking checking and by that i mean cross checking it and by that i mean cross checking it and by that i mean cross checking it against other data sources against other data sources against other data sources another thing to do is if we think about another thing to do is if we think about another thing to do is if we think about outliers rid of them outliers rid of them outliers rid of them so some outliers can really skew results so some outliers can really skew results so some outliers can really skew results they can take an obscure thing and put they can take an obscure thing and put they can take an obscure thing and put it into a training data set and make it into a training data set and make it into a training data set and make that situation seem more likely to occur that situation seem more likely to occur that situation seem more likely to occur again than it really is likely to again than it really is likely to again than it really is likely to and then finally you need to make sure and then finally you need to make sure and then finally you need to make sure that this data that this data that this data is timely is timely is timely outdated data outdated data outdated data is bad data is bad data is bad data now keep these three things in mind and now keep these three things in mind and now keep these three things in mind and you should be well on your way to you should be well on your way to you should be well on your way to developing better fitting models developing better fitting models developing better fitting models if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching",
    "chunks": [
      "Kind: captions Language: en if your forecasting model looked like if your forecasting model looked like if your forecasting model looked like this this this but in reality the numbers came out but in reality the numbers came out but in reality the numbers came out looking more like this looking more like this looking more like this well you might be asking yourself what well you might be asking yourself what well you might be asking yourself what went wrong went wrong went wrong we're going to",
      "look at three common data we're going to look at three common data we're going to look at three common data model forecasting pitfalls to understand model forecasting pitfalls to understand model forecasting pitfalls to understand what they are what they are what they are why they happen why they happen why they happen and how to avoid them so first up number and how to avoid them so first up number and how to avoid them so first up number one that is called one that is called one that is called",
      "under under under fitting now this is a scenario in data science now this is a scenario in data science now this is a scenario in data science where a data model is unable to capture where a data model is unable to capture where a data model is unable to capture the relationship between the input and the relationship between the input and the relationship between the input and the output variables accurately and the output variables accurately and the output variables accurately and under fitting",
      "usually occurs when a under fitting usually occurs when a under fitting usually occurs when a model is too simple it just cannot model is too simple it just cannot model is too simple it just cannot establish the dominant trend within the establish the dominant trend within the establish the dominant trend within the data and if a model can't generalize data and if a model can't generalize data and if a model can't generalize well to new data well to new data well to new data then it's not going",
      "to do a very good then it's not going to do a very good then it's not going to do a very good job with prediction tasks you'll get bad job with prediction tasks you'll get bad job with prediction tasks you'll get bad forecasting models now an optimal data forecasting models now an optimal data forecasting models now an optimal data model in this training data set it might model in this training data set it might model in this training data set it might look something look something look something",
      "roughly like that roughly like that roughly like that an underfit model well that will look an underfit model well that will look an underfit model well that will look more like more like more like this this this there's high bias here and low variance there's high bias here and low variance there's high bias here and low variance see how straight that line is and that see how straight that line is and that see how straight that line is and that is a good indicator that you have under is a good",
      "indicator that you have under is a good indicator that you have under fitting now fortunately under fitting is fitting now fortunately under fitting is fitting now fortunately under fitting is usually quite easy to spot it shows up usually quite easy to spot it shows up usually quite easy to spot it shows up even when modeling the training data set even when modeling the training data set even when modeling the training data set to fix it we need to better establish to fix it we need to better",
      "establish to fix it we need to better establish the dominant relationship between the the dominant relationship between the the dominant relationship between the input and the output variables at the input and the output variables at the input and the output variables at the onset to build a better fitting and onset to build a better fitting and onset to build a better fitting and likely a little bit more complex model likely a little bit more complex model likely a little bit more complex model",
      "now three ways that you can do that one now three ways that you can do that one now three ways that you can do that one of those of those of those is to decrease is to decrease is to decrease something called something called something called regular regular regular regularization now this really means that we're going now this really means that we're going now this really means that we're going to let the model be a little more free to let the model be a little more free to let the model be a",
      "little more free and how it draws its relationships and how it draws its relationships and how it draws its relationships between inputs and outputs and there are between inputs and outputs and there are between inputs and outputs and there are a number of methods like l1 a number of methods like l1 a number of methods like l1 regularization and lasso regularization regularization and lasso regularization regularization and lasso regularization which help to reduce the noise and which help to",
      "reduce the noise and which help to reduce the noise and outliers within a model outliers within a model outliers within a model second thing you could do is to increase second thing you could do is to increase second thing you could do is to increase your training your training your training data data data stopping training too soon is a common stopping training too soon is a common stopping training too soon is a common cause of underfitting cause of underfitting cause of underfitting more data",
      "can lead to a better fitting more data can lead to a better fitting more data can lead to a better fitting model and finally the other thing you model and finally the other thing you model and finally the other thing you might want to consider might want to consider might want to consider is called feature is called feature is called feature selection now this is used in any model where we now this is used in any model where we now this is used in any model where we want to take specific features",
      "to want to take specific features to want to take specific features to determine a given outcome and if there determine a given outcome and if there determine a given outcome and if there are not enough predictive features are not enough predictive features are not enough predictive features present then more features or features present then more features or features present then more features or features just of greater importance should be just of greater importance should be just of greater",
      "importance should be introduced that's what feature selection introduced that's what feature selection introduced that's what feature selection okay okay okay number two we've had underfitting number number two we've had underfitting number number two we've had underfitting number two is over two is over two is over fitting thing with overfitting is it occurs when thing with overfitting is it occurs when thing with overfitting is it occurs when a statistical model fits exactly exactly a",
      "statistical model fits exactly exactly a statistical model fits exactly exactly against its training data and when this against its training data and when this against its training data and when this happens the algorithm can't perform happens the algorithm can't perform happens the algorithm can't perform accurately against unseen data which accurately against unseen data which accurately against unseen data which well rather defeats its purpose oh and well rather defeats its purpose oh and well",
      "rather defeats its purpose oh and here's the rub with overfitting it can here's the rub with overfitting it can here's the rub with overfitting it can be caused by addressing be caused by addressing be caused by addressing underfitting a little too aggressively underfitting a little too aggressively underfitting a little too aggressively now an overfit model it might look a bit now an overfit model it might look a bit now an overfit model it might look a bit more more more like like like this it",
      "has a low error rate and a very high it has a low error rate and a very high it has a low error rate and a very high variance this is anything but straight variance this is anything but straight variance this is anything but straight now here the model is just so well tuned now here the model is just so well tuned now here the model is just so well tuned to the training data it's mistaken the to the training data it's mistaken the to the training data it's mistaken the noise or some of the",
      "irrelevant noise or some of the irrelevant noise or some of the irrelevant information from the training data set information from the training data set information from the training data set as the signal now unlike under fitting as the signal now unlike under fitting as the signal now unlike under fitting overfitting isn't always so easy to overfitting isn't always so easy to overfitting isn't always so easy to initially detect and to find it we need initially detect and to find it we need",
      "initially detect and to find it we need to test we can test for model fitness to test we can test for model fitness to test we can test for model fitness using techniques like k-fold using techniques like k-fold using techniques like k-fold cross-validation and that splits cross-validation and that splits cross-validation and that splits training data into equally sized subsets training data into equally sized subsets training data into equally sized subsets they're called folds and that results",
      "in they're called folds and that results in they're called folds and that results in an evaluation score for your model an evaluation score for your model an evaluation score for your model now to prevent overfitting you might now to prevent overfitting you might now to prevent overfitting you might want to consider some of the following want to consider some of the following want to consider some of the following techniques techniques techniques is data is data is data augmentation augmentation",
      "augmentation while it is better generally to inject while it is better generally to inject while it is better generally to inject clean relevant information into your clean relevant information into your clean relevant information into your training data set sometimes a little bit training data set sometimes a little bit training data set sometimes a little bit of noisy data is added to make the model of noisy data is added to make the model of noisy data is added to make the model a little bit",
      "more stable a little bit more stable a little bit more stable another method is another method is another method is ensembl methods well some more methods are made up of a well some more methods are made up of a well some more methods are made up of a set of classifiers and their predictors set of classifiers and their predictors set of classifiers and their predictors are aggregated to identify the most are aggregated to identify the most are aggregated to identify the most popular result",
      "bagging is one such popular result bagging is one such popular result bagging is one such method where multiple models are trained method where multiple models are trained method where multiple models are trained in parallel on different subsets of data in parallel on different subsets of data in parallel on different subsets of data and then and then and then option number three something called option number three something called option number three something called early early early stopping",
      "stopping stopping can be used and this method seeks to can be used and this method seeks to can be used and this method seeks to pause training before the model starts pause training before the model starts pause training before the model starts learning the noise within that model of learning the noise within that model of learning the noise within that model of training data training data training data of course you do need to be careful not of course you do need to be careful not of course you",
      "do need to be careful not to stop too soon or you'll be dealing to stop too soon or you'll be dealing to stop too soon or you'll be dealing with a bad case of with a bad case of with a bad case of underfitting underfitting underfitting okay so finally in addition to okay so finally in addition to okay so finally in addition to underfitting and overfitting another underfitting and overfitting another underfitting and overfitting another common problem is number three common problem is number three",
      "common problem is number three data now this is data that is incorrect or now this is data that is incorrect or now this is data that is incorrect or irrelevant or incomplete and bad irrelevant or incomplete and bad irrelevant or incomplete and bad training data can lead to higher error training data can lead to higher error training data can lead to higher error rates and biased decision-making even rates and biased decision-making even rates and biased decision-making even when the underlying",
      "model is sound when the underlying model is sound when the underlying model is sound data forecasting models are only as data forecasting models are only as data forecasting models are only as effective as the data they're trained on effective as the data they're trained on effective as the data they're trained on now some tips to avoid bad data now some tips to avoid bad data now some tips to avoid bad data firstly you want to ensure that your firstly you want to ensure that your firstly you",
      "want to ensure that your data is accurate and complete by data is accurate and complete by data is accurate and complete by performing cross performing cross performing cross checking checking checking and by that i mean cross checking it and by that i mean cross checking it and by that i mean cross checking it against other data sources against other data sources against other data sources another thing to do is if we think about another thing to do is if we think about another thing to do is if",
      "we think about outliers rid of them outliers rid of them outliers rid of them so some outliers can really skew results so some outliers can really skew results so some outliers can really skew results they can take an obscure thing and put they can take an obscure thing and put they can take an obscure thing and put it into a training data set and make it into a training data set and make it into a training data set and make that situation seem more likely to occur that situation seem more likely",
      "to occur that situation seem more likely to occur again than it really is likely to again than it really is likely to again than it really is likely to and then finally you need to make sure and then finally you need to make sure and then finally you need to make sure that this data that this data that this data is timely is timely is timely outdated data outdated data outdated data is bad data is bad data is bad data now keep these three things in mind and now keep these three things in mind and",
      "now keep these three things in mind and you should be well on your way to you should be well on your way to you should be well on your way to developing better fitting models developing better fitting models developing better fitting models if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like",
      "this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching"
    ],
    "chunk_count": 32,
    "content_id": "5759fa61-838b-41a6-b9ab-9d95917bdb5f",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554992"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=T7Wr7wVK5Wo": {
    "title": "Fluid vs. Crystallized Intelligence",
    "url": "https://www.youtube.com/watch?v=T7Wr7wVK5Wo",
    "description": "Learn more about WatsonX → https://ibm.biz/BdPuCk\nLearn more about AI → http://ibm.biz/what-is-AI\nCheck out IBM Watson → http://ibm.biz/IBM-Watson\nWatch \"What is Random Forest?\" ligthboard video → https://youtu.be/gkXX4h3qYm4\n\nFluid intelligence and crystallized intelligence are two parts of what is known as general intelligence.  But what are they exactly?\nIn this video, Martin Keen, Master Inventor at IBM, explains what fluid and crystallized intelligence are, the relationship between them, and how we use them everyday - and how these types of intelligence fit into the development of artificial intelligence.\n\nDownload a free AI ebook → http://ibm.biz/AI-eBook\nRead about the Journey to AI → http://ibm.biz/Journey-to-AI\n\nGet started for free on IBM Cloud → http://ibm.biz/Free-cloud-tier\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#AI #Software #ITModernization #watsonx",
    "duration": 352,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en pop quiz time let's test your pop quiz time let's test your pop quiz time let's test your intelligence so intelligence so intelligence so what is the capital of france what is the capital of france what is the capital of france why paris may we why paris may we why paris may we okay now complete the sequence okay now complete the sequence okay now complete the sequence did you say three did you say three did you say three yeah that's pretty easy but that wasn't yeah that's pretty easy but that wasn't yeah that's pretty easy but that wasn't the real quiz here's what i really want the real quiz here's what i really want the real quiz here's what i really want to know to know to know which form of thinking did you use to which form of thinking did you use to which form of thinking did you use to solve each question solve each question solve each question well recalling previously acquired well recalling previously acquired well recalling previously acquired information like a capital city is a information like a capital city is a information like a capital city is a form of intelligence form of intelligence form of intelligence known as crystallized intelligence whereas the use of reasoning and logic whereas the use of reasoning and logic whereas the use of reasoning and logic to deduce the next character in a to deduce the next character in a to deduce the next character in a sequence sequence sequence that is known as fluid that is known as fluid that is known as fluid intelligence intelligence intelligence my name is martin keane and i'm a master my name is martin keane and i'm a master my name is martin keane and i'm a master inventor at ibm so crystallized inventor at ibm so crystallized inventor at ibm so crystallized intelligence refers to knowledge that intelligence refers to knowledge that intelligence refers to knowledge that comes from previously acquired comes from previously acquired comes from previously acquired information it's dependent on a person's knowledge it's dependent on a person's knowledge it's dependent on a person's knowledge on their skills on their expertise on their skills on their expertise on their skills on their expertise developed over a lifetime of learning developed over a lifetime of learning developed over a lifetime of learning and experience crystallized intelligence and experience crystallized intelligence and experience crystallized intelligence is fact is fact is fact and experience based whereas fluid intelligence is the whereas fluid intelligence is the whereas fluid intelligence is the capacity to think logically and solve capacity to think logically and solve capacity to think logically and solve problems in new and unfamiliar problems in new and unfamiliar problems in new and unfamiliar situations independent of acquired situations independent of acquired situations independent of acquired knowledge knowledge knowledge fluid intelligence represents a person's fluid intelligence represents a person's fluid intelligence represents a person's ability to problem-solve using reasoning and by also using and by also using and by also using logic when you come across a new problem that when you come across a new problem that when you come across a new problem that your current knowledge can't address you your current knowledge can't address you your current knowledge can't address you call on fluid intelligence to resolve it call on fluid intelligence to resolve it call on fluid intelligence to resolve it the notion of fluid and crystallized the notion of fluid and crystallized the notion of fluid and crystallized intelligence dates back to 1963 intelligence dates back to 1963 intelligence dates back to 1963 when it was first formalized by when it was first formalized by when it was first formalized by psychologist raymond cattell now you might be asking yourself now you might be asking yourself now you might be asking yourself why am i why am i why am i talking about psychology in an ibm talking about psychology in an ibm talking about psychology in an ibm technology video well technology video well technology video well because these two forms of intelligence because these two forms of intelligence because these two forms of intelligence that we use to solve problems every day that we use to solve problems every day that we use to solve problems every day can also play an important role in can also play an important role in can also play an important role in machine learning so for example consider machine learning so for example consider machine learning so for example consider an ai system like ibm watson to answer a an ai system like ibm watson to answer a an ai system like ibm watson to answer a question it can sift through like a question it can sift through like a question it can sift through like a million books per second and perhaps the million books per second and perhaps the million books per second and perhaps the answer to a question is right there in answer to a question is right there in answer to a question is right there in one of those books like our paris one of those books like our paris one of those books like our paris example example example in that case it's a simple case of in that case it's a simple case of in that case it's a simple case of natural language understanding to answer natural language understanding to answer natural language understanding to answer the question what is the capital of the question what is the capital of the question what is the capital of france that's crystallized intelligence france that's crystallized intelligence france that's crystallized intelligence but most problems aren't that simple and but most problems aren't that simple and but most problems aren't that simple and to solve them we may need to combine to solve them we may need to combine to solve them we may need to combine both crystallized and fluid intelligence both crystallized and fluid intelligence both crystallized and fluid intelligence together together together so for example say we wanted an a i so for example say we wanted an a i so for example say we wanted an a i travel system travel system travel system let's build one let's build one let's build one an ai travel system and what we want it an ai travel system and what we want it an ai travel system and what we want it to do to do to do is to build us an itinerary of the best is to build us an itinerary of the best is to build us an itinerary of the best way to spend a day in paris so the way to spend a day in paris so the way to spend a day in paris so the output of this output of this output of this is an itinerary we need to use our knowledge of parisian we need to use our knowledge of parisian we need to use our knowledge of parisian geography and cultural history to build geography and cultural history to build geography and cultural history to build a corpus of what's available that's the crystallized intelligence that's the crystallized intelligence that's the crystallized intelligence part but we also need to apply those part but we also need to apply those part but we also need to apply those options to a derived understanding of options to a derived understanding of options to a derived understanding of the types of activities that we like to the types of activities that we like to the types of activities that we like to so these are the sorts of things that so these are the sorts of things that so these are the sorts of things that we'd be interested in doing what do we we'd be interested in doing what do we we'd be interested in doing what do we normally do when we visit a new city are normally do when we visit a new city are normally do when we visit a new city are there comparable options here in paris there comparable options here in paris there comparable options here in paris can we tailor this to our personality to can we tailor this to our personality to can we tailor this to our personality to our budget and our willingness to try our budget and our willingness to try our budget and our willingness to try new things all of that stuff well that's new things all of that stuff well that's new things all of that stuff well that's the fluid intelligence side of things the fluid intelligence side of things the fluid intelligence side of things so from there a system could derive a so from there a system could derive a so from there a system could derive a subset of all the possible things to do subset of all the possible things to do subset of all the possible things to do in paris that day distilled into a in paris that day distilled into a in paris that day distilled into a tailorized personalized tailorized personalized tailorized personalized itinerary itinerary itinerary so for us humans our crystallized so for us humans our crystallized so for us humans our crystallized intelligence is knowledge we acquire and intelligence is knowledge we acquire and intelligence is knowledge we acquire and fluid intelligence is how we apply it fluid intelligence is how we apply it fluid intelligence is how we apply it for an ai system you can think of a for an ai system you can think of a for an ai system you can think of a systems model as being crystallized systems model as being crystallized systems model as being crystallized intelligence because it teaches itself intelligence because it teaches itself intelligence because it teaches itself to do one task really well by training to do one task really well by training to do one task really well by training on massive data sets of prior experience on massive data sets of prior experience on massive data sets of prior experience then you can think of its ability to then you can think of its ability to then you can think of its ability to solve new problems as being fluid solve new problems as being fluid solve new problems as being fluid intelligence because it can intelligence because it can intelligence because it can apply that model to a previously unseen apply that model to a previously unseen apply that model to a previously unseen problem problem problem and as for me and as for me and as for me my crystallized and fluid intelligence my crystallized and fluid intelligence my crystallized and fluid intelligence is telling me that is telling me that is telling me that morning croissants under the eiffel morning croissants under the eiffel morning croissants under the eiffel tower tower tower should be top of my list if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching",
    "chunks": [
      "Kind: captions Language: en pop quiz time let's test your pop quiz time let's test your pop quiz time let's test your intelligence so intelligence so intelligence so what is the capital of france what is the capital of france what is the capital of france why paris may we why paris may we why paris may we okay now complete the sequence okay now complete the sequence okay now complete the sequence did you say three did you say three did you say three yeah that's pretty easy but that wasn't yeah",
      "that's pretty easy but that wasn't yeah that's pretty easy but that wasn't the real quiz here's what i really want the real quiz here's what i really want the real quiz here's what i really want to know to know to know which form of thinking did you use to which form of thinking did you use to which form of thinking did you use to solve each question solve each question solve each question well recalling previously acquired well recalling previously acquired well recalling previously acquired",
      "information like a capital city is a information like a capital city is a information like a capital city is a form of intelligence form of intelligence form of intelligence known as crystallized intelligence whereas the use of reasoning and logic whereas the use of reasoning and logic whereas the use of reasoning and logic to deduce the next character in a to deduce the next character in a to deduce the next character in a sequence sequence sequence that is known as fluid that is known as fluid",
      "that is known as fluid intelligence intelligence intelligence my name is martin keane and i'm a master my name is martin keane and i'm a master my name is martin keane and i'm a master inventor at ibm so crystallized inventor at ibm so crystallized inventor at ibm so crystallized intelligence refers to knowledge that intelligence refers to knowledge that intelligence refers to knowledge that comes from previously acquired comes from previously acquired comes from previously acquired information",
      "it's dependent on a person's knowledge it's dependent on a person's knowledge it's dependent on a person's knowledge on their skills on their expertise on their skills on their expertise on their skills on their expertise developed over a lifetime of learning developed over a lifetime of learning developed over a lifetime of learning and experience crystallized intelligence and experience crystallized intelligence and experience crystallized intelligence is fact is fact is fact and experience",
      "based whereas fluid intelligence is the whereas fluid intelligence is the whereas fluid intelligence is the capacity to think logically and solve capacity to think logically and solve capacity to think logically and solve problems in new and unfamiliar problems in new and unfamiliar problems in new and unfamiliar situations independent of acquired situations independent of acquired situations independent of acquired knowledge knowledge knowledge fluid intelligence represents a person's fluid",
      "intelligence represents a person's fluid intelligence represents a person's ability to problem-solve using reasoning and by also using and by also using and by also using logic when you come across a new problem that when you come across a new problem that when you come across a new problem that your current knowledge can't address you your current knowledge can't address you your current knowledge can't address you call on fluid intelligence to resolve it call on fluid intelligence to resolve it",
      "call on fluid intelligence to resolve it the notion of fluid and crystallized the notion of fluid and crystallized the notion of fluid and crystallized intelligence dates back to 1963 intelligence dates back to 1963 intelligence dates back to 1963 when it was first formalized by when it was first formalized by when it was first formalized by psychologist raymond cattell now you might be asking yourself now you might be asking yourself now you might be asking yourself why am i why am i why am i",
      "talking about psychology in an ibm talking about psychology in an ibm talking about psychology in an ibm technology video well technology video well technology video well because these two forms of intelligence because these two forms of intelligence because these two forms of intelligence that we use to solve problems every day that we use to solve problems every day that we use to solve problems every day can also play an important role in can also play an important role in can also play an",
      "important role in machine learning so for example consider machine learning so for example consider machine learning so for example consider an ai system like ibm watson to answer a an ai system like ibm watson to answer a an ai system like ibm watson to answer a question it can sift through like a question it can sift through like a question it can sift through like a million books per second and perhaps the million books per second and perhaps the million books per second and perhaps the answer",
      "to a question is right there in answer to a question is right there in answer to a question is right there in one of those books like our paris one of those books like our paris one of those books like our paris example example example in that case it's a simple case of in that case it's a simple case of in that case it's a simple case of natural language understanding to answer natural language understanding to answer natural language understanding to answer the question what is the capital of",
      "the question what is the capital of the question what is the capital of france that's crystallized intelligence france that's crystallized intelligence france that's crystallized intelligence but most problems aren't that simple and but most problems aren't that simple and but most problems aren't that simple and to solve them we may need to combine to solve them we may need to combine to solve them we may need to combine both crystallized and fluid intelligence both crystallized and fluid",
      "intelligence both crystallized and fluid intelligence together together together so for example say we wanted an a i so for example say we wanted an a i so for example say we wanted an a i travel system travel system travel system let's build one let's build one let's build one an ai travel system and what we want it an ai travel system and what we want it an ai travel system and what we want it to do to do to do is to build us an itinerary of the best is to build us an itinerary of the best is",
      "to build us an itinerary of the best way to spend a day in paris so the way to spend a day in paris so the way to spend a day in paris so the output of this output of this output of this is an itinerary we need to use our knowledge of parisian we need to use our knowledge of parisian we need to use our knowledge of parisian geography and cultural history to build geography and cultural history to build geography and cultural history to build a corpus of what's available that's the crystallized",
      "intelligence that's the crystallized intelligence that's the crystallized intelligence part but we also need to apply those part but we also need to apply those part but we also need to apply those options to a derived understanding of options to a derived understanding of options to a derived understanding of the types of activities that we like to the types of activities that we like to the types of activities that we like to so these are the sorts of things that so these are the sorts of",
      "things that so these are the sorts of things that we'd be interested in doing what do we we'd be interested in doing what do we we'd be interested in doing what do we normally do when we visit a new city are normally do when we visit a new city are normally do when we visit a new city are there comparable options here in paris there comparable options here in paris there comparable options here in paris can we tailor this to our personality to can we tailor this to our personality to can we",
      "tailor this to our personality to our budget and our willingness to try our budget and our willingness to try our budget and our willingness to try new things all of that stuff well that's new things all of that stuff well that's new things all of that stuff well that's the fluid intelligence side of things the fluid intelligence side of things the fluid intelligence side of things so from there a system could derive a so from there a system could derive a so from there a system could derive a",
      "subset of all the possible things to do subset of all the possible things to do subset of all the possible things to do in paris that day distilled into a in paris that day distilled into a in paris that day distilled into a tailorized personalized tailorized personalized tailorized personalized itinerary itinerary itinerary so for us humans our crystallized so for us humans our crystallized so for us humans our crystallized intelligence is knowledge we acquire and intelligence is knowledge we",
      "acquire and intelligence is knowledge we acquire and fluid intelligence is how we apply it fluid intelligence is how we apply it fluid intelligence is how we apply it for an ai system you can think of a for an ai system you can think of a for an ai system you can think of a systems model as being crystallized systems model as being crystallized systems model as being crystallized intelligence because it teaches itself intelligence because it teaches itself intelligence because it teaches itself",
      "to do one task really well by training to do one task really well by training to do one task really well by training on massive data sets of prior experience on massive data sets of prior experience on massive data sets of prior experience then you can think of its ability to then you can think of its ability to then you can think of its ability to solve new problems as being fluid solve new problems as being fluid solve new problems as being fluid intelligence because it can intelligence because",
      "it can intelligence because it can apply that model to a previously unseen apply that model to a previously unseen apply that model to a previously unseen problem problem problem and as for me and as for me and as for me my crystallized and fluid intelligence my crystallized and fluid intelligence my crystallized and fluid intelligence is telling me that is telling me that is telling me that morning croissants under the eiffel morning croissants under the eiffel morning croissants under the",
      "eiffel tower tower tower should be top of my list if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching"
    ],
    "chunk_count": 22,
    "content_id": "1541c707-00a0-481b-aa1a-553b43928bc4",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554995"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=Fu273ovPBmQ": {
    "title": "Activation Functions In Neural Networks Explained | Deep Learning Tutorial",
    "url": "https://www.youtube.com/watch?v=Fu273ovPBmQ",
    "description": "Get your Free Token for AssemblyAI Speech-To-Text API 👇\nhttps://www.assemblyai.com/?utm_source=youtube&utm_medium=referral&utm_campaign=yt_pat_2\n\nIn this video we are going to learn about Activation Functions in Neural Networks. We go over:\n\n* The definition of activation functions\n* Why they are used\n* Different activation functions\n* How to use them in code (TensorFlow and PyTorch)\n\nDeep Learning In 5 Minutes video:  https://youtu.be/dccdadl90vs \n\nDifferent activation functions we go over:\nStep Functions, Sigmoid, TanH, ReLU, Leaky ReLU, Softmax\n\nTimestamps:\n00:00 Introduction\n00:35 Activation Functions Explained\n01:48 Different activation functions\n05:23 How to implement them\n06:20 Get your Free AssemblyAI API link now!",
    "duration": 403,
    "uploader": "AssemblyAI",
    "transcript": "Kind: captions Language: en in this video we are going to learn in this video we are going to learn in this video we are going to learn about actuation functions we go over the about actuation functions we go over the about actuation functions we go over the definition of actuation functions why definition of actuation functions why definition of actuation functions why they are used then we have a look at they are used then we have a look at they are used then we have a look at different kinds of actuation functions different kinds of actuation functions different kinds of actuation functions and at the end i also show you how to and at the end i also show you how to and at the end i also show you how to use them in your code and don't worry use them in your code and don't worry use them in your code and don't worry because deep learning frameworks like because deep learning frameworks like because deep learning frameworks like pytorch and tensorflow make it extremely pytorch and tensorflow make it extremely pytorch and tensorflow make it extremely easy to apply them this video is part of easy to apply them this video is part of easy to apply them this video is part of the deep learning explained series by the deep learning explained series by the deep learning explained series by assembly ai which is a company that assembly ai which is a company that assembly ai which is a company that creates a state-of-the-art creates a state-of-the-art creates a state-of-the-art speech-to-text api and if you want to speech-to-text api and if you want to speech-to-text api and if you want to use assembly ai for free then grab your use assembly ai for free then grab your use assembly ai for free then grab your api token using the link in the api token using the link in the api token using the link in the description below and now let's get description below and now let's get description below and now let's get started so what are activation functions started so what are activation functions started so what are activation functions and why do we need them actuation and why do we need them actuation and why do we need them actuation functions apply a non-linear functions apply a non-linear functions apply a non-linear transformation and decide whether a transformation and decide whether a transformation and decide whether a neuron should be activated or not now neuron should be activated or not now neuron should be activated or not now let's take a step back and see what this let's take a step back and see what this let's take a step back and see what this means in a previous video we learned how means in a previous video we learned how means in a previous video we learned how neural networks work in a neural network neural networks work in a neural network neural networks work in a neural network we have the input layer where we accept we have the input layer where we accept we have the input layer where we accept an input and an output layer that gives an input and an output layer that gives an input and an output layer that gives the actual prediction or the outcome of the actual prediction or the outcome of the actual prediction or the outcome of the network and in between we have the the network and in between we have the the network and in between we have the hidden layers all of these layers hidden layers all of these layers hidden layers all of these layers consist of neurons and at each neuron we consist of neurons and at each neuron we consist of neurons and at each neuron we apply a linear transformation it apply a linear transformation it apply a linear transformation it multiplies the input with some weights multiplies the input with some weights multiplies the input with some weights and maybe adds a bias now this is fine and maybe adds a bias now this is fine and maybe adds a bias now this is fine as long as we have a simple problem like as long as we have a simple problem like as long as we have a simple problem like this where we can model the predictions this where we can model the predictions this where we can model the predictions with a linear function but let's say we with a linear function but let's say we with a linear function but let's say we have a more complex problem one thing we have a more complex problem one thing we have a more complex problem one thing we can do is of course add more layers to can do is of course add more layers to can do is of course add more layers to our network but here's a big problem our network but here's a big problem our network but here's a big problem without activation functions we only get without activation functions we only get without activation functions we only get linear transformations after each other linear transformations after each other linear transformations after each other so our whole network is basically just a so our whole network is basically just a so our whole network is basically just a stacked linear regression model that is stacked linear regression model that is stacked linear regression model that is not able to learn complex patterns and not able to learn complex patterns and not able to learn complex patterns and this is exactly why actuation functions this is exactly why actuation functions this is exactly why actuation functions come into play so after each layer we come into play so after each layer we come into play so after each layer we want to apply an activation function want to apply an activation function want to apply an activation function this applies a non-linear transformation this applies a non-linear transformation this applies a non-linear transformation and helps our network to solve complex and helps our network to solve complex and helps our network to solve complex tasks now let's have a look at different tasks now let's have a look at different tasks now let's have a look at different kinds of actuation functions there are kinds of actuation functions there are kinds of actuation functions there are many different actuation functions you many different actuation functions you many different actuation functions you can choose so we take a look at the most can choose so we take a look at the most can choose so we take a look at the most popular ones we'll have a look at the popular ones we'll have a look at the popular ones we'll have a look at the step function sigmoid hyperbolic tangent step function sigmoid hyperbolic tangent step function sigmoid hyperbolic tangent value leaky value and the softmax the value leaky value and the softmax the value leaky value and the softmax the step function will just output 1 if our step function will just output 1 if our step function will just output 1 if our input is greater than a threshold and 0 input is greater than a threshold and 0 input is greater than a threshold and 0 otherwise this perfectly demonstrates otherwise this perfectly demonstrates otherwise this perfectly demonstrates the underlying concept that the the underlying concept that the the underlying concept that the activation function decides if a neuron activation function decides if a neuron activation function decides if a neuron will be activated or not if the input is will be activated or not if the input is will be activated or not if the input is greater than the threshold the neuron is greater than the threshold the neuron is greater than the threshold the neuron is actuated and otherwise not while this actuated and otherwise not while this actuated and otherwise not while this transformation should be easy to transformation should be easy to transformation should be easy to understand the step function is actually understand the step function is actually understand the step function is actually a little bit too simple and not used in a little bit too simple and not used in a little bit too simple and not used in practice a very popular choice in practice a very popular choice in practice a very popular choice in practice is the sigmoid function the practice is the sigmoid function the practice is the sigmoid function the formula is 1 over 1 plus e to the minus formula is 1 over 1 plus e to the minus formula is 1 over 1 plus e to the minus x this outputs a probability between 0 x this outputs a probability between 0 x this outputs a probability between 0 and 1. if the input is a very negative and 1. if the input is a very negative and 1. if the input is a very negative number then sigmoid outputs a number number then sigmoid outputs a number number then sigmoid outputs a number close to 0 and for a very positive close to 0 and for a very positive close to 0 and for a very positive number sigmoid transforms it to a number number sigmoid transforms it to a number number sigmoid transforms it to a number close to 1 and for numbers close to 0 we close to 1 and for numbers close to 0 we close to 1 and for numbers close to 0 we have this rising curve between 0 and 1. have this rising curve between 0 and 1. have this rising curve between 0 and 1. this again means that the more positive this again means that the more positive this again means that the more positive the input number is the more our neuron the input number is the more our neuron the input number is the more our neuron will be activated the sigmoid function will be activated the sigmoid function will be activated the sigmoid function is sometimes used in hidden layers but is sometimes used in hidden layers but is sometimes used in hidden layers but most of the time it is used in the last most of the time it is used in the last most of the time it is used in the last layer for binary classification problems layer for binary classification problems layer for binary classification problems until now we have only seen activation until now we have only seen activation until now we have only seen activation functions that output numbers between 0 functions that output numbers between 0 functions that output numbers between 0 and 1 but this is not a requirement for and 1 but this is not a requirement for and 1 but this is not a requirement for actuation functions so in the next actuation functions so in the next actuation functions so in the next examples you will see transformations examples you will see transformations examples you will see transformations that can output numbers also in a that can output numbers also in a that can output numbers also in a different range the hyperbolic tangent different range the hyperbolic tangent different range the hyperbolic tangent is a common choice for hidden layers it is a common choice for hidden layers it is a common choice for hidden layers it is basically a scaled and shifted is basically a scaled and shifted is basically a scaled and shifted sigmoid function that outputs a number sigmoid function that outputs a number sigmoid function that outputs a number between -1 and plus 1. value is probably between -1 and plus 1. value is probably between -1 and plus 1. value is probably the most popular choice in hidden layers the most popular choice in hidden layers the most popular choice in hidden layers the formula is rather simple it just the formula is rather simple it just the formula is rather simple it just takes the maximum of 0 and the input x takes the maximum of 0 and the input x takes the maximum of 0 and the input x so if the input is negative it outputs 0 so if the input is negative it outputs 0 so if the input is negative it outputs 0 and if the input is positive it simply and if the input is positive it simply and if the input is positive it simply returns this output without modification returns this output without modification returns this output without modification it does not look that fancy but it can it does not look that fancy but it can it does not look that fancy but it can actually improve the learning of our actually improve the learning of our actually improve the learning of our neural network a lot so the rule of neural network a lot so the rule of neural network a lot so the rule of thumb is that if you are not sure which thumb is that if you are not sure which thumb is that if you are not sure which actuation function you should use in actuation function you should use in actuation function you should use in your hidden layers then just use value your hidden layers then just use value your hidden layers then just use value there is only one problem that sometimes there is only one problem that sometimes there is only one problem that sometimes happens during training this is the happens during training this is the happens during training this is the so-called dying value problem after many so-called dying value problem after many so-called dying value problem after many training iterations our neuron can reach training iterations our neuron can reach training iterations our neuron can reach a dead state where it only outputs 0 for a dead state where it only outputs 0 for a dead state where it only outputs 0 for any given input which means there will any given input which means there will any given input which means there will be no more updates for your weights so be no more updates for your weights so be no more updates for your weights so to avoid this problem you can use a to avoid this problem you can use a to avoid this problem you can use a slightly adapted function which is the slightly adapted function which is the slightly adapted function which is the leaky value the leaky value is the same leaky value the leaky value is the same leaky value the leaky value is the same as the regular value for positive as the regular value for positive as the regular value for positive numbers here it just returns the input numbers here it just returns the input numbers here it just returns the input but for negative numbers it does not but for negative numbers it does not but for negative numbers it does not simply return 0 but it applies a small simply return 0 but it applies a small simply return 0 but it applies a small scaling factor a times x a is usually scaling factor a times x a is usually scaling factor a times x a is usually very small for example very small for example very small for example 0.001 so the output is close to zero but 0.001 so the output is close to zero but 0.001 so the output is close to zero but it avoids that the neuron will be it avoids that the neuron will be it avoids that the neuron will be completely dead so this is also a very completely dead so this is also a very completely dead so this is also a very good choice for hidden layers so good choice for hidden layers so good choice for hidden layers so whenever you notice that your weights whenever you notice that your weights whenever you notice that your weights won't update during training then try won't update during training then try won't update during training then try using leaky value instead of the normal using leaky value instead of the normal using leaky value instead of the normal value and the last function i want to value and the last function i want to value and the last function i want to show you is the softmax function the show you is the softmax function the show you is the softmax function the softmax squashes the input numbers to softmax squashes the input numbers to softmax squashes the input numbers to output numbers between 0 and 1 so that output numbers between 0 and 1 so that output numbers between 0 and 1 so that you will get a probability value at the you will get a probability value at the you will get a probability value at the end so the higher the raw input number end so the higher the raw input number end so the higher the raw input number the higher will be the probability value the higher will be the probability value the higher will be the probability value this is usually used in the last layer this is usually used in the last layer this is usually used in the last layer in multi-class classification problems in multi-class classification problems in multi-class classification problems after applying the softmax in the end after applying the softmax in the end after applying the softmax in the end you then decide for the class with the you then decide for the class with the you then decide for the class with the highest probability now that we've seen highest probability now that we've seen highest probability now that we've seen different actuation functions in theory different actuation functions in theory different actuation functions in theory let's have a look at how we can use them let's have a look at how we can use them let's have a look at how we can use them in tensorflow and pytorch it is quite in tensorflow and pytorch it is quite in tensorflow and pytorch it is quite easy with both frameworks in tensorflow easy with both frameworks in tensorflow easy with both frameworks in tensorflow i recommend using the keras api with i recommend using the keras api with i recommend using the keras api with this we have two options for each layer this we have two options for each layer this we have two options for each layer we can specify the optional argument we can specify the optional argument we can specify the optional argument actuation and then just use the name of actuation and then just use the name of actuation and then just use the name of the actuation function or we just leave the actuation function or we just leave the actuation function or we just leave this actuation argument away and create this actuation argument away and create this actuation argument away and create the layer ourself all the functions i the layer ourself all the functions i the layer ourself all the functions i just showed you are available as a layer just showed you are available as a layer just showed you are available as a layer tensorflow.keras.layers in pytorch we tensorflow.keras.layers in pytorch we tensorflow.keras.layers in pytorch we also find all actuation functions as a also find all actuation functions as a also find all actuation functions as a layer under torch.nn in our init layer under torch.nn in our init layer under torch.nn in our init function of the neural network we can function of the neural network we can function of the neural network we can create instances of the actuation create instances of the actuation create instances of the actuation function layers and then in the forward function layers and then in the forward function layers and then in the forward pass we call these layers or as a second pass we call these layers or as a second pass we call these layers or as a second option we can use the functions directly option we can use the functions directly option we can use the functions directly in the forward pass by using the in the forward pass by using the in the forward pass by using the functions defined in torch.nn.functional functions defined in torch.nn.functional functions defined in torch.nn.functional and that's basically all we have to do and that's basically all we have to do and that's basically all we have to do to use actuation functions in our code to use actuation functions in our code to use actuation functions in our code alright so i hope you now have a clear alright so i hope you now have a clear alright so i hope you now have a clear understanding of what actuation understanding of what actuation understanding of what actuation functions are and how you can use them functions are and how you can use them functions are and how you can use them and if you have any questions let me and if you have any questions let me and if you have any questions let me know in the comments and also if you know in the comments and also if you know in the comments and also if you enjoyed this video then please hit the enjoyed this video then please hit the enjoyed this video then please hit the like button and consider subscribing to like button and consider subscribing to like button and consider subscribing to the channel for more content like this the channel for more content like this the channel for more content like this and before you leave don't forget to and before you leave don't forget to and before you leave don't forget to grab your free api token using the link grab your free api token using the link grab your free api token using the link in the description below and then i hope in the description below and then i hope in the description below and then i hope to see you in the next video bye",
    "chunks": [
      "Kind: captions Language: en in this video we are going to learn in this video we are going to learn in this video we are going to learn about actuation functions we go over the about actuation functions we go over the about actuation functions we go over the definition of actuation functions why definition of actuation functions why definition of actuation functions why they are used then we have a look at they are used then we have a look at they are used then we have a look at different kinds",
      "of actuation functions different kinds of actuation functions different kinds of actuation functions and at the end i also show you how to and at the end i also show you how to and at the end i also show you how to use them in your code and don't worry use them in your code and don't worry use them in your code and don't worry because deep learning frameworks like because deep learning frameworks like because deep learning frameworks like pytorch and tensorflow make it extremely pytorch and",
      "tensorflow make it extremely pytorch and tensorflow make it extremely easy to apply them this video is part of easy to apply them this video is part of easy to apply them this video is part of the deep learning explained series by the deep learning explained series by the deep learning explained series by assembly ai which is a company that assembly ai which is a company that assembly ai which is a company that creates a state-of-the-art creates a state-of-the-art creates a state-of-the-art",
      "speech-to-text api and if you want to speech-to-text api and if you want to speech-to-text api and if you want to use assembly ai for free then grab your use assembly ai for free then grab your use assembly ai for free then grab your api token using the link in the api token using the link in the api token using the link in the description below and now let's get description below and now let's get description below and now let's get started so what are activation functions started so what are",
      "activation functions started so what are activation functions and why do we need them actuation and why do we need them actuation and why do we need them actuation functions apply a non-linear functions apply a non-linear functions apply a non-linear transformation and decide whether a transformation and decide whether a transformation and decide whether a neuron should be activated or not now neuron should be activated or not now neuron should be activated or not now let's take a step back and",
      "see what this let's take a step back and see what this let's take a step back and see what this means in a previous video we learned how means in a previous video we learned how means in a previous video we learned how neural networks work in a neural network neural networks work in a neural network neural networks work in a neural network we have the input layer where we accept we have the input layer where we accept we have the input layer where we accept an input and an output layer that gives",
      "an input and an output layer that gives an input and an output layer that gives the actual prediction or the outcome of the actual prediction or the outcome of the actual prediction or the outcome of the network and in between we have the the network and in between we have the the network and in between we have the hidden layers all of these layers hidden layers all of these layers hidden layers all of these layers consist of neurons and at each neuron we consist of neurons and at each neuron we",
      "consist of neurons and at each neuron we apply a linear transformation it apply a linear transformation it apply a linear transformation it multiplies the input with some weights multiplies the input with some weights multiplies the input with some weights and maybe adds a bias now this is fine and maybe adds a bias now this is fine and maybe adds a bias now this is fine as long as we have a simple problem like as long as we have a simple problem like as long as we have a simple problem like this",
      "where we can model the predictions this where we can model the predictions this where we can model the predictions with a linear function but let's say we with a linear function but let's say we with a linear function but let's say we have a more complex problem one thing we have a more complex problem one thing we have a more complex problem one thing we can do is of course add more layers to can do is of course add more layers to can do is of course add more layers to our network but here's a",
      "big problem our network but here's a big problem our network but here's a big problem without activation functions we only get without activation functions we only get without activation functions we only get linear transformations after each other linear transformations after each other linear transformations after each other so our whole network is basically just a so our whole network is basically just a so our whole network is basically just a stacked linear regression model that is stacked",
      "linear regression model that is stacked linear regression model that is not able to learn complex patterns and not able to learn complex patterns and not able to learn complex patterns and this is exactly why actuation functions this is exactly why actuation functions this is exactly why actuation functions come into play so after each layer we come into play so after each layer we come into play so after each layer we want to apply an activation function want to apply an activation function want",
      "to apply an activation function this applies a non-linear transformation this applies a non-linear transformation this applies a non-linear transformation and helps our network to solve complex and helps our network to solve complex and helps our network to solve complex tasks now let's have a look at different tasks now let's have a look at different tasks now let's have a look at different kinds of actuation functions there are kinds of actuation functions there are kinds of actuation functions",
      "there are many different actuation functions you many different actuation functions you many different actuation functions you can choose so we take a look at the most can choose so we take a look at the most can choose so we take a look at the most popular ones we'll have a look at the popular ones we'll have a look at the popular ones we'll have a look at the step function sigmoid hyperbolic tangent step function sigmoid hyperbolic tangent step function sigmoid hyperbolic tangent value leaky",
      "value and the softmax the value leaky value and the softmax the value leaky value and the softmax the step function will just output 1 if our step function will just output 1 if our step function will just output 1 if our input is greater than a threshold and 0 input is greater than a threshold and 0 input is greater than a threshold and 0 otherwise this perfectly demonstrates otherwise this perfectly demonstrates otherwise this perfectly demonstrates the underlying concept that the the",
      "underlying concept that the the underlying concept that the activation function decides if a neuron activation function decides if a neuron activation function decides if a neuron will be activated or not if the input is will be activated or not if the input is will be activated or not if the input is greater than the threshold the neuron is greater than the threshold the neuron is greater than the threshold the neuron is actuated and otherwise not while this actuated and otherwise not while this",
      "actuated and otherwise not while this transformation should be easy to transformation should be easy to transformation should be easy to understand the step function is actually understand the step function is actually understand the step function is actually a little bit too simple and not used in a little bit too simple and not used in a little bit too simple and not used in practice a very popular choice in practice a very popular choice in practice a very popular choice in practice is the",
      "sigmoid function the practice is the sigmoid function the practice is the sigmoid function the formula is 1 over 1 plus e to the minus formula is 1 over 1 plus e to the minus formula is 1 over 1 plus e to the minus x this outputs a probability between 0 x this outputs a probability between 0 x this outputs a probability between 0 and 1. if the input is a very negative and 1. if the input is a very negative and 1. if the input is a very negative number then sigmoid outputs a number number then",
      "sigmoid outputs a number number then sigmoid outputs a number close to 0 and for a very positive close to 0 and for a very positive close to 0 and for a very positive number sigmoid transforms it to a number number sigmoid transforms it to a number number sigmoid transforms it to a number close to 1 and for numbers close to 0 we close to 1 and for numbers close to 0 we close to 1 and for numbers close to 0 we have this rising curve between 0 and 1. have this rising curve between 0 and 1. have",
      "this rising curve between 0 and 1. this again means that the more positive this again means that the more positive this again means that the more positive the input number is the more our neuron the input number is the more our neuron the input number is the more our neuron will be activated the sigmoid function will be activated the sigmoid function will be activated the sigmoid function is sometimes used in hidden layers but is sometimes used in hidden layers but is sometimes used in hidden",
      "layers but most of the time it is used in the last most of the time it is used in the last most of the time it is used in the last layer for binary classification problems layer for binary classification problems layer for binary classification problems until now we have only seen activation until now we have only seen activation until now we have only seen activation functions that output numbers between 0 functions that output numbers between 0 functions that output numbers between 0 and 1 but",
      "this is not a requirement for and 1 but this is not a requirement for and 1 but this is not a requirement for actuation functions so in the next actuation functions so in the next actuation functions so in the next examples you will see transformations examples you will see transformations examples you will see transformations that can output numbers also in a that can output numbers also in a that can output numbers also in a different range the hyperbolic tangent different range the hyperbolic",
      "tangent different range the hyperbolic tangent is a common choice for hidden layers it is a common choice for hidden layers it is a common choice for hidden layers it is basically a scaled and shifted is basically a scaled and shifted is basically a scaled and shifted sigmoid function that outputs a number sigmoid function that outputs a number sigmoid function that outputs a number between -1 and plus 1. value is probably between -1 and plus 1. value is probably between -1 and plus 1. value is",
      "probably the most popular choice in hidden layers the most popular choice in hidden layers the most popular choice in hidden layers the formula is rather simple it just the formula is rather simple it just the formula is rather simple it just takes the maximum of 0 and the input x takes the maximum of 0 and the input x takes the maximum of 0 and the input x so if the input is negative it outputs 0 so if the input is negative it outputs 0 so if the input is negative it outputs 0 and if the input",
      "is positive it simply and if the input is positive it simply and if the input is positive it simply returns this output without modification returns this output without modification returns this output without modification it does not look that fancy but it can it does not look that fancy but it can it does not look that fancy but it can actually improve the learning of our actually improve the learning of our actually improve the learning of our neural network a lot so the rule of neural network",
      "a lot so the rule of neural network a lot so the rule of thumb is that if you are not sure which thumb is that if you are not sure which thumb is that if you are not sure which actuation function you should use in actuation function you should use in actuation function you should use in your hidden layers then just use value your hidden layers then just use value your hidden layers then just use value there is only one problem that sometimes there is only one problem that sometimes there is only",
      "one problem that sometimes happens during training this is the happens during training this is the happens during training this is the so-called dying value problem after many so-called dying value problem after many so-called dying value problem after many training iterations our neuron can reach training iterations our neuron can reach training iterations our neuron can reach a dead state where it only outputs 0 for a dead state where it only outputs 0 for a dead state where it only outputs 0",
      "for any given input which means there will any given input which means there will any given input which means there will be no more updates for your weights so be no more updates for your weights so be no more updates for your weights so to avoid this problem you can use a to avoid this problem you can use a to avoid this problem you can use a slightly adapted function which is the slightly adapted function which is the slightly adapted function which is the leaky value the leaky value is the",
      "same leaky value the leaky value is the same leaky value the leaky value is the same as the regular value for positive as the regular value for positive as the regular value for positive numbers here it just returns the input numbers here it just returns the input numbers here it just returns the input but for negative numbers it does not but for negative numbers it does not but for negative numbers it does not simply return 0 but it applies a small simply return 0 but it applies a small simply",
      "return 0 but it applies a small scaling factor a times x a is usually scaling factor a times x a is usually scaling factor a times x a is usually very small for example very small for example very small for example 0.001 so the output is close to zero but 0.001 so the output is close to zero but 0.001 so the output is close to zero but it avoids that the neuron will be it avoids that the neuron will be it avoids that the neuron will be completely dead so this is also a very completely dead so",
      "this is also a very completely dead so this is also a very good choice for hidden layers so good choice for hidden layers so good choice for hidden layers so whenever you notice that your weights whenever you notice that your weights whenever you notice that your weights won't update during training then try won't update during training then try won't update during training then try using leaky value instead of the normal using leaky value instead of the normal using leaky value instead of the",
      "normal value and the last function i want to value and the last function i want to value and the last function i want to show you is the softmax function the show you is the softmax function the show you is the softmax function the softmax squashes the input numbers to softmax squashes the input numbers to softmax squashes the input numbers to output numbers between 0 and 1 so that output numbers between 0 and 1 so that output numbers between 0 and 1 so that you will get a probability value at",
      "the you will get a probability value at the you will get a probability value at the end so the higher the raw input number end so the higher the raw input number end so the higher the raw input number the higher will be the probability value the higher will be the probability value the higher will be the probability value this is usually used in the last layer this is usually used in the last layer this is usually used in the last layer in multi-class classification problems in multi-class",
      "classification problems in multi-class classification problems after applying the softmax in the end after applying the softmax in the end after applying the softmax in the end you then decide for the class with the you then decide for the class with the you then decide for the class with the highest probability now that we've seen highest probability now that we've seen highest probability now that we've seen different actuation functions in theory different actuation functions in theory",
      "different actuation functions in theory let's have a look at how we can use them let's have a look at how we can use them let's have a look at how we can use them in tensorflow and pytorch it is quite in tensorflow and pytorch it is quite in tensorflow and pytorch it is quite easy with both frameworks in tensorflow easy with both frameworks in tensorflow easy with both frameworks in tensorflow i recommend using the keras api with i recommend using the keras api with i recommend using the keras",
      "api with this we have two options for each layer this we have two options for each layer this we have two options for each layer we can specify the optional argument we can specify the optional argument we can specify the optional argument actuation and then just use the name of actuation and then just use the name of actuation and then just use the name of the actuation function or we just leave the actuation function or we just leave the actuation function or we just leave this actuation",
      "argument away and create this actuation argument away and create this actuation argument away and create the layer ourself all the functions i the layer ourself all the functions i the layer ourself all the functions i just showed you are available as a layer just showed you are available as a layer just showed you are available as a layer tensorflow.keras.layers in pytorch we tensorflow.keras.layers in pytorch we tensorflow.keras.layers in pytorch we also find all actuation functions as a also",
      "find all actuation functions as a also find all actuation functions as a layer under torch.nn in our init layer under torch.nn in our init layer under torch.nn in our init function of the neural network we can function of the neural network we can function of the neural network we can create instances of the actuation create instances of the actuation create instances of the actuation function layers and then in the forward function layers and then in the forward function layers and then in the",
      "forward pass we call these layers or as a second pass we call these layers or as a second pass we call these layers or as a second option we can use the functions directly option we can use the functions directly option we can use the functions directly in the forward pass by using the in the forward pass by using the in the forward pass by using the functions defined in torch.nn.functional functions defined in torch.nn.functional functions defined in torch.nn.functional and that's basically all",
      "we have to do and that's basically all we have to do and that's basically all we have to do to use actuation functions in our code to use actuation functions in our code to use actuation functions in our code alright so i hope you now have a clear alright so i hope you now have a clear alright so i hope you now have a clear understanding of what actuation understanding of what actuation understanding of what actuation functions are and how you can use them functions are and how you can use them",
      "functions are and how you can use them and if you have any questions let me and if you have any questions let me and if you have any questions let me know in the comments and also if you know in the comments and also if you know in the comments and also if you enjoyed this video then please hit the enjoyed this video then please hit the enjoyed this video then please hit the like button and consider subscribing to like button and consider subscribing to like button and consider subscribing to the",
      "channel for more content like this the channel for more content like this the channel for more content like this and before you leave don't forget to and before you leave don't forget to and before you leave don't forget to grab your free api token using the link grab your free api token using the link grab your free api token using the link in the description below and then i hope in the description below and then i hope in the description below and then i hope to see you in the next video bye"
    ],
    "chunk_count": 41,
    "content_id": "ac0451ec-4bbf-46a7-94fc-19dad2ed9d0c",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.554998"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=qiUEgSCyY5o": {
    "title": "What are Autoencoders?",
    "url": "https://www.youtube.com/watch?v=qiUEgSCyY5o",
    "description": "Learn about watsonx: https://ibm.biz/BdvxR8\n\nAn autoencoder is an unsupervised learning technique, but what does that mean?=\nIn this video, showing that two Martins are better than one, Martin Keen will explain more about what autoencoders are and how they work. \n\n#AI #Software #ITModernization",
    "duration": 299,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en if i input myself into an autoencoder it can create a pretty good al though it can create a pretty good al though it can create a pretty good al though not perfect not perfect not perfect reconstruction of me reconstruction of me reconstruction of me why don't i uh well i mean you explain why don't i uh well i mean you explain why don't i uh well i mean you explain sure so look auto encoders are an sure so look auto encoders are an sure so look auto encoders are an unsupervised neural network and they unsupervised neural network and they unsupervised neural network and they consist of two parts let's let's take a consist of two parts let's let's take a consist of two parts let's let's take a look there is first of all an encoder look there is first of all an encoder look there is first of all an encoder that's that's that's the first layer the first layer the first layer and that takes in some input and learns how to efficiently compress and learns how to efficiently compress and learns how to efficiently compress and encode that data into something that and encode that data into something that and encode that data into something that we call we call we call the code then we have a decoder that learns how then we have a decoder that learns how then we have a decoder that learns how to reconstruct that encoded data to reconstruct that encoded data to reconstruct that encoded data representation so representation so representation so decoder decoder decoder and that creates output output output and that output is as similar to the and that output is as similar to the and that output is as similar to the original input data as possible original input data as possible original input data as possible effectively an autoencoder learns to effectively an autoencoder learns to effectively an autoencoder learns to recognize which aspects of observable recognize which aspects of observable recognize which aspects of observable data are relevant and limit noise in data are relevant and limit noise in data are relevant and limit noise in data that can be discarded separate the data that can be discarded separate the data that can be discarded separate the signal signal signal from the noise from the noise from the noise okay great so is this is this all about okay great so is this is this all about okay great so is this is this all about creating smaller file sizes like the way creating smaller file sizes like the way creating smaller file sizes like the way i'll zip up some documents or compressor i'll zip up some documents or compressor i'll zip up some documents or compressor video no not at all video no not at all video no not at all so let's talk about some examples so let's talk about some examples so let's talk about some examples convolutional autoencoders have a convolutional autoencoders have a convolutional autoencoders have a variety of use cases related to images variety of use cases related to images variety of use cases related to images so for example i can draw an image like so for example i can draw an image like so for example i can draw an image like this number three this number three this number three and then through a process called and then through a process called and then through a process called feature extraction i can derive the feature extraction i can derive the feature extraction i can derive the required features of the image by required features of the image by required features of the image by removing noise removing noise removing noise something that looks a bit more like something that looks a bit more like something that looks a bit more like this and then i'm able to generate an output and then i'm able to generate an output and then i'm able to generate an output that approximates the original that approximates the original that approximates the original it's not exactly the same it's not exactly the same it's not exactly the same but it's pretty close but it's pretty close but it's pretty close now i can use this part called the code now i can use this part called the code now i can use this part called the code to do other things like create a higher to do other things like create a higher to do other things like create a higher resolution version of the output image resolution version of the output image resolution version of the output image or i can colorize an image so black and or i can colorize an image so black and or i can colorize an image so black and white input full color output white input full color output white input full color output now in this case the input and the now in this case the input and the now in this case the input and the output they look much the same which output they look much the same which output they look much the same which well that's what autoencoders are all well that's what autoencoders are all well that's what autoencoders are all about but they don't have to be about but they don't have to be about but they don't have to be we can provide input to an auto encoder we can provide input to an auto encoder we can provide input to an auto encoder in a corrupted form like the noisy image in a corrupted form like the noisy image in a corrupted form like the noisy image of a of a 3 of a of a 3 of a of a 3 and then train a denoising auto encoder and then train a denoising auto encoder and then train a denoising auto encoder to reconstruct our original image from to reconstruct our original image from to reconstruct our original image from the noisy version of it the noisy version of it the noisy version of it once we've trained our auto encoder to once we've trained our auto encoder to once we've trained our auto encoder to remove noise from a representation of a remove noise from a representation of a remove noise from a representation of a number or a picture or number or a picture or number or a picture or park bench park bench park bench we can apply that to all sorts of we can apply that to all sorts of we can apply that to all sorts of objects within an object element that objects within an object element that objects within an object element that displayed the same noise pattern displayed the same noise pattern displayed the same noise pattern so let's take a closer look inside an so let's take a closer look inside an so let's take a closer look inside an auto encoder auto encoder auto encoder the encoder itself compresses the input the encoder itself compresses the input the encoder itself compresses the input into a latent space representation so into a latent space representation so into a latent space representation so we have multiple layers here we have multiple layers here we have multiple layers here that represent the encoder the encoder the encoder each one a little smaller than the other each one a little smaller than the other each one a little smaller than the other so this part here that's the so this part here that's the so this part here that's the encoder encoder encoder the most compressed version we said the most compressed version we said the most compressed version we said that's called the code that's also known that's called the code that's also known that's called the code that's also known as the bottle neck because i suppose as the bottle neck because i suppose as the bottle neck because i suppose much like the neck of a bottle that's much like the neck of a bottle that's much like the neck of a bottle that's the most compressed part the most compressed part the most compressed part then we have the decoder then we have the decoder then we have the decoder which is reconstructed from that bottleneck from that bottleneck from that bottleneck that's the decoder that's the decoder that's the decoder and it's reconstructed from the latent and it's reconstructed from the latent and it's reconstructed from the latent space representation to generate the space representation to generate the space representation to generate the output output output by learning what makes up the signal and by learning what makes up the signal and by learning what makes up the signal and what makes up the noise we also have the what makes up the noise we also have the what makes up the noise we also have the ability to detect when something is not ability to detect when something is not ability to detect when something is not part of the status quo and that's part of the status quo and that's part of the status quo and that's anomaly detection anomalies are a anomaly detection anomalies are a anomaly detection anomalies are a significant deviation from the general significant deviation from the general significant deviation from the general behavior of the data and auto encoders behavior of the data and auto encoders behavior of the data and auto encoders are very good at telling us when are very good at telling us when are very good at telling us when something doesn't fit something doesn't fit something doesn't fit as a result auto encoders are widely as a result auto encoders are widely as a result auto encoders are widely used in anomaly detection in things such used in anomaly detection in things such used in anomaly detection in things such as fault as fault as fault fraud and intrusion detection fraud and intrusion detection fraud and intrusion detection so auto encoders are a great way of so auto encoders are a great way of so auto encoders are a great way of extracting noise extracting noise extracting noise recognizing relevant features and recognizing relevant features and recognizing relevant features and detecting anomalies a pretty handy detecting anomalies a pretty handy detecting anomalies a pretty handy toolbox for dealing with all sorts of toolbox for dealing with all sorts of toolbox for dealing with all sorts of data data data eerily similar clones if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching",
    "chunks": [
      "Kind: captions Language: en if i input myself into an autoencoder it can create a pretty good al though it can create a pretty good al though it can create a pretty good al though not perfect not perfect not perfect reconstruction of me reconstruction of me reconstruction of me why don't i uh well i mean you explain why don't i uh well i mean you explain why don't i uh well i mean you explain sure so look auto encoders are an sure so look auto encoders are an sure so look auto encoders are an",
      "unsupervised neural network and they unsupervised neural network and they unsupervised neural network and they consist of two parts let's let's take a consist of two parts let's let's take a consist of two parts let's let's take a look there is first of all an encoder look there is first of all an encoder look there is first of all an encoder that's that's that's the first layer the first layer the first layer and that takes in some input and learns how to efficiently compress and learns how to",
      "efficiently compress and learns how to efficiently compress and encode that data into something that and encode that data into something that and encode that data into something that we call we call we call the code then we have a decoder that learns how then we have a decoder that learns how then we have a decoder that learns how to reconstruct that encoded data to reconstruct that encoded data to reconstruct that encoded data representation so representation so representation so decoder decoder",
      "decoder and that creates output output output and that output is as similar to the and that output is as similar to the and that output is as similar to the original input data as possible original input data as possible original input data as possible effectively an autoencoder learns to effectively an autoencoder learns to effectively an autoencoder learns to recognize which aspects of observable recognize which aspects of observable recognize which aspects of observable data are relevant and",
      "limit noise in data are relevant and limit noise in data are relevant and limit noise in data that can be discarded separate the data that can be discarded separate the data that can be discarded separate the signal signal signal from the noise from the noise from the noise okay great so is this is this all about okay great so is this is this all about okay great so is this is this all about creating smaller file sizes like the way creating smaller file sizes like the way creating smaller file",
      "sizes like the way i'll zip up some documents or compressor i'll zip up some documents or compressor i'll zip up some documents or compressor video no not at all video no not at all video no not at all so let's talk about some examples so let's talk about some examples so let's talk about some examples convolutional autoencoders have a convolutional autoencoders have a convolutional autoencoders have a variety of use cases related to images variety of use cases related to images variety of use",
      "cases related to images so for example i can draw an image like so for example i can draw an image like so for example i can draw an image like this number three this number three this number three and then through a process called and then through a process called and then through a process called feature extraction i can derive the feature extraction i can derive the feature extraction i can derive the required features of the image by required features of the image by required features of the",
      "image by removing noise removing noise removing noise something that looks a bit more like something that looks a bit more like something that looks a bit more like this and then i'm able to generate an output and then i'm able to generate an output and then i'm able to generate an output that approximates the original that approximates the original that approximates the original it's not exactly the same it's not exactly the same it's not exactly the same but it's pretty close but it's pretty",
      "close but it's pretty close now i can use this part called the code now i can use this part called the code now i can use this part called the code to do other things like create a higher to do other things like create a higher to do other things like create a higher resolution version of the output image resolution version of the output image resolution version of the output image or i can colorize an image so black and or i can colorize an image so black and or i can colorize an image so black",
      "and white input full color output white input full color output white input full color output now in this case the input and the now in this case the input and the now in this case the input and the output they look much the same which output they look much the same which output they look much the same which well that's what autoencoders are all well that's what autoencoders are all well that's what autoencoders are all about but they don't have to be about but they don't have to be about but",
      "they don't have to be we can provide input to an auto encoder we can provide input to an auto encoder we can provide input to an auto encoder in a corrupted form like the noisy image in a corrupted form like the noisy image in a corrupted form like the noisy image of a of a 3 of a of a 3 of a of a 3 and then train a denoising auto encoder and then train a denoising auto encoder and then train a denoising auto encoder to reconstruct our original image from to reconstruct our original image from to",
      "reconstruct our original image from the noisy version of it the noisy version of it the noisy version of it once we've trained our auto encoder to once we've trained our auto encoder to once we've trained our auto encoder to remove noise from a representation of a remove noise from a representation of a remove noise from a representation of a number or a picture or number or a picture or number or a picture or park bench park bench park bench we can apply that to all sorts of we can apply that to",
      "all sorts of we can apply that to all sorts of objects within an object element that objects within an object element that objects within an object element that displayed the same noise pattern displayed the same noise pattern displayed the same noise pattern so let's take a closer look inside an so let's take a closer look inside an so let's take a closer look inside an auto encoder auto encoder auto encoder the encoder itself compresses the input the encoder itself compresses the input the",
      "encoder itself compresses the input into a latent space representation so into a latent space representation so into a latent space representation so we have multiple layers here we have multiple layers here we have multiple layers here that represent the encoder the encoder the encoder each one a little smaller than the other each one a little smaller than the other each one a little smaller than the other so this part here that's the so this part here that's the so this part here that's the",
      "encoder encoder encoder the most compressed version we said the most compressed version we said the most compressed version we said that's called the code that's also known that's called the code that's also known that's called the code that's also known as the bottle neck because i suppose as the bottle neck because i suppose as the bottle neck because i suppose much like the neck of a bottle that's much like the neck of a bottle that's much like the neck of a bottle that's the most compressed",
      "part the most compressed part the most compressed part then we have the decoder then we have the decoder then we have the decoder which is reconstructed from that bottleneck from that bottleneck from that bottleneck that's the decoder that's the decoder that's the decoder and it's reconstructed from the latent and it's reconstructed from the latent and it's reconstructed from the latent space representation to generate the space representation to generate the space representation to generate the",
      "output output output by learning what makes up the signal and by learning what makes up the signal and by learning what makes up the signal and what makes up the noise we also have the what makes up the noise we also have the what makes up the noise we also have the ability to detect when something is not ability to detect when something is not ability to detect when something is not part of the status quo and that's part of the status quo and that's part of the status quo and that's anomaly",
      "detection anomalies are a anomaly detection anomalies are a anomaly detection anomalies are a significant deviation from the general significant deviation from the general significant deviation from the general behavior of the data and auto encoders behavior of the data and auto encoders behavior of the data and auto encoders are very good at telling us when are very good at telling us when are very good at telling us when something doesn't fit something doesn't fit something doesn't fit as a",
      "result auto encoders are widely as a result auto encoders are widely as a result auto encoders are widely used in anomaly detection in things such used in anomaly detection in things such used in anomaly detection in things such as fault as fault as fault fraud and intrusion detection fraud and intrusion detection fraud and intrusion detection so auto encoders are a great way of so auto encoders are a great way of so auto encoders are a great way of extracting noise extracting noise extracting",
      "noise recognizing relevant features and recognizing relevant features and recognizing relevant features and detecting anomalies a pretty handy detecting anomalies a pretty handy detecting anomalies a pretty handy toolbox for dealing with all sorts of toolbox for dealing with all sorts of toolbox for dealing with all sorts of data data data eerily similar clones if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below",
      "and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like and subscribe like and subscribe like and subscribe thanks for watching"
    ],
    "chunk_count": 21,
    "content_id": "86cd4554-d776-4dc0-8f14-e26701b87ea3",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555002"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=i62czvwDlsw": {
    "title": "Gradient Descent Explained",
    "url": "https://www.youtube.com/watch?v=i62czvwDlsw",
    "description": "Learn more about WatsonX  →  https://ibm.biz/BdPu9e\nWhat is Gradient Descent?  →  https://ibm.biz/Gradient_Descent\nCreate Data Fabric instead of data silos  → https://ibm.biz/Grad_Descent_Data_Fabric\n\nGradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks.  Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. IBM Master Inventor Martin Keen explains.\n\nGet started for free on IBM Cloud → https://ibm.biz/sign-up-now\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#AI #Software #ITModernization #GradientDescent #lightboard #IBM #DataFabric #WatsonX",
    "duration": 424,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en gradient descent is like it's like gradient descent is like it's like gradient descent is like it's like trying to find your way down a dark trying to find your way down a dark trying to find your way down a dark Mountain you can't see where you're Mountain you can't see where you're Mountain you can't see where you're going so you have to feel your way going so you have to feel your way going so you have to feel your way around you take small steps in the around you take small steps in the around you take small steps in the direction that feels the most downhill direction that feels the most downhill direction that feels the most downhill eventually if you keep going you'll find eventually if you keep going you'll find eventually if you keep going you'll find your way to the bottom that's gradient your way to the bottom that's gradient your way to the bottom that's gradient descent let's get into it descent let's get into it descent let's get into it so gradient descent is a common so gradient descent is a common so gradient descent is a common optimization algorithm used to train optimization algorithm used to train optimization algorithm used to train machine learning models and neural machine learning models and neural machine learning models and neural networks by training on data these networks by training on data these networks by training on data these models can learn over time and because models can learn over time and because models can learn over time and because they're learning over time they can they're learning over time they can they're learning over time they can improve their accuracy now you see a improve their accuracy now you see a improve their accuracy now you see a neural network consists of connected neural network consists of connected neural network consists of connected neurons neurons neurons like this and these neurons are in like this and these neurons are in like this and these neurons are in layers and those layers have weights and layers and those layers have weights and layers and those layers have weights and biases which describe how we navigate biases which describe how we navigate biases which describe how we navigate through this network through this network through this network we provide the neural network with we provide the neural network with we provide the neural network with labeled training data to determine what labeled training data to determine what labeled training data to determine what we should set these weights and biases we should set these weights and biases we should set these weights and biases to to figure something out so like for to to figure something out so like for to to figure something out so like for example I could input a shape let's say example I could input a shape let's say example I could input a shape let's say like that and then we could use the like that and then we could use the like that and then we could use the neural network to learn that squiggle neural network to learn that squiggle neural network to learn that squiggle as our input represents this output as our input represents this output as our input represents this output for number three for number three for number three after we train the neural network we can after we train the neural network we can after we train the neural network we can provide it with more labeled data like provide it with more labeled data like provide it with more labeled data like this squiggle and then we can see if it this squiggle and then we can see if it this squiggle and then we can see if it could also correctly resolve that could also correctly resolve that could also correctly resolve that squiggle to squiggle to squiggle to the number six if it gets some of these the number six if it gets some of these the number six if it gets some of these squiggles wrong the the weights and squiggles wrong the the weights and squiggles wrong the the weights and biases here can be adjusted and then we biases here can be adjusted and then we biases here can be adjusted and then we just try it again just try it again just try it again now how can gradient descent help us now how can gradient descent help us now how can gradient descent help us here well gradient descent is used to here well gradient descent is used to here well gradient descent is used to find the minimum of something called a find the minimum of something called a find the minimum of something called a cost cost cost function function function so what is so what is so what is a cost function well it's a function a cost function well it's a function a cost function well it's a function that tells us how far off our that tells us how far off our that tells us how far off our predictions are from the actual values predictions are from the actual values predictions are from the actual values so the idea is that we want to minimize so the idea is that we want to minimize so the idea is that we want to minimize this cost function to get the best this cost function to get the best this cost function to get the best predictions now to do this we take small predictions now to do this we take small predictions now to do this we take small steps in the direction that reduces the steps in the direction that reduces the steps in the direction that reduces the cost function the most if we think about cost function the most if we think about cost function the most if we think about this on a graph we start here and we this on a graph we start here and we this on a graph we start here and we keep going downhill reducing our cost keep going downhill reducing our cost keep going downhill reducing our cost function as we go function as we go function as we go the size of the steps that we take so the size of the steps that we take so the size of the steps that we take so the size of the steps from here to here the size of the steps from here to here the size of the steps from here to here and to here that's called The Learning and to here that's called The Learning and to here that's called The Learning rate rate rate let's think about another example let's let's think about another example let's let's think about another example let's consider a neural network but instead of consider a neural network but instead of consider a neural network but instead of dealing with squiggles predicts how much dealing with squiggles predicts how much dealing with squiggles predicts how much a house will sell for so first we train a house will sell for so first we train a house will sell for so first we train the network on a labeled data set let's the network on a labeled data set let's the network on a labeled data set let's say that data has some information like say that data has some information like say that data has some information like um like the location of a house let's um like the location of a house let's um like the location of a house let's say the size of the house and then how say the size of the house and then how say the size of the house and then how much it sold for much it sold for much it sold for so with that we can then use our model so with that we can then use our model so with that we can then use our model to train new labeled data so here's a to train new labeled data so here's a to train new labeled data so here's a here's another example we've got a house here's another example we've got a house here's another example we've got a house uh it's location let's do it by ZIP code uh it's location let's do it by ZIP code uh it's location let's do it by ZIP code 275 from three how big is it 275 from three how big is it 275 from three how big is it uh 3 000 square feet input that into our uh 3 000 square feet input that into our uh 3 000 square feet input that into our neural network so how much does this neural network so how much does this neural network so how much does this house sell for well now our neural house sell for well now our neural house sell for well now our neural network will make a forecast it says we network will make a forecast it says we network will make a forecast it says we think think think is sold for three hundred thousand is sold for three hundred thousand is sold for three hundred thousand dollars dollars dollars and we compare that and we compare that and we compare that to the forecast of the actual sale price to the forecast of the actual sale price to the forecast of the actual sale price which was which was which was 000 dollars 000 dollars 000 dollars not a good guess we have a large cost not a good guess we have a large cost not a good guess we have a large cost function weights and biases now need to function weights and biases now need to function weights and biases now need to be adjusted and then the model can try be adjusted and then the model can try be adjusted and then the model can try again and did it do any better over the again and did it do any better over the again and did it do any better over the entire label data set or did it do worse entire label data set or did it do worse entire label data set or did it do worse that's what gradient descent can help us that's what gradient descent can help us that's what gradient descent can help us with with with now there are three types of gradient now there are three types of gradient now there are three types of gradient descent learning algorithms and let's descent learning algorithms and let's descent learning algorithms and let's take a look at some of those take a look at some of those take a look at some of those so first of all so first of all so first of all we've got a gradient descent called we've got a gradient descent called we've got a gradient descent called batch batch batch this sums the entries for each point in this sums the entries for each point in this sums the entries for each point in a training set updating the model only a training set updating the model only a training set updating the model only after all the training examples have after all the training examples have after all the training examples have been evaluated hence the term batch now been evaluated hence the term batch now been evaluated hence the term batch now in terms of how well does this do well in terms of how well does this do well in terms of how well does this do well computationally it is computationally computationally it is computationally computationally it is computationally effective effective effective you can give this a high rating you can give this a high rating you can give this a high rating because we're doing things in one big because we're doing things in one big because we're doing things in one big batch but what about processing time batch but what about processing time batch but what about processing time well with processing time we can end up well with processing time we can end up well with processing time we can end up with long processing times using batch with long processing times using batch with long processing times using batch gradient descent because well we've got gradient descent because well we've got gradient descent because well we've got large training data sets and it needs to large training data sets and it needs to large training data sets and it needs to store all of that data in memory and store all of that data in memory and store all of that data in memory and process it process it process it so that's batch another option is so that's batch another option is so that's batch another option is stochastic stochastic stochastic gradient descent and this evaluates each gradient descent and this evaluates each gradient descent and this evaluates each training example but one at a time training example but one at a time training example but one at a time instead of in a batch since you only instead of in a batch since you only instead of in a batch since you only need to hold one training example need to hold one training example need to hold one training example they're easy to store in memory and get they're easy to store in memory and get they're easy to store in memory and get individual responses much faster so in individual responses much faster so in individual responses much faster so in terms of speed terms of speed terms of speed that's that's that's fast but in terms of computational fast but in terms of computational fast but in terms of computational efficiency that's lower now there is a happy medium and that is now there is a happy medium and that is now there is a happy medium and that is called called called mini batch and mini batch gradient mini batch and mini batch gradient mini batch and mini batch gradient descent splits the training data set descent splits the training data set descent splits the training data set into small batch sizes and performs into small batch sizes and performs into small batch sizes and performs updates on each of those batches that is updates on each of those batches that is updates on each of those batches that is a nice balance of computational a nice balance of computational a nice balance of computational efficiency and of speed now gradient efficiency and of speed now gradient efficiency and of speed now gradient descent does come with its own descent does come with its own descent does come with its own challenges so for example it can challenges so for example it can challenges so for example it can struggle to find the global minimum in struggle to find the global minimum in struggle to find the global minimum in non-convex problems this was a nice non-convex problems this was a nice non-convex problems this was a nice convex problem with a clearly defined convex problem with a clearly defined convex problem with a clearly defined bottom bottom bottom so when are the slope of the cost so when are the slope of the cost so when are the slope of the cost function is close to zero or it's at function is close to zero or it's at function is close to zero or it's at zero the model stops learning but if we zero the model stops learning but if we zero the model stops learning but if we don't have this convex model here that don't have this convex model here that don't have this convex model here that we have something like we have something like we have something like this shape that's known as a saddle this shape that's known as a saddle this shape that's known as a saddle point and it can mislead the gradient point and it can mislead the gradient point and it can mislead the gradient descent because it thinks it's descent because it thinks it's descent because it thinks it's at the bottom at the bottom at the bottom before it really is this is going to before it really is this is going to before it really is this is going to keep going down further keep going down further keep going down further chord a subtle shape because it kind of chord a subtle shape because it kind of chord a subtle shape because it kind of looks like a horse saddle I guess looks like a horse saddle I guess looks like a horse saddle I guess another challenge is that in deeper another challenge is that in deeper another challenge is that in deeper neural learning networks a gradient neural learning networks a gradient neural learning networks a gradient descent can suffer from vanish descent can suffer from vanish descent can suffer from vanish ingredients or exploding gradients so ingredients or exploding gradients so ingredients or exploding gradients so Vanishing gradients are when the Vanishing gradients are when the Vanishing gradients are when the gradient is too small and the earlier gradient is too small and the earlier gradient is too small and the earlier layers in the network learn more slowly layers in the network learn more slowly layers in the network learn more slowly than the later layers as we go through than the later layers as we go through than the later layers as we go through this network here this network here this network here exploding gradients on the other hand exploding gradients on the other hand exploding gradients on the other hand are when the gradient is too large and are when the gradient is too large and are when the gradient is too large and that can create an unstable model but that can create an unstable model but that can create an unstable model but look despite those challenges gradient look despite those challenges gradient look despite those challenges gradient descent is a powerful optimization descent is a powerful optimization descent is a powerful optimization algorithm and it is commonly used to algorithm and it is commonly used to algorithm and it is commonly used to train machine learning models and neural train machine learning models and neural train machine learning models and neural networks today it's a clever way to get networks today it's a clever way to get networks today it's a clever way to get you back down that mountain safely you back down that mountain safely you back down that mountain safely if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like And subscribe thanks for watching",
    "chunks": [
      "Kind: captions Language: en gradient descent is like it's like gradient descent is like it's like gradient descent is like it's like trying to find your way down a dark trying to find your way down a dark trying to find your way down a dark Mountain you can't see where you're Mountain you can't see where you're Mountain you can't see where you're going so you have to feel your way going so you have to feel your way going so you have to feel your way around you take small steps in the around you",
      "take small steps in the around you take small steps in the direction that feels the most downhill direction that feels the most downhill direction that feels the most downhill eventually if you keep going you'll find eventually if you keep going you'll find eventually if you keep going you'll find your way to the bottom that's gradient your way to the bottom that's gradient your way to the bottom that's gradient descent let's get into it descent let's get into it descent let's get into it so",
      "gradient descent is a common so gradient descent is a common so gradient descent is a common optimization algorithm used to train optimization algorithm used to train optimization algorithm used to train machine learning models and neural machine learning models and neural machine learning models and neural networks by training on data these networks by training on data these networks by training on data these models can learn over time and because models can learn over time and because models",
      "can learn over time and because they're learning over time they can they're learning over time they can they're learning over time they can improve their accuracy now you see a improve their accuracy now you see a improve their accuracy now you see a neural network consists of connected neural network consists of connected neural network consists of connected neurons neurons neurons like this and these neurons are in like this and these neurons are in like this and these neurons are in layers and",
      "those layers have weights and layers and those layers have weights and layers and those layers have weights and biases which describe how we navigate biases which describe how we navigate biases which describe how we navigate through this network through this network through this network we provide the neural network with we provide the neural network with we provide the neural network with labeled training data to determine what labeled training data to determine what labeled training data to",
      "determine what we should set these weights and biases we should set these weights and biases we should set these weights and biases to to figure something out so like for to to figure something out so like for to to figure something out so like for example I could input a shape let's say example I could input a shape let's say example I could input a shape let's say like that and then we could use the like that and then we could use the like that and then we could use the neural network to learn",
      "that squiggle neural network to learn that squiggle neural network to learn that squiggle as our input represents this output as our input represents this output as our input represents this output for number three for number three for number three after we train the neural network we can after we train the neural network we can after we train the neural network we can provide it with more labeled data like provide it with more labeled data like provide it with more labeled data like this",
      "squiggle and then we can see if it this squiggle and then we can see if it this squiggle and then we can see if it could also correctly resolve that could also correctly resolve that could also correctly resolve that squiggle to squiggle to squiggle to the number six if it gets some of these the number six if it gets some of these the number six if it gets some of these squiggles wrong the the weights and squiggles wrong the the weights and squiggles wrong the the weights and biases here can be",
      "adjusted and then we biases here can be adjusted and then we biases here can be adjusted and then we just try it again just try it again just try it again now how can gradient descent help us now how can gradient descent help us now how can gradient descent help us here well gradient descent is used to here well gradient descent is used to here well gradient descent is used to find the minimum of something called a find the minimum of something called a find the minimum of something called a cost",
      "cost cost function function function so what is so what is so what is a cost function well it's a function a cost function well it's a function a cost function well it's a function that tells us how far off our that tells us how far off our that tells us how far off our predictions are from the actual values predictions are from the actual values predictions are from the actual values so the idea is that we want to minimize so the idea is that we want to minimize so the idea is that we want to",
      "minimize this cost function to get the best this cost function to get the best this cost function to get the best predictions now to do this we take small predictions now to do this we take small predictions now to do this we take small steps in the direction that reduces the steps in the direction that reduces the steps in the direction that reduces the cost function the most if we think about cost function the most if we think about cost function the most if we think about this on a graph we",
      "start here and we this on a graph we start here and we this on a graph we start here and we keep going downhill reducing our cost keep going downhill reducing our cost keep going downhill reducing our cost function as we go function as we go function as we go the size of the steps that we take so the size of the steps that we take so the size of the steps that we take so the size of the steps from here to here the size of the steps from here to here the size of the steps from here to here and to",
      "here that's called The Learning and to here that's called The Learning and to here that's called The Learning rate rate rate let's think about another example let's let's think about another example let's let's think about another example let's consider a neural network but instead of consider a neural network but instead of consider a neural network but instead of dealing with squiggles predicts how much dealing with squiggles predicts how much dealing with squiggles predicts how much a house",
      "will sell for so first we train a house will sell for so first we train a house will sell for so first we train the network on a labeled data set let's the network on a labeled data set let's the network on a labeled data set let's say that data has some information like say that data has some information like say that data has some information like um like the location of a house let's um like the location of a house let's um like the location of a house let's say the size of the house and then",
      "how say the size of the house and then how say the size of the house and then how much it sold for much it sold for much it sold for so with that we can then use our model so with that we can then use our model so with that we can then use our model to train new labeled data so here's a to train new labeled data so here's a to train new labeled data so here's a here's another example we've got a house here's another example we've got a house here's another example we've got a house uh it's",
      "location let's do it by ZIP code uh it's location let's do it by ZIP code uh it's location let's do it by ZIP code 275 from three how big is it 275 from three how big is it 275 from three how big is it uh 3 000 square feet input that into our uh 3 000 square feet input that into our uh 3 000 square feet input that into our neural network so how much does this neural network so how much does this neural network so how much does this house sell for well now our neural house sell for well now our",
      "neural house sell for well now our neural network will make a forecast it says we network will make a forecast it says we network will make a forecast it says we think think think is sold for three hundred thousand is sold for three hundred thousand is sold for three hundred thousand dollars dollars dollars and we compare that and we compare that and we compare that to the forecast of the actual sale price to the forecast of the actual sale price to the forecast of the actual sale price which was",
      "which was which was 000 dollars 000 dollars 000 dollars not a good guess we have a large cost not a good guess we have a large cost not a good guess we have a large cost function weights and biases now need to function weights and biases now need to function weights and biases now need to be adjusted and then the model can try be adjusted and then the model can try be adjusted and then the model can try again and did it do any better over the again and did it do any better over the again and did",
      "it do any better over the entire label data set or did it do worse entire label data set or did it do worse entire label data set or did it do worse that's what gradient descent can help us that's what gradient descent can help us that's what gradient descent can help us with with with now there are three types of gradient now there are three types of gradient now there are three types of gradient descent learning algorithms and let's descent learning algorithms and let's descent learning",
      "algorithms and let's take a look at some of those take a look at some of those take a look at some of those so first of all so first of all so first of all we've got a gradient descent called we've got a gradient descent called we've got a gradient descent called batch batch batch this sums the entries for each point in this sums the entries for each point in this sums the entries for each point in a training set updating the model only a training set updating the model only a training set",
      "updating the model only after all the training examples have after all the training examples have after all the training examples have been evaluated hence the term batch now been evaluated hence the term batch now been evaluated hence the term batch now in terms of how well does this do well in terms of how well does this do well in terms of how well does this do well computationally it is computationally computationally it is computationally computationally it is computationally effective",
      "effective effective you can give this a high rating you can give this a high rating you can give this a high rating because we're doing things in one big because we're doing things in one big because we're doing things in one big batch but what about processing time batch but what about processing time batch but what about processing time well with processing time we can end up well with processing time we can end up well with processing time we can end up with long processing times using batch",
      "with long processing times using batch with long processing times using batch gradient descent because well we've got gradient descent because well we've got gradient descent because well we've got large training data sets and it needs to large training data sets and it needs to large training data sets and it needs to store all of that data in memory and store all of that data in memory and store all of that data in memory and process it process it process it so that's batch another option is so",
      "that's batch another option is so that's batch another option is stochastic stochastic stochastic gradient descent and this evaluates each gradient descent and this evaluates each gradient descent and this evaluates each training example but one at a time training example but one at a time training example but one at a time instead of in a batch since you only instead of in a batch since you only instead of in a batch since you only need to hold one training example need to hold one training",
      "example need to hold one training example they're easy to store in memory and get they're easy to store in memory and get they're easy to store in memory and get individual responses much faster so in individual responses much faster so in individual responses much faster so in terms of speed terms of speed terms of speed that's that's that's fast but in terms of computational fast but in terms of computational fast but in terms of computational efficiency that's lower now there is a happy medium",
      "and that is now there is a happy medium and that is now there is a happy medium and that is called called called mini batch and mini batch gradient mini batch and mini batch gradient mini batch and mini batch gradient descent splits the training data set descent splits the training data set descent splits the training data set into small batch sizes and performs into small batch sizes and performs into small batch sizes and performs updates on each of those batches that is updates on each of",
      "those batches that is updates on each of those batches that is a nice balance of computational a nice balance of computational a nice balance of computational efficiency and of speed now gradient efficiency and of speed now gradient efficiency and of speed now gradient descent does come with its own descent does come with its own descent does come with its own challenges so for example it can challenges so for example it can challenges so for example it can struggle to find the global minimum in",
      "struggle to find the global minimum in struggle to find the global minimum in non-convex problems this was a nice non-convex problems this was a nice non-convex problems this was a nice convex problem with a clearly defined convex problem with a clearly defined convex problem with a clearly defined bottom bottom bottom so when are the slope of the cost so when are the slope of the cost so when are the slope of the cost function is close to zero or it's at function is close to zero or it's at",
      "function is close to zero or it's at zero the model stops learning but if we zero the model stops learning but if we zero the model stops learning but if we don't have this convex model here that don't have this convex model here that don't have this convex model here that we have something like we have something like we have something like this shape that's known as a saddle this shape that's known as a saddle this shape that's known as a saddle point and it can mislead the gradient point and it",
      "can mislead the gradient point and it can mislead the gradient descent because it thinks it's descent because it thinks it's descent because it thinks it's at the bottom at the bottom at the bottom before it really is this is going to before it really is this is going to before it really is this is going to keep going down further keep going down further keep going down further chord a subtle shape because it kind of chord a subtle shape because it kind of chord a subtle shape because it kind of",
      "looks like a horse saddle I guess looks like a horse saddle I guess looks like a horse saddle I guess another challenge is that in deeper another challenge is that in deeper another challenge is that in deeper neural learning networks a gradient neural learning networks a gradient neural learning networks a gradient descent can suffer from vanish descent can suffer from vanish descent can suffer from vanish ingredients or exploding gradients so ingredients or exploding gradients so ingredients or",
      "exploding gradients so Vanishing gradients are when the Vanishing gradients are when the Vanishing gradients are when the gradient is too small and the earlier gradient is too small and the earlier gradient is too small and the earlier layers in the network learn more slowly layers in the network learn more slowly layers in the network learn more slowly than the later layers as we go through than the later layers as we go through than the later layers as we go through this network here this",
      "network here this network here exploding gradients on the other hand exploding gradients on the other hand exploding gradients on the other hand are when the gradient is too large and are when the gradient is too large and are when the gradient is too large and that can create an unstable model but that can create an unstable model but that can create an unstable model but look despite those challenges gradient look despite those challenges gradient look despite those challenges gradient descent",
      "is a powerful optimization descent is a powerful optimization descent is a powerful optimization algorithm and it is commonly used to algorithm and it is commonly used to algorithm and it is commonly used to train machine learning models and neural train machine learning models and neural train machine learning models and neural networks today it's a clever way to get networks today it's a clever way to get networks today it's a clever way to get you back down that mountain safely you back down",
      "that mountain safely you back down that mountain safely if you have any questions please drop us if you have any questions please drop us if you have any questions please drop us a line below and if you want to see more a line below and if you want to see more a line below and if you want to see more videos like this in the future please videos like this in the future please videos like this in the future please like And subscribe thanks for watching"
    ],
    "chunk_count": 35,
    "content_id": "87515a8f-b031-4304-9983-6100db9692a2",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555005"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=lsMQRaeKNDk": {
    "title": "What is a REST API?",
    "url": "https://www.youtube.com/watch?v=lsMQRaeKNDk",
    "description": "Learn more about about APIs → http://ibm.biz/guide-to-apis\nLearn more about REST APIs → http://ibm.biz/rest-apis-guide\nWatch GraphQL vs REST: Which is Better for APIs? → https://ibm.biz/BdMpXN\nCheck out IBM API Connect → https://www.ibm.com/products/api-connect \n\nWhat is a REST API? What are the benefits and how are they fundamental to your cloud application? \n\nIn this lightboard video, Nathan Hekman with IBM Cloud, answers these questions and much more as he visually shows the benefits a company can gain with using REST API.\n\nRead SmartPaper to learn how to unlock the full potential of your APIs → https://ibm.biz/BdMpX6\nSign up for a live demo of API Connect, IBM's API management solution → https://ibm.biz/BdMpXU\nTry IBM API Connect free for 30 days → https://ibm.biz/BdMpX5\n\n#RESTAPI #APIs #IBMCloud",
    "duration": 552,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en What is a REST API? What are the benefits - and howare they fundamental to your cloud application development? I'm Nathan Heckman from IBM Cloud, and I'm going to answer that for you today, but before i do please hit that subscribe button. Let's jump in with an example. Let's say that you work for an ice cream shop and you're trying tobuild a web application to show the flavors of ice cream that are in stock that day and allow theworkers to actually make updates to those flavors. How do you do this? Well, with a REST API. Youhave your web app or web page communicate with a cloud-based server via a REST API. Great! So, let's jump into what exactly a REST API is. Starting out with: what does \"REST\" stand for? So, REST stands for Representational State Transfer. So it's \"REST\". Those words might not mean awhole lot to you but, basically, it's a standardized software architecture style, a specific typeof API that's industry known and used. So, the first thing that you should really know about REST APIs is they're all about communication. So, client with server and vice versa - that's how theytalk, and also you may have heard the term \"restful\". So, \"restful web service\", right, that's aservice that uses REST APIs to communicate. So, what are some of the benefits of REST APIs? First of all, they're a simple and standardized approach to communication. You don't have to worry about how to format your data, or howto format your request each time- it's all standardized and industry used. Secondof all, REST APIs are scalable and stateless. So, as your service grows in complexityyou can easily make modifications and just the fact that they're statelessmeans you don't have to worry about whatdata is in which state and keep track of thatacross client and server. It's truly stateless. Great, and then finally they have high performance in large part due to the fact that they supportcaching. So, that's all all good stuff - as your service gets more complex the performance staysvery high. Great! So, let's go back to our example. So, for the ice cream shop, whatwould the REST API look like? So, you have an endpoint which might looksomething like this: \"icecream.com/api/flavors\" Right? So, let's break that apart abit. So, \"api\" this just signifies that this is the API portion of the endpoint. Right? So, prettystraightforward there. \"Flavors\" is actually what's known as a \"resource\". So, this signifies that we'reworking with the flavors resource in this this REST API. Great! So, in our example, the main buildingblocks, or parts, of the REST API are: the \"request\", that is sent from the client to the server, and the\"response\", that is received back from the the server. So, let's let's break those apart a littlebit. Over here, let's put a big old box for request, and what are the type of things thatyou might want to do with a REST API? What actions or verbs would you want to make whenyou're working with one? You may have heard of \"CRUD\" - what does CRUD stand for? So, CRUD is \"Create, Read, Update, and Delete\". These are some of the main things that youmight want to do when you're communicatingwith your client and server. On a REST API, theequivalent of those are are actually HTTP \"methods\" or \"operations\". So, what's the equivalent of \"create\" in anHTTP method? Well, it's \"post\". And how about \"read\"? So, read is actually called \"get\". Update or replaceis \"put\", ... and delete? Guess what it is: \"delete\". Not too creative there. So, how about a requestitself? What are some of the pieces of it? So, first of all, you might have an operation, that's kind of what we just talked about here with the HTTP methods, you mighthave an endpoint, which is what we just talked about over here - this is your REST API endpoint, and then you might also have parameters or body, which is some data thatyou might send in the request, and we'll see some examplesin a moment, and finally are the headers. So, this is a specialpart of an REST API request  which might have things like an APIkey or some authentication data. Great - and then, in return, from the server, you'llreceive a response, right? So, we'll put a nice box here for response and this is typically inthe form of JSON data. Great, so let's jump into our example, and let's look at a few differentscenarios that might happen with your ice creamshop, right? So, first of all, let's say that youwant to display what's currently in stock, right? So, we want to get the flavors that are in stock. So, what does our rest API request look like? Well, you have a \"get\" as the operation, becauseyou're actually wanting to get those flavors, and then the end point is \"/api/flavors\", and, in response, you'll get an array of those flavor resources, right? So, we see strawberriesin stock, as well as mint chocolate ... yummy! but then, uh oh, mint chocolate is so popular that it actually runs out for the day, and the store is scrambling and they wantto replace that flavor with another one. They choose chocolate.  Which is a greatchoice! So, let's say they want to update the flavors, right? So, you want to updateor replace that mint chocolate with just chocolate. So, what does our request look like? It's a \"put\" operation, so that updates or replaces. The end point is \"/api/flavors/1\" to indicate the the ID of \"1\" you want to replace and then parameter body you actually specify the flavor of chocolate, which is going to be the new flavor that's replaced. And, in response,we see indeed acknowledging that the ID of \"1\" is replaced with a flavor of chocolate. Great! Good news: the store just received a shipment of a brand new experimental flavor called \"restful raspberry\", and you want to actually load this new ice cream up into yourwebsite, right? So, we want to create a new flavor. How do we do that with the REST API? Well, in our operation it's a \"post\" operation, so that actually will create a new flavor, theendpoint is once again \"/api/flavors\", and we included in the body the flavor thatwe want to create, which is \"restful raspberry\", and in response we see ... yep, this new ID of \"2\" wascreated, the flavor is restful raspberry. Great! So, hopefully this clarifies what exactly is a REST API? What are some of the benefits? What's a real world example look like? ... and how are REST APIs fundamental to your cloud application development? Thank you. If you have questionsplease drop us a line below. If you want to see more videos like this in the future please \"like\" and subscribe, - and don't forget: you can grow your skills and earn a badge with IBM Cloud labs, whichare free browser-based interactive Kubernetes labs.",
    "chunks": [
      "Kind: captions Language: en What is a REST API? What are the benefits - and howare they fundamental to your cloud application development? I'm Nathan Heckman from IBM Cloud, and I'm going to answer that for you today, but before i do please hit that subscribe button. Let's jump in with an example. Let's say that you work for an ice cream shop and you're trying tobuild a web application to show the flavors of ice cream that are in stock that day and allow theworkers to actually make updates to",
      "those flavors. How do you do this? Well, with a REST API. Youhave your web app or web page communicate with a cloud-based server via a REST API. Great! So, let's jump into what exactly a REST API is. Starting out with: what does \"REST\" stand for? So, REST stands for Representational State Transfer. So it's \"REST\". Those words might not mean awhole lot to you but, basically, it's a standardized software architecture style, a specific typeof API that's industry known and used. So, the first thing",
      "that you should really know about REST APIs is they're all about communication. So, client with server and vice versa - that's how theytalk, and also you may have heard the term \"restful\". So, \"restful web service\", right, that's aservice that uses REST APIs to communicate. So, what are some of the benefits of REST APIs? First of all, they're a simple and standardized approach to communication. You don't have to worry about how to format your data, or howto format your request each time- it's all",
      "standardized and industry used. Secondof all, REST APIs are scalable and stateless. So, as your service grows in complexityyou can easily make modifications and just the fact that they're statelessmeans you don't have to worry about whatdata is in which state and keep track of thatacross client and server. It's truly stateless. Great, and then finally they have high performance in large part due to the fact that they supportcaching. So, that's all all good stuff - as your service gets more",
      "complex the performance staysvery high. Great! So, let's go back to our example. So, for the ice cream shop, whatwould the REST API look like? So, you have an endpoint which might looksomething like this: \"icecream.com/api/flavors\" Right? So, let's break that apart abit. So, \"api\" this just signifies that this is the API portion of the endpoint. Right? So, prettystraightforward there. \"Flavors\" is actually what's known as a \"resource\". So, this signifies that we'reworking with the flavors",
      "resource in this this REST API. Great! So, in our example, the main buildingblocks, or parts, of the REST API are: the \"request\", that is sent from the client to the server, and the\"response\", that is received back from the the server. So, let's let's break those apart a littlebit. Over here, let's put a big old box for request, and what are the type of things thatyou might want to do with a REST API? What actions or verbs would you want to make whenyou're working with one? You may have heard of",
      "\"CRUD\" - what does CRUD stand for? So, CRUD is \"Create, Read, Update, and Delete\". These are some of the main things that youmight want to do when you're communicatingwith your client and server. On a REST API, theequivalent of those are are actually HTTP \"methods\" or \"operations\". So, what's the equivalent of \"create\" in anHTTP method? Well, it's \"post\". And how about \"read\"? So, read is actually called \"get\". Update or replaceis \"put\", ... and delete? Guess what it is: \"delete\". Not too",
      "creative there. So, how about a requestitself? What are some of the pieces of it? So, first of all, you might have an operation, that's kind of what we just talked about here with the HTTP methods, you mighthave an endpoint, which is what we just talked about over here - this is your REST API endpoint, and then you might also have parameters or body, which is some data thatyou might send in the request, and we'll see some examplesin a moment, and finally are the headers. So, this is a specialpart",
      "of an REST API request which might have things like an APIkey or some authentication data. Great - and then, in return, from the server, you'llreceive a response, right? So, we'll put a nice box here for response and this is typically inthe form of JSON data. Great, so let's jump into our example, and let's look at a few differentscenarios that might happen with your ice creamshop, right? So, first of all, let's say that youwant to display what's currently in stock, right? So, we want to get the",
      "flavors that are in stock. So, what does our rest API request look like? Well, you have a \"get\" as the operation, becauseyou're actually wanting to get those flavors, and then the end point is \"/api/flavors\", and, in response, you'll get an array of those flavor resources, right? So, we see strawberriesin stock, as well as mint chocolate ... yummy! but then, uh oh, mint chocolate is so popular that it actually runs out for the day, and the store is scrambling and they wantto replace that flavor",
      "with another one. They choose chocolate. Which is a greatchoice! So, let's say they want to update the flavors, right? So, you want to updateor replace that mint chocolate with just chocolate. So, what does our request look like? It's a \"put\" operation, so that updates or replaces. The end point is \"/api/flavors/1\" to indicate the the ID of \"1\" you want to replace and then parameter body you actually specify the flavor of chocolate, which is going to be the new flavor that's replaced. And, in",
      "response,we see indeed acknowledging that the ID of \"1\" is replaced with a flavor of chocolate. Great! Good news: the store just received a shipment of a brand new experimental flavor called \"restful raspberry\", and you want to actually load this new ice cream up into yourwebsite, right? So, we want to create a new flavor. How do we do that with the REST API? Well, in our operation it's a \"post\" operation, so that actually will create a new flavor, theendpoint is once again \"/api/flavors\", and we",
      "included in the body the flavor thatwe want to create, which is \"restful raspberry\", and in response we see ... yep, this new ID of \"2\" wascreated, the flavor is restful raspberry. Great! So, hopefully this clarifies what exactly is a REST API? What are some of the benefits? What's a real world example look like? ... and how are REST APIs fundamental to your cloud application development? Thank you. If you have questionsplease drop us a line below. If you want to see more videos like this in the",
      "future please \"like\" and subscribe, - and don't forget: you can grow your skills and earn a badge with IBM Cloud labs, whichare free browser-based interactive Kubernetes labs."
    ],
    "chunk_count": 14,
    "content_id": "9852a9b9-60e4-4211-96f7-554936a52b17",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555008"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=EqNe55IzjAw": {
    "title": "Cybersecurity Architecture: Fundamentals of Confidentiality, Integrity, and Availability",
    "url": "https://www.youtube.com/watch?v=EqNe55IzjAw",
    "description": "IBM Security QRadar EDR : https://ibm.biz/BdyRmv\nFull Playlist: https://www.youtube.com/playlist?list=PLOspHqNVtKADkWLFt9OcziQF7EatuANSY\nIBM Security X-Force Threat Intelligence Index 2024: https://ibm.biz/BdyRmm\n\nIn this next installment of the Cybersecurity Architecture series, Jeff \"the Security guy\" covers the three fundamentals that must be part of your security checklist: Confidentiality, Integrity, and Availability, also known as the \"CIA Triad\".\n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-up\n\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n#AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume\n\nChapters\n00:00 Confidentiality\n03:51 Integrity\n06:50 Availability",
    "duration": 753,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Welcome back to our Cybersecurity Architecture Series. In the last video, I talked about five security principles you should always follow and one you should never follow. In today's video, we're going to talk about the CIA. No, not the spy guy: Confidentiality, Integrity and Availability. So let's get started with the first of these, Confidentiality. So, confidentiality, we basically accomplish with two main types of technologies: One is access control, which consists of authentication and authorization. So authentication is answering the question, \"Who are you?\" In authorization, \"Are you allowed to do this or not?\" And let's take an example of how this might work. So let's say we've got a guy here who is an authorized user and he is going to try to come into access something down here. Could be some IoT device, could be a database, could be a server. He's going to access those things. He comes in and we're going to check him to see if he is, in fact, who he claims to be. We're going to do the authentication step. And in doing that, we might use a technology like multifactor authentication-- some way where he proves who he is by something he knows, something he has, something he is --and use them in combination. And if he's able to do that, then we allow him in. And another thing we might add to this is a role-based access control where I look and say, just because I know who you are doesn't necessarily mean I know what you're allowed to do. So I need to check now your privileges and see if they match what it is that you're asking for. If we pass both of these checks-- you're who you claim to be and you have the privileges --then I'm going to allow you through. So that's the positive case. In the negative case, here we have an unauthorized user who comes in and they try to authenticate to the system. And let's say, for instance, they're not able to; they don't have the right credentials. So we block them. Or, they come into the system and they're able to authenticate, but they don't have the privileges and therefore they're not allowed access any further. So that's how we're basically controlling confidentiality. Only the authorized user can see this, and we're using access control capabilities in order to enforce that. Now, what's another piece that we can do here? Encryption is the other component that is involved in ensuring confidentiality. Let's take an example here. Here we have a guy who is going to send a message to an authorized user. And we want to make sure that the person that is not authorized cannot read the message. How do we do that? Well, he takes his message and he encrypts it. So he encrypts it with a key and a cryptographic key we've shown here to look like an actual house key, but really, it's a string of bits and he's going to encrypt his message with that. The message then is going to go in an encrypted envelope, if you think of it that way, it's obscured. Somebody who is observing from the outside won't be able to read what it is, it looks scrambled. And then the message comes over here. This guy, who uses the very same key, because we call this symmetric encryption, because we're using the same key on both sides. In other words, it's a pre-shared key. Both of them have that knowledge in advance. How they get there is a whole other matter. But to keep this example simple we'll assume that they both know the key. He knows the key, so he's able to decrypt the message and therefore he can read it. So we get success. Now, this other guy here, however, does not have the key and therefore all he gets is an encrypted message, which he can't read. So these are two main things then that we're doing. We're using access control and encryption as ways to ensure confidentiality. Okay, we just covered Confidentiality. Now we're going to move on to cover Integrity. Integrity is the quality that says a message is true to itself. A transaction is true to itself. If it gets modified, then we can detect it. And if it's detected, then we can know not to trust that and we can take the appropriate countermeasures. So let's take a look at a couple of examples of this. Let's say we've got a good guy here and he goes on to a system and he logs in. Well, we log a record in the syslog to indicate that that occurred. Then he goes and does some transaction, and then maybe he logs off. So we're logging those activities as they occur. Now, let's say there's another guy here who-- the bad guy --he comes in and logs in and then he makes a copy of the database and exfiltrate it. Then he says, \"You know, I don't want anybody to see that. So what I'm going to do is go back here, elevate my privileged level to superuser, and I'm going to delete these log records so nobody sees what happened.\" Well, that's a big problem. What we need are technologies that allow us to know that this syslog is no longer trustworthy, that someone has tampered with it. And those technologies are these things right here. They are cryptographic functions-- digital signatures and message authentication codes that are used as ways to to tell if, when I compare one set of records to another, that there's been a change. So this is the way we can detect that and then take the appropriate countermeasures. Another example. Let's take a look at a blockchain, which is a distributed ledger that everyone would have access to. And as a result, we can all verify whether the results and the information in it is true or not. Here's this same good guy, and he has appended to the blockchain a few different records and done things like this. And in fact, in this middle record, let's say he's putting a transaction where he says, \"I want to order 100 widgets.\" And there we see that. Now what we want is for this thing to be immutable, for it to be unchangeable. You can add new entries, but you can't change the ones that are on and you can't delete ones that are on there. Let's say a bad guy wants to violate that. So his intention is to come along and say, \"Let's have some fun here and make this 400-- no more fun --400,000 widgets\" and really mess with this guy. That's what he wants to do. He may also want to come along and say, \"You know what? I'm just really like to just get rid of that one entirely.\" So those are the things that we are going to not allow to happen. And how do we keep those from not happening? Again, we're going to use these kinds of technologies, these cryptographic technologies that allow us to see that a record in either of these cases, if someone attempts to modify that, we can see that attempt and we can block it. Okay, now we've covered Confidentiality and Integrity. Let's do the last part of the triangle, Availability. Availability means that the system should be available, the resources should be available to authorized users--that they can get access when they need it. Well, let's take an example of what this would look like. Let's say we've got an authorized user here and he comes in and hits a web server, looks up his transaction balance and gets the results back. That's what we want to see occur. Well, there's always going to be somebody who's going to try to mess with this. And so we've got a bad guy and he's going to come in here and send a transaction and another and another and another. And he's just going to be flooding this system with all of these transaction requests, faster than the system can respond to them. And if it can't keep up, we end up with what's known as a \"denial of service\" because it now can't service other legitimate users for all the illegitimate traffic that's come in. So that's a basic denial of service case. How about a more complex case where we amplify the effect of one user and therefore have an even more devastating attack? Well, in this case, let's say this guy takes over control of his system. So this user is unsuspecting--Ignorance is bliss, he's happy as can be. Has no idea that this guy is controlling his system remotely. And he takes over a bunch of these systems, in fact. Now, all of these are under his control, and at any point he can send the command to marshal all of these systems and have them do the same thing. All of them now are going to start flooding this web server with traffic. And this thing then goes down even faster because of the the multiplier-- the force multiplier --that's been added in this case. And that's something we call a \"distributed denial of service\" attack. So it's been distributed across a number of different attackers. Now, in this case, unsuspecting. We call this thing a botnet because they're sitting out there under his control. Now, there's a lot of different variations on this. I just gave you the simple ones where it's just overwhelming amounts of traffic. In some cases we use other techniques. Like one of the original of these was called a SYN flood. And in a SYN flood, what occurs is we have-- in a normal TCP session setup, we have what's known as a three-way handshake. What occurs is you have someone who sends a SYN message. They get back from the server an acknowledgment (ACK). And then they're supposed to respond with a SYNACK. That's the three-way handshake. In between these two, the server is going to reserve some resources for that session. So it's sort of like knocking on your door, and then you go to the door, and you wait for someone to be there-- to come in to the door. If someone knocks on the door, you open the door and then you wait, and you wait forever, then there's eventually going to be no more doors and all of these things get used up. That's what happens in a SYN flood case. Someone in this case, the bad guy, sends the SYN. And so he sends a SYN down here, and when that comes in, this guy is going to reserve a resource for him to come in and use--a session. Then he sends back the acknowledgment and then this guy just goes quiet, just goes dark, doesn't answer. In fact, what he does is he starts another one, another SYN message. He gets an acknowledgment back, this guy holds a resource for him. And again, no answer. He does it again. Starts another-- ring, the doorbell --we reserve resources and send the acknowledgment. And again, he doesn't respond. Now what happens? We're out of resources. Nobody else, legitimate or otherwise, can get into this system. So obviously, the way we would have to guard against something like this is maybe put in a timeout that says, I'm only going to hold this for so long. I'm only going to stand at the front door so long waiting for you to come in. And after that, you know, I'm closing the door and letting somebody else try to come in. So that's an example of a SYN flood. There's a lot of other examples of denial of service attacks, where we do a reflection attacks, where we send information to someone else and then spoof the source address so that it comes back to where our intended target is. There are, in addition to reflection attacks, there are other types of force multipliers that we can do in these cases. But what we're trying to do is guard against these cases. We need to make sure that the system is up and available to the authorized users when they need it. So, if I'm working on an IT project, one of the things I want to be able to do is make sure that I've covered all the bases. And in covering all those bases, this is the checklist you should use. Have I met the confidentiality requirements of the project? Is the sensitive data only available to those who are authorized to see it? Is this system true to itself? Do I have integrity checking so that if someone modifies it or tampers with it, I can be aware of that and know to adjust my trust level? And do I have the system available all the time that it's supposed to be available? This is the CIA triad. If I've covered all of these three bases, then it's job done. Thanks for watching. Before you leave, don't forget to hit subscribe. That way you won't miss the next installment of the Cybersecurity Architecture series.",
    "chunks": [
      "Kind: captions Language: en Welcome back to our Cybersecurity Architecture Series. In the last video, I talked about five security principles you should always follow and one you should never follow. In today's video, we're going to talk about the CIA. No, not the spy guy: Confidentiality, Integrity and Availability. So let's get started with the first of these, Confidentiality. So, confidentiality, we basically accomplish with two main types of technologies: One is access control, which",
      "consists of authentication and authorization. So authentication is answering the question, \"Who are you?\" In authorization, \"Are you allowed to do this or not?\" And let's take an example of how this might work. So let's say we've got a guy here who is an authorized user and he is going to try to come into access something down here. Could be some IoT device, could be a database, could be a server. He's going to access those things. He comes in and we're going to check him to see if he is, in",
      "fact, who he claims to be. We're going to do the authentication step. And in doing that, we might use a technology like multifactor authentication-- some way where he proves who he is by something he knows, something he has, something he is --and use them in combination. And if he's able to do that, then we allow him in. And another thing we might add to this is a role-based access control where I look and say, just because I know who you are doesn't necessarily mean I know what you're allowed to",
      "do. So I need to check now your privileges and see if they match what it is that you're asking for. If we pass both of these checks-- you're who you claim to be and you have the privileges --then I'm going to allow you through. So that's the positive case. In the negative case, here we have an unauthorized user who comes in and they try to authenticate to the system. And let's say, for instance, they're not able to; they don't have the right credentials. So we block them. Or, they come into the",
      "system and they're able to authenticate, but they don't have the privileges and therefore they're not allowed access any further. So that's how we're basically controlling confidentiality. Only the authorized user can see this, and we're using access control capabilities in order to enforce that. Now, what's another piece that we can do here? Encryption is the other component that is involved in ensuring confidentiality. Let's take an example here. Here we have a guy who is going to send a",
      "message to an authorized user. And we want to make sure that the person that is not authorized cannot read the message. How do we do that? Well, he takes his message and he encrypts it. So he encrypts it with a key and a cryptographic key we've shown here to look like an actual house key, but really, it's a string of bits and he's going to encrypt his message with that. The message then is going to go in an encrypted envelope, if you think of it that way, it's obscured. Somebody who is observing",
      "from the outside won't be able to read what it is, it looks scrambled. And then the message comes over here. This guy, who uses the very same key, because we call this symmetric encryption, because we're using the same key on both sides. In other words, it's a pre-shared key. Both of them have that knowledge in advance. How they get there is a whole other matter. But to keep this example simple we'll assume that they both know the key. He knows the key, so he's able to decrypt the message and",
      "therefore he can read it. So we get success. Now, this other guy here, however, does not have the key and therefore all he gets is an encrypted message, which he can't read. So these are two main things then that we're doing. We're using access control and encryption as ways to ensure confidentiality. Okay, we just covered Confidentiality. Now we're going to move on to cover Integrity. Integrity is the quality that says a message is true to itself. A transaction is true to itself. If it gets",
      "modified, then we can detect it. And if it's detected, then we can know not to trust that and we can take the appropriate countermeasures. So let's take a look at a couple of examples of this. Let's say we've got a good guy here and he goes on to a system and he logs in. Well, we log a record in the syslog to indicate that that occurred. Then he goes and does some transaction, and then maybe he logs off. So we're logging those activities as they occur. Now, let's say there's another guy here",
      "who-- the bad guy --he comes in and logs in and then he makes a copy of the database and exfiltrate it. Then he says, \"You know, I don't want anybody to see that. So what I'm going to do is go back here, elevate my privileged level to superuser, and I'm going to delete these log records so nobody sees what happened.\" Well, that's a big problem. What we need are technologies that allow us to know that this syslog is no longer trustworthy, that someone has tampered with it. And those technologies",
      "are these things right here. They are cryptographic functions-- digital signatures and message authentication codes that are used as ways to to tell if, when I compare one set of records to another, that there's been a change. So this is the way we can detect that and then take the appropriate countermeasures. Another example. Let's take a look at a blockchain, which is a distributed ledger that everyone would have access to. And as a result, we can all verify whether the results and the",
      "information in it is true or not. Here's this same good guy, and he has appended to the blockchain a few different records and done things like this. And in fact, in this middle record, let's say he's putting a transaction where he says, \"I want to order 100 widgets.\" And there we see that. Now what we want is for this thing to be immutable, for it to be unchangeable. You can add new entries, but you can't change the ones that are on and you can't delete ones that are on there. Let's say a bad",
      "guy wants to violate that. So his intention is to come along and say, \"Let's have some fun here and make this 400-- no more fun --400,000 widgets\" and really mess with this guy. That's what he wants to do. He may also want to come along and say, \"You know what? I'm just really like to just get rid of that one entirely.\" So those are the things that we are going to not allow to happen. And how do we keep those from not happening? Again, we're going to use these kinds of technologies, these",
      "cryptographic technologies that allow us to see that a record in either of these cases, if someone attempts to modify that, we can see that attempt and we can block it. Okay, now we've covered Confidentiality and Integrity. Let's do the last part of the triangle, Availability. Availability means that the system should be available, the resources should be available to authorized users--that they can get access when they need it. Well, let's take an example of what this would look like. Let's say",
      "we've got an authorized user here and he comes in and hits a web server, looks up his transaction balance and gets the results back. That's what we want to see occur. Well, there's always going to be somebody who's going to try to mess with this. And so we've got a bad guy and he's going to come in here and send a transaction and another and another and another. And he's just going to be flooding this system with all of these transaction requests, faster than the system can respond to them. And",
      "if it can't keep up, we end up with what's known as a \"denial of service\" because it now can't service other legitimate users for all the illegitimate traffic that's come in. So that's a basic denial of service case. How about a more complex case where we amplify the effect of one user and therefore have an even more devastating attack? Well, in this case, let's say this guy takes over control of his system. So this user is unsuspecting--Ignorance is bliss, he's happy as can be. Has no idea that",
      "this guy is controlling his system remotely. And he takes over a bunch of these systems, in fact. Now, all of these are under his control, and at any point he can send the command to marshal all of these systems and have them do the same thing. All of them now are going to start flooding this web server with traffic. And this thing then goes down even faster because of the the multiplier-- the force multiplier --that's been added in this case. And that's something we call a \"distributed denial of",
      "service\" attack. So it's been distributed across a number of different attackers. Now, in this case, unsuspecting. We call this thing a botnet because they're sitting out there under his control. Now, there's a lot of different variations on this. I just gave you the simple ones where it's just overwhelming amounts of traffic. In some cases we use other techniques. Like one of the original of these was called a SYN flood. And in a SYN flood, what occurs is we have-- in a normal TCP session setup,",
      "we have what's known as a three-way handshake. What occurs is you have someone who sends a SYN message. They get back from the server an acknowledgment (ACK). And then they're supposed to respond with a SYNACK. That's the three-way handshake. In between these two, the server is going to reserve some resources for that session. So it's sort of like knocking on your door, and then you go to the door, and you wait for someone to be there-- to come in to the door. If someone knocks on the door, you",
      "open the door and then you wait, and you wait forever, then there's eventually going to be no more doors and all of these things get used up. That's what happens in a SYN flood case. Someone in this case, the bad guy, sends the SYN. And so he sends a SYN down here, and when that comes in, this guy is going to reserve a resource for him to come in and use--a session. Then he sends back the acknowledgment and then this guy just goes quiet, just goes dark, doesn't answer. In fact, what he does is he",
      "starts another one, another SYN message. He gets an acknowledgment back, this guy holds a resource for him. And again, no answer. He does it again. Starts another-- ring, the doorbell --we reserve resources and send the acknowledgment. And again, he doesn't respond. Now what happens? We're out of resources. Nobody else, legitimate or otherwise, can get into this system. So obviously, the way we would have to guard against something like this is maybe put in a timeout that says, I'm only going to",
      "hold this for so long. I'm only going to stand at the front door so long waiting for you to come in. And after that, you know, I'm closing the door and letting somebody else try to come in. So that's an example of a SYN flood. There's a lot of other examples of denial of service attacks, where we do a reflection attacks, where we send information to someone else and then spoof the source address so that it comes back to where our intended target is. There are, in addition to reflection attacks,",
      "there are other types of force multipliers that we can do in these cases. But what we're trying to do is guard against these cases. We need to make sure that the system is up and available to the authorized users when they need it. So, if I'm working on an IT project, one of the things I want to be able to do is make sure that I've covered all the bases. And in covering all those bases, this is the checklist you should use. Have I met the confidentiality requirements of the project? Is the",
      "sensitive data only available to those who are authorized to see it? Is this system true to itself? Do I have integrity checking so that if someone modifies it or tampers with it, I can be aware of that and know to adjust my trust level? And do I have the system available all the time that it's supposed to be available? This is the CIA triad. If I've covered all of these three bases, then it's job done. Thanks for watching. Before you leave, don't forget to hit subscribe. That way you won't miss",
      "the next installment of the Cybersecurity Architecture series."
    ],
    "chunk_count": 25,
    "content_id": "5bee0d82-69d2-4581-8787-af88feb58350",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555011"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=E9pHJRRfAhw": {
    "title": "Cybersecurity Architecture: Roles and Tools",
    "url": "https://www.youtube.com/watch?v=E9pHJRRfAhw",
    "description": "IBM Security QRadar EDR :  https://ibm.biz/Bdys93\n\nIBM Security X-Force Threat Intelligence Index 2023:  https://ibm.biz/Bdys9h\n\nLike a building architect, the cybersecurity architect has to think of the \"big picture\". But there's limits to this analogy, since the cybersecurity architect focuses as much on how things fail as how things work. In this video, Jeff \"the security guy\" delves into the role,  mindset, tools, and domains  they have to adopt in developing a secure solution.\n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-up\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n\n#AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume",
    "duration": 847,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en in the previous two videos on cyber in the previous two videos on cyber in the previous two videos on cyber security architecture fundamentals we security architecture fundamentals we security architecture fundamentals we discussed principles that you should discussed principles that you should discussed principles that you should follow essential security principles follow essential security principles follow essential security principles in the next video we discussed the CIA in the next video we discussed the CIA in the next video we discussed the CIA Triad where you could basically use this Triad where you could basically use this Triad where you could basically use this as a checklist to know that you've done as a checklist to know that you've done as a checklist to know that you've done a cyber security architecture correctly a cyber security architecture correctly a cyber security architecture correctly in this video we're going to focus on in this video we're going to focus on in this video we're going to focus on the cyber security architect in the cyber security architect in the cyber security architect in particular their role the mindset that particular their role the mindset that particular their role the mindset that they have to adopt in developing a they have to adopt in developing a they have to adopt in developing a secure solution the tools that they use secure solution the tools that they use secure solution the tools that they use tools of the trade and the domains that tools of the trade and the domains that tools of the trade and the domains that they have to operate in all right we're they have to operate in all right we're they have to operate in all right we're going to start off with the role in the going to start off with the role in the going to start off with the role in the mindset where this all begins is with mindset where this all begins is with mindset where this all begins is with stakeholders stakeholders stakeholders these are the people that have a vested these are the people that have a vested these are the people that have a vested interest in getting this solution right interest in getting this solution right interest in getting this solution right so we're going to take a look at two so we're going to take a look at two so we're going to take a look at two examples here of an architect who is examples here of an architect who is examples here of an architect who is working on a building and an I.T working on a building and an I.T working on a building and an I.T architect who's working on building an architect who's working on building an architect who's working on building an I.T system in both cases we're going to I.T system in both cases we're going to I.T system in both cases we're going to start with stakeholders and we're going start with stakeholders and we're going start with stakeholders and we're going to take their inputs into the architect to take their inputs into the architect to take their inputs into the architect the architect is going to wonder okay the architect is going to wonder okay the architect is going to wonder okay we're building a building but what kind we're building a building but what kind we're building a building but what kind of building is this going to be is it of building is this going to be is it of building is this going to be is it going to be a business is it going to be going to be a business is it going to be going to be a business is it going to be a home well in this case we're told it's a home well in this case we're told it's a home well in this case we're told it's going to be a home it's going to be a going to be a home it's going to be a going to be a home it's going to be a multi-family dwelling so it's a multi-family dwelling so it's a multi-family dwelling so it's a townhouse maybe for instance so we have townhouse maybe for instance so we have townhouse maybe for instance so we have an idea already what it's going to be an idea already what it's going to be an idea already what it's going to be what sort of size we want what kind of what sort of size we want what kind of what sort of size we want what kind of price range we want it to be in those price range we want it to be in those price range we want it to be in those are the things the stakeholders are are the things the stakeholders are are the things the stakeholders are giving the architect the architect is giving the architect the architect is giving the architect the architect is going to take that and develop a going to take that and develop a going to take that and develop a blueprint that blueprint then becomes blueprint that blueprint then becomes blueprint that blueprint then becomes the plan that the contractors come along the plan that the contractors come along the plan that the contractors come along and Implement so we've got contractors and Implement so we've got contractors and Implement so we've got contractors who are plumbers and Carpenter tours and who are plumbers and Carpenter tours and who are plumbers and Carpenter tours and things of that sort they're going to be things of that sort they're going to be things of that sort they're going to be the ones that do the actual the ones that do the actual the ones that do the actual implementation if the architect shows up implementation if the architect shows up implementation if the architect shows up on the job site with a hammer in hand on the job site with a hammer in hand on the job site with a hammer in hand you might be in trouble because that's you might be in trouble because that's you might be in trouble because that's not their area of expertise you want not their area of expertise you want not their area of expertise you want these people that are experts in doing these people that are experts in doing these people that are experts in doing and these guys who are experts in and these guys who are experts in and these guys who are experts in planning and coming up with the big planning and coming up with the big planning and coming up with the big Ideas so that's a little bit of analogy Ideas so that's a little bit of analogy Ideas so that's a little bit of analogy now an architect might say this is what now an architect might say this is what now an architect might say this is what I generally want this thing to look like I generally want this thing to look like I generally want this thing to look like but we need to take into account some but we need to take into account some but we need to take into account some other things after I've kind of come up other things after I've kind of come up other things after I've kind of come up with the basic sketch of what this is I with the basic sketch of what this is I with the basic sketch of what this is I need to think about Safety and Security need to think about Safety and Security need to think about Safety and Security as well with this building so for as well with this building so for as well with this building so for instance I want locks on the doors of instance I want locks on the doors of instance I want locks on the doors of course I don't want just anybody to be course I don't want just anybody to be course I don't want just anybody to be able to to walk in I might put security able to to walk in I might put security able to to walk in I might put security cameras uh in each of the the units at cameras uh in each of the the units at cameras uh in each of the the units at least on the outside maybe on the inside least on the outside maybe on the inside least on the outside maybe on the inside so that again I have an ability to so that again I have an ability to so that again I have an ability to monitor maybe even alarm systems I might monitor maybe even alarm systems I might monitor maybe even alarm systems I might be concerned about fire in one of the be concerned about fire in one of the be concerned about fire in one of the units so I put a smoke detector on the units so I put a smoke detector on the units so I put a smoke detector on the ceiling in each one of these so that we ceiling in each one of these so that we ceiling in each one of these so that we can detect that and then if I actually can detect that and then if I actually can detect that and then if I actually do have a fire well I'd like to have do have a fire well I'd like to have do have a fire well I'd like to have something that we call a firewall that something that we call a firewall that something that we call a firewall that slows the spread of fire from one unit slows the spread of fire from one unit slows the spread of fire from one unit to the next doesn't prevent it to the next doesn't prevent it to the next doesn't prevent it completely but at least it keeps it from completely but at least it keeps it from completely but at least it keeps it from spreading really really fast so these spreading really really fast so these spreading really really fast so these are kind of mitigations things that we are kind of mitigations things that we are kind of mitigations things that we add on to the architecture to make it add on to the architecture to make it add on to the architecture to make it more safe to make it more secure and the more safe to make it more secure and the more safe to make it more secure and the architect dreams those up and the architect dreams those up and the architect dreams those up and the contractors Implement and put those contractors Implement and put those contractors Implement and put those things together now let's take a look at things together now let's take a look at things together now let's take a look at an I.T example of the same sort of thing an I.T example of the same sort of thing an I.T example of the same sort of thing here once again here once again here once again we're going to start with stakeholders we're going to start with stakeholders we're going to start with stakeholders and they're going to work with the and they're going to work with the and they're going to work with the architect the architect instead of architect the architect instead of architect the architect instead of coming up with a blueprint is going to coming up with a blueprint is going to coming up with a blueprint is going to come up with an analogy to that which is come up with an analogy to that which is come up with an analogy to that which is going to be some type of reference going to be some type of reference going to be some type of reference architecture or some type of architecture or some type of architecture or some type of architecture overview diagram or architecture overview diagram or architecture overview diagram or diagrams that show the interrelations of diagrams that show the interrelations of diagrams that show the interrelations of the high level components of the system the high level components of the system the high level components of the system that then is going to get translated that then is going to get translated that then is going to get translated into an actual it architecture so here into an actual it architecture so here into an actual it architecture so here we've got in particular a user who is we've got in particular a user who is we've got in particular a user who is going to use a workstation maybe a going to use a workstation maybe a going to use a workstation maybe a mobile device or a desktop device mobile device or a desktop device mobile device or a desktop device they're going to come across a network they're going to come across a network they're going to come across a network to hit a web server that's going to hit to hit a web server that's going to hit to hit a web server that's going to hit an app server which is going to hit a an app server which is going to hit a an app server which is going to hit a database and we're going to get their database and we're going to get their database and we're going to get their data this is a very simple type of data this is a very simple type of data this is a very simple type of architecture architecture architecture now the the architect is then going to now the the architect is then going to now the the architect is then going to ask the engineers the architect has been ask the engineers the architect has been ask the engineers the architect has been doing their work basically from a doing their work basically from a doing their work basically from a whiteboard think of it this way whiteboard think of it this way whiteboard think of it this way Architects whiteboard Engineers keyboard Architects whiteboard Engineers keyboard Architects whiteboard Engineers keyboard This is where they're going to be doing This is where they're going to be doing This is where they're going to be doing their work as they start implementing their work as they start implementing their work as they start implementing this system this system this system this architect now also has to consider this architect now also has to consider this architect now also has to consider what might be some failure cases this is what might be some failure cases this is what might be some failure cases this is the difference between a sort of normal the difference between a sort of normal the difference between a sort of normal I.T architect and a cyber security I.T architect and a cyber security I.T architect and a cyber security architect the normal architect thinks architect the normal architect thinks architect the normal architect thinks about how a system will work about how a system will work about how a system will work the cyber security architect thinks the cyber security architect thinks the cyber security architect thinks about how it will fail now the cyber about how it will fail now the cyber about how it will fail now the cyber security architect has to First security architect has to First security architect has to First understand how the system is going to understand how the system is going to understand how the system is going to work or they don't know how it might work or they don't know how it might work or they don't know how it might fail so they have to have that level of fail so they have to have that level of fail so they have to have that level of understanding then they have to add on understanding then they have to add on understanding then they have to add on to it what are the possible things that to it what are the possible things that to it what are the possible things that could go wrong so let's ask what could could go wrong so let's ask what could could go wrong so let's ask what could go wrong with this user well it could be go wrong with this user well it could be go wrong with this user well it could be someone stole their password their someone stole their password their someone stole their password their credentials so it's not this user credentials so it's not this user credentials so it's not this user anymore so what do I need well I'm going anymore so what do I need well I'm going anymore so what do I need well I'm going to put in multi-factor authentication a to put in multi-factor authentication a to put in multi-factor authentication a mitigation a way to check and compensate mitigation a way to check and compensate mitigation a way to check and compensate for that particular risk for that particular risk for that particular risk what if we've got on this workstation a what if we've got on this workstation a what if we've got on this workstation a virus or if it's a mobile device maybe virus or if it's a mobile device maybe virus or if it's a mobile device maybe it's been jailbroken well if it's mobile it's been jailbroken well if it's mobile it's been jailbroken well if it's mobile device I'll add mobile device management device I'll add mobile device management device I'll add mobile device management software detect check for that if it's software detect check for that if it's software detect check for that if it's another type of device endpoint another type of device endpoint another type of device endpoint detection and response capabilities or detection and response capabilities or detection and response capabilities or antivirus capabilities to check there antivirus capabilities to check there antivirus capabilities to check there and we continue across this in the case and we continue across this in the case and we continue across this in the case of the network well just like over here of the network well just like over here of the network well just like over here on this building we added firewalls in on this building we added firewalls in on this building we added firewalls in order to keep the spread of a fire from order to keep the spread of a fire from order to keep the spread of a fire from one unit from immediately spreading to one unit from immediately spreading to one unit from immediately spreading to another and providing a level of another and providing a level of another and providing a level of protective isolation that's what we do protective isolation that's what we do protective isolation that's what we do with network firewalls that's where we with network firewalls that's where we with network firewalls that's where we got that term so I'm going to add got that term so I'm going to add got that term so I'm going to add Network firewalls here to slow the Network firewalls here to slow the Network firewalls here to slow the spread of contagion or attack across spread of contagion or attack across spread of contagion or attack across this infrastructure and then ultimately this infrastructure and then ultimately this infrastructure and then ultimately over here I'm going to encrypt the data over here I'm going to encrypt the data over here I'm going to encrypt the data that's in the database and I'm going to that's in the database and I'm going to that's in the database and I'm going to ask this it engineer whoever it is and ask this it engineer whoever it is and ask this it engineer whoever it is and by the way we'll have different by the way we'll have different by the way we'll have different Engineers that are specialized in each Engineers that are specialized in each Engineers that are specialized in each of these air areas so I might have a of these air areas so I might have a of these air areas so I might have a database administrator that does the database administrator that does the database administrator that does the database encryption a network database encryption a network database encryption a network administrator that implements the administrator that implements the administrator that implements the firewalls someone else who does the firewalls someone else who does the firewalls someone else who does the desktop someone else who does the desktop someone else who does the desktop someone else who does the identity and access management identity and access management identity and access management capabilities so all of these Engineers capabilities so all of these Engineers capabilities so all of these Engineers are analogous to the different are analogous to the different are analogous to the different contractors and the architect in both of contractors and the architect in both of contractors and the architect in both of these cases is coming up with the big these cases is coming up with the big these cases is coming up with the big picture the big plans so again think if picture the big plans so again think if picture the big plans so again think if you're thinking of a cyber security you're thinking of a cyber security you're thinking of a cyber security architect think whiteboard rather than architect think whiteboard rather than architect think whiteboard rather than keyboard and also think how will the keyboard and also think how will the keyboard and also think how will the system fail and what do I need to do to system fail and what do I need to do to system fail and what do I need to do to prevent that prevent that prevent that okay now we've covered the role and okay now we've covered the role and okay now we've covered the role and mindset of the cyber security architect mindset of the cyber security architect mindset of the cyber security architect now let's talk about now let's talk about now let's talk about the tools of the trade well it turns out the tools of the trade well it turns out the tools of the trade well it turns out that in the it architect World there are that in the it architect World there are that in the it architect World there are certain common diagrams that Architects certain common diagrams that Architects certain common diagrams that Architects use there's a business context diagram a use there's a business context diagram a use there's a business context diagram a system context diagram and an system context diagram and an system context diagram and an architecture overview diagram are just architecture overview diagram are just architecture overview diagram are just three examples that I think are three examples that I think are three examples that I think are particularly important so for instance particularly important so for instance particularly important so for instance we'll talk with a business context we'll talk with a business context we'll talk with a business context diagram here we're trying to show diagram here we're trying to show diagram here we're trying to show relationships among the different relationships among the different relationships among the different entities in the system so an example entities in the system so an example entities in the system so an example here we've got a builder we've got a here we've got a builder we've got a here we've got a builder we've got a marketing team we've got Tradesmen who marketing team we've got Tradesmen who marketing team we've got Tradesmen who are going to build the building and then are going to build the building and then are going to build the building and then a buyer and so we're showing the inner a buyer and so we're showing the inner a buyer and so we're showing the inner relationships amongst those various relationships amongst those various relationships amongst those various entities it's a very high level line of entities it's a very high level line of entities it's a very high level line of business sort of view business sort of view business sort of view in the next one the system context in the next one the system context in the next one the system context diagram we're going to take that and diagram we're going to take that and diagram we're going to take that and decompose it further into what it would decompose it further into what it would decompose it further into what it would look like in a system now this is just look like in a system now this is just look like in a system now this is just one aspect this doesn't show all of them one aspect this doesn't show all of them one aspect this doesn't show all of them by any means but here we have a project by any means but here we have a project by any means but here we have a project management system there's a finance management system there's a finance management system there's a finance system that's trying to oversee and make system that's trying to oversee and make system that's trying to oversee and make sure we can afford to build this thing sure we can afford to build this thing sure we can afford to build this thing the way we need to and on budget the way we need to and on budget the way we need to and on budget blueprints that we're going to call in blueprints that we're going to call in blueprints that we're going to call in and do the building with a permitting and do the building with a permitting and do the building with a permitting system that we need to go off and get system that we need to go off and get system that we need to go off and get those and then a graphical user those and then a graphical user those and then a graphical user interface that interfaces to all of it interface that interfaces to all of it interface that interfaces to all of it that's just a very simple example of of that's just a very simple example of of that's just a very simple example of of how the it system that supports this how the it system that supports this how the it system that supports this business model might look then we can business model might look then we can business model might look then we can move further down into an architecture move further down into an architecture move further down into an architecture overview diagram in this case now we've overview diagram in this case now we've overview diagram in this case now we've got a project database a scheduler that got a project database a scheduler that got a project database a scheduler that is getting status and reports that it's is getting status and reports that it's is getting status and reports that it's generating and then alerts whenever generating and then alerts whenever generating and then alerts whenever we're over budget or behind schedule or we're over budget or behind schedule or we're over budget or behind schedule or things like that so you notice with each things like that so you notice with each things like that so you notice with each one of these it's a further level of one of these it's a further level of one of these it's a further level of detail a further decomposition Mission detail a further decomposition Mission detail a further decomposition Mission and as I said this is sort of the lingua and as I said this is sort of the lingua and as I said this is sort of the lingua Franca the common language of the Franca the common language of the Franca the common language of the architect any I.T architect should be architect any I.T architect should be architect any I.T architect should be able to take these kinds of things and able to take these kinds of things and able to take these kinds of things and understand what they need to do now a understand what they need to do now a understand what they need to do now a cyber security architect will look at cyber security architect will look at cyber security architect will look at this and need to understand how the this and need to understand how the this and need to understand how the system works as I said before they also system works as I said before they also system works as I said before they also need to Envision how the system might need to Envision how the system might need to Envision how the system might fail so in doing that I'm going to take fail so in doing that I'm going to take fail so in doing that I'm going to take this architecture that my it my normal this architecture that my it my normal this architecture that my it my normal it architect came up with and I'm going it architect came up with and I'm going it architect came up with and I'm going to try to put the security into this now to try to put the security into this now to try to put the security into this now that's the typical practice and and the that's the typical practice and and the that's the typical practice and and the way we do it remember in the first video way we do it remember in the first video way we do it remember in the first video I talked about security principles five I talked about security principles five I talked about security principles five that you should always do and one you that you should always do and one you that you should always do and one you should never do should never do should never do and in the second video I talked about and in the second video I talked about and in the second video I talked about the CIA Triad confidentiality integrity the CIA Triad confidentiality integrity the CIA Triad confidentiality integrity and availability that's a checklist so and availability that's a checklist so and availability that's a checklist so we're going to use those things and in we're going to use those things and in we're going to use those things and in this video I'm going to add another tool this video I'm going to add another tool this video I'm going to add another tool to your toolbox and that's Frameworks in to your toolbox and that's Frameworks in to your toolbox and that's Frameworks in particular a framework like this one particular a framework like this one particular a framework like this one that comes from the National Institute that comes from the National Institute that comes from the National Institute of Standards in the U.S it's known as of Standards in the U.S it's known as of Standards in the U.S it's known as the cyber security framework and what it the cyber security framework and what it the cyber security framework and what it does is it spells out think of a an does is it spells out think of a an does is it spells out think of a an architect will need to follow certain architect will need to follow certain architect will need to follow certain building codes if they're in coming up building codes if they're in coming up building codes if they're in coming up with a building if you're an I.T with a building if you're an I.T with a building if you're an I.T architect we don't exactly have building architect we don't exactly have building architect we don't exactly have building codes that spell it out to that level of codes that spell it out to that level of codes that spell it out to that level of detail but this is an analogy to that so detail but this is an analogy to that so detail but this is an analogy to that so we're going to specify in the identify we're going to specify in the identify we're going to specify in the identify stage these are the things that you need stage these are the things that you need stage these are the things that you need to do to identify users and data and to do to identify users and data and to do to identify users and data and things of that sort we're going to spell things of that sort we're going to spell things of that sort we're going to spell out how we're going to protect those out how we're going to protect those out how we're going to protect those things once we've identified them what things once we've identified them what things once we've identified them what levels of encryption and access control levels of encryption and access control levels of encryption and access control and things like that that we need how and things like that that we need how and things like that that we need how we're going to detect when we have we're going to detect when we have we're going to detect when we have problems that we will spell that out problems that we will spell that out problems that we will spell that out it's this is all listed as a very nice it's this is all listed as a very nice it's this is all listed as a very nice comprehensive checklist for you to look comprehensive checklist for you to look comprehensive checklist for you to look at and consider if you've covered all at and consider if you've covered all at and consider if you've covered all the bases in the nist cyber security the bases in the nist cyber security the bases in the nist cyber security framework how are we going to respond framework how are we going to respond framework how are we going to respond once we've detected a problem and then once we've detected a problem and then once we've detected a problem and then how do we recover once we realize that how do we recover once we realize that how do we recover once we realize that we have now got to get the system all we have now got to get the system all we have now got to get the system all back and going again so think about this back and going again so think about this back and going again so think about this as a cyber security architect I'm going as a cyber security architect I'm going as a cyber security architect I'm going to apply these principles the CIA Triad to apply these principles the CIA Triad to apply these principles the CIA Triad and some of these Frameworks onto this and some of these Frameworks onto this and some of these Frameworks onto this now that's the typical practice what now that's the typical practice what now that's the typical practice what often happens is I get called in at this often happens is I get called in at this often happens is I get called in at this phase when the architecture is already phase when the architecture is already phase when the architecture is already done and they say Jeff make it secure done and they say Jeff make it secure done and they say Jeff make it secure well we can do it that's the typical well we can do it that's the typical well we can do it that's the typical practice but it's not the best practice practice but it's not the best practice practice but it's not the best practice it's not the best practice because the it's not the best practice because the it's not the best practice because the same way you wouldn't like to have the same way you wouldn't like to have the same way you wouldn't like to have the building architect say we've got the building architect say we've got the building architect say we've got the building built now come in in and make building built now come in in and make building built now come in in and make it earthquake proof it's a little hard it earthquake proof it's a little hard it earthquake proof it's a little hard to do now it would have been a whole lot to do now it would have been a whole lot to do now it would have been a whole lot better if instead of at the better if instead of at the better if instead of at the implementation or architecture phase you implementation or architecture phase you implementation or architecture phase you had engaged me up here this is the best had engaged me up here this is the best had engaged me up here this is the best practice this is when we ideally want to practice this is when we ideally want to practice this is when we ideally want to be bringing in the security architect be bringing in the security architect be bringing in the security architect and involve them at literally every step and involve them at literally every step and involve them at literally every step along the Project Life Cycle so I'm along the Project Life Cycle so I'm along the Project Life Cycle so I'm going to do risk analysis and I'm going going to do risk analysis and I'm going going to do risk analysis and I'm going to see what are the risks in each one of to see what are the risks in each one of to see what are the risks in each one of these areas and apply some of these these areas and apply some of these these areas and apply some of these principles and Frameworks I'm going to principles and Frameworks I'm going to principles and Frameworks I'm going to develop a security policy I'm going to develop a security policy I'm going to develop a security policy I'm going to develop then an architecture that goes develop then an architecture that goes develop then an architecture that goes along with the overall it architecture along with the overall it architecture along with the overall it architecture the normal mode architecture so that the normal mode architecture so that the normal mode architecture so that security is not just a bolt-on it's security is not just a bolt-on it's security is not just a bolt-on it's something that was baked in to begin something that was baked in to begin something that was baked in to begin with and then we add in the with and then we add in the with and then we add in the implementation we're looking implementation we're looking implementation we're looking architecturally at these security architecturally at these security architecturally at these security principles and these Frameworks and principles and these Frameworks and principles and these Frameworks and applying them throughout the process applying them throughout the process applying them throughout the process this is how the architect applies their this is how the architect applies their this is how the architect applies their mindset applies their role and uses the mindset applies their role and uses the mindset applies their role and uses the tools of the trade tools of the trade tools of the trade okay now we've covered the cyber okay now we've covered the cyber okay now we've covered the cyber security architect's role and their security architect's role and their security architect's role and their mindset also the tools of the trade now mindset also the tools of the trade now mindset also the tools of the trade now we're going to talk a little bit about we're going to talk a little bit about we're going to talk a little bit about the domains that they operate in these the domains that they operate in these the domains that they operate in these are the cyber security domains that are are the cyber security domains that are are the cyber security domains that are the focus of the cyber security the focus of the cyber security the focus of the cyber security architect so for instance they're going architect so for instance they're going architect so for instance they're going to take a look at a user who is coming to take a look at a user who is coming to take a look at a user who is coming into a system off of some endpoint into a system off of some endpoint into a system off of some endpoint device traversing a network hitting an device traversing a network hitting an device traversing a network hitting an application application application which pulls data from a database now we which pulls data from a database now we which pulls data from a database now we each one of these are domains in cyber each one of these are domains in cyber each one of these are domains in cyber security identity and access management security identity and access management security identity and access management is where we're looking at the user we're is where we're looking at the user we're is where we're looking at the user we're looking at making sure they're who they looking at making sure they're who they looking at making sure they're who they claim to be that they have the right claim to be that they have the right claim to be that they have the right access rights and things of that sort access rights and things of that sort access rights and things of that sort that's a whole domain endpoint security that's a whole domain endpoint security that's a whole domain endpoint security making sure their device is secure and making sure their device is secure and making sure their device is secure and can be trusted the network itself being can be trusted the network itself being can be trusted the network itself being secure the applications can't be broken secure the applications can't be broken secure the applications can't be broken into and the data is protected we'll into and the data is protected we'll into and the data is protected we'll talk about each one of those domains and talk about each one of those domains and talk about each one of those domains and then add two more on top of that because then add two more on top of that because then add two more on top of that because in fact what we need to be able to do is in fact what we need to be able to do is in fact what we need to be able to do is take security Telemetry and information take security Telemetry and information take security Telemetry and information from all of these parts of the working from all of these parts of the working from all of these parts of the working system the functional system and feed system the functional system and feed system the functional system and feed those into a monitoring system a those into a monitoring system a those into a monitoring system a security information and event security information and event security information and event management capability that monitors all management capability that monitors all management capability that monitors all of this and lets us know if there's an of this and lets us know if there's an of this and lets us know if there's an intrusion or if there's some reason that intrusion or if there's some reason that intrusion or if there's some reason that we need to go do an investigation and we need to go do an investigation and we need to go do an investigation and then ultimately a response if I find a then ultimately a response if I find a then ultimately a response if I find a problem I need to be able to orchestrate problem I need to be able to orchestrate problem I need to be able to orchestrate my response to that problem so that we my response to that problem so that we my response to that problem so that we get it resolved as quickly as possible get it resolved as quickly as possible get it resolved as quickly as possible these are the seven domains that we're these are the seven domains that we're these are the seven domains that we're going to be covering in the rest of the going to be covering in the rest of the going to be covering in the rest of the series series series thanks for watching before you leave thanks for watching before you leave thanks for watching before you leave don't forget to hit subscribe that way don't forget to hit subscribe that way don't forget to hit subscribe that way you won't miss the next installment of you won't miss the next installment of you won't miss the next installment of the cyber security architecture series",
    "chunks": [
      "Kind: captions Language: en in the previous two videos on cyber in the previous two videos on cyber in the previous two videos on cyber security architecture fundamentals we security architecture fundamentals we security architecture fundamentals we discussed principles that you should discussed principles that you should discussed principles that you should follow essential security principles follow essential security principles follow essential security principles in the next video we",
      "discussed the CIA in the next video we discussed the CIA in the next video we discussed the CIA Triad where you could basically use this Triad where you could basically use this Triad where you could basically use this as a checklist to know that you've done as a checklist to know that you've done as a checklist to know that you've done a cyber security architecture correctly a cyber security architecture correctly a cyber security architecture correctly in this video we're going to focus on in",
      "this video we're going to focus on in this video we're going to focus on the cyber security architect in the cyber security architect in the cyber security architect in particular their role the mindset that particular their role the mindset that particular their role the mindset that they have to adopt in developing a they have to adopt in developing a they have to adopt in developing a secure solution the tools that they use secure solution the tools that they use secure solution the tools that",
      "they use tools of the trade and the domains that tools of the trade and the domains that tools of the trade and the domains that they have to operate in all right we're they have to operate in all right we're they have to operate in all right we're going to start off with the role in the going to start off with the role in the going to start off with the role in the mindset where this all begins is with mindset where this all begins is with mindset where this all begins is with stakeholders",
      "stakeholders stakeholders these are the people that have a vested these are the people that have a vested these are the people that have a vested interest in getting this solution right interest in getting this solution right interest in getting this solution right so we're going to take a look at two so we're going to take a look at two so we're going to take a look at two examples here of an architect who is examples here of an architect who is examples here of an architect who is working on a",
      "building and an I.T working on a building and an I.T working on a building and an I.T architect who's working on building an architect who's working on building an architect who's working on building an I.T system in both cases we're going to I.T system in both cases we're going to I.T system in both cases we're going to start with stakeholders and we're going start with stakeholders and we're going start with stakeholders and we're going to take their inputs into the architect to take their",
      "inputs into the architect to take their inputs into the architect the architect is going to wonder okay the architect is going to wonder okay the architect is going to wonder okay we're building a building but what kind we're building a building but what kind we're building a building but what kind of building is this going to be is it of building is this going to be is it of building is this going to be is it going to be a business is it going to be going to be a business is it going to be going",
      "to be a business is it going to be a home well in this case we're told it's a home well in this case we're told it's a home well in this case we're told it's going to be a home it's going to be a going to be a home it's going to be a going to be a home it's going to be a multi-family dwelling so it's a multi-family dwelling so it's a multi-family dwelling so it's a townhouse maybe for instance so we have townhouse maybe for instance so we have townhouse maybe for instance so we have an idea",
      "already what it's going to be an idea already what it's going to be an idea already what it's going to be what sort of size we want what kind of what sort of size we want what kind of what sort of size we want what kind of price range we want it to be in those price range we want it to be in those price range we want it to be in those are the things the stakeholders are are the things the stakeholders are are the things the stakeholders are giving the architect the architect is giving the",
      "architect the architect is giving the architect the architect is going to take that and develop a going to take that and develop a going to take that and develop a blueprint that blueprint then becomes blueprint that blueprint then becomes blueprint that blueprint then becomes the plan that the contractors come along the plan that the contractors come along the plan that the contractors come along and Implement so we've got contractors and Implement so we've got contractors and Implement so we've",
      "got contractors who are plumbers and Carpenter tours and who are plumbers and Carpenter tours and who are plumbers and Carpenter tours and things of that sort they're going to be things of that sort they're going to be things of that sort they're going to be the ones that do the actual the ones that do the actual the ones that do the actual implementation if the architect shows up implementation if the architect shows up implementation if the architect shows up on the job site with a hammer in",
      "hand on the job site with a hammer in hand on the job site with a hammer in hand you might be in trouble because that's you might be in trouble because that's you might be in trouble because that's not their area of expertise you want not their area of expertise you want not their area of expertise you want these people that are experts in doing these people that are experts in doing these people that are experts in doing and these guys who are experts in and these guys who are experts in and",
      "these guys who are experts in planning and coming up with the big planning and coming up with the big planning and coming up with the big Ideas so that's a little bit of analogy Ideas so that's a little bit of analogy Ideas so that's a little bit of analogy now an architect might say this is what now an architect might say this is what now an architect might say this is what I generally want this thing to look like I generally want this thing to look like I generally want this thing to look like",
      "but we need to take into account some but we need to take into account some but we need to take into account some other things after I've kind of come up other things after I've kind of come up other things after I've kind of come up with the basic sketch of what this is I with the basic sketch of what this is I with the basic sketch of what this is I need to think about Safety and Security need to think about Safety and Security need to think about Safety and Security as well with this building",
      "so for as well with this building so for as well with this building so for instance I want locks on the doors of instance I want locks on the doors of instance I want locks on the doors of course I don't want just anybody to be course I don't want just anybody to be course I don't want just anybody to be able to to walk in I might put security able to to walk in I might put security able to to walk in I might put security cameras uh in each of the the units at cameras uh in each of the the units",
      "at cameras uh in each of the the units at least on the outside maybe on the inside least on the outside maybe on the inside least on the outside maybe on the inside so that again I have an ability to so that again I have an ability to so that again I have an ability to monitor maybe even alarm systems I might monitor maybe even alarm systems I might monitor maybe even alarm systems I might be concerned about fire in one of the be concerned about fire in one of the be concerned about fire in one",
      "of the units so I put a smoke detector on the units so I put a smoke detector on the units so I put a smoke detector on the ceiling in each one of these so that we ceiling in each one of these so that we ceiling in each one of these so that we can detect that and then if I actually can detect that and then if I actually can detect that and then if I actually do have a fire well I'd like to have do have a fire well I'd like to have do have a fire well I'd like to have something that we call a",
      "firewall that something that we call a firewall that something that we call a firewall that slows the spread of fire from one unit slows the spread of fire from one unit slows the spread of fire from one unit to the next doesn't prevent it to the next doesn't prevent it to the next doesn't prevent it completely but at least it keeps it from completely but at least it keeps it from completely but at least it keeps it from spreading really really fast so these spreading really really fast so these",
      "spreading really really fast so these are kind of mitigations things that we are kind of mitigations things that we are kind of mitigations things that we add on to the architecture to make it add on to the architecture to make it add on to the architecture to make it more safe to make it more secure and the more safe to make it more secure and the more safe to make it more secure and the architect dreams those up and the architect dreams those up and the architect dreams those up and the",
      "contractors Implement and put those contractors Implement and put those contractors Implement and put those things together now let's take a look at things together now let's take a look at things together now let's take a look at an I.T example of the same sort of thing an I.T example of the same sort of thing an I.T example of the same sort of thing here once again here once again here once again we're going to start with stakeholders we're going to start with stakeholders we're going to start",
      "with stakeholders and they're going to work with the and they're going to work with the and they're going to work with the architect the architect instead of architect the architect instead of architect the architect instead of coming up with a blueprint is going to coming up with a blueprint is going to coming up with a blueprint is going to come up with an analogy to that which is come up with an analogy to that which is come up with an analogy to that which is going to be some type of",
      "reference going to be some type of reference going to be some type of reference architecture or some type of architecture or some type of architecture or some type of architecture overview diagram or architecture overview diagram or architecture overview diagram or diagrams that show the interrelations of diagrams that show the interrelations of diagrams that show the interrelations of the high level components of the system the high level components of the system the high level components of the",
      "system that then is going to get translated that then is going to get translated that then is going to get translated into an actual it architecture so here into an actual it architecture so here into an actual it architecture so here we've got in particular a user who is we've got in particular a user who is we've got in particular a user who is going to use a workstation maybe a going to use a workstation maybe a going to use a workstation maybe a mobile device or a desktop device mobile device",
      "or a desktop device mobile device or a desktop device they're going to come across a network they're going to come across a network they're going to come across a network to hit a web server that's going to hit to hit a web server that's going to hit to hit a web server that's going to hit an app server which is going to hit a an app server which is going to hit a an app server which is going to hit a database and we're going to get their database and we're going to get their database and we're",
      "going to get their data this is a very simple type of data this is a very simple type of data this is a very simple type of architecture architecture architecture now the the architect is then going to now the the architect is then going to now the the architect is then going to ask the engineers the architect has been ask the engineers the architect has been ask the engineers the architect has been doing their work basically from a doing their work basically from a doing their work basically",
      "from a whiteboard think of it this way whiteboard think of it this way whiteboard think of it this way Architects whiteboard Engineers keyboard Architects whiteboard Engineers keyboard Architects whiteboard Engineers keyboard This is where they're going to be doing This is where they're going to be doing This is where they're going to be doing their work as they start implementing their work as they start implementing their work as they start implementing this system this system this system this",
      "architect now also has to consider this architect now also has to consider this architect now also has to consider what might be some failure cases this is what might be some failure cases this is what might be some failure cases this is the difference between a sort of normal the difference between a sort of normal the difference between a sort of normal I.T architect and a cyber security I.T architect and a cyber security I.T architect and a cyber security architect the normal architect thinks",
      "architect the normal architect thinks architect the normal architect thinks about how a system will work about how a system will work about how a system will work the cyber security architect thinks the cyber security architect thinks the cyber security architect thinks about how it will fail now the cyber about how it will fail now the cyber about how it will fail now the cyber security architect has to First security architect has to First security architect has to First understand how the",
      "system is going to understand how the system is going to understand how the system is going to work or they don't know how it might work or they don't know how it might work or they don't know how it might fail so they have to have that level of fail so they have to have that level of fail so they have to have that level of understanding then they have to add on understanding then they have to add on understanding then they have to add on to it what are the possible things that to it what are the",
      "possible things that to it what are the possible things that could go wrong so let's ask what could could go wrong so let's ask what could could go wrong so let's ask what could go wrong with this user well it could be go wrong with this user well it could be go wrong with this user well it could be someone stole their password their someone stole their password their someone stole their password their credentials so it's not this user credentials so it's not this user credentials so it's not",
      "this user anymore so what do I need well I'm going anymore so what do I need well I'm going anymore so what do I need well I'm going to put in multi-factor authentication a to put in multi-factor authentication a to put in multi-factor authentication a mitigation a way to check and compensate mitigation a way to check and compensate mitigation a way to check and compensate for that particular risk for that particular risk for that particular risk what if we've got on this workstation a what if",
      "we've got on this workstation a what if we've got on this workstation a virus or if it's a mobile device maybe virus or if it's a mobile device maybe virus or if it's a mobile device maybe it's been jailbroken well if it's mobile it's been jailbroken well if it's mobile it's been jailbroken well if it's mobile device I'll add mobile device management device I'll add mobile device management device I'll add mobile device management software detect check for that if it's software detect check for",
      "that if it's software detect check for that if it's another type of device endpoint another type of device endpoint another type of device endpoint detection and response capabilities or detection and response capabilities or detection and response capabilities or antivirus capabilities to check there antivirus capabilities to check there antivirus capabilities to check there and we continue across this in the case and we continue across this in the case and we continue across this in the case of",
      "the network well just like over here of the network well just like over here of the network well just like over here on this building we added firewalls in on this building we added firewalls in on this building we added firewalls in order to keep the spread of a fire from order to keep the spread of a fire from order to keep the spread of a fire from one unit from immediately spreading to one unit from immediately spreading to one unit from immediately spreading to another and providing a level",
      "of another and providing a level of another and providing a level of protective isolation that's what we do protective isolation that's what we do protective isolation that's what we do with network firewalls that's where we with network firewalls that's where we with network firewalls that's where we got that term so I'm going to add got that term so I'm going to add got that term so I'm going to add Network firewalls here to slow the Network firewalls here to slow the Network firewalls here to",
      "slow the spread of contagion or attack across spread of contagion or attack across spread of contagion or attack across this infrastructure and then ultimately this infrastructure and then ultimately this infrastructure and then ultimately over here I'm going to encrypt the data over here I'm going to encrypt the data over here I'm going to encrypt the data that's in the database and I'm going to that's in the database and I'm going to that's in the database and I'm going to ask this it engineer",
      "whoever it is and ask this it engineer whoever it is and ask this it engineer whoever it is and by the way we'll have different by the way we'll have different by the way we'll have different Engineers that are specialized in each Engineers that are specialized in each Engineers that are specialized in each of these air areas so I might have a of these air areas so I might have a of these air areas so I might have a database administrator that does the database administrator that does the",
      "database administrator that does the database encryption a network database encryption a network database encryption a network administrator that implements the administrator that implements the administrator that implements the firewalls someone else who does the firewalls someone else who does the firewalls someone else who does the desktop someone else who does the desktop someone else who does the desktop someone else who does the identity and access management identity and access management",
      "identity and access management capabilities so all of these Engineers capabilities so all of these Engineers capabilities so all of these Engineers are analogous to the different are analogous to the different are analogous to the different contractors and the architect in both of contractors and the architect in both of contractors and the architect in both of these cases is coming up with the big these cases is coming up with the big these cases is coming up with the big picture the big plans",
      "so again think if picture the big plans so again think if picture the big plans so again think if you're thinking of a cyber security you're thinking of a cyber security you're thinking of a cyber security architect think whiteboard rather than architect think whiteboard rather than architect think whiteboard rather than keyboard and also think how will the keyboard and also think how will the keyboard and also think how will the system fail and what do I need to do to system fail and what do I",
      "need to do to system fail and what do I need to do to prevent that prevent that prevent that okay now we've covered the role and okay now we've covered the role and okay now we've covered the role and mindset of the cyber security architect mindset of the cyber security architect mindset of the cyber security architect now let's talk about now let's talk about now let's talk about the tools of the trade well it turns out the tools of the trade well it turns out the tools of the trade well it",
      "turns out that in the it architect World there are that in the it architect World there are that in the it architect World there are certain common diagrams that Architects certain common diagrams that Architects certain common diagrams that Architects use there's a business context diagram a use there's a business context diagram a use there's a business context diagram a system context diagram and an system context diagram and an system context diagram and an architecture overview diagram are",
      "just architecture overview diagram are just architecture overview diagram are just three examples that I think are three examples that I think are three examples that I think are particularly important so for instance particularly important so for instance particularly important so for instance we'll talk with a business context we'll talk with a business context we'll talk with a business context diagram here we're trying to show diagram here we're trying to show diagram here we're trying to",
      "show relationships among the different relationships among the different relationships among the different entities in the system so an example entities in the system so an example entities in the system so an example here we've got a builder we've got a here we've got a builder we've got a here we've got a builder we've got a marketing team we've got Tradesmen who marketing team we've got Tradesmen who marketing team we've got Tradesmen who are going to build the building and then are going to",
      "build the building and then are going to build the building and then a buyer and so we're showing the inner a buyer and so we're showing the inner a buyer and so we're showing the inner relationships amongst those various relationships amongst those various relationships amongst those various entities it's a very high level line of entities it's a very high level line of entities it's a very high level line of business sort of view business sort of view business sort of view in the next one the",
      "system context in the next one the system context in the next one the system context diagram we're going to take that and diagram we're going to take that and diagram we're going to take that and decompose it further into what it would decompose it further into what it would decompose it further into what it would look like in a system now this is just look like in a system now this is just look like in a system now this is just one aspect this doesn't show all of them one aspect this doesn't",
      "show all of them one aspect this doesn't show all of them by any means but here we have a project by any means but here we have a project by any means but here we have a project management system there's a finance management system there's a finance management system there's a finance system that's trying to oversee and make system that's trying to oversee and make system that's trying to oversee and make sure we can afford to build this thing sure we can afford to build this thing sure we can",
      "afford to build this thing the way we need to and on budget the way we need to and on budget the way we need to and on budget blueprints that we're going to call in blueprints that we're going to call in blueprints that we're going to call in and do the building with a permitting and do the building with a permitting and do the building with a permitting system that we need to go off and get system that we need to go off and get system that we need to go off and get those and then a graphical",
      "user those and then a graphical user those and then a graphical user interface that interfaces to all of it interface that interfaces to all of it interface that interfaces to all of it that's just a very simple example of of that's just a very simple example of of that's just a very simple example of of how the it system that supports this how the it system that supports this how the it system that supports this business model might look then we can business model might look then we can business",
      "model might look then we can move further down into an architecture move further down into an architecture move further down into an architecture overview diagram in this case now we've overview diagram in this case now we've overview diagram in this case now we've got a project database a scheduler that got a project database a scheduler that got a project database a scheduler that is getting status and reports that it's is getting status and reports that it's is getting status and reports that",
      "it's generating and then alerts whenever generating and then alerts whenever generating and then alerts whenever we're over budget or behind schedule or we're over budget or behind schedule or we're over budget or behind schedule or things like that so you notice with each things like that so you notice with each things like that so you notice with each one of these it's a further level of one of these it's a further level of one of these it's a further level of detail a further decomposition",
      "Mission detail a further decomposition Mission detail a further decomposition Mission and as I said this is sort of the lingua and as I said this is sort of the lingua and as I said this is sort of the lingua Franca the common language of the Franca the common language of the Franca the common language of the architect any I.T architect should be architect any I.T architect should be architect any I.T architect should be able to take these kinds of things and able to take these kinds of things",
      "and able to take these kinds of things and understand what they need to do now a understand what they need to do now a understand what they need to do now a cyber security architect will look at cyber security architect will look at cyber security architect will look at this and need to understand how the this and need to understand how the this and need to understand how the system works as I said before they also system works as I said before they also system works as I said before they also",
      "need to Envision how the system might need to Envision how the system might need to Envision how the system might fail so in doing that I'm going to take fail so in doing that I'm going to take fail so in doing that I'm going to take this architecture that my it my normal this architecture that my it my normal this architecture that my it my normal it architect came up with and I'm going it architect came up with and I'm going it architect came up with and I'm going to try to put the security",
      "into this now to try to put the security into this now to try to put the security into this now that's the typical practice and and the that's the typical practice and and the that's the typical practice and and the way we do it remember in the first video way we do it remember in the first video way we do it remember in the first video I talked about security principles five I talked about security principles five I talked about security principles five that you should always do and one you that",
      "you should always do and one you that you should always do and one you should never do should never do should never do and in the second video I talked about and in the second video I talked about and in the second video I talked about the CIA Triad confidentiality integrity the CIA Triad confidentiality integrity the CIA Triad confidentiality integrity and availability that's a checklist so and availability that's a checklist so and availability that's a checklist so we're going to use those",
      "things and in we're going to use those things and in we're going to use those things and in this video I'm going to add another tool this video I'm going to add another tool this video I'm going to add another tool to your toolbox and that's Frameworks in to your toolbox and that's Frameworks in to your toolbox and that's Frameworks in particular a framework like this one particular a framework like this one particular a framework like this one that comes from the National Institute that comes",
      "from the National Institute that comes from the National Institute of Standards in the U.S it's known as of Standards in the U.S it's known as of Standards in the U.S it's known as the cyber security framework and what it the cyber security framework and what it the cyber security framework and what it does is it spells out think of a an does is it spells out think of a an does is it spells out think of a an architect will need to follow certain architect will need to follow certain architect",
      "will need to follow certain building codes if they're in coming up building codes if they're in coming up building codes if they're in coming up with a building if you're an I.T with a building if you're an I.T with a building if you're an I.T architect we don't exactly have building architect we don't exactly have building architect we don't exactly have building codes that spell it out to that level of codes that spell it out to that level of codes that spell it out to that level of detail but",
      "this is an analogy to that so detail but this is an analogy to that so detail but this is an analogy to that so we're going to specify in the identify we're going to specify in the identify we're going to specify in the identify stage these are the things that you need stage these are the things that you need stage these are the things that you need to do to identify users and data and to do to identify users and data and to do to identify users and data and things of that sort we're going to",
      "spell things of that sort we're going to spell things of that sort we're going to spell out how we're going to protect those out how we're going to protect those out how we're going to protect those things once we've identified them what things once we've identified them what things once we've identified them what levels of encryption and access control levels of encryption and access control levels of encryption and access control and things like that that we need how and things like that that",
      "we need how and things like that that we need how we're going to detect when we have we're going to detect when we have we're going to detect when we have problems that we will spell that out problems that we will spell that out problems that we will spell that out it's this is all listed as a very nice it's this is all listed as a very nice it's this is all listed as a very nice comprehensive checklist for you to look comprehensive checklist for you to look comprehensive checklist for you to",
      "look at and consider if you've covered all at and consider if you've covered all at and consider if you've covered all the bases in the nist cyber security the bases in the nist cyber security the bases in the nist cyber security framework how are we going to respond framework how are we going to respond framework how are we going to respond once we've detected a problem and then once we've detected a problem and then once we've detected a problem and then how do we recover once we realize that",
      "how do we recover once we realize that how do we recover once we realize that we have now got to get the system all we have now got to get the system all we have now got to get the system all back and going again so think about this back and going again so think about this back and going again so think about this as a cyber security architect I'm going as a cyber security architect I'm going as a cyber security architect I'm going to apply these principles the CIA Triad to apply these principles",
      "the CIA Triad to apply these principles the CIA Triad and some of these Frameworks onto this and some of these Frameworks onto this and some of these Frameworks onto this now that's the typical practice what now that's the typical practice what now that's the typical practice what often happens is I get called in at this often happens is I get called in at this often happens is I get called in at this phase when the architecture is already phase when the architecture is already phase when the",
      "architecture is already done and they say Jeff make it secure done and they say Jeff make it secure done and they say Jeff make it secure well we can do it that's the typical well we can do it that's the typical well we can do it that's the typical practice but it's not the best practice practice but it's not the best practice practice but it's not the best practice it's not the best practice because the it's not the best practice because the it's not the best practice because the same way you",
      "wouldn't like to have the same way you wouldn't like to have the same way you wouldn't like to have the building architect say we've got the building architect say we've got the building architect say we've got the building built now come in in and make building built now come in in and make building built now come in in and make it earthquake proof it's a little hard it earthquake proof it's a little hard it earthquake proof it's a little hard to do now it would have been a whole lot to do now",
      "it would have been a whole lot to do now it would have been a whole lot better if instead of at the better if instead of at the better if instead of at the implementation or architecture phase you implementation or architecture phase you implementation or architecture phase you had engaged me up here this is the best had engaged me up here this is the best had engaged me up here this is the best practice this is when we ideally want to practice this is when we ideally want to practice this is",
      "when we ideally want to be bringing in the security architect be bringing in the security architect be bringing in the security architect and involve them at literally every step and involve them at literally every step and involve them at literally every step along the Project Life Cycle so I'm along the Project Life Cycle so I'm along the Project Life Cycle so I'm going to do risk analysis and I'm going going to do risk analysis and I'm going going to do risk analysis and I'm going to see what",
      "are the risks in each one of to see what are the risks in each one of to see what are the risks in each one of these areas and apply some of these these areas and apply some of these these areas and apply some of these principles and Frameworks I'm going to principles and Frameworks I'm going to principles and Frameworks I'm going to develop a security policy I'm going to develop a security policy I'm going to develop a security policy I'm going to develop then an architecture that goes develop",
      "then an architecture that goes develop then an architecture that goes along with the overall it architecture along with the overall it architecture along with the overall it architecture the normal mode architecture so that the normal mode architecture so that the normal mode architecture so that security is not just a bolt-on it's security is not just a bolt-on it's security is not just a bolt-on it's something that was baked in to begin something that was baked in to begin something that was",
      "baked in to begin with and then we add in the with and then we add in the with and then we add in the implementation we're looking implementation we're looking implementation we're looking architecturally at these security architecturally at these security architecturally at these security principles and these Frameworks and principles and these Frameworks and principles and these Frameworks and applying them throughout the process applying them throughout the process applying them throughout the",
      "process this is how the architect applies their this is how the architect applies their this is how the architect applies their mindset applies their role and uses the mindset applies their role and uses the mindset applies their role and uses the tools of the trade tools of the trade tools of the trade okay now we've covered the cyber okay now we've covered the cyber okay now we've covered the cyber security architect's role and their security architect's role and their security architect's role",
      "and their mindset also the tools of the trade now mindset also the tools of the trade now mindset also the tools of the trade now we're going to talk a little bit about we're going to talk a little bit about we're going to talk a little bit about the domains that they operate in these the domains that they operate in these the domains that they operate in these are the cyber security domains that are are the cyber security domains that are are the cyber security domains that are the focus of the",
      "cyber security the focus of the cyber security the focus of the cyber security architect so for instance they're going architect so for instance they're going architect so for instance they're going to take a look at a user who is coming to take a look at a user who is coming to take a look at a user who is coming into a system off of some endpoint into a system off of some endpoint into a system off of some endpoint device traversing a network hitting an device traversing a network hitting an",
      "device traversing a network hitting an application application application which pulls data from a database now we which pulls data from a database now we which pulls data from a database now we each one of these are domains in cyber each one of these are domains in cyber each one of these are domains in cyber security identity and access management security identity and access management security identity and access management is where we're looking at the user we're is where we're looking at",
      "the user we're is where we're looking at the user we're looking at making sure they're who they looking at making sure they're who they looking at making sure they're who they claim to be that they have the right claim to be that they have the right claim to be that they have the right access rights and things of that sort access rights and things of that sort access rights and things of that sort that's a whole domain endpoint security that's a whole domain endpoint security that's a whole",
      "domain endpoint security making sure their device is secure and making sure their device is secure and making sure their device is secure and can be trusted the network itself being can be trusted the network itself being can be trusted the network itself being secure the applications can't be broken secure the applications can't be broken secure the applications can't be broken into and the data is protected we'll into and the data is protected we'll into and the data is protected we'll talk",
      "about each one of those domains and talk about each one of those domains and talk about each one of those domains and then add two more on top of that because then add two more on top of that because then add two more on top of that because in fact what we need to be able to do is in fact what we need to be able to do is in fact what we need to be able to do is take security Telemetry and information take security Telemetry and information take security Telemetry and information from all of these",
      "parts of the working from all of these parts of the working from all of these parts of the working system the functional system and feed system the functional system and feed system the functional system and feed those into a monitoring system a those into a monitoring system a those into a monitoring system a security information and event security information and event security information and event management capability that monitors all management capability that monitors all management",
      "capability that monitors all of this and lets us know if there's an of this and lets us know if there's an of this and lets us know if there's an intrusion or if there's some reason that intrusion or if there's some reason that intrusion or if there's some reason that we need to go do an investigation and we need to go do an investigation and we need to go do an investigation and then ultimately a response if I find a then ultimately a response if I find a then ultimately a response if I find a",
      "problem I need to be able to orchestrate problem I need to be able to orchestrate problem I need to be able to orchestrate my response to that problem so that we my response to that problem so that we my response to that problem so that we get it resolved as quickly as possible get it resolved as quickly as possible get it resolved as quickly as possible these are the seven domains that we're these are the seven domains that we're these are the seven domains that we're going to be covering in the",
      "rest of the going to be covering in the rest of the going to be covering in the rest of the series series series thanks for watching before you leave thanks for watching before you leave thanks for watching before you leave don't forget to hit subscribe that way don't forget to hit subscribe that way don't forget to hit subscribe that way you won't miss the next installment of you won't miss the next installment of you won't miss the next installment of the cyber security architecture series"
    ],
    "chunk_count": 83,
    "content_id": "ee077dbb-f80b-4693-99e7-d80876891c5c",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555014"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=Njqid_JpqTs": {
    "title": "Cybersecurity Architecture: Endpoints Are the IT Front Door - Guard Them",
    "url": "https://www.youtube.com/watch?v=Njqid_JpqTs",
    "description": "IBM Security QRadar EDR :   https://ibm.biz/BdySEq\n\nIBM Security X-Force Threat Intelligence Index 2023:  https://ibm.biz/BdySEf\n\nThe prior video in the series covered identity and access management (IAM), which Jeff \"the security guy\" described as the new perimeter. Of course, none of those access precautions, like multi-factor authentication, will matter if you can't trust the platform that it's coming from. In this video, Jeff Crume explains how to assure that endpoints are in fact secure and can be trusted. \n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-up\n\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n#AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume #endpointsecurity",
    "duration": 861,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Welcome back to the Cybersecurity Architecture Series. In the previous videos, I talked about some of the fundamentals of cybersecurity. And then in the last video, we started a seven part series on the various domains of cybersecurity. In particular, last time we talked about identity and access management, which I said is the new perimeter. Well, all of that stuff, for instance, multi-factor authentication that we do in the IAM space depends on us having a trusted platform that it's coming from. It won't matter how strong the biometric is, if it's coming from a jailbroken device. So I need to be able to secure that to make sure that that endpoint is in fact secure and can be trusted. So today we're going to talk about endpoint security. First off, what is an endpoint? What do I mean by that? Well, it involves a lot of different things, as you see here. For instance, from a hardware perspective, it involves different platforms. It could be a server, although a lot of times people skip over that and assume that's handled by the server group. Well, it can be. but in fact, I want to think about this in much more holistic terms. I want to see the server as a computing platform, someone's desktop system, a laptop system, that they have a mobile device. And in fact, we need to also consider IoT, that is, the Internet of Things. All of the stuff that is now getting computing capability built into it that we wouldn't necessarily consider a computing device in the past. You can see here we pictured it as a camera, but it could be a lot of other things. It could be household appliances, for that matter. So these all are the hardware platforms that in fact are on our systems; they're on our networks, and in some cases we're coming into the corporate network from those. Another thing to consider. So while along this axis, we've got varying hardware platforms, we've got also this sort of a continuum between business use of these devices and personal use. And in fact, I'll tell you that business and personal, there are a few people that have servers in their homes, and there are a lot of people, of course, that have mobile devices they use for work, for business. So the idea that there's a distinction between business and personal, I'd say that's largely a fiction anymore. People are using all of these devices in all cases. And again, the home appliances on the home network, which then connects to the corporate network. So these are all part of the scope that we have to consider as an architect. I like this term holistic. I want to keep staying with holistic views and look at all of the endpoints that are out there. Because in fact, the other thing that every single one of these is doing is it's contributing to our attack surface. This bad guy--every one of these platforms represents another way that he can come in and potentially attack us. Each one of these will have different vulnerabilities, and some of them, like these devices, maybe we handle the kid and let them play games with. This guy will love that. It makes it even easier for him to attack the expanding size of the perimeter. The expanding size of the attack surface is, in fact, creating a lot of challenges for us. And this is looking at it strictly from a hardware standpoint. There's also a software view of all of this, and that is across these different devices. I've got lots of different operating systems to deal with. We've got Windows, MacOS Linux, Unix, we've got mainframes, we've got mobile devices, we've got all kinds of others and some of these IoT devices, who knows what kind of operating system is running on those? Every single one of these create more complexity. And more complexity? Remember, complexity is the enemy of security. So all of this endpoint mess is why, in fact, we need to do controls. Okay, we just talked about what is an endpoint. Now we're going to talk about what are the security controls that we need to put on those endpoints in order to make them secure. So let's talk about endpoint management systems and how we operate these things. The typical practice we'll start with and then we'll move to the best practice. Typical practice is, we've got this guy down here, he logs into a console which then manages the servers. In a perfect world, we at least have all of the servers managed together. In a lot of cases, that's not even the case. Then we've got another administrator who logs into a different system to manage all the desktops and laptops. Sometimes those are running different OSes and therefore different tools and things like that. So another! Then we've got another administrator over here who is using his mobile device management system in order to to deal with the mobile phones, tablets, this kind of stuff. And then when it comes to the IoT, well, most cases we've got nothing. So there is nothing down here managing those at all, which is a whole other problem in and of itself. But you can see what's happened here. We've got multiple administrators managing different kinds of things. And the good news is they at least are domain experts in those particular areas. But it's not the most efficient way and it's not the simplest way. And again, complexity is the enemy of security. If I want to implement a single security policy across all of these, maybe with a few tweaks in here and there, but you get the idea, I'd like to be able to do it from one console. I could have logically one administrator who does this across all of these different platforms so I can push down policies and patches that then go across the entire infrastructure and then get up information and alerts about all of these different systems into the one console. It's much more efficient. I don't need necessarily all that much domain expertise because I have all of this in one system and I have the ability to control it all. Visibility and control are keys to security. If I can do both of those, then I have a fighting chance. So this would be the best practice is to integrate all of those into a holistic --the word I used in the previous section, *holistic* Endpoint security Management System. Now I've talked about policies. What might these policies be? What are the things that I'm trying to enforce over here? Let's take a look at what some of those controls might be. So for one thing, I'd like the system to be able to query over here and tell me what are all the different systems that I have. I may know about all of them. I may not. I'd like to be able to discover the ones I don't know and the ones that I do. I'd like to know what's the hardware level, what's the software level on these? In fact, a lot of organizations will have a particular security policy, which is this next part which dictates what types of hardware and software we will allow in the organization in the IT systems. For instance, one of the things I might say is for software levels, I'm going to allow the current release N and N minus one release. In other words, the current release of whatever that accepted software package is and maybe one level back, but two levels back. Now, you probably are missing a lot of security patches and we need you to have those on there. So we're going to disconnect you from the sensitive data because now you've got a system that's too old for us to be able to secure it. So that's a typical security policy that we might enforce. Other things we might enforce would be password policies. So I'm going to say on these devices, you need a password of a certain length, strength, expiry date, these kinds of things. So it's a way of controlling that across a lot of different systems. Patching I mentioned. The systems are only as good as the latest software. And if they've got old software again, from an operating system perspective, it might be in minus one, but other things might be that we've just come out with a new patch on an application. I need all of those patches applied because the likelihood is that there are security fixes in there that if we don't apply those, the bad guys can take advantage. Other things, an encryption policy. I might want the any of these devices that can hold data, which is probably all of them to have some sort of encryption policy so that I can make sure that if the device is lost or stolen, then nobody can get any information off of it because all the data that's on it is encrypted. Remote wipe capability? Again, in the case this mobile device maybe goes missing, someone loses it, it gets stolen, something like that, it would be really great if I could automatically wipe all of the data that's on this device and maybe do the same thing on some of these others as well. If I see that they have gone missing, I want to be able to blank all the data and do that remotely. Location tracking. In case I want to find one of these things, if it's gone. Now, some organizations may choose to not turn that on because these may be personal devices that are getting used. That's understandable. But the capability exists, certainly for the corporate devices to be able to track where those locations could be. Antivirus or endpoint detection and response. That is to make sure that I don't have malware on these systems. And then finally, what's my policy for disposing of these devices? These things don't last forever. The battery starts to die on this. We need to upgrade one of someone's laptop or something like that. How are we going to get rid of the device in a way that doesn't expose our information? Our security controls should take into account all of these things as well. Okay, now we've covered what are endpoints and what are the controls that we need for them. Now, let's take a look at BYOD. Well, what do I mean BYOD? It's bring your own device. But let me tell you, it's more complicated than that because a lot of people have figured out how to bring their own IT. And some people are even bringing their own cloud. So we've really got this whole collection of acronyms that we're having to kind of deal with. And the endpoint is just part of this when it comes to these bring your own programs. I'm going to tell you there's really two types of organizations out there. There are the ones who have a well-defined program. The ones who have a poorly defined program. And then there's another group that claims that's not allowed. So I'm going to tell you their program is actually an unsanctioned program. It comes back and really maps to just being one of these. So it's a poorly defined program. In other words, there really is no third category. Everybody is either a well-defined program when it comes to these things or a poorly defined program. So what would it be if we were to have a well-defined program? Because this obviously is not going to do our security needs. If the security organization says \"no\", the end user will say \"how\" and they will do it this way. Better if we define it in advance for them this way. What are some of the elements that would go into this? First of all, consent, especially if the person is bringing their own device that could have their own data. It's their property. So we need to make sure that they understand what are the rules. What are the things that we're going to put on your system? What are the things we're likely to do to your system with your system, that sort of thing. So we've got to have consent from the end user who owns this thing. And we're going to tell them, for instance, \"Oh yeah, we're going to monitor certain of your usage or not.\". Your policy should state whether you're monitoring their usage or not and under what conditions you might do that. We're going to look at how you're using your system. We're going to look and see if you're using it. It may be just the corporate things that you're doing and monitor only that. And then we might also want to reserve the right to remotely wipe the device and remove all corporate data. Now, we can do a selective wipe so that I remove only the corporate data and not all of the personal data. So if this person has a mobile mobile phone and they've taken family photos on their vacation, that stuff doesn't go away. But all the corporate data goes away. If they report the device as lost or stolen or they leave the organization. So I need that kind of capability. I might also specify what levels of software are required. As I mentioned in the previous section, the version, the current, the N, and the N minus one. But I might also get down to certain applications and say there are certain applications that are required, certain things that must be on your system. If it's a mobile device, there might be different kinds of things that would be required than if it was, say, a desktop device where maybe I'm going to be requiring antivirus that I might not require on a mobile device. There's other things that I might say. There are some applications you should never have. We don't want you having this on on your device and we're going to check for it and if we see it on there, we're going to report you or we're going to remove our data from your system, because we believe these devices or these applications, I should say, are going to make our data vulnerable or they're going to expose us to certain other types of threats. Then from a hardware perspective, an organization may very well say, we're only going to support you bringing in your device, but it has to be of a certain hardware configuration. We can't support every single device that anyone might ever come up with. So we're going to say \"This is the type of desktop, laptop, mobile device that we're going to support. We're going to support only those and not more.\". But we need to be able to specify what that is as well the services that you're going to use from these devices. Cloud's a good example. So I might use only authorized services. If I want to do file sharing, then the organizations and say we have a cloud-based file sharing program and everyone needs to use that. Don't use all of these others. And we're going to monitor and make sure that that's what you're using, for instance. So those are examples. Again, it's best not to say no. It's better to say how. And if I can say how on these kinds of things, I can guide the users to do the right thing. Always remember if we make it easier to do the wrong thing than it is to do the right thing, the users are going to basically do the wrong thing. So we want to make it easier and enable that. Okay, we've talked about endpoint security and covered that. In the next video, we're going to talk about network security. In case you've missed any in the series, take a look here. And if you want to make sure you don't miss any in the future, make sure to click, subscribe and notify so that you'll be notified when the next video comes out.",
    "chunks": [
      "Kind: captions Language: en Welcome back to the Cybersecurity Architecture Series. In the previous videos, I talked about some of the fundamentals of cybersecurity. And then in the last video, we started a seven part series on the various domains of cybersecurity. In particular, last time we talked about identity and access management, which I said is the new perimeter. Well, all of that stuff, for instance, multi-factor authentication that we do in the IAM space depends on us having a trusted",
      "platform that it's coming from. It won't matter how strong the biometric is, if it's coming from a jailbroken device. So I need to be able to secure that to make sure that that endpoint is in fact secure and can be trusted. So today we're going to talk about endpoint security. First off, what is an endpoint? What do I mean by that? Well, it involves a lot of different things, as you see here. For instance, from a hardware perspective, it involves different platforms. It could be a server,",
      "although a lot of times people skip over that and assume that's handled by the server group. Well, it can be. but in fact, I want to think about this in much more holistic terms. I want to see the server as a computing platform, someone's desktop system, a laptop system, that they have a mobile device. And in fact, we need to also consider IoT, that is, the Internet of Things. All of the stuff that is now getting computing capability built into it that we wouldn't necessarily consider a computing",
      "device in the past. You can see here we pictured it as a camera, but it could be a lot of other things. It could be household appliances, for that matter. So these all are the hardware platforms that in fact are on our systems; they're on our networks, and in some cases we're coming into the corporate network from those. Another thing to consider. So while along this axis, we've got varying hardware platforms, we've got also this sort of a continuum between business use of these devices and",
      "personal use. And in fact, I'll tell you that business and personal, there are a few people that have servers in their homes, and there are a lot of people, of course, that have mobile devices they use for work, for business. So the idea that there's a distinction between business and personal, I'd say that's largely a fiction anymore. People are using all of these devices in all cases. And again, the home appliances on the home network, which then connects to the corporate network. So these are",
      "all part of the scope that we have to consider as an architect. I like this term holistic. I want to keep staying with holistic views and look at all of the endpoints that are out there. Because in fact, the other thing that every single one of these is doing is it's contributing to our attack surface. This bad guy--every one of these platforms represents another way that he can come in and potentially attack us. Each one of these will have different vulnerabilities, and some of them, like these",
      "devices, maybe we handle the kid and let them play games with. This guy will love that. It makes it even easier for him to attack the expanding size of the perimeter. The expanding size of the attack surface is, in fact, creating a lot of challenges for us. And this is looking at it strictly from a hardware standpoint. There's also a software view of all of this, and that is across these different devices. I've got lots of different operating systems to deal with. We've got Windows, MacOS Linux,",
      "Unix, we've got mainframes, we've got mobile devices, we've got all kinds of others and some of these IoT devices, who knows what kind of operating system is running on those? Every single one of these create more complexity. And more complexity? Remember, complexity is the enemy of security. So all of this endpoint mess is why, in fact, we need to do controls. Okay, we just talked about what is an endpoint. Now we're going to talk about what are the security controls that we need to put on those",
      "endpoints in order to make them secure. So let's talk about endpoint management systems and how we operate these things. The typical practice we'll start with and then we'll move to the best practice. Typical practice is, we've got this guy down here, he logs into a console which then manages the servers. In a perfect world, we at least have all of the servers managed together. In a lot of cases, that's not even the case. Then we've got another administrator who logs into a different system to",
      "manage all the desktops and laptops. Sometimes those are running different OSes and therefore different tools and things like that. So another! Then we've got another administrator over here who is using his mobile device management system in order to to deal with the mobile phones, tablets, this kind of stuff. And then when it comes to the IoT, well, most cases we've got nothing. So there is nothing down here managing those at all, which is a whole other problem in and of itself. But you can see",
      "what's happened here. We've got multiple administrators managing different kinds of things. And the good news is they at least are domain experts in those particular areas. But it's not the most efficient way and it's not the simplest way. And again, complexity is the enemy of security. If I want to implement a single security policy across all of these, maybe with a few tweaks in here and there, but you get the idea, I'd like to be able to do it from one console. I could have logically one",
      "administrator who does this across all of these different platforms so I can push down policies and patches that then go across the entire infrastructure and then get up information and alerts about all of these different systems into the one console. It's much more efficient. I don't need necessarily all that much domain expertise because I have all of this in one system and I have the ability to control it all. Visibility and control are keys to security. If I can do both of those, then I have",
      "a fighting chance. So this would be the best practice is to integrate all of those into a holistic --the word I used in the previous section, *holistic* Endpoint security Management System. Now I've talked about policies. What might these policies be? What are the things that I'm trying to enforce over here? Let's take a look at what some of those controls might be. So for one thing, I'd like the system to be able to query over here and tell me what are all the different systems that I have. I",
      "may know about all of them. I may not. I'd like to be able to discover the ones I don't know and the ones that I do. I'd like to know what's the hardware level, what's the software level on these? In fact, a lot of organizations will have a particular security policy, which is this next part which dictates what types of hardware and software we will allow in the organization in the IT systems. For instance, one of the things I might say is for software levels, I'm going to allow the current",
      "release N and N minus one release. In other words, the current release of whatever that accepted software package is and maybe one level back, but two levels back. Now, you probably are missing a lot of security patches and we need you to have those on there. So we're going to disconnect you from the sensitive data because now you've got a system that's too old for us to be able to secure it. So that's a typical security policy that we might enforce. Other things we might enforce would be",
      "password policies. So I'm going to say on these devices, you need a password of a certain length, strength, expiry date, these kinds of things. So it's a way of controlling that across a lot of different systems. Patching I mentioned. The systems are only as good as the latest software. And if they've got old software again, from an operating system perspective, it might be in minus one, but other things might be that we've just come out with a new patch on an application. I need all of those",
      "patches applied because the likelihood is that there are security fixes in there that if we don't apply those, the bad guys can take advantage. Other things, an encryption policy. I might want the any of these devices that can hold data, which is probably all of them to have some sort of encryption policy so that I can make sure that if the device is lost or stolen, then nobody can get any information off of it because all the data that's on it is encrypted. Remote wipe capability? Again, in the",
      "case this mobile device maybe goes missing, someone loses it, it gets stolen, something like that, it would be really great if I could automatically wipe all of the data that's on this device and maybe do the same thing on some of these others as well. If I see that they have gone missing, I want to be able to blank all the data and do that remotely. Location tracking. In case I want to find one of these things, if it's gone. Now, some organizations may choose to not turn that on because these",
      "may be personal devices that are getting used. That's understandable. But the capability exists, certainly for the corporate devices to be able to track where those locations could be. Antivirus or endpoint detection and response. That is to make sure that I don't have malware on these systems. And then finally, what's my policy for disposing of these devices? These things don't last forever. The battery starts to die on this. We need to upgrade one of someone's laptop or something like that. How",
      "are we going to get rid of the device in a way that doesn't expose our information? Our security controls should take into account all of these things as well. Okay, now we've covered what are endpoints and what are the controls that we need for them. Now, let's take a look at BYOD. Well, what do I mean BYOD? It's bring your own device. But let me tell you, it's more complicated than that because a lot of people have figured out how to bring their own IT. And some people are even bringing their",
      "own cloud. So we've really got this whole collection of acronyms that we're having to kind of deal with. And the endpoint is just part of this when it comes to these bring your own programs. I'm going to tell you there's really two types of organizations out there. There are the ones who have a well-defined program. The ones who have a poorly defined program. And then there's another group that claims that's not allowed. So I'm going to tell you their program is actually an unsanctioned program.",
      "It comes back and really maps to just being one of these. So it's a poorly defined program. In other words, there really is no third category. Everybody is either a well-defined program when it comes to these things or a poorly defined program. So what would it be if we were to have a well-defined program? Because this obviously is not going to do our security needs. If the security organization says \"no\", the end user will say \"how\" and they will do it this way. Better if we define it in advance",
      "for them this way. What are some of the elements that would go into this? First of all, consent, especially if the person is bringing their own device that could have their own data. It's their property. So we need to make sure that they understand what are the rules. What are the things that we're going to put on your system? What are the things we're likely to do to your system with your system, that sort of thing. So we've got to have consent from the end user who owns this thing. And we're",
      "going to tell them, for instance, \"Oh yeah, we're going to monitor certain of your usage or not.\". Your policy should state whether you're monitoring their usage or not and under what conditions you might do that. We're going to look at how you're using your system. We're going to look and see if you're using it. It may be just the corporate things that you're doing and monitor only that. And then we might also want to reserve the right to remotely wipe the device and remove all corporate data.",
      "Now, we can do a selective wipe so that I remove only the corporate data and not all of the personal data. So if this person has a mobile mobile phone and they've taken family photos on their vacation, that stuff doesn't go away. But all the corporate data goes away. If they report the device as lost or stolen or they leave the organization. So I need that kind of capability. I might also specify what levels of software are required. As I mentioned in the previous section, the version, the",
      "current, the N, and the N minus one. But I might also get down to certain applications and say there are certain applications that are required, certain things that must be on your system. If it's a mobile device, there might be different kinds of things that would be required than if it was, say, a desktop device where maybe I'm going to be requiring antivirus that I might not require on a mobile device. There's other things that I might say. There are some applications you should never have. We",
      "don't want you having this on on your device and we're going to check for it and if we see it on there, we're going to report you or we're going to remove our data from your system, because we believe these devices or these applications, I should say, are going to make our data vulnerable or they're going to expose us to certain other types of threats. Then from a hardware perspective, an organization may very well say, we're only going to support you bringing in your device, but it has to be of",
      "a certain hardware configuration. We can't support every single device that anyone might ever come up with. So we're going to say \"This is the type of desktop, laptop, mobile device that we're going to support. We're going to support only those and not more.\". But we need to be able to specify what that is as well the services that you're going to use from these devices. Cloud's a good example. So I might use only authorized services. If I want to do file sharing, then the organizations and say",
      "we have a cloud-based file sharing program and everyone needs to use that. Don't use all of these others. And we're going to monitor and make sure that that's what you're using, for instance. So those are examples. Again, it's best not to say no. It's better to say how. And if I can say how on these kinds of things, I can guide the users to do the right thing. Always remember if we make it easier to do the wrong thing than it is to do the right thing, the users are going to basically do the wrong",
      "thing. So we want to make it easier and enable that. Okay, we've talked about endpoint security and covered that. In the next video, we're going to talk about network security. In case you've missed any in the series, take a look here. And if you want to make sure you don't miss any in the future, make sure to click, subscribe and notify so that you'll be notified when the next video comes out."
    ],
    "chunk_count": 30,
    "content_id": "6236337e-78d9-45b1-96c6-98cb4ec0dd36",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555017"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=UpkqXK0B2E0": {
    "title": "Cybersecurity Architecture:  Data Security",
    "url": "https://www.youtube.com/watch?v=UpkqXK0B2E0",
    "description": "IBM Security QRadar EDR : https://ibm.biz/BdyJLn\n\nIBM Security X-Force Threat Intelligence Index 2023: https://ibm.biz/BdyJLp\n\nMany companies refer to their data as the \"crown jewels\" for a reason - it represents one of a company's most valuable assets. That's also why preventing a data breach should be at the top of your security priorities. In this video, IBM Distinguished Engineer and Adjunct Professor lays out the concerns of data security and explains the processes you should have in place to discover, secure, and govern your data across your enterprise.\n\n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-up\n\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n#AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume #datasecurity",
    "duration": 887,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en 4.35 million. 9.44 million. 83%. What do those numbers have to do with data security? I'll tell you in a minute. Welcome back to the Cyber Security Architecture series, where in the first few videos we talked about principles, fundamentals, about the various domains of cybersecurity. We covered identity and access. We looked at the endpoint, the network, applications in the last video. Today, we're going to talk about data security, which is what these numbers have to do with. So the first one, 4.35 million. Well, according to the Ponemon Institute, that's the average cost of a data breach worldwide. That's a lot of money every time somebody breaks in, that's what it's costing the organization on average. And by the way, that's taking out some of the really large numbers that might have skewed the average. So this is the baseline that we're looking at, 9.44 million--cost of a data breach in the US. Twice as much, if you're in the US, if you get hit with this. And 83%, what does that have to do with anything? That is the number of organizations that have been hit by more than one data breach. So this is answering the question \"Why do we need to care about data security?\" Now we're going to talk about in the rest of the video about some of these aspects, about governance, about discovery, about protection, compliance, detection, response. Hang around to the end. And I'm going to tell you the top five things that this survey found will reduce the cost of a data breach for you. So stay tuned for that. First of all, we're going to cover governance and discovery. So think of this as a data security ecosystem. We're going to fill this thing out. These are the technologies that go into allowing us to secure our data. First of all, we have to have some sort of governance plan. This is our way of saying this is what we want to do. If I don't define where the finish line is, I can expect people to end up there. So first of all, we're going to have a policy, a data security policy, and in that, we're going to spell out what kinds of things are sensitive and what kinds of things aren't. We're going to spell out a classification criteria that says this is keys to the kingdom and this is the lunchroom menu. Nobody needs to protect that. We don't really care, that sort of thing. We need to spell out that criteria. We need to also spell out what kinds of protections go with those different levels of sensitivity. So we may have confidential, unclassified. We may have super confidential, super top secret, super duper top secret, whatever your classification scheme is. But you need to spell out what those are. We can't expect the users in our company to protect the data if we haven't spelled out what those guidelines are. So it all starts with this governance policy classification. Build a catalog that tells us this is where the data is. This is where the really sensitive stuff is, and you want to know where that is all the time. We're going to be updating that as well. And then build a resilience plan that says if we lose data, how do we recover? So we'll talk a little more about that later as well. So that's the first stage is build the plan, then we're going to move into discovery. Now, I've said this is what I want to do to cover our data and protect it. Now let's go find where the data is. A couple of different places that we can look for data types--databases. That's where we expect that's going to be structured data in most cases. That's we're going to see a lot of the keys to the kingdom. But don't overlook this other area. Files, emails, spreadsheets, all kinds of other things that make up unstructured data. We need to be able to discover sensitive content in unstructured data as well. In other words, in places where we might not otherwise look. We expect the keys to the kingdom to be in the very sensitive database. But what if somebody makes a copy of that? What if somebody excerpts parts of it and emails it to someone? Now we've got sensitive data flying around a lot of other places, and I'd like to look on the network and see as this data is moving around as well, and maybe discover when there are certain types of issues that we could be running into. And one of those discovery things will help us in particular with a thing called data loss protection. So DLP is a technology that's important that allows us to discover this stuff in real time on various systems and as data is flowing across our networks. Okay. Now, we've covered the why of data security. These numbers down here should be sufficient motivation for you. How do we do governance and discovery, some of the general technologies there. Now, let's talk about protection and compliance. I figured out how I need to secure things and where the stuff is. Now how am I going to actually protect it? Well, encryption is a huge part of that. That is, I scramble the data so that only I can read it and the bad guys can't. And I need to be able to do that for data at rest. Like sitting in a database, for instance, or data at motion. Data that's in motion could be moving from one user to another, could be on a web server, and then being served up over across the Internet. So there's I wouldn't need to be able to keep the data secure all the time at rest and in motion if I'm going to be encrypting data. The other thing I need to be able to do a really good job of is managing the keys. If you lose the keys, you lose the data. So remember that we want to generate keys in a random way. If anyone can predict the way keys are generated, then all bets are off. They can read all your data. So it has to be done randomly and we need to be able to keep it. We also need to be able to keep up with the lifecycle of a key. That is, don't encrypt at once and forget it. It's not encrypt and forget, it's encrypt and then continuously follow the lifecycle and re-encrypt at appropriate times. We have keys that we rotate in and we rotate out. Another aspect of all this that you need to be paying attention to is this notion of quantum-safe crypto. Quantum computers are going to be able to break all of our existing cryptography in the next number of years. We don't know exactly when, but it's not that far off. And so we're going to need to keep an eye on this space right here so that we use new algorithms that will keep our data safe against the quantum threat that would be out there if someone had a quantum computer and started trying to crack all of our data that we had previously secured. Access control is another part of data security. Access control is also part of identity and access management, which was the first of the seven domains that we covered. But it relates here as well, because it doesn't matter how strong I've encrypted the information. If a user that has access to this set their password to the word \"password\", then it doesn't matter because they're getting in. If we don't have a good way of authenticating and authorizing users, then all of this strong crypto isn't going to matter. So access controls need to be there. Backup. If we face in particular a disaster recovery scenario or a ransomware type scenario where someone says, I've got your data and I'm not giving it back. Well, the best defense against that is saying, guess what, ransomware guy. I've also got a copy of my data and I'll just restore it and you can go pound sand. So this is the type of protection that we need not only to keep the data from prying eyes, but also keep the data so that we have a level of resilience that I mentioned earlier in case of attack or disaster or things of that sort. Also, I need to be able to ensure that I am complying to some of the industry regulations that might exist. The Generalized Data Protection Regulation in Europe, GDPR we have in the US, HIPAA for health care information. There are lots of regulations in lots of industries and lots of parts of the world. It's essentially, if you've got any information about somebody, there's a chance that you have would be subject to one of these regulatory requirements. So I need to be able to report on am I complying with that? Just as much, I need to be able to report when I'm not complying because if I don't know that, then one of the costs that goes into these data breaches are the fines that go along with it and with GDPR in particular. It's very substantial. And if you think because you're operating a company that's not in the in Europe, in the European Union, that you're exempt from this. Think again. If you've got European citizen data, even if you don't operate there, you could potentially be subject to that. Check with your lawyers to find out. And then a policy. I need an ability to retain records, but only as long as is necessary according to the law or other regulation. It really doesn't do us good to keep a lot of information and just store everything forever. So what we need to do, though, is the law will require that we store data for a certain period of time and we want to store it for that long and probably not longer because it gets more expensive. And the longer we're holding this data, the longer we're holding the burden that if in fact, we have one of these breaches, we could be held liable. So it's best not to have it in the first place if you don't really need it. Okay. Now, we've covered the first bunch of security protections that are needed about governance, about discovery and things of that sort. Now we're going to take a look at two more-- detection and response. Security is about prevention, detection and response. These first ones are mostly about the prevention. This is about the detection and response. Detection. Now, what does that involve? Well, I need to be able to monitor my systems and see how data is being used. How is it moving around my organization, who's using it, under what conditions? And in fact, I also may want to use a technology called user behavior analytics (UBA) that will monitor users and the way that they're looking at the data, if they're downloading normally a thousand files a day and suddenly they start downloading a million files in a day, then that could be suspicious. Or if someone who normally doesn't access certain data starts having a lot of interest in that data, then that could be an issue. If someone is doing something different than their peer group, then that could be an issue. So that's what user behavior analytics is looking for, those kind of cases. So we're trying to detect misuse and abuse of the data. And ultimately, I'm going to generate alerts that are going to go up to some console and someone then can go take an action on that. And the action is our response part. So one of the things I might do is open a case and then assign that case to someone and then they begin an investigation. We might guide all of those efforts with something called a dynamic playbook. A dynamic playbook would tell you, this is what just happened. Now you need to do the following steps. And based upon the results of those steps, I'm going to specify other steps for you to follow. So that's the dynamic nature of it, as opposed to just a a hardcoded script that someone follows. This is more like a dynamic script that they can follow. But it leads people through and allows them to automate the recovery and orchestration and automation of these problems. What's the difference between those two? In a perfect world, I'd automate all of my responses. But we don't live in a perfect world because we see a lot of these things that are first of a kind type of situations. A first of a kind, I can't automate because I don't know what I should have done because I've never seen it before. Orchestrate is what I have to do in a lot of these cases where basically I'm looking for certain things and I'm providing guidance, but like an orchestra, the conductor of the orchestra is conducting who's going to come in, when do the trumpets come in, when do the the saxophones come in. This sort of thing. They're going to basically direct all of this, and that's what we're going to do in our response. So think about those things. We're going to talk about these actually in more detail in our next two videos where we're going to talk about security monitoring and security response. Okay. Now we've done a quick flyover view of data security. We started off talking about governance, setting the plan and saying this is what we intend to do. And if we don't have that right, we can't expect to do the other parts right. Then we move into discovery, find out where all the data is that we need to apply those policies to. Then we're going to put those protections in place. Then we're going to check our compliance and see if, in fact we're doing what we intended to do. We're going to look for anomalies. Then we're going to respond when we find those and feed that information back into our policy. So the whole thing then becomes this ecosystem of data security, and it involves a lot of different technologies. So that's in general how we want to do this. And now what I told you is if you would stay to the end, I'm going to tell you the top five things and you can see them here already. So no need for a drum roll. But according to the cost of data breach survey, these were the top five things that reduced the cost of a data breach. Number one, I using artificial intelligence was top on the list. And in fact we started using AI a good deal already in this detect phase. You'll hear more about that in the future videos as we as we cover that in the next one. But you can expect to see AI be infused in all of these different spaces. So look for that moving forward. DevSecOps. That was a big part of the discussion in the previous video on application security. If you missed that, make sure you go back and check that out. But that breaks down the walls between dev[elopment], sec[urity] and ops organizations and put security in a shift left position. Incident response. We talked about that here. That's this response capability and it's going to be the subject of our video two videos from now. So again, stay tuned for that. Cryptography. We've talked about that here in this particular space. And as I said, if you can't encrypt the data, you can't protect it. And then ultimately employee training, because at the end of the day, it's not all about the technology. There's an end user and the user, the human, is almost always the weakest link in any security system. So don't ignore this part. If you do, it's at your own peril. Okay. There we go. Those are the top five things that you can do to reduce the cost of a data breach, to reduce the likelihood that it ever happens to you in the first place. Make sure you have a good plan for doing all of these things. All right. Now, in the next video, we will cover monitoring and then the following video security response. So make sure you stay tuned to check those out--[click] like, subscribe, and hit the notify button so you'll know and not miss any videos in the series.",
    "chunks": [
      "Kind: captions Language: en 4.35 million. 9.44 million. 83%. What do those numbers have to do with data security? I'll tell you in a minute. Welcome back to the Cyber Security Architecture series, where in the first few videos we talked about principles, fundamentals, about the various domains of cybersecurity. We covered identity and access. We looked at the endpoint, the network, applications in the last video. Today, we're going to talk about data security, which is what these numbers have to",
      "do with. So the first one, 4.35 million. Well, according to the Ponemon Institute, that's the average cost of a data breach worldwide. That's a lot of money every time somebody breaks in, that's what it's costing the organization on average. And by the way, that's taking out some of the really large numbers that might have skewed the average. So this is the baseline that we're looking at, 9.44 million--cost of a data breach in the US. Twice as much, if you're in the US, if you get hit with this.",
      "And 83%, what does that have to do with anything? That is the number of organizations that have been hit by more than one data breach. So this is answering the question \"Why do we need to care about data security?\" Now we're going to talk about in the rest of the video about some of these aspects, about governance, about discovery, about protection, compliance, detection, response. Hang around to the end. And I'm going to tell you the top five things that this survey found will reduce the cost of",
      "a data breach for you. So stay tuned for that. First of all, we're going to cover governance and discovery. So think of this as a data security ecosystem. We're going to fill this thing out. These are the technologies that go into allowing us to secure our data. First of all, we have to have some sort of governance plan. This is our way of saying this is what we want to do. If I don't define where the finish line is, I can expect people to end up there. So first of all, we're going to have a",
      "policy, a data security policy, and in that, we're going to spell out what kinds of things are sensitive and what kinds of things aren't. We're going to spell out a classification criteria that says this is keys to the kingdom and this is the lunchroom menu. Nobody needs to protect that. We don't really care, that sort of thing. We need to spell out that criteria. We need to also spell out what kinds of protections go with those different levels of sensitivity. So we may have confidential,",
      "unclassified. We may have super confidential, super top secret, super duper top secret, whatever your classification scheme is. But you need to spell out what those are. We can't expect the users in our company to protect the data if we haven't spelled out what those guidelines are. So it all starts with this governance policy classification. Build a catalog that tells us this is where the data is. This is where the really sensitive stuff is, and you want to know where that is all the time. We're",
      "going to be updating that as well. And then build a resilience plan that says if we lose data, how do we recover? So we'll talk a little more about that later as well. So that's the first stage is build the plan, then we're going to move into discovery. Now, I've said this is what I want to do to cover our data and protect it. Now let's go find where the data is. A couple of different places that we can look for data types--databases. That's where we expect that's going to be structured data in",
      "most cases. That's we're going to see a lot of the keys to the kingdom. But don't overlook this other area. Files, emails, spreadsheets, all kinds of other things that make up unstructured data. We need to be able to discover sensitive content in unstructured data as well. In other words, in places where we might not otherwise look. We expect the keys to the kingdom to be in the very sensitive database. But what if somebody makes a copy of that? What if somebody excerpts parts of it and emails it",
      "to someone? Now we've got sensitive data flying around a lot of other places, and I'd like to look on the network and see as this data is moving around as well, and maybe discover when there are certain types of issues that we could be running into. And one of those discovery things will help us in particular with a thing called data loss protection. So DLP is a technology that's important that allows us to discover this stuff in real time on various systems and as data is flowing across our",
      "networks. Okay. Now, we've covered the why of data security. These numbers down here should be sufficient motivation for you. How do we do governance and discovery, some of the general technologies there. Now, let's talk about protection and compliance. I figured out how I need to secure things and where the stuff is. Now how am I going to actually protect it? Well, encryption is a huge part of that. That is, I scramble the data so that only I can read it and the bad guys can't. And I need to be",
      "able to do that for data at rest. Like sitting in a database, for instance, or data at motion. Data that's in motion could be moving from one user to another, could be on a web server, and then being served up over across the Internet. So there's I wouldn't need to be able to keep the data secure all the time at rest and in motion if I'm going to be encrypting data. The other thing I need to be able to do a really good job of is managing the keys. If you lose the keys, you lose the data. So",
      "remember that we want to generate keys in a random way. If anyone can predict the way keys are generated, then all bets are off. They can read all your data. So it has to be done randomly and we need to be able to keep it. We also need to be able to keep up with the lifecycle of a key. That is, don't encrypt at once and forget it. It's not encrypt and forget, it's encrypt and then continuously follow the lifecycle and re-encrypt at appropriate times. We have keys that we rotate in and we rotate",
      "out. Another aspect of all this that you need to be paying attention to is this notion of quantum-safe crypto. Quantum computers are going to be able to break all of our existing cryptography in the next number of years. We don't know exactly when, but it's not that far off. And so we're going to need to keep an eye on this space right here so that we use new algorithms that will keep our data safe against the quantum threat that would be out there if someone had a quantum computer and started",
      "trying to crack all of our data that we had previously secured. Access control is another part of data security. Access control is also part of identity and access management, which was the first of the seven domains that we covered. But it relates here as well, because it doesn't matter how strong I've encrypted the information. If a user that has access to this set their password to the word \"password\", then it doesn't matter because they're getting in. If we don't have a good way of",
      "authenticating and authorizing users, then all of this strong crypto isn't going to matter. So access controls need to be there. Backup. If we face in particular a disaster recovery scenario or a ransomware type scenario where someone says, I've got your data and I'm not giving it back. Well, the best defense against that is saying, guess what, ransomware guy. I've also got a copy of my data and I'll just restore it and you can go pound sand. So this is the type of protection that we need not",
      "only to keep the data from prying eyes, but also keep the data so that we have a level of resilience that I mentioned earlier in case of attack or disaster or things of that sort. Also, I need to be able to ensure that I am complying to some of the industry regulations that might exist. The Generalized Data Protection Regulation in Europe, GDPR we have in the US, HIPAA for health care information. There are lots of regulations in lots of industries and lots of parts of the world. It's",
      "essentially, if you've got any information about somebody, there's a chance that you have would be subject to one of these regulatory requirements. So I need to be able to report on am I complying with that? Just as much, I need to be able to report when I'm not complying because if I don't know that, then one of the costs that goes into these data breaches are the fines that go along with it and with GDPR in particular. It's very substantial. And if you think because you're operating a company",
      "that's not in the in Europe, in the European Union, that you're exempt from this. Think again. If you've got European citizen data, even if you don't operate there, you could potentially be subject to that. Check with your lawyers to find out. And then a policy. I need an ability to retain records, but only as long as is necessary according to the law or other regulation. It really doesn't do us good to keep a lot of information and just store everything forever. So what we need to do, though, is",
      "the law will require that we store data for a certain period of time and we want to store it for that long and probably not longer because it gets more expensive. And the longer we're holding this data, the longer we're holding the burden that if in fact, we have one of these breaches, we could be held liable. So it's best not to have it in the first place if you don't really need it. Okay. Now, we've covered the first bunch of security protections that are needed about governance, about",
      "discovery and things of that sort. Now we're going to take a look at two more-- detection and response. Security is about prevention, detection and response. These first ones are mostly about the prevention. This is about the detection and response. Detection. Now, what does that involve? Well, I need to be able to monitor my systems and see how data is being used. How is it moving around my organization, who's using it, under what conditions? And in fact, I also may want to use a technology",
      "called user behavior analytics (UBA) that will monitor users and the way that they're looking at the data, if they're downloading normally a thousand files a day and suddenly they start downloading a million files in a day, then that could be suspicious. Or if someone who normally doesn't access certain data starts having a lot of interest in that data, then that could be an issue. If someone is doing something different than their peer group, then that could be an issue. So that's what user",
      "behavior analytics is looking for, those kind of cases. So we're trying to detect misuse and abuse of the data. And ultimately, I'm going to generate alerts that are going to go up to some console and someone then can go take an action on that. And the action is our response part. So one of the things I might do is open a case and then assign that case to someone and then they begin an investigation. We might guide all of those efforts with something called a dynamic playbook. A dynamic playbook",
      "would tell you, this is what just happened. Now you need to do the following steps. And based upon the results of those steps, I'm going to specify other steps for you to follow. So that's the dynamic nature of it, as opposed to just a a hardcoded script that someone follows. This is more like a dynamic script that they can follow. But it leads people through and allows them to automate the recovery and orchestration and automation of these problems. What's the difference between those two? In a",
      "perfect world, I'd automate all of my responses. But we don't live in a perfect world because we see a lot of these things that are first of a kind type of situations. A first of a kind, I can't automate because I don't know what I should have done because I've never seen it before. Orchestrate is what I have to do in a lot of these cases where basically I'm looking for certain things and I'm providing guidance, but like an orchestra, the conductor of the orchestra is conducting who's going to",
      "come in, when do the trumpets come in, when do the the saxophones come in. This sort of thing. They're going to basically direct all of this, and that's what we're going to do in our response. So think about those things. We're going to talk about these actually in more detail in our next two videos where we're going to talk about security monitoring and security response. Okay. Now we've done a quick flyover view of data security. We started off talking about governance, setting the plan and",
      "saying this is what we intend to do. And if we don't have that right, we can't expect to do the other parts right. Then we move into discovery, find out where all the data is that we need to apply those policies to. Then we're going to put those protections in place. Then we're going to check our compliance and see if, in fact we're doing what we intended to do. We're going to look for anomalies. Then we're going to respond when we find those and feed that information back into our policy. So the",
      "whole thing then becomes this ecosystem of data security, and it involves a lot of different technologies. So that's in general how we want to do this. And now what I told you is if you would stay to the end, I'm going to tell you the top five things and you can see them here already. So no need for a drum roll. But according to the cost of data breach survey, these were the top five things that reduced the cost of a data breach. Number one, I using artificial intelligence was top on the list.",
      "And in fact we started using AI a good deal already in this detect phase. You'll hear more about that in the future videos as we as we cover that in the next one. But you can expect to see AI be infused in all of these different spaces. So look for that moving forward. DevSecOps. That was a big part of the discussion in the previous video on application security. If you missed that, make sure you go back and check that out. But that breaks down the walls between dev[elopment], sec[urity] and ops",
      "organizations and put security in a shift left position. Incident response. We talked about that here. That's this response capability and it's going to be the subject of our video two videos from now. So again, stay tuned for that. Cryptography. We've talked about that here in this particular space. And as I said, if you can't encrypt the data, you can't protect it. And then ultimately employee training, because at the end of the day, it's not all about the technology. There's an end user and",
      "the user, the human, is almost always the weakest link in any security system. So don't ignore this part. If you do, it's at your own peril. Okay. There we go. Those are the top five things that you can do to reduce the cost of a data breach, to reduce the likelihood that it ever happens to you in the first place. Make sure you have a good plan for doing all of these things. All right. Now, in the next video, we will cover monitoring and then the following video security response. So make sure",
      "you stay tuned to check those out--[click] like, subscribe, and hit the notify button so you'll know and not miss any videos in the series."
    ],
    "chunk_count": 31,
    "content_id": "960b20d4-0d23-4ac4-a2ad-079512bab2d0",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555020"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=VEu326IZpsc": {
    "title": "Cybersecurity Architecture: Detection",
    "url": "https://www.youtube.com/watch?v=VEu326IZpsc",
    "description": "IBM Security QRadar EDR : https://ibm.biz/BdyQeU\n\nIBM Security X-Force Threat Intelligence Index 2023: https://ibm.biz/BdyQbx\n\nSecurity is about prevention, detection, and response. In this installment, IBM Distinguished Engineer and adjunct professor Jeff Crume explains the tools that are key to detecting an attack as soon as possible. He also clarifies the often-confused distinction between SIEM and XDR systems by delving into their historical usage, noting they're not either/or choices, but complementary tools that experienced cybersecurity analysts use with the help of federated search capabilities.\n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-up\n\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume #detection",
    "duration": 1030,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Here's a formula for you to remember. S equals P plus D plus R. What does that mean? Security is about prevention, detection and response. Remember the CIA triad I mentioned in the second video of this series? It's about confidentiality, integrity and availability. And I said everything we do in security is about trying to achieve one or more of those things. Well, that's kind of the “what” of cybersecurity. This is what we're trying to do. This equation is the “how”. This is how we're going to go about doing that. And that is with prevention, detection and response. Now, what we've covered as we've gone through the domains up to this point: Identity and access management. Endpoint security, network security, application security and data security. This is all largely been about prevention. Not 100%, but mostly with the controls that we put in place down here are about trying to prevent a data attack, a breach, any of those kinds of things. So that's what that's about. Now we're going to start looking at the other parts of the equation. Today in particular, we're going to focus on the D part of this-- this detection aspect. And then in the next video, we'll cover response. Now, how do we do detection? Well, it basically means I need to get information from all of these different domains that I've been discussing previously and feed them into some sort of monitoring engine. So that's what we're going to be taking a look at. Monitoring. Then we're going to analyze. Then we're going to report. And then we're going to do this thing called threat hunting. This is what the purview of this area is. And then in that final video, we'll take a look at the response. What do I do with all of this information once I realize that I've got a problem? Now, these two functions are largely done by an organization called the SOC, the Security Operations Center. So bear in mind, this is kind of all coming together in that one organization that's going to do that particular work. And what are the technologies that they're used to do, this kind of detection, to do this kind of work? Well, it's basically two predominant things. It's a security information and event management system or an XDR, an extended detection and response system. We're going to take a look at both of those as we go through this video. Okay. Now, we've introduced this idea of detection. Let's go into it a little bit deeper. And specifically, we're going to start off talking about this thing. The security information and event management system or SIEM. Some people pronounce it “seam”, you can choose to pronounce it however you like. So I'll call it a SIEM. What is a SIEM do? What's its purpose? Well, it's if you think of this way, we look at all the different domains that we've talked about in the past. Each one of these could be a source of security information. And in fact, what typically happens this is not best practice-- it’s typical practice --is I have a console some sort of security management system that is unique to that particular domain. The identity management security console, the access management security console, the endpoint management console and so on and so forth. In fact, I've got multiples of those consoles and really multiples of the individuals, the security operators, analysts that have to deal with those particular domains and have that specific domain knowledge. You can see this is very expensive. And also, what else is missing here? I don't have any consistent single view of what's happening. So the left hand doesn't know what the right hand is doing. And if an attacker comes in and hits one of these systems, it may generate alarms in lots of these systems. And then we've got a lot of people chasing all the same problem. It's not very efficient. So this is why the SIEM came into existence. Its purpose then was to say, instead of operating all of these things independently, let's come along with a layer on top of them where we take all of these information systems, feed it into a higher level system. That's our SIEM. So we create a database up here that is about this collection stuff. We're going to collect logs, we're going to collect alarms and events that occur, and we're going to collect flow data that goes across the network. So each one of these systems would be able to give us different types of information. We take all of that and we bring that up to the SIEM-- big database, and then we start applying some analytics to it. One of the things we're going to do is correlate because as I mentioned, a single attack might generate an alarm in multiples of these domains and across multiples of these systems. So what I'd like to be able to do is not see this as four different events or four different alarms. I want to see this as a single. So one of the first things the SIEM will do after it's collected is correlate all that information and get it down to a smaller, more manageable subset. Another thing we're going to do then is start analyzing the information we have. We're going to take a look at, for instance, rules that are based upon our security policies. I might say if a certain condition, such as traffic coming from a particular geo and then it meets some other criteria, like someone tries to log in too many times or something like that and some other criteria. So we can start building these very complex rules. And a good SIEM will have a lot of these that already come out of the box. But I can also customize them and build all these rules. Then if all of those things happen, I want to take a specific action. I want to generate an alarm. I want it to be of a certain priority. I want it to be assigned to a particular person. And so ultimately, what I'm going to do with those priorities is I'm going to assign them as well as high, medium and low. The SIEM system ought to be able to do that. And I could do that from a rule or I could have the system do that automatically based upon its confidence level, based upon calculations that it's done and things of that sort. Another thing in the analysis I want to do is look for anomalies. So these are things where I know I'm looking for a specific use case, a specific set of examples, a specific set of indicators of compromise. And when I see those, then I know I've got a problem, or at least I know I have a high probability there's a problem and someone should investigate. In this case, I may be looking to say, just tell me if something looks weird. I don't know what weird is. You figure it out. And so this is where things like artificial intelligence, in particular machine learning is particularly good because it can find patterns that we might not otherwise find. Feed it tons of this kind of information, and then tell it to look for what's the anomaly. And a particular technology we do we use to do this is called user behavior analytics. So UBA might leverage these underlying technologies as a way to find what's the thing that doesn't belong. Why is this user doing something different than all of his peers? Or why is certain things happening at certain times when we don't expect them to happen? So that's looking for the anomalies. Here we say, this is what I'm looking for; here I say, tell me for something that I don't really know what I'm looking for. And then ultimately I look for trends. I want to see because I want to generate reports to management to say, because remember, this whole organization I mentioned in the previous portion is the SOC, the security operations center. And the SOC wants to know, are we doing better this month than we were last month? Are we detecting more alarms? Are we not? Are we resolving those more quickly? And so the reporting of all of this, it would be important to know, These are some of the major functions then of a SIEM. It's about trying to reduce the footprint that we have down here and give us a single point where I can look and see the visibility of all of my systems, gather all of that information, bring it up and do these kinds of analysis activities. Okay, we've just covered the SIEM. Now let's take a look at the other technology I mentioned, XDR. This is extended detection and response. Let's do a little compare and contrast of these two different technologies so we can see how they fit together. Is it really SIEM versus XDR or not? We'll see. So, first of all, SIEMs, these again came into existence-- Largely, the vendors that did these came from one of two different camps. They were either log management vendors and they would the idea was they take the system logs from all the different devices, operating systems, databases, applications, and manage all of those things and bring them up to some centralized database. And then we do the analysis I mentioned previously. Or they were focused on the network side of things. So it's network behavior, anomaly detection, this kind of technology. Most of the SIEM vendors came either from the log management or the network management view of security, and the SIEM was designed to basically be able to reach across both of those. Well, the SIEMs could always do more than that, but that was where they traditionally came from. How about this newer technology called XDR, extended detection response? It grew out of a thing called endpoint detection and response. So we already talked about how we did detection and response here in the SIEM. The idea here was we're taking most of the information up. It was kind of a bottoms up approach. With the XDR, it's really more of a top down. And here's what I mean. What we would do with an EDR system is we would actually install some kind of capability, some kind of agent on each one of these systems, and that would sit there and would do detection and would do a certain level of response. And the idea here is we're pushing the the actions down. It's more of a instead of a “let's bring everything up and then take action”, let's do as much as we can and automate the response there on the platform as close to the source of the outage, as close to the source of the attack as we can make it. And that's what this did. With XDR, though we still need an ability to bring this information up. And this would be from servers, from desktops, from laptops. Those are the systems that we're trying to enforce policy on and look for anomalous behavior and things of that sort. So the EDR capabilities basically needed a way to report up and so that they could all give a more comprehensive view. And that's really where XDR came into existence, was to do those kinds of things. Now, it turns out you could take an XDR system and read all of these endpoint devices into it. You could actually even take the information from the SIEM and forward it into an XDR, just as you could have taken the endpoint information and fed it to the SIEM. So there's a lot of different ways that you can make these things work. But what's really interesting is that some vendors have come up with this idea is we'll keep both of these here, but we're going to add a capability here to the XDR that's called Federated Search. And what Federated Search does is it says, I want to look for particular indicators of compromise or particular incidents, particular alerts, particular conditions. And I'm going to take those and I'm going to say I'm going to query all my systems and say, do any of you have these kind of conditions happening on your system right now? The advantage to that is I don't need all of the data pre-fetched and stored in advance in some big database. I go out and get it just in time. So we leave the data in place and then we go out and gather it just as we need it. And a federated search basically tells each one of these systems, search your local database of information and see if there's a problem that matches the specific conditions that I'm spelling out. And if you have that, then report this back up. And that's the way these things work. It's a lot like the card game that a lot of kids play called Go Fish, where you say, does anybody have any threes? And everyone looks in their hand to see if they have any three cards. And if they do, then they have to turn that in. It's the same thing here. We're saying everyone run a search on your system locally and then only report the results. It's much more efficient, but in fact, we kind of need both of these because the SIEM is particularly good at doing alarms since all the information is coming up. But what we want to do is have high quality alarms. We don't want to just have tons and tons of information there. SIEMs tend to get more expensive the more information you feed into them. XDRs get around that problem by saying leave most of the data here and I'll go fetch it just in time. But still, the XDR operator needs to know that there's a reason they need to go out and look in the first place. So an alarm coming in from a SIEM might be a trigger to then cause an investigation to occur. So it's really not XDR versus SIEM. I want to leave you with the point that it's XDR plus. SIEM. These two work together and can complement each other and be part of a stronger security response. Okay. Now, we've talked about the SIEM and XDR technologies, which basically allow us to monitor, to analyze and report on the stuff that we see happening in our environment. Now let's talk about hunting. What is hunting about? Well, the reason we want to do this in the first place is this is an attack scenario, a timeline. And the first thing the bad guy does is reconnaissance. They basically check out your site, they case the joint. They try to figure out where your weak points are. So they're going to spend some time doing that initially. Then, according to Ponemon Institute’s Cost of a Data Breach survey, there's a delay in time until we have the mean-time-to-identify (MTTI). In other words, the guy attacks me at this point after he's finished his reconnaissance, he goes in. Now, how long does it take before the organization is aware that they've been attacked? Well, it turns out this is on the order of 200 days. That's a huge problem, because imagine if a bad guy was in your house for 200 days before you realized that you had been broken into. And then the mean-time-to-contain (MTTC), that is, after I am aware that there's a problem, how long before I actually have it fixed? Now we're taking a look at it about another 70 days. You put those two together, 270 days. It's the better part of a year that the since when you were attacked until you finally have recovered from all of this. So what would we like to be able to do in this? I'd like to be able to move awareness back earlier into this. If I can't completely prevent the attack, at least become aware of it sooner. And the way we do that is with threat hunting. Now, threat hunting, as compared to a basic investigation. With an investigation, we're reacting. So the system is giving me an alarm, a guy has broken in and now I'm doing the forensic investigation to find out what happened. That's what we typically do with SIEM and XDR tools. But there's something else we could do, and that's this idea of threat hunting, where I'm going to be more proactive. I'm going to basically use the skills, the experience and the instincts of a skilled cybersecurity analyst who has seen everything-- hopefully --and kind of comes up with what is essentially a hypothesis. They say, I wonder if someone has done this or that or the other thing. We don't have an alarm yet. No one has told us that we've been broken into. But I want to get ahead of this before anyone even allows the alarm to be sounded. So they develop a hypothesis based upon their experience and their instincts about what would someone go after? How might they attack us? What kinds of things would they do? We're looking at the way other attackers are breaching networks and systems and using that in our hypothesis as well. And the threat hunter then uses tools like these, the SIEM and the XDR, to go off and look for searches and look for indicators of compromise. And if they do it correctly, what we end up with is early detection. We basically move the bar back. In a perfect world, we'd be able to detect future crimes and we'd arrest the bad guys before they even break in. But we don't live in that world. The next best thing we can do is try to find out as close to the attack as possible if we can't prevent it at all. Now, what we've done with this so far is we've talked about the detection aspect of all of these things. What we want to do in the final video in the series is talk about response. So make sure you don't miss it. Thanks for watching. Please remember to hit like and subscribe and don't miss the notify bell so that you don't miss any videos in this series.",
    "chunks": [
      "Kind: captions Language: en Here's a formula for you to remember. S equals P plus D plus R. What does that mean? Security is about prevention, detection and response. Remember the CIA triad I mentioned in the second video of this series? It's about confidentiality, integrity and availability. And I said everything we do in security is about trying to achieve one or more of those things. Well, that's kind of the “what” of cybersecurity. This is what we're trying to do. This equation is the “how”.",
      "This is how we're going to go about doing that. And that is with prevention, detection and response. Now, what we've covered as we've gone through the domains up to this point: Identity and access management. Endpoint security, network security, application security and data security. This is all largely been about prevention. Not 100%, but mostly with the controls that we put in place down here are about trying to prevent a data attack, a breach, any of those kinds of things. So that's what",
      "that's about. Now we're going to start looking at the other parts of the equation. Today in particular, we're going to focus on the D part of this-- this detection aspect. And then in the next video, we'll cover response. Now, how do we do detection? Well, it basically means I need to get information from all of these different domains that I've been discussing previously and feed them into some sort of monitoring engine. So that's what we're going to be taking a look at. Monitoring. Then we're",
      "going to analyze. Then we're going to report. And then we're going to do this thing called threat hunting. This is what the purview of this area is. And then in that final video, we'll take a look at the response. What do I do with all of this information once I realize that I've got a problem? Now, these two functions are largely done by an organization called the SOC, the Security Operations Center. So bear in mind, this is kind of all coming together in that one organization that's going to do",
      "that particular work. And what are the technologies that they're used to do, this kind of detection, to do this kind of work? Well, it's basically two predominant things. It's a security information and event management system or an XDR, an extended detection and response system. We're going to take a look at both of those as we go through this video. Okay. Now, we've introduced this idea of detection. Let's go into it a little bit deeper. And specifically, we're going to start off talking about",
      "this thing. The security information and event management system or SIEM. Some people pronounce it “seam”, you can choose to pronounce it however you like. So I'll call it a SIEM. What is a SIEM do? What's its purpose? Well, it's if you think of this way, we look at all the different domains that we've talked about in the past. Each one of these could be a source of security information. And in fact, what typically happens this is not best practice-- it’s typical practice --is I have a console",
      "some sort of security management system that is unique to that particular domain. The identity management security console, the access management security console, the endpoint management console and so on and so forth. In fact, I've got multiples of those consoles and really multiples of the individuals, the security operators, analysts that have to deal with those particular domains and have that specific domain knowledge. You can see this is very expensive. And also, what else is missing here?",
      "I don't have any consistent single view of what's happening. So the left hand doesn't know what the right hand is doing. And if an attacker comes in and hits one of these systems, it may generate alarms in lots of these systems. And then we've got a lot of people chasing all the same problem. It's not very efficient. So this is why the SIEM came into existence. Its purpose then was to say, instead of operating all of these things independently, let's come along with a layer on top of them where",
      "we take all of these information systems, feed it into a higher level system. That's our SIEM. So we create a database up here that is about this collection stuff. We're going to collect logs, we're going to collect alarms and events that occur, and we're going to collect flow data that goes across the network. So each one of these systems would be able to give us different types of information. We take all of that and we bring that up to the SIEM-- big database, and then we start applying some",
      "analytics to it. One of the things we're going to do is correlate because as I mentioned, a single attack might generate an alarm in multiples of these domains and across multiples of these systems. So what I'd like to be able to do is not see this as four different events or four different alarms. I want to see this as a single. So one of the first things the SIEM will do after it's collected is correlate all that information and get it down to a smaller, more manageable subset. Another thing",
      "we're going to do then is start analyzing the information we have. We're going to take a look at, for instance, rules that are based upon our security policies. I might say if a certain condition, such as traffic coming from a particular geo and then it meets some other criteria, like someone tries to log in too many times or something like that and some other criteria. So we can start building these very complex rules. And a good SIEM will have a lot of these that already come out of the box.",
      "But I can also customize them and build all these rules. Then if all of those things happen, I want to take a specific action. I want to generate an alarm. I want it to be of a certain priority. I want it to be assigned to a particular person. And so ultimately, what I'm going to do with those priorities is I'm going to assign them as well as high, medium and low. The SIEM system ought to be able to do that. And I could do that from a rule or I could have the system do that automatically based",
      "upon its confidence level, based upon calculations that it's done and things of that sort. Another thing in the analysis I want to do is look for anomalies. So these are things where I know I'm looking for a specific use case, a specific set of examples, a specific set of indicators of compromise. And when I see those, then I know I've got a problem, or at least I know I have a high probability there's a problem and someone should investigate. In this case, I may be looking to say, just tell me",
      "if something looks weird. I don't know what weird is. You figure it out. And so this is where things like artificial intelligence, in particular machine learning is particularly good because it can find patterns that we might not otherwise find. Feed it tons of this kind of information, and then tell it to look for what's the anomaly. And a particular technology we do we use to do this is called user behavior analytics. So UBA might leverage these underlying technologies as a way to find what's",
      "the thing that doesn't belong. Why is this user doing something different than all of his peers? Or why is certain things happening at certain times when we don't expect them to happen? So that's looking for the anomalies. Here we say, this is what I'm looking for; here I say, tell me for something that I don't really know what I'm looking for. And then ultimately I look for trends. I want to see because I want to generate reports to management to say, because remember, this whole organization I",
      "mentioned in the previous portion is the SOC, the security operations center. And the SOC wants to know, are we doing better this month than we were last month? Are we detecting more alarms? Are we not? Are we resolving those more quickly? And so the reporting of all of this, it would be important to know, These are some of the major functions then of a SIEM. It's about trying to reduce the footprint that we have down here and give us a single point where I can look and see the visibility of all",
      "of my systems, gather all of that information, bring it up and do these kinds of analysis activities. Okay, we've just covered the SIEM. Now let's take a look at the other technology I mentioned, XDR. This is extended detection and response. Let's do a little compare and contrast of these two different technologies so we can see how they fit together. Is it really SIEM versus XDR or not? We'll see. So, first of all, SIEMs, these again came into existence-- Largely, the vendors that did these came",
      "from one of two different camps. They were either log management vendors and they would the idea was they take the system logs from all the different devices, operating systems, databases, applications, and manage all of those things and bring them up to some centralized database. And then we do the analysis I mentioned previously. Or they were focused on the network side of things. So it's network behavior, anomaly detection, this kind of technology. Most of the SIEM vendors came either from the",
      "log management or the network management view of security, and the SIEM was designed to basically be able to reach across both of those. Well, the SIEMs could always do more than that, but that was where they traditionally came from. How about this newer technology called XDR, extended detection response? It grew out of a thing called endpoint detection and response. So we already talked about how we did detection and response here in the SIEM. The idea here was we're taking most of the",
      "information up. It was kind of a bottoms up approach. With the XDR, it's really more of a top down. And here's what I mean. What we would do with an EDR system is we would actually install some kind of capability, some kind of agent on each one of these systems, and that would sit there and would do detection and would do a certain level of response. And the idea here is we're pushing the the actions down. It's more of a instead of a “let's bring everything up and then take action”, let's do as",
      "much as we can and automate the response there on the platform as close to the source of the outage, as close to the source of the attack as we can make it. And that's what this did. With XDR, though we still need an ability to bring this information up. And this would be from servers, from desktops, from laptops. Those are the systems that we're trying to enforce policy on and look for anomalous behavior and things of that sort. So the EDR capabilities basically needed a way to report up and so",
      "that they could all give a more comprehensive view. And that's really where XDR came into existence, was to do those kinds of things. Now, it turns out you could take an XDR system and read all of these endpoint devices into it. You could actually even take the information from the SIEM and forward it into an XDR, just as you could have taken the endpoint information and fed it to the SIEM. So there's a lot of different ways that you can make these things work. But what's really interesting is",
      "that some vendors have come up with this idea is we'll keep both of these here, but we're going to add a capability here to the XDR that's called Federated Search. And what Federated Search does is it says, I want to look for particular indicators of compromise or particular incidents, particular alerts, particular conditions. And I'm going to take those and I'm going to say I'm going to query all my systems and say, do any of you have these kind of conditions happening on your system right now?",
      "The advantage to that is I don't need all of the data pre-fetched and stored in advance in some big database. I go out and get it just in time. So we leave the data in place and then we go out and gather it just as we need it. And a federated search basically tells each one of these systems, search your local database of information and see if there's a problem that matches the specific conditions that I'm spelling out. And if you have that, then report this back up. And that's the way these",
      "things work. It's a lot like the card game that a lot of kids play called Go Fish, where you say, does anybody have any threes? And everyone looks in their hand to see if they have any three cards. And if they do, then they have to turn that in. It's the same thing here. We're saying everyone run a search on your system locally and then only report the results. It's much more efficient, but in fact, we kind of need both of these because the SIEM is particularly good at doing alarms since all the",
      "information is coming up. But what we want to do is have high quality alarms. We don't want to just have tons and tons of information there. SIEMs tend to get more expensive the more information you feed into them. XDRs get around that problem by saying leave most of the data here and I'll go fetch it just in time. But still, the XDR operator needs to know that there's a reason they need to go out and look in the first place. So an alarm coming in from a SIEM might be a trigger to then cause an",
      "investigation to occur. So it's really not XDR versus SIEM. I want to leave you with the point that it's XDR plus. SIEM. These two work together and can complement each other and be part of a stronger security response. Okay. Now, we've talked about the SIEM and XDR technologies, which basically allow us to monitor, to analyze and report on the stuff that we see happening in our environment. Now let's talk about hunting. What is hunting about? Well, the reason we want to do this in the first",
      "place is this is an attack scenario, a timeline. And the first thing the bad guy does is reconnaissance. They basically check out your site, they case the joint. They try to figure out where your weak points are. So they're going to spend some time doing that initially. Then, according to Ponemon Institute’s Cost of a Data Breach survey, there's a delay in time until we have the mean-time-to-identify (MTTI). In other words, the guy attacks me at this point after he's finished his reconnaissance,",
      "he goes in. Now, how long does it take before the organization is aware that they've been attacked? Well, it turns out this is on the order of 200 days. That's a huge problem, because imagine if a bad guy was in your house for 200 days before you realized that you had been broken into. And then the mean-time-to-contain (MTTC), that is, after I am aware that there's a problem, how long before I actually have it fixed? Now we're taking a look at it about another 70 days. You put those two together,",
      "270 days. It's the better part of a year that the since when you were attacked until you finally have recovered from all of this. So what would we like to be able to do in this? I'd like to be able to move awareness back earlier into this. If I can't completely prevent the attack, at least become aware of it sooner. And the way we do that is with threat hunting. Now, threat hunting, as compared to a basic investigation. With an investigation, we're reacting. So the system is giving me an alarm, a",
      "guy has broken in and now I'm doing the forensic investigation to find out what happened. That's what we typically do with SIEM and XDR tools. But there's something else we could do, and that's this idea of threat hunting, where I'm going to be more proactive. I'm going to basically use the skills, the experience and the instincts of a skilled cybersecurity analyst who has seen everything-- hopefully --and kind of comes up with what is essentially a hypothesis. They say, I wonder if someone has",
      "done this or that or the other thing. We don't have an alarm yet. No one has told us that we've been broken into. But I want to get ahead of this before anyone even allows the alarm to be sounded. So they develop a hypothesis based upon their experience and their instincts about what would someone go after? How might they attack us? What kinds of things would they do? We're looking at the way other attackers are breaching networks and systems and using that in our hypothesis as well. And the",
      "threat hunter then uses tools like these, the SIEM and the XDR, to go off and look for searches and look for indicators of compromise. And if they do it correctly, what we end up with is early detection. We basically move the bar back. In a perfect world, we'd be able to detect future crimes and we'd arrest the bad guys before they even break in. But we don't live in that world. The next best thing we can do is try to find out as close to the attack as possible if we can't prevent it at all. Now,",
      "what we've done with this so far is we've talked about the detection aspect of all of these things. What we want to do in the final video in the series is talk about response. So make sure you don't miss it. Thanks for watching. Please remember to hit like and subscribe and don't miss the notify bell so that you don't miss any videos in this series."
    ],
    "chunk_count": 34,
    "content_id": "42a95204-7447-4514-b58d-34e401a54474",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555023"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=Jk79QJCxPkM": {
    "title": "Cybersecurity Architecture: Response",
    "url": "https://www.youtube.com/watch?v=Jk79QJCxPkM",
    "description": "IBM Security QRadar EDR :  https://ibm.biz/Bdy3nu\n\nIBM Security X-Force Threat Intelligence Index 2023:  https://ibm.biz/Bdy3nL\n\nCost of a Data Breach Report 2023: https://ibm.biz/breach_report_2023\n\nRemember that security = prevention + detection + response. In this final episode of the Cybersecurity Architecture series, Jeff “the security guy” covers incident response. Who is responsible? What systems do they use to do their job? What can be automated and what can’t? How about the potential consequences of a data breach — do you need to be worried about your bottomline? All these questions will be answered.\n\nWatch all videos in the Cybersecurity Architecture series: https://ibm.biz/cybersecurity_video_series\n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-up\n\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n#AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume #response",
    "duration": 1017,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en foreign remember that equation remember that equation remember that equation this we covered in the last episode of this we covered in the last episode of this we covered in the last episode of the cyber security architecture series the cyber security architecture series the cyber security architecture series welcome back this was about security welcome back this was about security welcome back this was about security equals prevention plus detection plus equals prevention plus detection plus equals prevention plus detection plus response this is basically what we're response this is basically what we're response this is basically what we're doing in security and we talked about at doing in security and we talked about at doing in security and we talked about at the very beginning of the series some the very beginning of the series some the very beginning of the series some security principles and roles and things security principles and roles and things security principles and roles and things like that tools of the trade then we like that tools of the trade then we like that tools of the trade then we moved into prevention and we looked at moved into prevention and we looked at moved into prevention and we looked at each of these different domains identity each of these different domains identity each of these different domains identity and access management an endpoint a and access management an endpoint a and access management an endpoint a network an application and data and this network an application and data and this network an application and data and this is basically that mostly those controls is basically that mostly those controls is basically that mostly those controls we put in place are about prevention we put in place are about prevention we put in place are about prevention trying to prevent someone from breaking trying to prevent someone from breaking trying to prevent someone from breaking in or doing damage to us then in the in or doing damage to us then in the in or doing damage to us then in the last video if you didn't see that make last video if you didn't see that make last video if you didn't see that make sure you check it out in the last video sure you check it out in the last video sure you check it out in the last video we really focused on the detection we really focused on the detection we really focused on the detection portion of this the the thing that's up portion of this the the thing that's up portion of this the the thing that's up here that does the monitoring and things here that does the monitoring and things here that does the monitoring and things of that sort today we're going to move of that sort today we're going to move of that sort today we're going to move on over into once we've discovered that on over into once we've discovered that on over into once we've discovered that how do we do response so that's what how do we do response so that's what how do we do response so that's what we're covering today let's start off we're covering today let's start off we're covering today let's start off with some of the basics of security with some of the basics of security with some of the basics of security response well remember also I talked response well remember also I talked response well remember also I talked about this diagram where basically the about this diagram where basically the about this diagram where basically the bad guy comes along and he begins his bad guy comes along and he begins his bad guy comes along and he begins his attack by doing reconnaissance that's attack by doing reconnaissance that's attack by doing reconnaissance that's some number of days where they're some number of days where they're some number of days where they're basically casing The Joint they're basically casing The Joint they're basically casing The Joint they're looking to see where are the weak spots looking to see where are the weak spots looking to see where are the weak spots where can I get in then the attack where can I get in then the attack where can I get in then the attack occurs and this is where the damage occurs and this is where the damage occurs and this is where the damage begins unfortunately where the attack begins unfortunately where the attack begins unfortunately where the attack occurs and when we're aware of it is occurs and when we're aware of it is occurs and when we're aware of it is separated by a period of time and that separated by a period of time and that separated by a period of time and that period of time we call the mean time to period of time we call the mean time to period of time we call the mean time to identify and the according to the identify and the according to the identify and the according to the ponymon institute's cost of a data ponymon institute's cost of a data ponymon institute's cost of a data breach survey which I've mentioned breach survey which I've mentioned breach survey which I've mentioned before this is roughly I'm going to make before this is roughly I'm going to make before this is roughly I'm going to make it approximate roughly 200 days on it approximate roughly 200 days on it approximate roughly 200 days on average between when the attackers get average between when the attackers get average between when the attackers get in and when we're actually aware of it in and when we're actually aware of it in and when we're actually aware of it mean time to identify 200 days the bad mean time to identify 200 days the bad mean time to identify 200 days the bad guy is sitting in your house doing who guy is sitting in your house doing who guy is sitting in your house doing who knows what before you finally realize knows what before you finally realize knows what before you finally realize that it's happened that's a big problem that it's happened that's a big problem that it's happened that's a big problem but that's what we covered in the but that's what we covered in the but that's what we covered in the detection portion then what we're going detection portion then what we're going detection portion then what we're going to look at today is this next bit here to look at today is this next bit here to look at today is this next bit here the mean time to contain how long does the mean time to contain how long does the mean time to contain how long does it take for us once we're aware to get it take for us once we're aware to get it take for us once we're aware to get the damage controlled and the bad guys the damage controlled and the bad guys the damage controlled and the bad guys out and get back to operation and it out and get back to operation and it out and get back to operation and it turns out this is on the order of 70 turns out this is on the order of 70 turns out this is on the order of 70 days so this is the response portion of days so this is the response portion of days so this is the response portion of all of this that is clearly not working all of this that is clearly not working all of this that is clearly not working well we in fact if you look at the well we in fact if you look at the well we in fact if you look at the numbers over the years these numbers numbers over the years these numbers numbers over the years these numbers stay roughly the same they don't really stay roughly the same they don't really stay roughly the same they don't really change much so even though we've had change much so even though we've had change much so even though we've had more tooling we've had more more tooling we've had more more tooling we've had more understanding we've had a lot of things understanding we've had a lot of things understanding we've had a lot of things that we tried to do we've not been as an that we tried to do we've not been as an that we tried to do we've not been as an industry all that terribly effective at industry all that terribly effective at industry all that terribly effective at reducing that mean time to identify in reducing that mean time to identify in reducing that mean time to identify in meantime to to contain so what are we meantime to to contain so what are we meantime to to contain so what are we going to do we're going to look at this going to do we're going to look at this going to do we're going to look at this this stuff in the response part and see this stuff in the response part and see this stuff in the response part and see if we can shrink this 70 days and make if we can shrink this 70 days and make if we can shrink this 70 days and make that some number that's shorter than that some number that's shorter than that some number that's shorter than that so for instance the group that's up that so for instance the group that's up that so for instance the group that's up here doing this I talked about this also here doing this I talked about this also here doing this I talked about this also in the last video this is what we know in the last video this is what we know in the last video this is what we know as the sock the security operations as the sock the security operations as the sock the security operations center and it is a centralized team of center and it is a centralized team of center and it is a centralized team of people whose job it is to Monitor and people whose job it is to Monitor and people whose job it is to Monitor and look for across all the different memory look for across all the different memory look for across all the different memory each one of these domains is sending each one of these domains is sending each one of these domains is sending their information up so that we can see their information up so that we can see their information up so that we can see what's going on we're going to detect what's going on we're going to detect what's going on we're going to detect whenever we see anomalous behaviors whenever we see anomalous behaviors whenever we see anomalous behaviors alerts things like that where core laid alerts things like that where core laid alerts things like that where core laid all of that and then we're going to all of that and then we're going to all of that and then we're going to respond which again is the focus of what respond which again is the focus of what respond which again is the focus of what we're talking about today and that we're talking about today and that we're talking about today and that response business has traditionally been response business has traditionally been response business has traditionally been called incident response or ir and one called incident response or ir and one called incident response or ir and one of the big things that can cut the cost of the big things that can cut the cost of the big things that can cut the cost of a data breach is having a good of a data breach is having a good of a data breach is having a good incident response capability in place incident response capability in place incident response capability in place traditionally again that's the traditionally again that's the traditionally again that's the terminology we've used it's not wrong terminology we've used it's not wrong terminology we've used it's not wrong but that's just where we've come from but that's just where we've come from but that's just where we've come from and what it is is traditionally we've and what it is is traditionally we've and what it is is traditionally we've also made this largely a manual process also made this largely a manual process also made this largely a manual process it relies on Heroes and experts people it relies on Heroes and experts people it relies on Heroes and experts people who just happen to have knowledge in who just happen to have knowledge in who just happen to have knowledge in their heads and a gut feel on what to do their heads and a gut feel on what to do their heads and a gut feel on what to do and that's how we do this unfortunately and that's how we do this unfortunately and that's how we do this unfortunately that really doesn't scale well and it's that really doesn't scale well and it's that really doesn't scale well and it's not necessarily all that repeatable but not necessarily all that repeatable but not necessarily all that repeatable but their job is to first of all triage that their job is to first of all triage that their job is to first of all triage that is to when we get these alarms that come is to when we get these alarms that come is to when we get these alarms that come in we've got to determine is that a real in we've got to determine is that a real in we've got to determine is that a real attack or is this just some noise that's attack or is this just some noise that's attack or is this just some noise that's out there and if it's a real Attack is out there and if it's a real Attack is out there and if it's a real Attack is it a significant attack or is it not so it a significant attack or is it not so it a significant attack or is it not so significant and which ones because we significant and which ones because we significant and which ones because we never have time to respond to everything never have time to respond to everything never have time to respond to everything as quickly as we'd like to what's the as quickly as we'd like to what's the as quickly as we'd like to what's the order of importance that's the triage order of importance that's the triage order of importance that's the triage aspect of that looking and figuring out aspect of that looking and figuring out aspect of that looking and figuring out which patients do we need to see first which patients do we need to see first which patients do we need to see first that's where that word comes from is in that's where that word comes from is in that's where that word comes from is in the health care industry and then once the health care industry and then once the health care industry and then once we've figured out the pecking order then we've figured out the pecking order then we've figured out the pecking order then we're going to do remediation that is we're going to do remediation that is we're going to do remediation that is we're going to fix whatever it is we're we're going to fix whatever it is we're we're going to fix whatever it is we're going to block we're going to shut going to block we're going to shut going to block we're going to shut things down we're going to apply things down we're going to apply things down we're going to apply software patches we're going to put in software patches we're going to put in software patches we're going to put in controls so that we're no longer leaking controls so that we're no longer leaking controls so that we're no longer leaking data or our systems are able to get back data or our systems are able to get back data or our systems are able to get back up and running again up and running again up and running again now the more modern approach that we now the more modern approach that we now the more modern approach that we hear the terminology is sore and I'm hear the terminology is sore and I'm hear the terminology is sore and I'm going to do my best to resort to not going to do my best to resort to not going to do my best to resort to not using any puns when it comes to a word using any puns when it comes to a word using any puns when it comes to a word like this but it's awfully tempting like this but it's awfully tempting like this but it's awfully tempting anyway soar stands for security anyway soar stands for security anyway soar stands for security orchestration Automation and response so orchestration Automation and response so orchestration Automation and response so it's Automation and orchestration we'll it's Automation and orchestration we'll it's Automation and orchestration we'll talk a little bit more about those talk a little bit more about those talk a little bit more about those coming up and the difference between the coming up and the difference between the coming up and the difference between the two but just say where this tends to be two but just say where this tends to be two but just say where this tends to be traditionally much more manual there the traditionally much more manual there the traditionally much more manual there the idea was soar is that we're going to try idea was soar is that we're going to try idea was soar is that we're going to try to make things as much as possible to make things as much as possible to make things as much as possible automated and that hopefully will help automated and that hopefully will help automated and that hopefully will help us reduce the time that it takes to us reduce the time that it takes to us reduce the time that it takes to contain contain contain okay now we've covered the basics of okay now we've covered the basics of okay now we've covered the basics of response now let's take a look in a response now let's take a look in a response now let's take a look in a little more detail about cases little more detail about cases little more detail about cases and investigations because that's the and investigations because that's the and investigations because that's the thing we're going to do next the way thing we're going to do next the way thing we're going to do next the way this works is we have an attack remember this works is we have an attack remember this works is we have an attack remember over here well now I've depicted it here over here well now I've depicted it here over here well now I've depicted it here and this attack then is going to and this attack then is going to and this attack then is going to generate hopefully some sort of alarm generate hopefully some sort of alarm generate hopefully some sort of alarm some sort of event some sort of some sort of event some sort of some sort of event some sort of indication that we have a problem and indication that we have a problem and indication that we have a problem and that's going to flow on up here into the that's going to flow on up here into the that's going to flow on up here into the Sim the security information and event Sim the security information and event Sim the security information and event management system which makes up this management system which makes up this management system which makes up this detection portion of our security detection portion of our security detection portion of our security architecture architecture architecture now we may also have the Sim feeding in now we may also have the Sim feeding in now we may also have the Sim feeding in to an xdr as I mentioned in the previous to an xdr as I mentioned in the previous to an xdr as I mentioned in the previous video or they may be separate you may video or they may be separate you may video or they may be separate you may only have xdr or only Sim so I'm just only have xdr or only Sim so I'm just only have xdr or only Sim so I'm just going to depict it this way for the going to depict it this way for the going to depict it this way for the purpose of this illustration purpose of this illustration purpose of this illustration what I'd like to do then if this system what I'd like to do then if this system what I'd like to do then if this system is able to identify that we in fact is able to identify that we in fact is able to identify that we in fact think this is a problem and someone think this is a problem and someone think this is a problem and someone needs to go into more detail and needs to go into more detail and needs to go into more detail and investigate this then one of the things investigate this then one of the things investigate this then one of the things I want to do is create a case and I'm I want to do is create a case and I'm I want to do is create a case and I'm going to create that case here going to create that case here going to create that case here either one of these systems could open either one of these systems could open either one of these systems could open the case automatically in our solar the case automatically in our solar the case automatically in our solar system the security orchestration system the security orchestration system the security orchestration Automation and response system so the Automation and response system so the Automation and response system so the soar is depicted here it's got this case soar is depicted here it's got this case soar is depicted here it's got this case management as one of its components we management as one of its components we management as one of its components we can also use that to modify cases we can can also use that to modify cases we can can also use that to modify cases we can use it to assign cases so I'm going to use it to assign cases so I'm going to use it to assign cases so I'm going to assign this case over here to this guy assign this case over here to this guy assign this case over here to this guy right here and he's going to be right here and he's going to be right here and he's going to be responsible for looking at all the cases responsible for looking at all the cases responsible for looking at all the cases that are assigned to him and when he that are assigned to him and when he that are assigned to him and when he does that one of the things he'll see is does that one of the things he'll see is does that one of the things he'll see is that if we've done a really good job our that if we've done a really good job our that if we've done a really good job our xdr or our Sim not only created the case xdr or our Sim not only created the case xdr or our Sim not only created the case but also added in these things right but also added in these things right but also added in these things right here artifacts indicators of compromise here artifacts indicators of compromise here artifacts indicators of compromise useful information that the cyber useful information that the cyber useful information that the cyber security analyst is going to use later security analyst is going to use later security analyst is going to use later when they start trying to do their when they start trying to do their when they start trying to do their investigation so we take that investigation so we take that investigation so we take that information and add that into the case information and add that into the case information and add that into the case automatically so we enrich the case as automatically so we enrich the case as automatically so we enrich the case as another way of looking at it and that another way of looking at it and that another way of looking at it and that way this person isn't starting from zero way this person isn't starting from zero way this person isn't starting from zero so they've got some information there's so they've got some information there's so they've got some information there's a problem here's where it started here's a problem here's where it started here's a problem here's where it started here's some information I have about it now we some information I have about it now we some information I have about it now we can also with this soar case management can also with this soar case management can also with this soar case management system track and maybe we have an extra system track and maybe we have an extra system track and maybe we have an extra dashboard that allows us to figure out dashboard that allows us to figure out dashboard that allows us to figure out which cases are open which ones are high which cases are open which ones are high which cases are open which ones are high priority who's investigating these and priority who's investigating these and priority who's investigating these and even reassign these as we need to and even reassign these as we need to and even reassign these as we need to and make kinds of adjustments of that sort make kinds of adjustments of that sort make kinds of adjustments of that sort now this person is going to need to do now this person is going to need to do now this person is going to need to do this investigation business they're this investigation business they're this investigation business they're going to have to dig in and figure out going to have to dig in and figure out going to have to dig in and figure out what's going on how do they know what to what's going on how do they know what to what's going on how do they know what to do do they just start guessing do they do do they just start guessing do they do do they just start guessing do they start poking around Well we'd hoped that start poking around Well we'd hoped that start poking around Well we'd hoped that they would have a more consistent they would have a more consistent they would have a more consistent repeatable way of figuring out what the repeatable way of figuring out what the repeatable way of figuring out what the problems are so we'd like to guide their problems are so we'd like to guide their problems are so we'd like to guide their activities especially if this person is activities especially if this person is activities especially if this person is not very experienced and doesn't know not very experienced and doesn't know not very experienced and doesn't know where to start first so we have these where to start first so we have these where to start first so we have these things called Dynamic playbooks so a things called Dynamic playbooks so a things called Dynamic playbooks so a dynamic Playbook is something where we dynamic Playbook is something where we dynamic Playbook is something where we have gone in in advance and determined have gone in in advance and determined have gone in in advance and determined when you see this then run this routine when you see this then run this routine when you see this then run this routine if you start here and then you go off if you start here and then you go off if you start here and then you go off and run these two and run these two and run these two events maybe their scripts maybe their events maybe their scripts maybe their events maybe their scripts maybe their particular procedures that a person's particular procedures that a person's particular procedures that a person's supposed to go through and they do those supposed to go through and they do those supposed to go through and they do those and based upon the results of those and based upon the results of those and based upon the results of those things then they might do other things things then they might do other things things then they might do other things and depending on the results of that and depending on the results of that and depending on the results of that they do something different that's the they do something different that's the they do something different that's the dynamic aspect of this so it's not just dynamic aspect of this so it's not just dynamic aspect of this so it's not just a static standard operating procedure a static standard operating procedure a static standard operating procedure that spells out you do number one that spells out you do number one that spells out you do number one through ten statically every single time through ten statically every single time through ten statically every single time there may be some of those things but in there may be some of those things but in there may be some of those things but in fact what we find is that there's a lot fact what we find is that there's a lot fact what we find is that there's a lot of cases where we have to be more of cases where we have to be more of cases where we have to be more Dynamic and more flexible and what you Dynamic and more flexible and what you Dynamic and more flexible and what you get from this step will depend on what get from this step will depend on what get from this step will depend on what you want to do in the next steps that's you want to do in the next steps that's you want to do in the next steps that's why it needs to be dynamic and if we're why it needs to be dynamic and if we're why it needs to be dynamic and if we're able to capture that kind of information able to capture that kind of information able to capture that kind of information in a dynamic Playbook we can guide this in a dynamic Playbook we can guide this in a dynamic Playbook we can guide this person they don't have to have all of person they don't have to have all of person they don't have to have all of the expertise that knows everything the expertise that knows everything the expertise that knows everything about everything but they can follow the about everything but they can follow the about everything but they can follow the Playbook and let it guide their Playbook and let it guide their Playbook and let it guide their activities ultimately they're going to activities ultimately they're going to activities ultimately they're going to figure out where the source of the figure out where the source of the figure out where the source of the problem is and we're going to spell out problem is and we're going to spell out problem is and we're going to spell out with the remediation steps they should with the remediation steps they should with the remediation steps they should take and then they can go back here to take and then they can go back here to take and then they can go back here to this system and figure out what they this system and figure out what they this system and figure out what they need to do in order to get things need to do in order to get things need to do in order to get things if we're leaking data stop that if we're leaking data stop that if we're leaking data stop that hemorrhaging if it's a system that's hemorrhaging if it's a system that's hemorrhaging if it's a system that's down how to bring it back up and protect down how to bring it back up and protect down how to bring it back up and protect it do the remediation that's necessary it do the remediation that's necessary it do the remediation that's necessary with that system with that system with that system okay now we have gotten an indication okay now we have gotten an indication okay now we have gotten an indication that there was a problem we created a that there was a problem we created a that there was a problem we created a case opened it in the case management case opened it in the case management case opened it in the case management system and now we have the cyber system and now we have the cyber system and now we have the cyber security analyst that's going to go off security analyst that's going to go off security analyst that's going to go off and investigate this figure out what the and investigate this figure out what the and investigate this figure out what the problem is and remediate why wouldn't we problem is and remediate why wouldn't we problem is and remediate why wouldn't we just automate everything well I'm going just automate everything well I'm going just automate everything well I'm going to address that in this next business of to address that in this next business of to address that in this next business of automation versus orchestration so think automation versus orchestration so think automation versus orchestration so think about this here's the manual approach about this here's the manual approach about this here's the manual approach and we can think about everything as and we can think about everything as and we can think about everything as being on the Spectrum either it's done being on the Spectrum either it's done being on the Spectrum either it's done entirely manually or it's done entirely entirely manually or it's done entirely entirely manually or it's done entirely in an automated way and as much as in an automated way and as much as in an automated way and as much as possible I'd rather do it this way the possible I'd rather do it this way the possible I'd rather do it this way the problem is in security we see a lot of problem is in security we see a lot of problem is in security we see a lot of things that sometimes we refer to as things that sometimes we refer to as things that sometimes we refer to as Black Swan events you know swans are Black Swan events you know swans are Black Swan events you know swans are normally white sometimes they're black normally white sometimes they're black normally white sometimes they're black it's not impossible it's just more rare it's not impossible it's just more rare it's not impossible it's just more rare and it's not what we expect and I can and it's not what we expect and I can and it's not what we expect and I can only automate what I've seen before only automate what I've seen before only automate what I've seen before sometimes we even get insecurity some of sometimes we even get insecurity some of sometimes we even get insecurity some of these things we call First of a Kind these things we call First of a Kind these things we call First of a Kind events and a first of a kind mind okay events and a first of a kind mind okay events and a first of a kind mind okay I'm probably going to have to figure I'm probably going to have to figure I'm probably going to have to figure that out manually because I won't have that out manually because I won't have that out manually because I won't have known how to set up a script to handle known how to set up a script to handle known how to set up a script to handle that in advance that in advance that in advance but what I'd like to do is as much as but what I'd like to do is as much as but what I'd like to do is as much as possible do as much of this as I can in possible do as much of this as I can in possible do as much of this as I can in an automated way and for the things I an automated way and for the things I an automated way and for the things I can't I'll stop somewhere along this can't I'll stop somewhere along this can't I'll stop somewhere along this Continuum and we'll do what we call Continuum and we'll do what we call Continuum and we'll do what we call orchestration think of this as sort of orchestration think of this as sort of orchestration think of this as sort of like semi-automated where we have a like semi-automated where we have a like semi-automated where we have a human who is directing the system and human who is directing the system and human who is directing the system and saying okay go I'm going to push this saying okay go I'm going to push this saying okay go I'm going to push this button it's going to go off and do these button it's going to go off and do these button it's going to go off and do these procedures and I'm going to push this procedures and I'm going to push this procedures and I'm going to push this button it's going to go off and do these button it's going to go off and do these button it's going to go off and do these things so think of this also another way things so think of this also another way things so think of this also another way analogy here is the conductor in an analogy here is the conductor in an analogy here is the conductor in an orchestra who is saying okay now I want orchestra who is saying okay now I want orchestra who is saying okay now I want the violins to come in here and I want the violins to come in here and I want the violins to come in here and I want the drums to exit there and they're the drums to exit there and they're the drums to exit there and they're orchestrating what is happening in this orchestrating what is happening in this orchestrating what is happening in this case so orchestration is a step that's case so orchestration is a step that's case so orchestration is a step that's not fully automated but it's in that not fully automated but it's in that not fully automated but it's in that direction and the whole goal of this direction and the whole goal of this direction and the whole goal of this modern soar capability is to as much as modern soar capability is to as much as modern soar capability is to as much as possible move things in that direction possible move things in that direction possible move things in that direction where it's more automated and less where it's more automated and less where it's more automated and less manual and Orchestra duration is a step manual and Orchestra duration is a step manual and Orchestra duration is a step along that way well what are the other along that way well what are the other along that way well what are the other things that we have to do it turns out things that we have to do it turns out things that we have to do it turns out if someone attacked us and there was if someone attacked us and there was if someone attacked us and there was data that was sensitive to individuals data that was sensitive to individuals data that was sensitive to individuals that then got breached or got that then got breached or got that then got breached or got compromised and bad guys got a hold of compromised and bad guys got a hold of compromised and bad guys got a hold of it then we might have a responsibility it then we might have a responsibility it then we might have a responsibility here for notification this whole here for notification this whole here for notification this whole business of breach notification one of business of breach notification one of business of breach notification one of the first things we have to ask is what the first things we have to ask is what the first things we have to ask is what kind of data was involved well maybe kind of data was involved well maybe kind of data was involved well maybe there were names there might be Social Security numbers there might be Social Security numbers there might be Social Security numbers if you're in the U.S other types of ID if you're in the U.S other types of ID if you're in the U.S other types of ID numbers uh credit card numbers and numbers uh credit card numbers and numbers uh credit card numbers and things of that sort that's the kind of things of that sort that's the kind of things of that sort that's the kind of information so that's the data I need to information so that's the data I need to information so that's the data I need to know what kind of data was compromised know what kind of data was compromised know what kind of data was compromised then I need to know what geography where then I need to know what geography where then I need to know what geography where the people whose data was compromised uh the people whose data was compromised uh the people whose data was compromised uh involved so what nation are they in involved so what nation are they in involved so what nation are they in maybe even what state or region within maybe even what state or region within maybe even what state or region within that because it turns out we have that because it turns out we have that because it turns out we have different breach notification laws in different breach notification laws in different breach notification laws in different countries in different uh different countries in different uh different countries in different uh parts of the world and even within a parts of the world and even within a parts of the world and even within a country we have different breach country we have different breach country we have different breach notification rules so for instance one notification rules so for instance one notification rules so for instance one of these major ones is called the of these major ones is called the of these major ones is called the generalized data protection regulation generalized data protection regulation generalized data protection regulation gdpr that comes from the European Union gdpr that comes from the European Union gdpr that comes from the European Union and gdpr specifies very specific very and gdpr specifies very specific very and gdpr specifies very specific very heavy penalties in fact if you don't heavy penalties in fact if you don't heavy penalties in fact if you don't respond within a timely fashion the respond within a timely fashion the respond within a timely fashion the penalties for this can be on the order penalties for this can be on the order penalties for this can be on the order of 4 percent of worldwide Revenue so of 4 percent of worldwide Revenue so of 4 percent of worldwide Revenue so four percent is a huge number for most four percent is a huge number for most four percent is a huge number for most organizations or it can be 20 million organizations or it can be 20 million organizations or it can be 20 million euros whichever is greater that's a big euros whichever is greater that's a big euros whichever is greater that's a big penalty if you don't report in a timely penalty if you don't report in a timely penalty if you don't report in a timely way when consumers data citizen data has way when consumers data citizen data has way when consumers data citizen data has been compromised and again you might say been compromised and again you might say been compromised and again you might say but I don't care I am in the U.S or I'm but I don't care I am in the U.S or I'm but I don't care I am in the U.S or I'm in Australia I don't have EU uh you know in Australia I don't have EU uh you know in Australia I don't have EU uh you know really hanging over my head really hanging over my head really hanging over my head that think again if you've got EU that think again if you've got EU that think again if you've got EU citizen data you might be subject to EU citizen data you might be subject to EU citizen data you might be subject to EU rules and penalties so then it's a rules and penalties so then it's a rules and penalties so then it's a question of prosecution but regardless question of prosecution but regardless question of prosecution but regardless there are other types of rules in the there are other types of rules in the there are other types of rules in the U.S each individual state is kind of U.S each individual state is kind of U.S each individual state is kind of coming out with their own sets of rules coming out with their own sets of rules coming out with their own sets of rules with their own set of regulations and so with their own set of regulations and so with their own set of regulations and so forth so there are lots of these and forth so there are lots of these and forth so there are lots of these and this is what makes it really complicated this is what makes it really complicated this is what makes it really complicated to make sure that you have complied with to make sure that you have complied with to make sure that you have complied with all of these regulations because there all of these regulations because there all of these regulations because there are so many it's really good to have a are so many it's really good to have a are so many it's really good to have a tool point being that would help you tool point being that would help you tool point being that would help you with all of this once I realized as part with all of this once I realized as part with all of this once I realized as part of my investigation that there has been of my investigation that there has been of my investigation that there has been a data breach I'd like to go into the a data breach I'd like to go into the a data breach I'd like to go into the tool and say here are the types of data tool and say here are the types of data tool and say here are the types of data that were compromised here are the that were compromised here are the that were compromised here are the geographies where it was compromised and geographies where it was compromised and geographies where it was compromised and here are the different regulatory here are the different regulatory here are the different regulatory requirements that I have to follow and requirements that I have to follow and requirements that I have to follow and based upon that the system would come based upon that the system would come based upon that the system would come out and tell me exactly who do I need to out and tell me exactly who do I need to out and tell me exactly who do I need to notify and that way I don't end up with notify and that way I don't end up with notify and that way I don't end up with a bunch of this because that gets to be a bunch of this because that gets to be a bunch of this because that gets to be very expensive very expensive very expensive I hope you've enjoyed watching this I hope you've enjoyed watching this I hope you've enjoyed watching this series as much as I've enjoyed making it series as much as I've enjoyed making it series as much as I've enjoyed making it as I told you at the beginning this is a as I told you at the beginning this is a as I told you at the beginning this is a condensed version of a course I teach at condensed version of a course I teach at condensed version of a course I teach at a local University so you don't get any a local University so you don't get any a local University so you don't get any college credit but on the positive side college credit but on the positive side college credit but on the positive side you also didn't have any homework and you also didn't have any homework and you also didn't have any homework and you didn't have to take a final exam so you didn't have to take a final exam so you didn't have to take a final exam so good news in that regard I hope this good news in that regard I hope this good news in that regard I hope this also wedded your appetite and increased also wedded your appetite and increased also wedded your appetite and increased your desire to learn more about cyber your desire to learn more about cyber your desire to learn more about cyber security and that you find this topic as security and that you find this topic as security and that you find this topic as interesting as I do interesting as I do interesting as I do what we'd like is to get some feedback what we'd like is to get some feedback what we'd like is to get some feedback from you if you can add in the comments from you if you can add in the comments from you if you can add in the comments what did you learn what was a particular what did you learn what was a particular what did you learn what was a particular value to you in this series that helps value to you in this series that helps value to you in this series that helps us know what kinds of things we should us know what kinds of things we should us know what kinds of things we should be doing in future videos so also take a be doing in future videos so also take a be doing in future videos so also take a look look look at in the description below and you'll at in the description below and you'll at in the description below and you'll see a playlist that shows you all 10 see a playlist that shows you all 10 see a playlist that shows you all 10 videos that were in this series in case videos that were in this series in case videos that were in this series in case you missed one you don't want to miss you missed one you don't want to miss you missed one you don't want to miss you want to catch all of them so please you want to catch all of them so please you want to catch all of them so please go take a look at that and as always go take a look at that and as always go take a look at that and as always like subscribe and hit the notify bill like subscribe and hit the notify bill like subscribe and hit the notify bill so that you're aware of new videos as so that you're aware of new videos as so that you're aware of new videos as they come out thank you",
    "chunks": [
      "Kind: captions Language: en foreign remember that equation remember that equation remember that equation this we covered in the last episode of this we covered in the last episode of this we covered in the last episode of the cyber security architecture series the cyber security architecture series the cyber security architecture series welcome back this was about security welcome back this was about security welcome back this was about security equals prevention plus detection plus equals",
      "prevention plus detection plus equals prevention plus detection plus response this is basically what we're response this is basically what we're response this is basically what we're doing in security and we talked about at doing in security and we talked about at doing in security and we talked about at the very beginning of the series some the very beginning of the series some the very beginning of the series some security principles and roles and things security principles and roles and things",
      "security principles and roles and things like that tools of the trade then we like that tools of the trade then we like that tools of the trade then we moved into prevention and we looked at moved into prevention and we looked at moved into prevention and we looked at each of these different domains identity each of these different domains identity each of these different domains identity and access management an endpoint a and access management an endpoint a and access management an endpoint a",
      "network an application and data and this network an application and data and this network an application and data and this is basically that mostly those controls is basically that mostly those controls is basically that mostly those controls we put in place are about prevention we put in place are about prevention we put in place are about prevention trying to prevent someone from breaking trying to prevent someone from breaking trying to prevent someone from breaking in or doing damage to us",
      "then in the in or doing damage to us then in the in or doing damage to us then in the last video if you didn't see that make last video if you didn't see that make last video if you didn't see that make sure you check it out in the last video sure you check it out in the last video sure you check it out in the last video we really focused on the detection we really focused on the detection we really focused on the detection portion of this the the thing that's up portion of this the the thing",
      "that's up portion of this the the thing that's up here that does the monitoring and things here that does the monitoring and things here that does the monitoring and things of that sort today we're going to move of that sort today we're going to move of that sort today we're going to move on over into once we've discovered that on over into once we've discovered that on over into once we've discovered that how do we do response so that's what how do we do response so that's what how do we do",
      "response so that's what we're covering today let's start off we're covering today let's start off we're covering today let's start off with some of the basics of security with some of the basics of security with some of the basics of security response well remember also I talked response well remember also I talked response well remember also I talked about this diagram where basically the about this diagram where basically the about this diagram where basically the bad guy comes along and he",
      "begins his bad guy comes along and he begins his bad guy comes along and he begins his attack by doing reconnaissance that's attack by doing reconnaissance that's attack by doing reconnaissance that's some number of days where they're some number of days where they're some number of days where they're basically casing The Joint they're basically casing The Joint they're basically casing The Joint they're looking to see where are the weak spots looking to see where are the weak spots looking to",
      "see where are the weak spots where can I get in then the attack where can I get in then the attack where can I get in then the attack occurs and this is where the damage occurs and this is where the damage occurs and this is where the damage begins unfortunately where the attack begins unfortunately where the attack begins unfortunately where the attack occurs and when we're aware of it is occurs and when we're aware of it is occurs and when we're aware of it is separated by a period of time and",
      "that separated by a period of time and that separated by a period of time and that period of time we call the mean time to period of time we call the mean time to period of time we call the mean time to identify and the according to the identify and the according to the identify and the according to the ponymon institute's cost of a data ponymon institute's cost of a data ponymon institute's cost of a data breach survey which I've mentioned breach survey which I've mentioned breach survey which",
      "I've mentioned before this is roughly I'm going to make before this is roughly I'm going to make before this is roughly I'm going to make it approximate roughly 200 days on it approximate roughly 200 days on it approximate roughly 200 days on average between when the attackers get average between when the attackers get average between when the attackers get in and when we're actually aware of it in and when we're actually aware of it in and when we're actually aware of it mean time to identify",
      "200 days the bad mean time to identify 200 days the bad mean time to identify 200 days the bad guy is sitting in your house doing who guy is sitting in your house doing who guy is sitting in your house doing who knows what before you finally realize knows what before you finally realize knows what before you finally realize that it's happened that's a big problem that it's happened that's a big problem that it's happened that's a big problem but that's what we covered in the but that's what we",
      "covered in the but that's what we covered in the detection portion then what we're going detection portion then what we're going detection portion then what we're going to look at today is this next bit here to look at today is this next bit here to look at today is this next bit here the mean time to contain how long does the mean time to contain how long does the mean time to contain how long does it take for us once we're aware to get it take for us once we're aware to get it take for us once",
      "we're aware to get the damage controlled and the bad guys the damage controlled and the bad guys the damage controlled and the bad guys out and get back to operation and it out and get back to operation and it out and get back to operation and it turns out this is on the order of 70 turns out this is on the order of 70 turns out this is on the order of 70 days so this is the response portion of days so this is the response portion of days so this is the response portion of all of this that is",
      "clearly not working all of this that is clearly not working all of this that is clearly not working well we in fact if you look at the well we in fact if you look at the well we in fact if you look at the numbers over the years these numbers numbers over the years these numbers numbers over the years these numbers stay roughly the same they don't really stay roughly the same they don't really stay roughly the same they don't really change much so even though we've had change much so even though",
      "we've had change much so even though we've had more tooling we've had more more tooling we've had more more tooling we've had more understanding we've had a lot of things understanding we've had a lot of things understanding we've had a lot of things that we tried to do we've not been as an that we tried to do we've not been as an that we tried to do we've not been as an industry all that terribly effective at industry all that terribly effective at industry all that terribly effective at",
      "reducing that mean time to identify in reducing that mean time to identify in reducing that mean time to identify in meantime to to contain so what are we meantime to to contain so what are we meantime to to contain so what are we going to do we're going to look at this going to do we're going to look at this going to do we're going to look at this this stuff in the response part and see this stuff in the response part and see this stuff in the response part and see if we can shrink this 70 days",
      "and make if we can shrink this 70 days and make if we can shrink this 70 days and make that some number that's shorter than that some number that's shorter than that some number that's shorter than that so for instance the group that's up that so for instance the group that's up that so for instance the group that's up here doing this I talked about this also here doing this I talked about this also here doing this I talked about this also in the last video this is what we know in the last video",
      "this is what we know in the last video this is what we know as the sock the security operations as the sock the security operations as the sock the security operations center and it is a centralized team of center and it is a centralized team of center and it is a centralized team of people whose job it is to Monitor and people whose job it is to Monitor and people whose job it is to Monitor and look for across all the different memory look for across all the different memory look for across all",
      "the different memory each one of these domains is sending each one of these domains is sending each one of these domains is sending their information up so that we can see their information up so that we can see their information up so that we can see what's going on we're going to detect what's going on we're going to detect what's going on we're going to detect whenever we see anomalous behaviors whenever we see anomalous behaviors whenever we see anomalous behaviors alerts things like that",
      "where core laid alerts things like that where core laid alerts things like that where core laid all of that and then we're going to all of that and then we're going to all of that and then we're going to respond which again is the focus of what respond which again is the focus of what respond which again is the focus of what we're talking about today and that we're talking about today and that we're talking about today and that response business has traditionally been response business has",
      "traditionally been response business has traditionally been called incident response or ir and one called incident response or ir and one called incident response or ir and one of the big things that can cut the cost of the big things that can cut the cost of the big things that can cut the cost of a data breach is having a good of a data breach is having a good of a data breach is having a good incident response capability in place incident response capability in place incident response",
      "capability in place traditionally again that's the traditionally again that's the traditionally again that's the terminology we've used it's not wrong terminology we've used it's not wrong terminology we've used it's not wrong but that's just where we've come from but that's just where we've come from but that's just where we've come from and what it is is traditionally we've and what it is is traditionally we've and what it is is traditionally we've also made this largely a manual process also",
      "made this largely a manual process also made this largely a manual process it relies on Heroes and experts people it relies on Heroes and experts people it relies on Heroes and experts people who just happen to have knowledge in who just happen to have knowledge in who just happen to have knowledge in their heads and a gut feel on what to do their heads and a gut feel on what to do their heads and a gut feel on what to do and that's how we do this unfortunately and that's how we do this",
      "unfortunately and that's how we do this unfortunately that really doesn't scale well and it's that really doesn't scale well and it's that really doesn't scale well and it's not necessarily all that repeatable but not necessarily all that repeatable but not necessarily all that repeatable but their job is to first of all triage that their job is to first of all triage that their job is to first of all triage that is to when we get these alarms that come is to when we get these alarms that come is",
      "to when we get these alarms that come in we've got to determine is that a real in we've got to determine is that a real in we've got to determine is that a real attack or is this just some noise that's attack or is this just some noise that's attack or is this just some noise that's out there and if it's a real Attack is out there and if it's a real Attack is out there and if it's a real Attack is it a significant attack or is it not so it a significant attack or is it not so it a significant",
      "attack or is it not so significant and which ones because we significant and which ones because we significant and which ones because we never have time to respond to everything never have time to respond to everything never have time to respond to everything as quickly as we'd like to what's the as quickly as we'd like to what's the as quickly as we'd like to what's the order of importance that's the triage order of importance that's the triage order of importance that's the triage aspect of",
      "that looking and figuring out aspect of that looking and figuring out aspect of that looking and figuring out which patients do we need to see first which patients do we need to see first which patients do we need to see first that's where that word comes from is in that's where that word comes from is in that's where that word comes from is in the health care industry and then once the health care industry and then once the health care industry and then once we've figured out the pecking order",
      "then we've figured out the pecking order then we've figured out the pecking order then we're going to do remediation that is we're going to do remediation that is we're going to do remediation that is we're going to fix whatever it is we're we're going to fix whatever it is we're we're going to fix whatever it is we're going to block we're going to shut going to block we're going to shut going to block we're going to shut things down we're going to apply things down we're going to apply things",
      "down we're going to apply software patches we're going to put in software patches we're going to put in software patches we're going to put in controls so that we're no longer leaking controls so that we're no longer leaking controls so that we're no longer leaking data or our systems are able to get back data or our systems are able to get back data or our systems are able to get back up and running again up and running again up and running again now the more modern approach that we now the more",
      "modern approach that we now the more modern approach that we hear the terminology is sore and I'm hear the terminology is sore and I'm hear the terminology is sore and I'm going to do my best to resort to not going to do my best to resort to not going to do my best to resort to not using any puns when it comes to a word using any puns when it comes to a word using any puns when it comes to a word like this but it's awfully tempting like this but it's awfully tempting like this but it's awfully",
      "tempting anyway soar stands for security anyway soar stands for security anyway soar stands for security orchestration Automation and response so orchestration Automation and response so orchestration Automation and response so it's Automation and orchestration we'll it's Automation and orchestration we'll it's Automation and orchestration we'll talk a little bit more about those talk a little bit more about those talk a little bit more about those coming up and the difference between the coming",
      "up and the difference between the coming up and the difference between the two but just say where this tends to be two but just say where this tends to be two but just say where this tends to be traditionally much more manual there the traditionally much more manual there the traditionally much more manual there the idea was soar is that we're going to try idea was soar is that we're going to try idea was soar is that we're going to try to make things as much as possible to make things as much as",
      "possible to make things as much as possible automated and that hopefully will help automated and that hopefully will help automated and that hopefully will help us reduce the time that it takes to us reduce the time that it takes to us reduce the time that it takes to contain contain contain okay now we've covered the basics of okay now we've covered the basics of okay now we've covered the basics of response now let's take a look in a response now let's take a look in a response now let's take a",
      "look in a little more detail about cases little more detail about cases little more detail about cases and investigations because that's the and investigations because that's the and investigations because that's the thing we're going to do next the way thing we're going to do next the way thing we're going to do next the way this works is we have an attack remember this works is we have an attack remember this works is we have an attack remember over here well now I've depicted it here over here",
      "well now I've depicted it here over here well now I've depicted it here and this attack then is going to and this attack then is going to and this attack then is going to generate hopefully some sort of alarm generate hopefully some sort of alarm generate hopefully some sort of alarm some sort of event some sort of some sort of event some sort of some sort of event some sort of indication that we have a problem and indication that we have a problem and indication that we have a problem and that's",
      "going to flow on up here into the that's going to flow on up here into the that's going to flow on up here into the Sim the security information and event Sim the security information and event Sim the security information and event management system which makes up this management system which makes up this management system which makes up this detection portion of our security detection portion of our security detection portion of our security architecture architecture architecture now we may",
      "also have the Sim feeding in now we may also have the Sim feeding in now we may also have the Sim feeding in to an xdr as I mentioned in the previous to an xdr as I mentioned in the previous to an xdr as I mentioned in the previous video or they may be separate you may video or they may be separate you may video or they may be separate you may only have xdr or only Sim so I'm just only have xdr or only Sim so I'm just only have xdr or only Sim so I'm just going to depict it this way for the going",
      "to depict it this way for the going to depict it this way for the purpose of this illustration purpose of this illustration purpose of this illustration what I'd like to do then if this system what I'd like to do then if this system what I'd like to do then if this system is able to identify that we in fact is able to identify that we in fact is able to identify that we in fact think this is a problem and someone think this is a problem and someone think this is a problem and someone needs to go",
      "into more detail and needs to go into more detail and needs to go into more detail and investigate this then one of the things investigate this then one of the things investigate this then one of the things I want to do is create a case and I'm I want to do is create a case and I'm I want to do is create a case and I'm going to create that case here going to create that case here going to create that case here either one of these systems could open either one of these systems could open either",
      "one of these systems could open the case automatically in our solar the case automatically in our solar the case automatically in our solar system the security orchestration system the security orchestration system the security orchestration Automation and response system so the Automation and response system so the Automation and response system so the soar is depicted here it's got this case soar is depicted here it's got this case soar is depicted here it's got this case management as one of",
      "its components we management as one of its components we management as one of its components we can also use that to modify cases we can can also use that to modify cases we can can also use that to modify cases we can use it to assign cases so I'm going to use it to assign cases so I'm going to use it to assign cases so I'm going to assign this case over here to this guy assign this case over here to this guy assign this case over here to this guy right here and he's going to be right here and",
      "he's going to be right here and he's going to be responsible for looking at all the cases responsible for looking at all the cases responsible for looking at all the cases that are assigned to him and when he that are assigned to him and when he that are assigned to him and when he does that one of the things he'll see is does that one of the things he'll see is does that one of the things he'll see is that if we've done a really good job our that if we've done a really good job our that if we've",
      "done a really good job our xdr or our Sim not only created the case xdr or our Sim not only created the case xdr or our Sim not only created the case but also added in these things right but also added in these things right but also added in these things right here artifacts indicators of compromise here artifacts indicators of compromise here artifacts indicators of compromise useful information that the cyber useful information that the cyber useful information that the cyber security analyst",
      "is going to use later security analyst is going to use later security analyst is going to use later when they start trying to do their when they start trying to do their when they start trying to do their investigation so we take that investigation so we take that investigation so we take that information and add that into the case information and add that into the case information and add that into the case automatically so we enrich the case as automatically so we enrich the case as",
      "automatically so we enrich the case as another way of looking at it and that another way of looking at it and that another way of looking at it and that way this person isn't starting from zero way this person isn't starting from zero way this person isn't starting from zero so they've got some information there's so they've got some information there's so they've got some information there's a problem here's where it started here's a problem here's where it started here's a problem here's where",
      "it started here's some information I have about it now we some information I have about it now we some information I have about it now we can also with this soar case management can also with this soar case management can also with this soar case management system track and maybe we have an extra system track and maybe we have an extra system track and maybe we have an extra dashboard that allows us to figure out dashboard that allows us to figure out dashboard that allows us to figure out which",
      "cases are open which ones are high which cases are open which ones are high which cases are open which ones are high priority who's investigating these and priority who's investigating these and priority who's investigating these and even reassign these as we need to and even reassign these as we need to and even reassign these as we need to and make kinds of adjustments of that sort make kinds of adjustments of that sort make kinds of adjustments of that sort now this person is going to need to",
      "do now this person is going to need to do now this person is going to need to do this investigation business they're this investigation business they're this investigation business they're going to have to dig in and figure out going to have to dig in and figure out going to have to dig in and figure out what's going on how do they know what to what's going on how do they know what to what's going on how do they know what to do do they just start guessing do they do do they just start guessing do",
      "they do do they just start guessing do they start poking around Well we'd hoped that start poking around Well we'd hoped that start poking around Well we'd hoped that they would have a more consistent they would have a more consistent they would have a more consistent repeatable way of figuring out what the repeatable way of figuring out what the repeatable way of figuring out what the problems are so we'd like to guide their problems are so we'd like to guide their problems are so we'd like to",
      "guide their activities especially if this person is activities especially if this person is activities especially if this person is not very experienced and doesn't know not very experienced and doesn't know not very experienced and doesn't know where to start first so we have these where to start first so we have these where to start first so we have these things called Dynamic playbooks so a things called Dynamic playbooks so a things called Dynamic playbooks so a dynamic Playbook is something",
      "where we dynamic Playbook is something where we dynamic Playbook is something where we have gone in in advance and determined have gone in in advance and determined have gone in in advance and determined when you see this then run this routine when you see this then run this routine when you see this then run this routine if you start here and then you go off if you start here and then you go off if you start here and then you go off and run these two and run these two and run these two events",
      "maybe their scripts maybe their events maybe their scripts maybe their events maybe their scripts maybe their particular procedures that a person's particular procedures that a person's particular procedures that a person's supposed to go through and they do those supposed to go through and they do those supposed to go through and they do those and based upon the results of those and based upon the results of those and based upon the results of those things then they might do other things things",
      "then they might do other things things then they might do other things and depending on the results of that and depending on the results of that and depending on the results of that they do something different that's the they do something different that's the they do something different that's the dynamic aspect of this so it's not just dynamic aspect of this so it's not just dynamic aspect of this so it's not just a static standard operating procedure a static standard operating procedure a",
      "static standard operating procedure that spells out you do number one that spells out you do number one that spells out you do number one through ten statically every single time through ten statically every single time through ten statically every single time there may be some of those things but in there may be some of those things but in there may be some of those things but in fact what we find is that there's a lot fact what we find is that there's a lot fact what we find is that there's a",
      "lot of cases where we have to be more of cases where we have to be more of cases where we have to be more Dynamic and more flexible and what you Dynamic and more flexible and what you Dynamic and more flexible and what you get from this step will depend on what get from this step will depend on what get from this step will depend on what you want to do in the next steps that's you want to do in the next steps that's you want to do in the next steps that's why it needs to be dynamic and if we're",
      "why it needs to be dynamic and if we're why it needs to be dynamic and if we're able to capture that kind of information able to capture that kind of information able to capture that kind of information in a dynamic Playbook we can guide this in a dynamic Playbook we can guide this in a dynamic Playbook we can guide this person they don't have to have all of person they don't have to have all of person they don't have to have all of the expertise that knows everything the expertise that knows",
      "everything the expertise that knows everything about everything but they can follow the about everything but they can follow the about everything but they can follow the Playbook and let it guide their Playbook and let it guide their Playbook and let it guide their activities ultimately they're going to activities ultimately they're going to activities ultimately they're going to figure out where the source of the figure out where the source of the figure out where the source of the problem is",
      "and we're going to spell out problem is and we're going to spell out problem is and we're going to spell out with the remediation steps they should with the remediation steps they should with the remediation steps they should take and then they can go back here to take and then they can go back here to take and then they can go back here to this system and figure out what they this system and figure out what they this system and figure out what they need to do in order to get things need to do in",
      "order to get things need to do in order to get things if we're leaking data stop that if we're leaking data stop that if we're leaking data stop that hemorrhaging if it's a system that's hemorrhaging if it's a system that's hemorrhaging if it's a system that's down how to bring it back up and protect down how to bring it back up and protect down how to bring it back up and protect it do the remediation that's necessary it do the remediation that's necessary it do the remediation that's necessary",
      "with that system with that system with that system okay now we have gotten an indication okay now we have gotten an indication okay now we have gotten an indication that there was a problem we created a that there was a problem we created a that there was a problem we created a case opened it in the case management case opened it in the case management case opened it in the case management system and now we have the cyber system and now we have the cyber system and now we have the cyber security",
      "analyst that's going to go off security analyst that's going to go off security analyst that's going to go off and investigate this figure out what the and investigate this figure out what the and investigate this figure out what the problem is and remediate why wouldn't we problem is and remediate why wouldn't we problem is and remediate why wouldn't we just automate everything well I'm going just automate everything well I'm going just automate everything well I'm going to address that in this",
      "next business of to address that in this next business of to address that in this next business of automation versus orchestration so think automation versus orchestration so think automation versus orchestration so think about this here's the manual approach about this here's the manual approach about this here's the manual approach and we can think about everything as and we can think about everything as and we can think about everything as being on the Spectrum either it's done being on the",
      "Spectrum either it's done being on the Spectrum either it's done entirely manually or it's done entirely entirely manually or it's done entirely entirely manually or it's done entirely in an automated way and as much as in an automated way and as much as in an automated way and as much as possible I'd rather do it this way the possible I'd rather do it this way the possible I'd rather do it this way the problem is in security we see a lot of problem is in security we see a lot of problem is in",
      "security we see a lot of things that sometimes we refer to as things that sometimes we refer to as things that sometimes we refer to as Black Swan events you know swans are Black Swan events you know swans are Black Swan events you know swans are normally white sometimes they're black normally white sometimes they're black normally white sometimes they're black it's not impossible it's just more rare it's not impossible it's just more rare it's not impossible it's just more rare and it's not what",
      "we expect and I can and it's not what we expect and I can and it's not what we expect and I can only automate what I've seen before only automate what I've seen before only automate what I've seen before sometimes we even get insecurity some of sometimes we even get insecurity some of sometimes we even get insecurity some of these things we call First of a Kind these things we call First of a Kind these things we call First of a Kind events and a first of a kind mind okay events and a first of a",
      "kind mind okay events and a first of a kind mind okay I'm probably going to have to figure I'm probably going to have to figure I'm probably going to have to figure that out manually because I won't have that out manually because I won't have that out manually because I won't have known how to set up a script to handle known how to set up a script to handle known how to set up a script to handle that in advance that in advance that in advance but what I'd like to do is as much as but what I'd",
      "like to do is as much as but what I'd like to do is as much as possible do as much of this as I can in possible do as much of this as I can in possible do as much of this as I can in an automated way and for the things I an automated way and for the things I an automated way and for the things I can't I'll stop somewhere along this can't I'll stop somewhere along this can't I'll stop somewhere along this Continuum and we'll do what we call Continuum and we'll do what we call Continuum and we'll",
      "do what we call orchestration think of this as sort of orchestration think of this as sort of orchestration think of this as sort of like semi-automated where we have a like semi-automated where we have a like semi-automated where we have a human who is directing the system and human who is directing the system and human who is directing the system and saying okay go I'm going to push this saying okay go I'm going to push this saying okay go I'm going to push this button it's going to go off and",
      "do these button it's going to go off and do these button it's going to go off and do these procedures and I'm going to push this procedures and I'm going to push this procedures and I'm going to push this button it's going to go off and do these button it's going to go off and do these button it's going to go off and do these things so think of this also another way things so think of this also another way things so think of this also another way analogy here is the conductor in an analogy here",
      "is the conductor in an analogy here is the conductor in an orchestra who is saying okay now I want orchestra who is saying okay now I want orchestra who is saying okay now I want the violins to come in here and I want the violins to come in here and I want the violins to come in here and I want the drums to exit there and they're the drums to exit there and they're the drums to exit there and they're orchestrating what is happening in this orchestrating what is happening in this orchestrating",
      "what is happening in this case so orchestration is a step that's case so orchestration is a step that's case so orchestration is a step that's not fully automated but it's in that not fully automated but it's in that not fully automated but it's in that direction and the whole goal of this direction and the whole goal of this direction and the whole goal of this modern soar capability is to as much as modern soar capability is to as much as modern soar capability is to as much as possible move",
      "things in that direction possible move things in that direction possible move things in that direction where it's more automated and less where it's more automated and less where it's more automated and less manual and Orchestra duration is a step manual and Orchestra duration is a step manual and Orchestra duration is a step along that way well what are the other along that way well what are the other along that way well what are the other things that we have to do it turns out things that we",
      "have to do it turns out things that we have to do it turns out if someone attacked us and there was if someone attacked us and there was if someone attacked us and there was data that was sensitive to individuals data that was sensitive to individuals data that was sensitive to individuals that then got breached or got that then got breached or got that then got breached or got compromised and bad guys got a hold of compromised and bad guys got a hold of compromised and bad guys got a hold of it",
      "then we might have a responsibility it then we might have a responsibility it then we might have a responsibility here for notification this whole here for notification this whole here for notification this whole business of breach notification one of business of breach notification one of business of breach notification one of the first things we have to ask is what the first things we have to ask is what the first things we have to ask is what kind of data was involved well maybe kind of data",
      "was involved well maybe kind of data was involved well maybe there were names there might be Social Security numbers there might be Social Security numbers there might be Social Security numbers if you're in the U.S other types of ID if you're in the U.S other types of ID if you're in the U.S other types of ID numbers uh credit card numbers and numbers uh credit card numbers and numbers uh credit card numbers and things of that sort that's the kind of things of that sort that's the kind of things",
      "of that sort that's the kind of information so that's the data I need to information so that's the data I need to information so that's the data I need to know what kind of data was compromised know what kind of data was compromised know what kind of data was compromised then I need to know what geography where then I need to know what geography where then I need to know what geography where the people whose data was compromised uh the people whose data was compromised uh the people whose data",
      "was compromised uh involved so what nation are they in involved so what nation are they in involved so what nation are they in maybe even what state or region within maybe even what state or region within maybe even what state or region within that because it turns out we have that because it turns out we have that because it turns out we have different breach notification laws in different breach notification laws in different breach notification laws in different countries in different uh",
      "different countries in different uh different countries in different uh parts of the world and even within a parts of the world and even within a parts of the world and even within a country we have different breach country we have different breach country we have different breach notification rules so for instance one notification rules so for instance one notification rules so for instance one of these major ones is called the of these major ones is called the of these major ones is called the",
      "generalized data protection regulation generalized data protection regulation generalized data protection regulation gdpr that comes from the European Union gdpr that comes from the European Union gdpr that comes from the European Union and gdpr specifies very specific very and gdpr specifies very specific very and gdpr specifies very specific very heavy penalties in fact if you don't heavy penalties in fact if you don't heavy penalties in fact if you don't respond within a timely fashion the",
      "respond within a timely fashion the respond within a timely fashion the penalties for this can be on the order penalties for this can be on the order penalties for this can be on the order of 4 percent of worldwide Revenue so of 4 percent of worldwide Revenue so of 4 percent of worldwide Revenue so four percent is a huge number for most four percent is a huge number for most four percent is a huge number for most organizations or it can be 20 million organizations or it can be 20 million",
      "organizations or it can be 20 million euros whichever is greater that's a big euros whichever is greater that's a big euros whichever is greater that's a big penalty if you don't report in a timely penalty if you don't report in a timely penalty if you don't report in a timely way when consumers data citizen data has way when consumers data citizen data has way when consumers data citizen data has been compromised and again you might say been compromised and again you might say been compromised",
      "and again you might say but I don't care I am in the U.S or I'm but I don't care I am in the U.S or I'm but I don't care I am in the U.S or I'm in Australia I don't have EU uh you know in Australia I don't have EU uh you know in Australia I don't have EU uh you know really hanging over my head really hanging over my head really hanging over my head that think again if you've got EU that think again if you've got EU that think again if you've got EU citizen data you might be subject to EU citizen",
      "data you might be subject to EU citizen data you might be subject to EU rules and penalties so then it's a rules and penalties so then it's a rules and penalties so then it's a question of prosecution but regardless question of prosecution but regardless question of prosecution but regardless there are other types of rules in the there are other types of rules in the there are other types of rules in the U.S each individual state is kind of U.S each individual state is kind of U.S each individual",
      "state is kind of coming out with their own sets of rules coming out with their own sets of rules coming out with their own sets of rules with their own set of regulations and so with their own set of regulations and so with their own set of regulations and so forth so there are lots of these and forth so there are lots of these and forth so there are lots of these and this is what makes it really complicated this is what makes it really complicated this is what makes it really complicated to make",
      "sure that you have complied with to make sure that you have complied with to make sure that you have complied with all of these regulations because there all of these regulations because there all of these regulations because there are so many it's really good to have a are so many it's really good to have a are so many it's really good to have a tool point being that would help you tool point being that would help you tool point being that would help you with all of this once I realized as part",
      "with all of this once I realized as part with all of this once I realized as part of my investigation that there has been of my investigation that there has been of my investigation that there has been a data breach I'd like to go into the a data breach I'd like to go into the a data breach I'd like to go into the tool and say here are the types of data tool and say here are the types of data tool and say here are the types of data that were compromised here are the that were compromised here are",
      "the that were compromised here are the geographies where it was compromised and geographies where it was compromised and geographies where it was compromised and here are the different regulatory here are the different regulatory here are the different regulatory requirements that I have to follow and requirements that I have to follow and requirements that I have to follow and based upon that the system would come based upon that the system would come based upon that the system would come out",
      "and tell me exactly who do I need to out and tell me exactly who do I need to out and tell me exactly who do I need to notify and that way I don't end up with notify and that way I don't end up with notify and that way I don't end up with a bunch of this because that gets to be a bunch of this because that gets to be a bunch of this because that gets to be very expensive very expensive very expensive I hope you've enjoyed watching this I hope you've enjoyed watching this I hope you've enjoyed",
      "watching this series as much as I've enjoyed making it series as much as I've enjoyed making it series as much as I've enjoyed making it as I told you at the beginning this is a as I told you at the beginning this is a as I told you at the beginning this is a condensed version of a course I teach at condensed version of a course I teach at condensed version of a course I teach at a local University so you don't get any a local University so you don't get any a local University so you don't get",
      "any college credit but on the positive side college credit but on the positive side college credit but on the positive side you also didn't have any homework and you also didn't have any homework and you also didn't have any homework and you didn't have to take a final exam so you didn't have to take a final exam so you didn't have to take a final exam so good news in that regard I hope this good news in that regard I hope this good news in that regard I hope this also wedded your appetite and",
      "increased also wedded your appetite and increased also wedded your appetite and increased your desire to learn more about cyber your desire to learn more about cyber your desire to learn more about cyber security and that you find this topic as security and that you find this topic as security and that you find this topic as interesting as I do interesting as I do interesting as I do what we'd like is to get some feedback what we'd like is to get some feedback what we'd like is to get some",
      "feedback from you if you can add in the comments from you if you can add in the comments from you if you can add in the comments what did you learn what was a particular what did you learn what was a particular what did you learn what was a particular value to you in this series that helps value to you in this series that helps value to you in this series that helps us know what kinds of things we should us know what kinds of things we should us know what kinds of things we should be doing in",
      "future videos so also take a be doing in future videos so also take a be doing in future videos so also take a look look look at in the description below and you'll at in the description below and you'll at in the description below and you'll see a playlist that shows you all 10 see a playlist that shows you all 10 see a playlist that shows you all 10 videos that were in this series in case videos that were in this series in case videos that were in this series in case you missed one you don't",
      "want to miss you missed one you don't want to miss you missed one you don't want to miss you want to catch all of them so please you want to catch all of them so please you want to catch all of them so please go take a look at that and as always go take a look at that and as always go take a look at that and as always like subscribe and hit the notify bill like subscribe and hit the notify bill like subscribe and hit the notify bill so that you're aware of new videos as so that you're aware of",
      "new videos as so that you're aware of new videos as they come out thank you"
    ],
    "chunk_count": 96,
    "content_id": "bd780eb1-f099-422a-b426-bdb465e5500b",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555026"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=cjy5jpRS_S0": {
    "title": "Will AI Help or Hurt Cybersecurity? Definitely!",
    "url": "https://www.youtube.com/watch?v=cjy5jpRS_S0",
    "description": "AI for Cybersecurity: https://ibm.biz/BdM6ae\nCost of a Data Breach: https://ibm.biz/BdM6aL\nCheck out the AI and Cybersecurity eBook → https://ibm.biz/BdSkcA\n\nAccording to a recent survey, the average cost of a data breach is $4.5 million dollars. That's the bad news! The good news is that AI technology is estimated to represent a potential savings of $1.76 million per data breach. In this video, IBM Distinguished Engineer Jeff Crume shows how artificial intelligence can be exploited by hackers AND how it can be used by cybersecurity experts to better defend your company against attacks. \n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-upd\n\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#AI #Software #Dev #lightboard #IBM #Cybersecurity #JeffCrume",
    "duration": 600,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en what are two of the hottest topics not what are two of the hottest topics not what are two of the hottest topics not only in I.T but in society these days only in I.T but in society these days only in I.T but in society these days well if you said artificial intelligence well if you said artificial intelligence well if you said artificial intelligence and cyber security and cyber security and cyber security I'd agree with you both are really hot I'd agree with you both are really hot I'd agree with you both are really hot in fact even your non-technical friends in fact even your non-technical friends in fact even your non-technical friends have heard of these and may be talking have heard of these and may be talking have heard of these and may be talking about them and asking you questions and about them and asking you questions and about them and asking you questions and I'm going to suggest to you this I'm going to suggest to you this I'm going to suggest to you this intersection between the two even hotter intersection between the two even hotter intersection between the two even hotter still so what are we going to talk about still so what are we going to talk about still so what are we going to talk about in this video I'm going to talk about in this video I'm going to talk about in this video I'm going to talk about what from a cyber security standpoint AI what from a cyber security standpoint AI what from a cyber security standpoint AI can do to you and what it can do for you can do to you and what it can do for you can do to you and what it can do for you so let's take a look at that I'm going so let's take a look at that I'm going so let's take a look at that I'm going to start with some of the downsides to start with some of the downsides to start with some of the downsides first and then we'll conclude with some first and then we'll conclude with some first and then we'll conclude with some positive things on the downside what positive things on the downside what positive things on the downside what could AI do to us from a cyber security could AI do to us from a cyber security could AI do to us from a cyber security standpoint well it turns out that a lot standpoint well it turns out that a lot standpoint well it turns out that a lot of times we're able to tell about a of times we're able to tell about a of times we're able to tell about a phishing attack because the English phishing attack because the English phishing attack because the English language of the rider is not so good language of the rider is not so good language of the rider is not so good it's not their first language however it's not their first language however it's not their first language however you could now go into a chat bot and use you could now go into a chat bot and use you could now go into a chat bot and use it to generate very natural sounding it to generate very natural sounding it to generate very natural sounding language even though you might say but language even though you might say but language even though you might say but but Jeff there are Protections in some but Jeff there are Protections in some but Jeff there are Protections in some of these chat Bots that if you tell it of these chat Bots that if you tell it of these chat Bots that if you tell it to write you a phishing email it won't to write you a phishing email it won't to write you a phishing email it won't do to it there are also ways of do to it there are also ways of do to it there are also ways of re-engineering your prompt so that you re-engineering your prompt so that you re-engineering your prompt so that you can get past that so this is one area can get past that so this is one area can get past that so this is one area where phishing attacks are going to get where phishing attacks are going to get where phishing attacks are going to get better and the ways that we've been able better and the ways that we've been able better and the ways that we've been able to detect them in the past are not going to detect them in the past are not going to detect them in the past are not going to be so effective anymore to be so effective anymore to be so effective anymore what's another thing well on the what's another thing well on the what's another thing well on the positive side this generative Ai and positive side this generative Ai and positive side this generative Ai and chat Bots and things like that are able chat Bots and things like that are able chat Bots and things like that are able to write code for us so if I want to I to write code for us so if I want to I to write code for us so if I want to I can have it write code and do it really can have it write code and do it really can have it write code and do it really quickly and effectively it also means it quickly and effectively it also means it quickly and effectively it also means it could write malware as well it also could write malware as well it also could write malware as well it also means it could insert malware into the means it could insert malware into the means it could insert malware into the code that I have it also means it could code that I have it also means it could code that I have it also means it could insert back doors into the code that I insert back doors into the code that I insert back doors into the code that I have so we got we have got to also have so we got we have got to also have so we got we have got to also verify when we ask it to write code for verify when we ask it to write code for verify when we ask it to write code for us then in fact the code that it's us then in fact the code that it's us then in fact the code that it's giving us is pure and is doing what we giving us is pure and is doing what we giving us is pure and is doing what we intend for it to do intend for it to do intend for it to do another thing it could do to us another thing it could do to us another thing it could do to us misinformation misinformation misinformation how does this happen well these are how does this happen well these are how does this happen well these are generative AIS so one of the things that generative AIS so one of the things that generative AIS so one of the things that they suffer from is this issue we call they suffer from is this issue we call they suffer from is this issue we call hallucination where it may make up hallucination where it may make up hallucination where it may make up information or conflate two things that information or conflate two things that information or conflate two things that are not really related to each other and are not really related to each other and are not really related to each other and give a false impression also we could give a false impression also we could give a false impression also we could have a determined attacker who is doing have a determined attacker who is doing have a determined attacker who is doing what's known as a prompt injection where what's known as a prompt injection where what's known as a prompt injection where they're inserting bad information into they're inserting bad information into they're inserting bad information into the system or they're attacking the the system or they're attacking the the system or they're attacking the Corpus that is the body of knowledge Corpus that is the body of knowledge Corpus that is the body of knowledge that the system is based on and if they that the system is based on and if they that the system is based on and if they were able to do that then what comes out were able to do that then what comes out were able to do that then what comes out would be wrong information so we have to would be wrong information so we have to would be wrong information so we have to be careful to guard against be careful to guard against be careful to guard against over-reliance and make sure that we're over-reliance and make sure that we're over-reliance and make sure that we're verifying and testing our sources so verifying and testing our sources so verifying and testing our sources so that we can make sure that they're that we can make sure that they're that we can make sure that they're trustworthy one other example I'll give trustworthy one other example I'll give trustworthy one other example I'll give you here and there are actually many but you here and there are actually many but you here and there are actually many but I think this one's particularly I think this one's particularly I think this one's particularly interesting is this idea of a deep fake interesting is this idea of a deep fake interesting is this idea of a deep fake a deep fake is where we basically have a deep fake is where we basically have a deep fake is where we basically have an AI system that is able to copy your an AI system that is able to copy your an AI system that is able to copy your image and likeness your mannerisms your image and likeness your mannerisms your image and likeness your mannerisms your voice your appearance all of these voice your appearance all of these voice your appearance all of these things to the point where someone is things to the point where someone is things to the point where someone is looking at a video of you and they can't looking at a video of you and they can't looking at a video of you and they can't tell if it really was an actual video of tell if it really was an actual video of tell if it really was an actual video of you or a deep fake where we could have you or a deep fake where we could have you or a deep fake where we could have you saying things that weren't true and you saying things that weren't true and you saying things that weren't true and therefore if we're going to trust this therefore if we're going to trust this therefore if we're going to trust this kind of system we need a way to verify kind of system we need a way to verify kind of system we need a way to verify these things but right now the Deep fake these things but right now the Deep fake these things but right now the Deep fake technology has gone so far ahead in a technology has gone so far ahead in a technology has gone so far ahead in a very short period of time that it's very short period of time that it's very short period of time that it's going to be hard to verify those kinds going to be hard to verify those kinds going to be hard to verify those kinds of things of things of things okay we've just talked about what AI can okay we've just talked about what AI can okay we've just talked about what AI can do to us now let's look at some do to us now let's look at some do to us now let's look at some positives what can AI do for us in the positives what can AI do for us in the positives what can AI do for us in the cyber security space it turns out a lot cyber security space it turns out a lot cyber security space it turns out a lot in fact we do a survey each year that we in fact we do a survey each year that we in fact we do a survey each year that we call the cost of a data breach survey call the cost of a data breach survey call the cost of a data breach survey and the report that came back this year and the report that came back this year and the report that came back this year indicated that the number one thing you indicated that the number one thing you indicated that the number one thing you can do to save on the cost of a data can do to save on the cost of a data can do to save on the cost of a data breach and improve your response time is breach and improve your response time is breach and improve your response time is the extensive use of AI and Automation the extensive use of AI and Automation the extensive use of AI and Automation and here's what it can do on the one and here's what it can do on the one and here's what it can do on the one hand it can save on average 176 million hand it can save on average 176 million hand it can save on average 176 million dollars per data breach with the average dollars per data breach with the average dollars per data breach with the average data breach costing four and a half data breach costing four and a half data breach costing four and a half million that's a significant savings million that's a significant savings million that's a significant savings it can also cut down the mean time to it can also cut down the mean time to it can also cut down the mean time to identify and contain a breach by a identify and contain a breach by a identify and contain a breach by a hundred and eight days that makes a big hundred and eight days that makes a big hundred and eight days that makes a big difference so we know this is effective difference so we know this is effective difference so we know this is effective now what are we doing to make these now what are we doing to make these now what are we doing to make these kinds of results well it turns out a lot kinds of results well it turns out a lot kinds of results well it turns out a lot of what we do in this space is to do of what we do in this space is to do of what we do in this space is to do better analysis better analysis better analysis we're going to analyze large data sets we're going to analyze large data sets we're going to analyze large data sets lots of information that we have out lots of information that we have out lots of information that we have out there it's very hard to find patterns if there it's very hard to find patterns if there it's very hard to find patterns if I give you a whole large data set but if I give you a whole large data set but if I give you a whole large data set but if I use a technology called machine I use a technology called machine I use a technology called machine learning I can do a lot better job of learning I can do a lot better job of learning I can do a lot better job of spotting outliers and anomalies which is spotting outliers and anomalies which is spotting outliers and anomalies which is what we want to do in security a lot now what we want to do in security a lot now what we want to do in security a lot now I mentioned machine learning what is I mentioned machine learning what is I mentioned machine learning what is that well if you think about AI in that well if you think about AI in that well if you think about AI in particular as this large sort of particular as this large sort of particular as this large sort of umbrella term with a number of umbrella term with a number of umbrella term with a number of Technologies involved well Machine Technologies involved well Machine Technologies involved well Machine learning is a subset of that that learning is a subset of that that learning is a subset of that that specifically deals with some of these specifically deals with some of these specifically deals with some of these kind of analyzes that I've just referred kind of analyzes that I've just referred kind of analyzes that I've just referred to machine learning is what is often to machine learning is what is often to machine learning is what is often used in the security space we do it a used in the security space we do it a used in the security space we do it a lot because again it's very good at lot because again it's very good at lot because again it's very good at spotting anomalies and outliers and spotting anomalies and outliers and spotting anomalies and outliers and patterns and that's what we need a lot patterns and that's what we need a lot patterns and that's what we need a lot of in the security space so we're doing of in the security space so we're doing of in the security space so we're doing a lot of this today and a lot of these a lot of this today and a lot of these a lot of this today and a lot of these results come from leveraging machine results come from leveraging machine results come from leveraging machine learning which is a subfield of AI what learning which is a subfield of AI what learning which is a subfield of AI what else I mentioned automation well AI can else I mentioned automation well AI can else I mentioned automation well AI can help us in the automation task as well help us in the automation task as well help us in the automation task as well and I'll give you a few examples coming and I'll give you a few examples coming and I'll give you a few examples coming up but some of the things it can do is up but some of the things it can do is up but some of the things it can do is anticipate what we need to do next and anticipate what we need to do next and anticipate what we need to do next and some of those kind of things really some of those kind of things really some of those kind of things really start coming in from the area of deep start coming in from the area of deep start coming in from the area of deep learning which is a subfield of machine learning which is a subfield of machine learning which is a subfield of machine learning and then now this really new learning and then now this really new learning and then now this really new area that everyone is talking about area that everyone is talking about area that everyone is talking about these days Foundation models or you may these days Foundation models or you may these days Foundation models or you may hear them called large language models hear them called large language models hear them called large language models generative AI chat Bots they all exist generative AI chat Bots they all exist generative AI chat Bots they all exist in this space down here what can we in this space down here what can we in this space down here what can we start doing as I said security has start doing as I said security has start doing as I said security has mostly leveraged this in the past what mostly leveraged this in the past what mostly leveraged this in the past what can we start doing to leverage some of can we start doing to leverage some of can we start doing to leverage some of this stuff going forward well it turns this stuff going forward well it turns this stuff going forward well it turns out a lot of things because one of the out a lot of things because one of the out a lot of things because one of the things that Foundation models are really things that Foundation models are really things that Foundation models are really good at is summarizing good at is summarizing good at is summarizing they can be fed a lot of information and they can be fed a lot of information and they can be fed a lot of information and then it can give you a very quick then it can give you a very quick then it can give you a very quick summary of that why would that be useful summary of that why would that be useful summary of that why would that be useful well if you've got tons of documents well if you've got tons of documents well if you've got tons of documents you're trying to review it could give you're trying to review it could give you're trying to review it could give you the net the cliff notes of that you the net the cliff notes of that you the net the cliff notes of that another good use case for this would be another good use case for this would be another good use case for this would be incident summarization and case incident summarization and case incident summarization and case summarization if I'm seeing lots and summarization if I'm seeing lots and summarization if I'm seeing lots and lots of cases in my environment this lots of cases in my environment this lots of cases in my environment this kind of Technology could be used to tell kind of Technology could be used to tell kind of Technology could be used to tell me what are the trends among those cases me what are the trends among those cases me what are the trends among those cases are these things all related or are they are these things all related or are they are these things all related or are they all very different and my guess is all very different and my guess is all very different and my guess is they're probably at least a few things they're probably at least a few things they're probably at least a few things that are similar about these so that's that are similar about these so that's that are similar about these so that's an another nice use case that we'll see an another nice use case that we'll see an another nice use case that we'll see coming in the future from generative AI coming in the future from generative AI coming in the future from generative AI Foundation models into cyber security Foundation models into cyber security Foundation models into cyber security some other things we can do we know some other things we can do we know some other things we can do we know these kind of chat Bots are good at these kind of chat Bots are good at these kind of chat Bots are good at interacting so you can respond to them in natural so you can respond to them in natural so you can respond to them in natural language you don't have to format your language you don't have to format your language you don't have to format your queries using a particular query queries using a particular query queries using a particular query language or using a particular syntax language or using a particular syntax language or using a particular syntax you use the natural language that you're you use the natural language that you're you use the natural language that you're used to so for me I would state in used to so for me I would state in used to so for me I would state in English English English what are we being affected by this what are we being affected by this what are we being affected by this particular kind of malware and maybe particular kind of malware and maybe particular kind of malware and maybe what it could do is build a query for me what it could do is build a query for me what it could do is build a query for me that I can then run into my environment that I can then run into my environment that I can then run into my environment and it comes back and tells me am I and it comes back and tells me am I and it comes back and tells me am I affected or not and I can then ask more affected or not and I can then ask more affected or not and I can then ask more questions tell me more about this kind questions tell me more about this kind questions tell me more about this kind of malware what kind of indicators of of malware what kind of indicators of of malware what kind of indicators of compromise are there that are associated compromise are there that are associated compromise are there that are associated with this all of that stuff gives me a with this all of that stuff gives me a with this all of that stuff gives me a very easy intuitive way to get very easy intuitive way to get very easy intuitive way to get information that is highly technical out information that is highly technical out information that is highly technical out of the system and do this much faster of the system and do this much faster of the system and do this much faster another thing we might want to do is another thing we might want to do is another thing we might want to do is generate playbooks generate playbooks generate playbooks playbooks are the things that we use in playbooks are the things that we use in playbooks are the things that we use in incident response when we're trying to incident response when we're trying to incident response when we're trying to figure out what do we need to do once figure out what do we need to do once figure out what do we need to do once we've had an incident so generating we've had an incident so generating we've had an incident so generating these on the Fly generative AI these on the Fly generative AI these on the Fly generative AI generating playbooks you can see where generating playbooks you can see where generating playbooks you can see where there might be some type of crossover there might be some type of crossover there might be some type of crossover this is a good use case also for this this is a good use case also for this this is a good use case also for this technology so expect to see more of that technology so expect to see more of that technology so expect to see more of that and in fact there could be other types and in fact there could be other types and in fact there could be other types of things where we're using generative of things where we're using generative of things where we're using generative creative technology because these things creative technology because these things creative technology because these things really are creating for instance with really are creating for instance with really are creating for instance with threat hunting threat hunting threat hunting a threat Hunter is basically coming up a threat Hunter is basically coming up a threat Hunter is basically coming up with a hypothesis and saying I wonder if with a hypothesis and saying I wonder if with a hypothesis and saying I wonder if someone were to attack us maybe they someone were to attack us maybe they someone were to attack us maybe they would do the following things would do the following things would do the following things and we have a limitation in terms of our and we have a limitation in terms of our and we have a limitation in terms of our imagination sometimes the bad guys may imagination sometimes the bad guys may imagination sometimes the bad guys may dream up scenarios that we don't so it dream up scenarios that we don't so it dream up scenarios that we don't so it might be useful to have a system that might be useful to have a system that might be useful to have a system that can dream up scenarios we didn't think can dream up scenarios we didn't think can dream up scenarios we didn't think of using a generative AI to generate of using a generative AI to generate of using a generative AI to generate hypothetical cases that we then go out hypothetical cases that we then go out hypothetical cases that we then go out and automate and do a threat hunt in our and automate and do a threat hunt in our and automate and do a threat hunt in our environment this is all really super environment this is all really super environment this is all really super exciting stuff I think and it shows exciting stuff I think and it shows exciting stuff I think and it shows exactly what we'll be able to do in this exactly what we'll be able to do in this exactly what we'll be able to do in this space because what we want to be able to space because what we want to be able to space because what we want to be able to do is move away from being purely do is move away from being purely do is move away from being purely reactive to a more proactive reactive to a more proactive reactive to a more proactive way of doing cyber security and that's way of doing cyber security and that's way of doing cyber security and that's the good news in this story we've got Ai the good news in this story we've got Ai the good news in this story we've got Ai and cyber security and if they're and cyber security and if they're and cyber security and if they're working together as you see here we can working together as you see here we can working together as you see here we can end up with a more proactive Solution end up with a more proactive Solution end up with a more proactive Solution that's more cost effective and keeps us that's more cost effective and keeps us that's more cost effective and keeps us all much safer all much safer all much safer thanks for watching if you found this thanks for watching if you found this thanks for watching if you found this video interesting and would like to video interesting and would like to video interesting and would like to learn more about cyber security please learn more about cyber security please learn more about cyber security please remember to hit like And subscribe to remember to hit like And subscribe to remember to hit like And subscribe to this channel",
    "chunks": [
      "Kind: captions Language: en what are two of the hottest topics not what are two of the hottest topics not what are two of the hottest topics not only in I.T but in society these days only in I.T but in society these days only in I.T but in society these days well if you said artificial intelligence well if you said artificial intelligence well if you said artificial intelligence and cyber security and cyber security and cyber security I'd agree with you both are really hot I'd agree with you",
      "both are really hot I'd agree with you both are really hot in fact even your non-technical friends in fact even your non-technical friends in fact even your non-technical friends have heard of these and may be talking have heard of these and may be talking have heard of these and may be talking about them and asking you questions and about them and asking you questions and about them and asking you questions and I'm going to suggest to you this I'm going to suggest to you this I'm going to",
      "suggest to you this intersection between the two even hotter intersection between the two even hotter intersection between the two even hotter still so what are we going to talk about still so what are we going to talk about still so what are we going to talk about in this video I'm going to talk about in this video I'm going to talk about in this video I'm going to talk about what from a cyber security standpoint AI what from a cyber security standpoint AI what from a cyber security standpoint",
      "AI can do to you and what it can do for you can do to you and what it can do for you can do to you and what it can do for you so let's take a look at that I'm going so let's take a look at that I'm going so let's take a look at that I'm going to start with some of the downsides to start with some of the downsides to start with some of the downsides first and then we'll conclude with some first and then we'll conclude with some first and then we'll conclude with some positive things on the",
      "downside what positive things on the downside what positive things on the downside what could AI do to us from a cyber security could AI do to us from a cyber security could AI do to us from a cyber security standpoint well it turns out that a lot standpoint well it turns out that a lot standpoint well it turns out that a lot of times we're able to tell about a of times we're able to tell about a of times we're able to tell about a phishing attack because the English phishing attack because the",
      "English phishing attack because the English language of the rider is not so good language of the rider is not so good language of the rider is not so good it's not their first language however it's not their first language however it's not their first language however you could now go into a chat bot and use you could now go into a chat bot and use you could now go into a chat bot and use it to generate very natural sounding it to generate very natural sounding it to generate very natural",
      "sounding language even though you might say but language even though you might say but language even though you might say but but Jeff there are Protections in some but Jeff there are Protections in some but Jeff there are Protections in some of these chat Bots that if you tell it of these chat Bots that if you tell it of these chat Bots that if you tell it to write you a phishing email it won't to write you a phishing email it won't to write you a phishing email it won't do to it there are also",
      "ways of do to it there are also ways of do to it there are also ways of re-engineering your prompt so that you re-engineering your prompt so that you re-engineering your prompt so that you can get past that so this is one area can get past that so this is one area can get past that so this is one area where phishing attacks are going to get where phishing attacks are going to get where phishing attacks are going to get better and the ways that we've been able better and the ways that we've been",
      "able better and the ways that we've been able to detect them in the past are not going to detect them in the past are not going to detect them in the past are not going to be so effective anymore to be so effective anymore to be so effective anymore what's another thing well on the what's another thing well on the what's another thing well on the positive side this generative Ai and positive side this generative Ai and positive side this generative Ai and chat Bots and things like that are able",
      "chat Bots and things like that are able chat Bots and things like that are able to write code for us so if I want to I to write code for us so if I want to I to write code for us so if I want to I can have it write code and do it really can have it write code and do it really can have it write code and do it really quickly and effectively it also means it quickly and effectively it also means it quickly and effectively it also means it could write malware as well it also could write malware as",
      "well it also could write malware as well it also means it could insert malware into the means it could insert malware into the means it could insert malware into the code that I have it also means it could code that I have it also means it could code that I have it also means it could insert back doors into the code that I insert back doors into the code that I insert back doors into the code that I have so we got we have got to also have so we got we have got to also have so we got we have got",
      "to also verify when we ask it to write code for verify when we ask it to write code for verify when we ask it to write code for us then in fact the code that it's us then in fact the code that it's us then in fact the code that it's giving us is pure and is doing what we giving us is pure and is doing what we giving us is pure and is doing what we intend for it to do intend for it to do intend for it to do another thing it could do to us another thing it could do to us another thing it could do",
      "to us misinformation misinformation misinformation how does this happen well these are how does this happen well these are how does this happen well these are generative AIS so one of the things that generative AIS so one of the things that generative AIS so one of the things that they suffer from is this issue we call they suffer from is this issue we call they suffer from is this issue we call hallucination where it may make up hallucination where it may make up hallucination where it may make",
      "up information or conflate two things that information or conflate two things that information or conflate two things that are not really related to each other and are not really related to each other and are not really related to each other and give a false impression also we could give a false impression also we could give a false impression also we could have a determined attacker who is doing have a determined attacker who is doing have a determined attacker who is doing what's known as a",
      "prompt injection where what's known as a prompt injection where what's known as a prompt injection where they're inserting bad information into they're inserting bad information into they're inserting bad information into the system or they're attacking the the system or they're attacking the the system or they're attacking the Corpus that is the body of knowledge Corpus that is the body of knowledge Corpus that is the body of knowledge that the system is based on and if they that the system is",
      "based on and if they that the system is based on and if they were able to do that then what comes out were able to do that then what comes out were able to do that then what comes out would be wrong information so we have to would be wrong information so we have to would be wrong information so we have to be careful to guard against be careful to guard against be careful to guard against over-reliance and make sure that we're over-reliance and make sure that we're over-reliance and make sure that",
      "we're verifying and testing our sources so verifying and testing our sources so verifying and testing our sources so that we can make sure that they're that we can make sure that they're that we can make sure that they're trustworthy one other example I'll give trustworthy one other example I'll give trustworthy one other example I'll give you here and there are actually many but you here and there are actually many but you here and there are actually many but I think this one's particularly I",
      "think this one's particularly I think this one's particularly interesting is this idea of a deep fake interesting is this idea of a deep fake interesting is this idea of a deep fake a deep fake is where we basically have a deep fake is where we basically have a deep fake is where we basically have an AI system that is able to copy your an AI system that is able to copy your an AI system that is able to copy your image and likeness your mannerisms your image and likeness your mannerisms your image",
      "and likeness your mannerisms your voice your appearance all of these voice your appearance all of these voice your appearance all of these things to the point where someone is things to the point where someone is things to the point where someone is looking at a video of you and they can't looking at a video of you and they can't looking at a video of you and they can't tell if it really was an actual video of tell if it really was an actual video of tell if it really was an actual video of you",
      "or a deep fake where we could have you or a deep fake where we could have you or a deep fake where we could have you saying things that weren't true and you saying things that weren't true and you saying things that weren't true and therefore if we're going to trust this therefore if we're going to trust this therefore if we're going to trust this kind of system we need a way to verify kind of system we need a way to verify kind of system we need a way to verify these things but right now the",
      "Deep fake these things but right now the Deep fake these things but right now the Deep fake technology has gone so far ahead in a technology has gone so far ahead in a technology has gone so far ahead in a very short period of time that it's very short period of time that it's very short period of time that it's going to be hard to verify those kinds going to be hard to verify those kinds going to be hard to verify those kinds of things of things of things okay we've just talked about what AI can",
      "okay we've just talked about what AI can okay we've just talked about what AI can do to us now let's look at some do to us now let's look at some do to us now let's look at some positives what can AI do for us in the positives what can AI do for us in the positives what can AI do for us in the cyber security space it turns out a lot cyber security space it turns out a lot cyber security space it turns out a lot in fact we do a survey each year that we in fact we do a survey each year that we in",
      "fact we do a survey each year that we call the cost of a data breach survey call the cost of a data breach survey call the cost of a data breach survey and the report that came back this year and the report that came back this year and the report that came back this year indicated that the number one thing you indicated that the number one thing you indicated that the number one thing you can do to save on the cost of a data can do to save on the cost of a data can do to save on the cost of a",
      "data breach and improve your response time is breach and improve your response time is breach and improve your response time is the extensive use of AI and Automation the extensive use of AI and Automation the extensive use of AI and Automation and here's what it can do on the one and here's what it can do on the one and here's what it can do on the one hand it can save on average 176 million hand it can save on average 176 million hand it can save on average 176 million dollars per data breach",
      "with the average dollars per data breach with the average dollars per data breach with the average data breach costing four and a half data breach costing four and a half data breach costing four and a half million that's a significant savings million that's a significant savings million that's a significant savings it can also cut down the mean time to it can also cut down the mean time to it can also cut down the mean time to identify and contain a breach by a identify and contain a breach by a",
      "identify and contain a breach by a hundred and eight days that makes a big hundred and eight days that makes a big hundred and eight days that makes a big difference so we know this is effective difference so we know this is effective difference so we know this is effective now what are we doing to make these now what are we doing to make these now what are we doing to make these kinds of results well it turns out a lot kinds of results well it turns out a lot kinds of results well it turns out a",
      "lot of what we do in this space is to do of what we do in this space is to do of what we do in this space is to do better analysis better analysis better analysis we're going to analyze large data sets we're going to analyze large data sets we're going to analyze large data sets lots of information that we have out lots of information that we have out lots of information that we have out there it's very hard to find patterns if there it's very hard to find patterns if there it's very hard to find",
      "patterns if I give you a whole large data set but if I give you a whole large data set but if I give you a whole large data set but if I use a technology called machine I use a technology called machine I use a technology called machine learning I can do a lot better job of learning I can do a lot better job of learning I can do a lot better job of spotting outliers and anomalies which is spotting outliers and anomalies which is spotting outliers and anomalies which is what we want to do in",
      "security a lot now what we want to do in security a lot now what we want to do in security a lot now I mentioned machine learning what is I mentioned machine learning what is I mentioned machine learning what is that well if you think about AI in that well if you think about AI in that well if you think about AI in particular as this large sort of particular as this large sort of particular as this large sort of umbrella term with a number of umbrella term with a number of umbrella term with a",
      "number of Technologies involved well Machine Technologies involved well Machine Technologies involved well Machine learning is a subset of that that learning is a subset of that that learning is a subset of that that specifically deals with some of these specifically deals with some of these specifically deals with some of these kind of analyzes that I've just referred kind of analyzes that I've just referred kind of analyzes that I've just referred to machine learning is what is often to machine",
      "learning is what is often to machine learning is what is often used in the security space we do it a used in the security space we do it a used in the security space we do it a lot because again it's very good at lot because again it's very good at lot because again it's very good at spotting anomalies and outliers and spotting anomalies and outliers and spotting anomalies and outliers and patterns and that's what we need a lot patterns and that's what we need a lot patterns and that's what we",
      "need a lot of in the security space so we're doing of in the security space so we're doing of in the security space so we're doing a lot of this today and a lot of these a lot of this today and a lot of these a lot of this today and a lot of these results come from leveraging machine results come from leveraging machine results come from leveraging machine learning which is a subfield of AI what learning which is a subfield of AI what learning which is a subfield of AI what else I mentioned",
      "automation well AI can else I mentioned automation well AI can else I mentioned automation well AI can help us in the automation task as well help us in the automation task as well help us in the automation task as well and I'll give you a few examples coming and I'll give you a few examples coming and I'll give you a few examples coming up but some of the things it can do is up but some of the things it can do is up but some of the things it can do is anticipate what we need to do next and",
      "anticipate what we need to do next and anticipate what we need to do next and some of those kind of things really some of those kind of things really some of those kind of things really start coming in from the area of deep start coming in from the area of deep start coming in from the area of deep learning which is a subfield of machine learning which is a subfield of machine learning which is a subfield of machine learning and then now this really new learning and then now this really new",
      "learning and then now this really new area that everyone is talking about area that everyone is talking about area that everyone is talking about these days Foundation models or you may these days Foundation models or you may these days Foundation models or you may hear them called large language models hear them called large language models hear them called large language models generative AI chat Bots they all exist generative AI chat Bots they all exist generative AI chat Bots they all exist",
      "in this space down here what can we in this space down here what can we in this space down here what can we start doing as I said security has start doing as I said security has start doing as I said security has mostly leveraged this in the past what mostly leveraged this in the past what mostly leveraged this in the past what can we start doing to leverage some of can we start doing to leverage some of can we start doing to leverage some of this stuff going forward well it turns this stuff",
      "going forward well it turns this stuff going forward well it turns out a lot of things because one of the out a lot of things because one of the out a lot of things because one of the things that Foundation models are really things that Foundation models are really things that Foundation models are really good at is summarizing good at is summarizing good at is summarizing they can be fed a lot of information and they can be fed a lot of information and they can be fed a lot of information and",
      "then it can give you a very quick then it can give you a very quick then it can give you a very quick summary of that why would that be useful summary of that why would that be useful summary of that why would that be useful well if you've got tons of documents well if you've got tons of documents well if you've got tons of documents you're trying to review it could give you're trying to review it could give you're trying to review it could give you the net the cliff notes of that you the net the",
      "cliff notes of that you the net the cliff notes of that another good use case for this would be another good use case for this would be another good use case for this would be incident summarization and case incident summarization and case incident summarization and case summarization if I'm seeing lots and summarization if I'm seeing lots and summarization if I'm seeing lots and lots of cases in my environment this lots of cases in my environment this lots of cases in my environment this kind of",
      "Technology could be used to tell kind of Technology could be used to tell kind of Technology could be used to tell me what are the trends among those cases me what are the trends among those cases me what are the trends among those cases are these things all related or are they are these things all related or are they are these things all related or are they all very different and my guess is all very different and my guess is all very different and my guess is they're probably at least a few",
      "things they're probably at least a few things they're probably at least a few things that are similar about these so that's that are similar about these so that's that are similar about these so that's an another nice use case that we'll see an another nice use case that we'll see an another nice use case that we'll see coming in the future from generative AI coming in the future from generative AI coming in the future from generative AI Foundation models into cyber security Foundation models",
      "into cyber security Foundation models into cyber security some other things we can do we know some other things we can do we know some other things we can do we know these kind of chat Bots are good at these kind of chat Bots are good at these kind of chat Bots are good at interacting so you can respond to them in natural so you can respond to them in natural so you can respond to them in natural language you don't have to format your language you don't have to format your language you don't have",
      "to format your queries using a particular query queries using a particular query queries using a particular query language or using a particular syntax language or using a particular syntax language or using a particular syntax you use the natural language that you're you use the natural language that you're you use the natural language that you're used to so for me I would state in used to so for me I would state in used to so for me I would state in English English English what are we being",
      "affected by this what are we being affected by this what are we being affected by this particular kind of malware and maybe particular kind of malware and maybe particular kind of malware and maybe what it could do is build a query for me what it could do is build a query for me what it could do is build a query for me that I can then run into my environment that I can then run into my environment that I can then run into my environment and it comes back and tells me am I and it comes back and",
      "tells me am I and it comes back and tells me am I affected or not and I can then ask more affected or not and I can then ask more affected or not and I can then ask more questions tell me more about this kind questions tell me more about this kind questions tell me more about this kind of malware what kind of indicators of of malware what kind of indicators of of malware what kind of indicators of compromise are there that are associated compromise are there that are associated compromise are",
      "there that are associated with this all of that stuff gives me a with this all of that stuff gives me a with this all of that stuff gives me a very easy intuitive way to get very easy intuitive way to get very easy intuitive way to get information that is highly technical out information that is highly technical out information that is highly technical out of the system and do this much faster of the system and do this much faster of the system and do this much faster another thing we might want",
      "to do is another thing we might want to do is another thing we might want to do is generate playbooks generate playbooks generate playbooks playbooks are the things that we use in playbooks are the things that we use in playbooks are the things that we use in incident response when we're trying to incident response when we're trying to incident response when we're trying to figure out what do we need to do once figure out what do we need to do once figure out what do we need to do once we've had",
      "an incident so generating we've had an incident so generating we've had an incident so generating these on the Fly generative AI these on the Fly generative AI these on the Fly generative AI generating playbooks you can see where generating playbooks you can see where generating playbooks you can see where there might be some type of crossover there might be some type of crossover there might be some type of crossover this is a good use case also for this this is a good use case also for this",
      "this is a good use case also for this technology so expect to see more of that technology so expect to see more of that technology so expect to see more of that and in fact there could be other types and in fact there could be other types and in fact there could be other types of things where we're using generative of things where we're using generative of things where we're using generative creative technology because these things creative technology because these things creative technology",
      "because these things really are creating for instance with really are creating for instance with really are creating for instance with threat hunting threat hunting threat hunting a threat Hunter is basically coming up a threat Hunter is basically coming up a threat Hunter is basically coming up with a hypothesis and saying I wonder if with a hypothesis and saying I wonder if with a hypothesis and saying I wonder if someone were to attack us maybe they someone were to attack us maybe they someone",
      "were to attack us maybe they would do the following things would do the following things would do the following things and we have a limitation in terms of our and we have a limitation in terms of our and we have a limitation in terms of our imagination sometimes the bad guys may imagination sometimes the bad guys may imagination sometimes the bad guys may dream up scenarios that we don't so it dream up scenarios that we don't so it dream up scenarios that we don't so it might be useful to have a",
      "system that might be useful to have a system that might be useful to have a system that can dream up scenarios we didn't think can dream up scenarios we didn't think can dream up scenarios we didn't think of using a generative AI to generate of using a generative AI to generate of using a generative AI to generate hypothetical cases that we then go out hypothetical cases that we then go out hypothetical cases that we then go out and automate and do a threat hunt in our and automate and do a",
      "threat hunt in our and automate and do a threat hunt in our environment this is all really super environment this is all really super environment this is all really super exciting stuff I think and it shows exciting stuff I think and it shows exciting stuff I think and it shows exactly what we'll be able to do in this exactly what we'll be able to do in this exactly what we'll be able to do in this space because what we want to be able to space because what we want to be able to space because",
      "what we want to be able to do is move away from being purely do is move away from being purely do is move away from being purely reactive to a more proactive reactive to a more proactive reactive to a more proactive way of doing cyber security and that's way of doing cyber security and that's way of doing cyber security and that's the good news in this story we've got Ai the good news in this story we've got Ai the good news in this story we've got Ai and cyber security and if they're and cyber",
      "security and if they're and cyber security and if they're working together as you see here we can working together as you see here we can working together as you see here we can end up with a more proactive Solution end up with a more proactive Solution end up with a more proactive Solution that's more cost effective and keeps us that's more cost effective and keeps us that's more cost effective and keeps us all much safer all much safer all much safer thanks for watching if you found this thanks",
      "for watching if you found this thanks for watching if you found this video interesting and would like to video interesting and would like to video interesting and would like to learn more about cyber security please learn more about cyber security please learn more about cyber security please remember to hit like And subscribe to remember to hit like And subscribe to remember to hit like And subscribe to this channel"
    ],
    "chunk_count": 56,
    "content_id": "39e6eddb-6368-4123-8410-8922f4a01459",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555029"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=XtT5i0ZeHHE": {
    "title": "AI Inference: The Secret to AI's Superpowers",
    "url": "https://www.youtube.com/watch?v=XtT5i0ZeHHE",
    "description": "Download the AI model guide to learn more → https://ibm.biz/BdaJTb\nLearn more about the technology → https://ibm.biz/BdaJTp\n\nExplore the world of AI Inference, a game-changing technology that's transforming the way we make decisions and interact with machines. Martin Keen gets into the basics of AI Inference, including what it is, how it works, and its exciting applications in real-world scenarios. By leveraging Data-Driven Decision Making, AI Inference enables organizations to make more accurate and informed decisions, leading to improved outcomes and increased efficiency. \n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdaJT8",
    "duration": 641,
    "uploader": "IBM Technology",
    "transcript": "Video Description: Download the AI model guide to learn more → https://ibm.biz/BdaJTb\nLearn more about the technology → https://ibm.biz/BdaJTp\n\nExplore the world of AI Inference, a game-changing technology that's transforming the way we make decisions and interact with machines. Martin Keen gets into the basics of AI Inference, including what it is, how it works, and its exciting applications in real-world scenarios. By leveraging Data-Driven Decision Making, AI Inference enables organizations to make more accurate and informed decisions, leading to improved outcomes and increased efficiency. \n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdaJT8",
    "chunks": [
      "Video Description: Download the AI model guide to learn more → https://ibm.biz/BdaJTb Learn more about the technology → https://ibm.biz/BdaJTp Explore the world of AI Inference, a game-changing technology that's transforming the way we make decisions and interact with machines. Martin Keen gets into the basics of AI Inference, including what it is, how it works, and its exciting applications in real-world scenarios. By leveraging Data-Driven Decision Making, AI Inference enables organizations to",
      "make more accurate and informed decisions, leading to improved outcomes and increased efficiency. AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdaJT8"
    ],
    "chunk_count": 2,
    "content_id": "60132308-6835-4e9c-a113-cbb22c84e207",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555033"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=sYDlVVyJYn4": {
    "title": "What is Mixture of Experts?",
    "url": "https://www.youtube.com/watch?v=sYDlVVyJYn4",
    "description": "Want to play with the technology yourself? Explore our interactive demo → https://ibm.biz/BdK8fn\nLearn more about the technology → https://ibm.biz/BdK8fe\n\nIn this video, Master Inventor Martin Keen explains the concept of Mixture of Experts (MoE), a machine learning approach that divides an AI model into separate subnetworks or experts, each focusing on a subset of the input data. Martin discusses the architecture, advantages, and challenges of MoE, including sparse layers, routing, and load balancing.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdK8fb",
    "duration": 477,
    "uploader": "IBM Technology",
    "transcript": "Video Description: Want to play with the technology yourself? Explore our interactive demo → https://ibm.biz/BdK8fn\nLearn more about the technology → https://ibm.biz/BdK8fe\n\nIn this video, Master Inventor Martin Keen explains the concept of Mixture of Experts (MoE), a machine learning approach that divides an AI model into separate subnetworks or experts, each focusing on a subset of the input data. Martin discusses the architecture, advantages, and challenges of MoE, including sparse layers, routing, and load balancing.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdK8fb",
    "chunks": [
      "Video Description: Want to play with the technology yourself? Explore our interactive demo → https://ibm.biz/BdK8fn Learn more about the technology → https://ibm.biz/BdK8fe In this video, Master Inventor Martin Keen explains the concept of Mixture of Experts (MoE), a machine learning approach that divides an AI model into separate subnetworks or experts, each focusing on a subset of the input data. Martin discusses the architecture, advantages, and challenges of MoE, including sparse layers,",
      "routing, and load balancing. AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdK8fb"
    ],
    "chunk_count": 2,
    "content_id": "5d266287-e3e0-456d-99ed-d52288be73ce",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555036"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=OThahaOga20": {
    "title": "How to Add AI to Your Apps Faster with Embedded AI",
    "url": "https://www.youtube.com/watch?v=OThahaOga20",
    "description": "Try out our embeddable AI technology → https://ibm.biz/BdMx6m\n\nWondering what embeddable AI is, how one actually goes about embedding AI ... and where?\nIn this video, Martin Keen talks about AI deployment, specifically how to deploy embeddable AI, and centers his discussion on the 2 major methods: containerized libraries and applications.\n\nGet started for free on IBM Cloud → https://ibm.biz/sign-up-now\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#ai #watsonx",
    "duration": 455,
    "uploader": "IBM Technology",
    "transcript": "Video Description: Try out our embeddable AI technology → https://ibm.biz/BdMx6m\n\nWondering what embeddable AI is, how one actually goes about embedding AI ... and where?\nIn this video, Martin Keen talks about AI deployment, specifically how to deploy embeddable AI, and centers his discussion on the 2 major methods: containerized libraries and applications.\n\nGet started for free on IBM Cloud → https://ibm.biz/sign-up-now\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#ai #watsonx",
    "chunks": [
      "Video Description: Try out our embeddable AI technology → https://ibm.biz/BdMx6m Wondering what embeddable AI is, how one actually goes about embedding AI ... and where? In this video, Martin Keen talks about AI deployment, specifically how to deploy embeddable AI, and centers his discussion on the 2 major methods: containerized libraries and applications. Get started for free on IBM Cloud → https://ibm.biz/sign-up-now Subscribe to see more videos like this in the future →",
      "http://ibm.biz/subscribe-now #ai #watsonx"
    ],
    "chunk_count": 2,
    "content_id": "9422da50-3212-4450-9a2d-bcc179b28b5c",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555041"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=Gafjk7_w1i8": {
    "title": "The Power of Recurrent Neural Networks (RNN)",
    "url": "https://www.youtube.com/watch?v=Gafjk7_w1i8",
    "description": "Want to play with the technology yourself? Explore our interactive demo → https://ibm.biz/BdK5Un\nLearn more about the technology → https://ibm.biz/BdK5Ue\n\nUnlock the power of Recurrent Neural Networks (RNN) Whether you're a beginner or looking to refresh your knowledge, this video will provide a clear and concise overview of RNNs, including their architecture, applications, and how they differ from other neural networks.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdK5Ub\n\n#Recurrent Neural Networks #RNN #Deep Learning #Neural Networks",
    "duration": 466,
    "uploader": "IBM Technology",
    "transcript": "Video Description: Want to play with the technology yourself? Explore our interactive demo → https://ibm.biz/BdK5Un\nLearn more about the technology → https://ibm.biz/BdK5Ue\n\nUnlock the power of Recurrent Neural Networks (RNN) Whether you're a beginner or looking to refresh your knowledge, this video will provide a clear and concise overview of RNNs, including their architecture, applications, and how they differ from other neural networks.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdK5Ub\n\n#Recurrent Neural Networks #RNN #Deep Learning #Neural Networks",
    "chunks": [
      "Video Description: Want to play with the technology yourself? Explore our interactive demo → https://ibm.biz/BdK5Un Learn more about the technology → https://ibm.biz/BdK5Ue Unlock the power of Recurrent Neural Networks (RNN) Whether you're a beginner or looking to refresh your knowledge, this video will provide a clear and concise overview of RNNs, including their architecture, applications, and how they differ from other neural networks. AI news moves fast. Sign up for a monthly newsletter for",
      "AI updates from IBM → https://ibm.biz/BdK5Ub #Recurrent Neural Networks #RNN #Deep Learning #Neural Networks"
    ],
    "chunk_count": 2,
    "content_id": "abedf294-7600-45ea-bdd3-38f190228593",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555044"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=5uNifnVlBy4": {
    "title": "Cybersecurity Architecture: Who Are You? Identity and Access Management",
    "url": "https://www.youtube.com/watch?v=5uNifnVlBy4",
    "description": "IBM Security QRadar EDR :   https://ibm.biz/BdyzJA\n\nIBM Security X-Force Threat Intelligence Index 2023:  https://ibm.biz/BdyzJ9 \n\nWho are you? What system capabilities are you permitted? If you leave the company, what accesses need to be removed? These are all questions that fall in the identity and access management domain. In the fourth episode of the series, Jeff Crume explains IAM, which he defines as \"the new perimeter\" of cybersecurity.\n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-up\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n#AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume",
    "duration": 1874,
    "uploader": "IBM Technology",
    "transcript": "Video Description: IBM Security QRadar EDR :   https://ibm.biz/BdyzJA\n\nIBM Security X-Force Threat Intelligence Index 2023:  https://ibm.biz/BdyzJ9 \n\nWho are you? What system capabilities are you permitted? If you leave the company, what accesses need to be removed? These are all questions that fall in the identity and access management domain. In the fourth episode of the series, Jeff Crume explains IAM, which he defines as \"the new perimeter\" of cybersecurity.\n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-up\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n#AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume",
    "chunks": [
      "Video Description: IBM Security QRadar EDR : https://ibm.biz/BdyzJA IBM Security X-Force Threat Intelligence Index 2023: https://ibm.biz/BdyzJ9 Who are you? What system capabilities are you permitted? If you leave the company, what accesses need to be removed? These are all questions that fall in the identity and access management domain. In the fourth episode of the series, Jeff Crume explains IAM, which he defines as \"the new perimeter\" of cybersecurity. Get started for free on IBM Cloud →",
      "https://ibm.biz/ibm-cloud-sign-up Subscribe to see more videos like this in the future → http://ibm.biz/subscribe-now #AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume"
    ],
    "chunk_count": 2,
    "content_id": "96f9a762-c163-4ecb-a6b1-d86468688313",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555047"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=sesacY7Xz3c": {
    "title": "Cybersecurity Architecture: Networks",
    "url": "https://www.youtube.com/watch?v=sesacY7Xz3c",
    "description": "IBM Security QRadar EDR  → https://ibm.biz/BdymsM \nIBM Security X-Force Threat Intelligence Index 2023 → https://ibm.biz/Bdymsv  \n\nNetworks are your company's connection to the world, and therefore one of they key players in a cybersecurity architecture. In the sixth installment of the series, IBM Distinguished Engineer and Adjunct Professor Jeff Crume introduces and explains the elements of network security, including firewalls, VPNs, and lower-level topics like network packet security-risk detection.\n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-up\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume #networksecurity",
    "duration": 1651,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Welcome back to the Cybersecurity Architecture Series. In the previous two videos, I talked about identity management and endpoint security, and now we're going to focus on the network. The network security involves a lot of different elements, and we're going to talk about each of these-- --about firewalls, which are a fundamental component of this, about segmentation, which we're able to do with using firewalls, about virtual private networks, about SASE. You'll hear more about that in a few minutes. And then, actually, the topic is so large and so mature, you really can't cover it all in this space. So there's going to be some things I'm not going to get a chance to talk about. But needless to say, there's a lot more that could be discussed here. So, first thing we're going to talk about are firewalls. So let's talk about what are we going to do with firewalls. The idea behind them in the first place where the technology and the notion even came from is in the physical world, a physical firewall. Let's say we've got three townhouses all connected. And this guy has a fire in his unit. Well, what we'd like is to have some way to limit the spread of that fire from one unit to the next. It might not make it completely go away, but it slows it down so that the fire department can get there and do something about it. So it's a way of creating isolation and protection from a dangerous event. So take that concept in mind and then apply it to a network scenario. Here we've got a user on a workstation hitting a web server, and that web server then goes against a database. The typical architecture we put in here is we add a firewall here and a firewall here. One that's Internet facing and one that is internal facing. And what we're going to do, the reason we're putting two of these here, we'll talk more about it in segmentation. But simply what we're going to do here, in the most basic form, is we're going to filter based upon certain things that are happening here. This is this notion of packet filtering. With packet filtering, I'm going to look into the packet, that is, the information that's being sent from this guy to this guy. And in that, he's going to include things like his source address, that is, the address that he's coming from, his destination address, where he wants it to go, which is going to be this web server and the port that he's going to use, which is a way of designating what kind of traffic it is. So this firewall can then filter based upon that information that's in the header of that packet. And he's going to look at and say, okay, port 80, that's the standard industry standard for unencrypted Internet traffic. So I'm going to allow that web traffic to come through on port 80, that we will allow. We will also allow encrypted traffic. So this is your SSL or now TLS encrypted web traffic. So I'm going to open this first firewall to allow that information to flow through. But I'm also going to add some more things to make it a little tighter. I'm going to say the source address has to be in the range of the external Internet. Why would we look at that? Well, I want to make sure that somebody isn't spoofing and acting like they're coming from inside. Sometimes we'll trust traffic that comes from the inside and give it more privileges and sort of drop our guard. We don't want that to happen from external traffic. So if this packet is literally coming from the outside and claims to be coming from the inside, we're going to block it. It's just going to be blocked right here. The destination address, we're going to say, where you can go is you must in fact only go there. If this guy tries to put any other address, like here, if he tries to go to the database directly, that will be blocked, because that destination address is not something we're going to allow through the first firewall. Now we have a second firewall here that's going to add even a little more security. It's going to say this is the traffic, this is the port we're going to open up to allow this kind of traffic between the web server and the database. We're going to allow a source address only of the web server. So I'm not going to allow anything that came from the Internet, the external world, if it's got that address as its source, where it started from, I'm not going to allow it. It has to have originated here and its destination can only be that. So what we've done in creating that set of rules is any traffic from the outside can only get to and must stop over here where it can be inspected and where we can implement some sort of security controls. It cannot get anywhere else. So that's what we're basically doing here, and you can keep applying these kinds of rules in order to tighten up your security. And what we're doing, what I've described here, is basically this idea of packet filtering where we're essentially just looking here at the packet that is, the first part of the packet, the header. So think of it as like an envelope. If I'm mailing a letter to you, what's going to be on the envelope? Well, we're going to have a to address and we're going to have a return address. This is your to address. This is your return address. So that's what we've basically done here. But we have not looked inside. So there's no inspection, here. But in this next example, I'm going to talk about stateful packet inspection. Where in this case, we're going to look not only at the packet, but we may also go ahead and look at the full thing, look at what's in the payload as well. And there are other things like application firewalls that are specific and they'll do even more inspection of this payload. That is the data that you're actually sending and make sure that it's not going to do harm to us. And in this case, it's more like the open envelope so that we're able to see the contents, not only the to and the from, but we can also see the contents as well. So that's an analogy to help you understand what this is. Stateful packet inspection also looks at the context of the packet. So it's looking at you sent one of these, and then one of these, but you should never have sent that next thing because that breaks all the rules. So it can look at more than just individual packets in isolation, which is what packet filtering does. This was sort of our first generation of firewalling, and then it got more sophisticated when we added stateful packet inspection. But there are other things here as well in this sort of firewall and technologies. Think about these all as a collection of technologies that can be part or not of a particular firewall. And the next one is this idea of a proxy. A proxy is something that acts on behalf of something else. So we have here a workstation that's coming in from the outside and it wants to hit this web server, for instance. Except what I'm going to do is say I'm going to put another server here in between that is the proxy. And what's going to happen is I'm going to break this session. Right now what you see depicted is a direct connection, end-to-end. I insert a proxy, I'm going to tell this guy, you're going to communicate directly with me and he is going to communicate directly with the back end Web server. Now we have two sessions. And what it is, is this guy thinks he's talking here, but in fact, he's talking to this guy. This guy thinks he's talking to this. In fact, he's talking to that. Now we have effectively a man-in-the-middle. But it's a good man in the middle. It's one that we put in there so that we can inspect the traffic. By the way, traffic coming in from the outside into my internal network. Maybe I want to have a look at that before I allow it in, see if it's got viruses or things like that. So I can inspect and I can enforce a security policy, if I need to. That's one of the advantages of putting in a proxy. By the way, sometimes people also put in proxies for privacy reasons, so that they can guard against who's seeing exactly who's doing what. It just looks like everything is coming from the proxy, not the actual end user, in that case. The final bit that I want to talk about here is actually maybe one of the most pervasive of these technologies, network address translation, or NAT for short. This is something that is probably in your home and you're probably using it right now and you may not know it. With NAT, what we do is, the Internet-- So by convention, we've all agreed by industry standard that there is a range of addresses that are reliable across the Internet and a range that are not reliable across the Internet. So this is specified in the in the rules for Internet traffic. If the address starts with a 10, then it doesn't matter what the other numbers are after that, because Internet addresses are always these four numbers separated by periods. That's the way we depict those. If it starts with a 10, if it's a 10 dot address, it is not routable across the Internet. It is routable across an internal intranet or across your home network. If you've got a home router, Wi-Fi router, whatever, you probably will find that it used a 10 dot address or more commonly it's used one of these 192.168 dot something dot something. That's very common in home setups. So this is an internal address that cannot be routed across the internet. That's why we need this NAT box. The NAT box does the translation, that's the T in NAT. So if this guy wants to send traffic that goes out to the Internet, his internal address, if he put that out directly, it would hit the first router on the internet and get blocked. It wouldn't go anywhere. But in fact, what we're going to do, is the NAT table, the NAT router or NAT firewall maintains a table where it's going to translate this into an external address, usually just a single address that is recognized for everything that's behind here. So it actually conserves IP addresses. I don't need I could have 100 of these different devices back here. They all look like just one address going out to the Internet. So I'm going to translate the traffic as it comes here into something that's routable so it can get out there and it will come back and then the NAT box will turn it back and send it back to the workstation that it needed to go to. Now that's just to preserve existing functionality. Where's the protection? The protection comes in the fact that if this guy out here wants to directly hit this workstation, he can't because the address for this workstation, this 192.168.1.1 is example is not routable across the internet. If he tries to send that, it won't go anywhere. So this way we have internal traffic and external traffic and we can flow this way. But it prevent someone from being able to get from the outside directly to the inside. And as I said, this is very common technology. It's usually built into all of the home routers. Okay, we just talked about firewalls. Now we're going to talk about segmentation. That is, how are we going to apply these firewalls in various network architectures to achieve different levels of security? Let's take the first one. This is the most primitive. I don't recommend this. Don't ever do this. But this in the early days of the Internet was a viable option for a lot of people. It's a bastion host. We basically take our web server and put it on the Internet because what we don't want is the internal network-- intranet --being exposed directly to the Internet. And if I don't put this somewhere outside here, then that means I have to allow all the Internet traffic into my internal network. And that's a really bad idea. So in the early days, people would put a single firewall right here, put their web server out here, or whatever devices, it had to be a bastion, last bastion on on the defense right out here on the edge. Again, not recommended. We have better ways to do it. The sort of next generation of this was a tri-homed network where in this case we've created essentially a firewall that we've carved off here into three different networks. So the firewall that's sitting here recognizes traffic coming in on one network interface card. And this traffic, for instance, all of the Internet traffic will be directed directly into where our Web server is. It's coming in on this network interface card, it will automatically be directed there. And maybe we apply some rules like the packet filtering and stuff like that. But that's where the traffic is destined to go in most cases. And likely in this case, the internal traffic, it's coming in on this network interface card, it could be routed here if somebody's internal user wants to hit our website. But it could also just as equally be trying to go out somewhere here. So here we have a single firewall sitting there, but it's got three different network interface cards. So it's trying to kind of do a lot of work in one place. And that's why it's called tri-homed. This is a DMZ of sorts. A demilitarized zone is the term we use there to refer to an area that's a buffer between an untrusted environment and a more trusted environment. Now, I'm going to use those terms very simplistically with apologies to people who understand what zero trust means, that there's really no trusted networks. But that's why we're essentially trying to show here's the red zone, the untrusted. The yellow zone is the semi trusted and the green zone is the more trusted zone. That's the idea here behind the color coding. Okay, move to the next one, the basic DMZ. And this is very popular. By the way, this last one is one that's often done in home networks. If you want to have an internal network and you want to allow guests to have access or your IoT devices or things like that. If you're hosting your own web server on your home network, not a great idea, but you could do it. You might want to separated out like this because again, it's a low cost option. That's the advantage is, it's very scalable. It's cheap. But on the downside, it's a single point of failure. If this thing doesn't do its job, everything's wide open potentially. Okay, moving on to the basic DMZ, which in this case I'm going to use two firewalls. So automatically I end up with more costs because I've got multiple security protections that I'm putting in place. And it's going to be more complex because I have to administer different rules and different capabilities here. And I gave an example of this in the first frame. In the first example before, when I talked about packet filtering and traffic coming in here and coming out there and so forth. That would be a basic DMZ. We got a red zone, a yellow zone, a green zone. Think of this as the traffic light. This is the danger untrusted, semi trusted caution. And then this is where it's more trusted. And we've built a kind of firewall rules to make sure that this can be trusted, because again, someone cannot go from here to here. We block that. We block that actually at this first firewall and then we have a secondary block here. And as a result, because we've got one block here and another block here, we have defense in depth. You remember going back to the very first video in this series, one of the principles I talked about that was important is this notion of defense in depth. I don't rely on any single security mechanism to protect me. If this firewall fails for some reason, I still can't get traffic from here to there because I've built in a rule that said the source address has to be traffic coming from this web server, for instance. And if this failed and all the traffic was able to come through it, then it would still be blocked by this second. So we've got defense in depth. It's also more scalable. So I could build up multiples of these, multiples of a lot of these kinds of things. So the opportunities are a lot greater. Again, not a single point of failure, but defense in depth. And then finally, I'll talk about a multi-tiered DMZ. So the multi-tiered DMZ, we basically put a firewall here and here. So now we've got this this diagram essentially replicated here. But in this case, I've split out the web server from the application server from the database, in this example. In this case, I'm going to implement yet another firewall, a third firewall in this case. So as you would guess, one of the downsides is it's going to be even more expensive than these others. It is going to be more costly and complex than these others, because now I've got three firewalls to administer and different rules on each one of them. However, we've got defense in depth on steroids. We've got even more because now any one of these mechanisms, it would have to be that all three of them failed. If one of them failed, it wouldn't necessarily be a huge problem for us. We also have greater granularity. That is, I can allow traffic to only go to here from this zone to this zone. I can allow traffic from this zone only to go to that zone and traffic from this zone only to go to that zone and do the reverse back. So more granularity, more firewalls, more cost and complexity. But potentially, if I do it right, more security. Okay, in the previous section, we talked about firewalls and segmentation. Now we're going to cover the next subject of virtual private networking VPNs. Now, what are VPNs designed to do? They're basically trying to give us a secure channel over an untrusted network. That's the idea. I can't necessarily trust the Internet because I don't control I don't have visibility into all of the aspects of that. But I'd like to be able to send secure information or information in a secure way over it. So I want a secure channel over an untrusted network. That would be a great capability. And the way I do that, I accomplish it by encrypting my information and then sending it over the network. The idea there is I get confidentiality. And I get that because people can't see what is in the packet. All they'll see is the encrypted information. We lots of times think about this as a pipe or as a tunnel. You'll hear those kind of analogies used here. Think about if we've got a user here with a browser trying to get to a web server and we're building a secure pipe, a connection from one end to the other. And I'm encrypting all the packets as they go across. So that way someone who looks here sees nothing that they can interpret, nothing that means anything. That's the idea behind a secure pipe, secure channel over an untrusted network. So that's the good stuff. And security guys love that we can do that. What they don't love is this. That is a limited inspection capability, this ability to-- so the good guy can send their traffic without everybody seeing it. It also means a bad guy could send their traffic without everybody seeing it and that would be a problem. So it limits our ability to inspect and therefore see if someone's putting malware into my system or someone's initiating an attack. So it's one of those blessings and a curse at the same time. All right. There are different kinds of VPN technologies. And to understand them, we really need a little bit of understanding on network technologies. A 7 layer OSI stack. This is classic stuff. We're not going to go into it in detail. But the notion is, is that there are different layers. For every packet I send, different concerns, different aspects that are implemented at different layers here. And what happens in the real world is most people, if you're an application programmer, you're really concerned more up here with the application, presentation layers, these kinds of things. And the networking infrastructure, people are much more concerned with the stuff down here at the transport network data link, physical layers and those kinds of things. So there's a little bit off concerns that separate there. But the other thing that's really important about this is, with this model, we have a way of, if you implement a security capability, for instance, at one of these layers, it's inherited by the upper layers. So if I encrypt all my traffic here, then it's encrypted by all the higher layers as well. So from a simple security standpoint, it might be easier to put the encryption down lower in the stack. We'll talk in a minute what hat's not necessarily what you always want to do, though. So there are different examples of how we do this. For instance, at the application layer. You may have heard of this protocol--secure shell. That's an example of an application layer or application specific VPN. It encrypts the data so that you can connect into a particular device console, this sort of thing. There's a secure FTP and other examples like that. Another one that's very common and you're going to run across this all the time, whether you're aware of it or not, is TLS or SSL--transport layer security or secure sockets layer. This is the older term. The newer name for this standard is TLS. It's implemented at the transport layer. That's what you usually see when you're a browser connected to a web server and you see that little lock in the browser up on the the URL line. That's what is is being implemented there, TLS. So that is everything that's going to that Web server then is going to be encrypted. There are other examples. There's a thing called IPsec, which is implemented at the network layer. If you do that, then everything between two network addresses will be encrypted as opposed to between the web server or for a very specific application. And then we have some other examples of point-to-point tunneling protocol, P2PTP or L2TP, which is the layer 2 tunneling protocol. These are some examples of even lower on the stack. So not to go into details on it, but just to give you an idea, there's not a single type of VPN or VPN technology. They all share some of these qualities. And if you see these things, you should think, ah that's a type of VPN. Now what's happening these days, is we're tending to move away from these broad network based VPNs more toward application specific VPNs. What's the reason for that? Well, on the advantage side for the broad ones, is that they're relatively simple. I set up a connection, for instance, between two endpoints or me into a particular network. And everything I do, for instance, if I set up an IPsec session, everything I send into that whole network now will be encrypted. So that's a very simple type of thing to do. However, it doesn't give us the granularity that we get over on this side with a very application specific firewall or VPN like we're doing here with SSH. The other advantage on the broad based side, network-based side, is a catch all. Again, if I encrypt at this layer, then all the traffic, all the different applications can benefit from that basic VPN. I don't have to set up separate ones. So it's simpler in that regard. However, I don't have as much control and as much granularity. The ability to control and say I have a VPN for my email, a VPN for my file sharing application, a VPN for my instant messaging application and control all of those separately gives me more control so that if I need to shut a particular service down or a particular user down, I can do that. So a lot of different possibilities here in this area of VPNs. Okay, now we've covered firewalls, segmentation, VPNs, and our last one is SASE. What is SASE?. It's secure access service edge. That's what the acronym stands for. It's actually a very relevant and important topic these days as part of the larger subject of zero trust. And in fact, another aspect of zero trust that's very relevant is micro-segmentation. And we talked about segmentation before, but micro-segmentation just carries that to the extreme and puts lots and lots of zones within your network with little micro networks. But SASE in particular. What we're trying to do here is create some sort of of secure capability that's delivered on the edge. To think about it this way. Let's look at a Venn diagram. Here, we've got networking concerns here. We've got security things and we've got the cloud. If you think about the intersection of network security and cloud, this is the world where SASE lives. Because-- and depending on how you want to think about this, if you think more mathematically, you might like this description, if you think more visually, you might like this one. We'll go through the mathematical one first. SASE is basically network security plus WAN, wide area networking capability. So that's the network and the security all delivered from the cloud. So that's the way of taking that Venn diagram and expressing it kind of as a loose mathematical equation. So if we were to decompose this a little more, what does it mean? What does the NetSEC mean, network security? Well, it's basically firewalling, which we've talked about. It's secure web gateways, which we didn't really go into detail, but think about those as application-specific firewalls and things of that sort. And DLP, data loss prevention, which is something I'll talk about in the data security domain when we get to that topic. But all of these things and more, delivered on the edge, so that's the network security component of all this. The WAN, specifically, is a software-defined WAN. Which is a way of creating a dynamic network where you can change where the boundaries of the network are and provision these in real-time, effectively. So it gives you a lot more agility and flexibility. So we're adding that capability, this SD-WAN marrying it, merging it, with the network security components and then delivering the thing from the cloud, because the cloud gives us the ability to do scalability. We can scale up and scale down elasticity and agility. Again, lots and lots of flexibility. That's what people are looking for in this case. If I take all of those things and then maybe even add in another thing within the security space, identity management, specifically, authentication and authorization. So some access controls, then this is what SASE is about. It's combining all of these functions into a single logical component and delivering that from the cloud, at the edge of the network. Another way to look at it is this way. So here we've got our users, here we've got the external network, and then I've got this SASE capability that's here in the middle. And what this delivers is on one end, the networking capabilities I mentioned previously, on the other end, also the security capabilities-- firewall and DLP and so forth. So all of this is is a way of combining these functions and delivering them. This is a more modern way of delivering all of these capabilities as opposed to what would have in the past each one of these would have been a separate appliance, a separate component, a separate administrative capability, a separate person to administer it and all of those kinds of things. So it's bringing these functions together and having them operate in a more holistic way. All right, now, we finished the networking topic. A couple of things that I didn't cover and I put here in the etc. just because of the interest of time, I didn't really get into very much in the physical networking side, things like 5G and Wi-Fi and the network security capabilities of those. If there's interest in that, put that down in the comments and maybe we'll revisit that in a future video. Until then, now we've completed the networking portion of our domains. The next one and, we want you to stay tuned and look for that one. The next one will be in the area of application security as we move along the various domains. Please remember to like, subscribe and hit notify so that you'll be aware when future videos in the series are available.",
    "chunks": [
      "Kind: captions Language: en Welcome back to the Cybersecurity Architecture Series. In the previous two videos, I talked about identity management and endpoint security, and now we're going to focus on the network. The network security involves a lot of different elements, and we're going to talk about each of these-- --about firewalls, which are a fundamental component of this, about segmentation, which we're able to do with using firewalls, about virtual private networks, about SASE. You'll",
      "hear more about that in a few minutes. And then, actually, the topic is so large and so mature, you really can't cover it all in this space. So there's going to be some things I'm not going to get a chance to talk about. But needless to say, there's a lot more that could be discussed here. So, first thing we're going to talk about are firewalls. So let's talk about what are we going to do with firewalls. The idea behind them in the first place where the technology and the notion even came from is",
      "in the physical world, a physical firewall. Let's say we've got three townhouses all connected. And this guy has a fire in his unit. Well, what we'd like is to have some way to limit the spread of that fire from one unit to the next. It might not make it completely go away, but it slows it down so that the fire department can get there and do something about it. So it's a way of creating isolation and protection from a dangerous event. So take that concept in mind and then apply it to a network",
      "scenario. Here we've got a user on a workstation hitting a web server, and that web server then goes against a database. The typical architecture we put in here is we add a firewall here and a firewall here. One that's Internet facing and one that is internal facing. And what we're going to do, the reason we're putting two of these here, we'll talk more about it in segmentation. But simply what we're going to do here, in the most basic form, is we're going to filter based upon certain things that",
      "are happening here. This is this notion of packet filtering. With packet filtering, I'm going to look into the packet, that is, the information that's being sent from this guy to this guy. And in that, he's going to include things like his source address, that is, the address that he's coming from, his destination address, where he wants it to go, which is going to be this web server and the port that he's going to use, which is a way of designating what kind of traffic it is. So this firewall",
      "can then filter based upon that information that's in the header of that packet. And he's going to look at and say, okay, port 80, that's the standard industry standard for unencrypted Internet traffic. So I'm going to allow that web traffic to come through on port 80, that we will allow. We will also allow encrypted traffic. So this is your SSL or now TLS encrypted web traffic. So I'm going to open this first firewall to allow that information to flow through. But I'm also going to add some more",
      "things to make it a little tighter. I'm going to say the source address has to be in the range of the external Internet. Why would we look at that? Well, I want to make sure that somebody isn't spoofing and acting like they're coming from inside. Sometimes we'll trust traffic that comes from the inside and give it more privileges and sort of drop our guard. We don't want that to happen from external traffic. So if this packet is literally coming from the outside and claims to be coming from the",
      "inside, we're going to block it. It's just going to be blocked right here. The destination address, we're going to say, where you can go is you must in fact only go there. If this guy tries to put any other address, like here, if he tries to go to the database directly, that will be blocked, because that destination address is not something we're going to allow through the first firewall. Now we have a second firewall here that's going to add even a little more security. It's going to say this is",
      "the traffic, this is the port we're going to open up to allow this kind of traffic between the web server and the database. We're going to allow a source address only of the web server. So I'm not going to allow anything that came from the Internet, the external world, if it's got that address as its source, where it started from, I'm not going to allow it. It has to have originated here and its destination can only be that. So what we've done in creating that set of rules is any traffic from the",
      "outside can only get to and must stop over here where it can be inspected and where we can implement some sort of security controls. It cannot get anywhere else. So that's what we're basically doing here, and you can keep applying these kinds of rules in order to tighten up your security. And what we're doing, what I've described here, is basically this idea of packet filtering where we're essentially just looking here at the packet that is, the first part of the packet, the header. So think of",
      "it as like an envelope. If I'm mailing a letter to you, what's going to be on the envelope? Well, we're going to have a to address and we're going to have a return address. This is your to address. This is your return address. So that's what we've basically done here. But we have not looked inside. So there's no inspection, here. But in this next example, I'm going to talk about stateful packet inspection. Where in this case, we're going to look not only at the packet, but we may also go ahead",
      "and look at the full thing, look at what's in the payload as well. And there are other things like application firewalls that are specific and they'll do even more inspection of this payload. That is the data that you're actually sending and make sure that it's not going to do harm to us. And in this case, it's more like the open envelope so that we're able to see the contents, not only the to and the from, but we can also see the contents as well. So that's an analogy to help you understand what",
      "this is. Stateful packet inspection also looks at the context of the packet. So it's looking at you sent one of these, and then one of these, but you should never have sent that next thing because that breaks all the rules. So it can look at more than just individual packets in isolation, which is what packet filtering does. This was sort of our first generation of firewalling, and then it got more sophisticated when we added stateful packet inspection. But there are other things here as well in",
      "this sort of firewall and technologies. Think about these all as a collection of technologies that can be part or not of a particular firewall. And the next one is this idea of a proxy. A proxy is something that acts on behalf of something else. So we have here a workstation that's coming in from the outside and it wants to hit this web server, for instance. Except what I'm going to do is say I'm going to put another server here in between that is the proxy. And what's going to happen is I'm",
      "going to break this session. Right now what you see depicted is a direct connection, end-to-end. I insert a proxy, I'm going to tell this guy, you're going to communicate directly with me and he is going to communicate directly with the back end Web server. Now we have two sessions. And what it is, is this guy thinks he's talking here, but in fact, he's talking to this guy. This guy thinks he's talking to this. In fact, he's talking to that. Now we have effectively a man-in-the-middle. But it's a",
      "good man in the middle. It's one that we put in there so that we can inspect the traffic. By the way, traffic coming in from the outside into my internal network. Maybe I want to have a look at that before I allow it in, see if it's got viruses or things like that. So I can inspect and I can enforce a security policy, if I need to. That's one of the advantages of putting in a proxy. By the way, sometimes people also put in proxies for privacy reasons, so that they can guard against who's seeing",
      "exactly who's doing what. It just looks like everything is coming from the proxy, not the actual end user, in that case. The final bit that I want to talk about here is actually maybe one of the most pervasive of these technologies, network address translation, or NAT for short. This is something that is probably in your home and you're probably using it right now and you may not know it. With NAT, what we do is, the Internet-- So by convention, we've all agreed by industry standard that there is",
      "a range of addresses that are reliable across the Internet and a range that are not reliable across the Internet. So this is specified in the in the rules for Internet traffic. If the address starts with a 10, then it doesn't matter what the other numbers are after that, because Internet addresses are always these four numbers separated by periods. That's the way we depict those. If it starts with a 10, if it's a 10 dot address, it is not routable across the Internet. It is routable across an",
      "internal intranet or across your home network. If you've got a home router, Wi-Fi router, whatever, you probably will find that it used a 10 dot address or more commonly it's used one of these 192.168 dot something dot something. That's very common in home setups. So this is an internal address that cannot be routed across the internet. That's why we need this NAT box. The NAT box does the translation, that's the T in NAT. So if this guy wants to send traffic that goes out to the Internet, his",
      "internal address, if he put that out directly, it would hit the first router on the internet and get blocked. It wouldn't go anywhere. But in fact, what we're going to do, is the NAT table, the NAT router or NAT firewall maintains a table where it's going to translate this into an external address, usually just a single address that is recognized for everything that's behind here. So it actually conserves IP addresses. I don't need I could have 100 of these different devices back here. They all",
      "look like just one address going out to the Internet. So I'm going to translate the traffic as it comes here into something that's routable so it can get out there and it will come back and then the NAT box will turn it back and send it back to the workstation that it needed to go to. Now that's just to preserve existing functionality. Where's the protection? The protection comes in the fact that if this guy out here wants to directly hit this workstation, he can't because the address for this",
      "workstation, this 192.168.1.1 is example is not routable across the internet. If he tries to send that, it won't go anywhere. So this way we have internal traffic and external traffic and we can flow this way. But it prevent someone from being able to get from the outside directly to the inside. And as I said, this is very common technology. It's usually built into all of the home routers. Okay, we just talked about firewalls. Now we're going to talk about segmentation. That is, how are we going",
      "to apply these firewalls in various network architectures to achieve different levels of security? Let's take the first one. This is the most primitive. I don't recommend this. Don't ever do this. But this in the early days of the Internet was a viable option for a lot of people. It's a bastion host. We basically take our web server and put it on the Internet because what we don't want is the internal network-- intranet --being exposed directly to the Internet. And if I don't put this somewhere",
      "outside here, then that means I have to allow all the Internet traffic into my internal network. And that's a really bad idea. So in the early days, people would put a single firewall right here, put their web server out here, or whatever devices, it had to be a bastion, last bastion on on the defense right out here on the edge. Again, not recommended. We have better ways to do it. The sort of next generation of this was a tri-homed network where in this case we've created essentially a firewall",
      "that we've carved off here into three different networks. So the firewall that's sitting here recognizes traffic coming in on one network interface card. And this traffic, for instance, all of the Internet traffic will be directed directly into where our Web server is. It's coming in on this network interface card, it will automatically be directed there. And maybe we apply some rules like the packet filtering and stuff like that. But that's where the traffic is destined to go in most cases. And",
      "likely in this case, the internal traffic, it's coming in on this network interface card, it could be routed here if somebody's internal user wants to hit our website. But it could also just as equally be trying to go out somewhere here. So here we have a single firewall sitting there, but it's got three different network interface cards. So it's trying to kind of do a lot of work in one place. And that's why it's called tri-homed. This is a DMZ of sorts. A demilitarized zone is the term we use",
      "there to refer to an area that's a buffer between an untrusted environment and a more trusted environment. Now, I'm going to use those terms very simplistically with apologies to people who understand what zero trust means, that there's really no trusted networks. But that's why we're essentially trying to show here's the red zone, the untrusted. The yellow zone is the semi trusted and the green zone is the more trusted zone. That's the idea here behind the color coding. Okay, move to the next",
      "one, the basic DMZ. And this is very popular. By the way, this last one is one that's often done in home networks. If you want to have an internal network and you want to allow guests to have access or your IoT devices or things like that. If you're hosting your own web server on your home network, not a great idea, but you could do it. You might want to separated out like this because again, it's a low cost option. That's the advantage is, it's very scalable. It's cheap. But on the downside,",
      "it's a single point of failure. If this thing doesn't do its job, everything's wide open potentially. Okay, moving on to the basic DMZ, which in this case I'm going to use two firewalls. So automatically I end up with more costs because I've got multiple security protections that I'm putting in place. And it's going to be more complex because I have to administer different rules and different capabilities here. And I gave an example of this in the first frame. In the first example before, when I",
      "talked about packet filtering and traffic coming in here and coming out there and so forth. That would be a basic DMZ. We got a red zone, a yellow zone, a green zone. Think of this as the traffic light. This is the danger untrusted, semi trusted caution. And then this is where it's more trusted. And we've built a kind of firewall rules to make sure that this can be trusted, because again, someone cannot go from here to here. We block that. We block that actually at this first firewall and then we",
      "have a secondary block here. And as a result, because we've got one block here and another block here, we have defense in depth. You remember going back to the very first video in this series, one of the principles I talked about that was important is this notion of defense in depth. I don't rely on any single security mechanism to protect me. If this firewall fails for some reason, I still can't get traffic from here to there because I've built in a rule that said the source address has to be",
      "traffic coming from this web server, for instance. And if this failed and all the traffic was able to come through it, then it would still be blocked by this second. So we've got defense in depth. It's also more scalable. So I could build up multiples of these, multiples of a lot of these kinds of things. So the opportunities are a lot greater. Again, not a single point of failure, but defense in depth. And then finally, I'll talk about a multi-tiered DMZ. So the multi-tiered DMZ, we basically",
      "put a firewall here and here. So now we've got this this diagram essentially replicated here. But in this case, I've split out the web server from the application server from the database, in this example. In this case, I'm going to implement yet another firewall, a third firewall in this case. So as you would guess, one of the downsides is it's going to be even more expensive than these others. It is going to be more costly and complex than these others, because now I've got three firewalls to",
      "administer and different rules on each one of them. However, we've got defense in depth on steroids. We've got even more because now any one of these mechanisms, it would have to be that all three of them failed. If one of them failed, it wouldn't necessarily be a huge problem for us. We also have greater granularity. That is, I can allow traffic to only go to here from this zone to this zone. I can allow traffic from this zone only to go to that zone and traffic from this zone only to go to that",
      "zone and do the reverse back. So more granularity, more firewalls, more cost and complexity. But potentially, if I do it right, more security. Okay, in the previous section, we talked about firewalls and segmentation. Now we're going to cover the next subject of virtual private networking VPNs. Now, what are VPNs designed to do? They're basically trying to give us a secure channel over an untrusted network. That's the idea. I can't necessarily trust the Internet because I don't control I don't",
      "have visibility into all of the aspects of that. But I'd like to be able to send secure information or information in a secure way over it. So I want a secure channel over an untrusted network. That would be a great capability. And the way I do that, I accomplish it by encrypting my information and then sending it over the network. The idea there is I get confidentiality. And I get that because people can't see what is in the packet. All they'll see is the encrypted information. We lots of times",
      "think about this as a pipe or as a tunnel. You'll hear those kind of analogies used here. Think about if we've got a user here with a browser trying to get to a web server and we're building a secure pipe, a connection from one end to the other. And I'm encrypting all the packets as they go across. So that way someone who looks here sees nothing that they can interpret, nothing that means anything. That's the idea behind a secure pipe, secure channel over an untrusted network. So that's the good",
      "stuff. And security guys love that we can do that. What they don't love is this. That is a limited inspection capability, this ability to-- so the good guy can send their traffic without everybody seeing it. It also means a bad guy could send their traffic without everybody seeing it and that would be a problem. So it limits our ability to inspect and therefore see if someone's putting malware into my system or someone's initiating an attack. So it's one of those blessings and a curse at the same",
      "time. All right. There are different kinds of VPN technologies. And to understand them, we really need a little bit of understanding on network technologies. A 7 layer OSI stack. This is classic stuff. We're not going to go into it in detail. But the notion is, is that there are different layers. For every packet I send, different concerns, different aspects that are implemented at different layers here. And what happens in the real world is most people, if you're an application programmer,",
      "you're really concerned more up here with the application, presentation layers, these kinds of things. And the networking infrastructure, people are much more concerned with the stuff down here at the transport network data link, physical layers and those kinds of things. So there's a little bit off concerns that separate there. But the other thing that's really important about this is, with this model, we have a way of, if you implement a security capability, for instance, at one of these",
      "layers, it's inherited by the upper layers. So if I encrypt all my traffic here, then it's encrypted by all the higher layers as well. So from a simple security standpoint, it might be easier to put the encryption down lower in the stack. We'll talk in a minute what hat's not necessarily what you always want to do, though. So there are different examples of how we do this. For instance, at the application layer. You may have heard of this protocol--secure shell. That's an example of an",
      "application layer or application specific VPN. It encrypts the data so that you can connect into a particular device console, this sort of thing. There's a secure FTP and other examples like that. Another one that's very common and you're going to run across this all the time, whether you're aware of it or not, is TLS or SSL--transport layer security or secure sockets layer. This is the older term. The newer name for this standard is TLS. It's implemented at the transport layer. That's what you",
      "usually see when you're a browser connected to a web server and you see that little lock in the browser up on the the URL line. That's what is is being implemented there, TLS. So that is everything that's going to that Web server then is going to be encrypted. There are other examples. There's a thing called IPsec, which is implemented at the network layer. If you do that, then everything between two network addresses will be encrypted as opposed to between the web server or for a very specific",
      "application. And then we have some other examples of point-to-point tunneling protocol, P2PTP or L2TP, which is the layer 2 tunneling protocol. These are some examples of even lower on the stack. So not to go into details on it, but just to give you an idea, there's not a single type of VPN or VPN technology. They all share some of these qualities. And if you see these things, you should think, ah that's a type of VPN. Now what's happening these days, is we're tending to move away from these",
      "broad network based VPNs more toward application specific VPNs. What's the reason for that? Well, on the advantage side for the broad ones, is that they're relatively simple. I set up a connection, for instance, between two endpoints or me into a particular network. And everything I do, for instance, if I set up an IPsec session, everything I send into that whole network now will be encrypted. So that's a very simple type of thing to do. However, it doesn't give us the granularity that we get",
      "over on this side with a very application specific firewall or VPN like we're doing here with SSH. The other advantage on the broad based side, network-based side, is a catch all. Again, if I encrypt at this layer, then all the traffic, all the different applications can benefit from that basic VPN. I don't have to set up separate ones. So it's simpler in that regard. However, I don't have as much control and as much granularity. The ability to control and say I have a VPN for my email, a VPN for",
      "my file sharing application, a VPN for my instant messaging application and control all of those separately gives me more control so that if I need to shut a particular service down or a particular user down, I can do that. So a lot of different possibilities here in this area of VPNs. Okay, now we've covered firewalls, segmentation, VPNs, and our last one is SASE. What is SASE?. It's secure access service edge. That's what the acronym stands for. It's actually a very relevant and important topic",
      "these days as part of the larger subject of zero trust. And in fact, another aspect of zero trust that's very relevant is micro-segmentation. And we talked about segmentation before, but micro-segmentation just carries that to the extreme and puts lots and lots of zones within your network with little micro networks. But SASE in particular. What we're trying to do here is create some sort of of secure capability that's delivered on the edge. To think about it this way. Let's look at a Venn",
      "diagram. Here, we've got networking concerns here. We've got security things and we've got the cloud. If you think about the intersection of network security and cloud, this is the world where SASE lives. Because-- and depending on how you want to think about this, if you think more mathematically, you might like this description, if you think more visually, you might like this one. We'll go through the mathematical one first. SASE is basically network security plus WAN, wide area networking",
      "capability. So that's the network and the security all delivered from the cloud. So that's the way of taking that Venn diagram and expressing it kind of as a loose mathematical equation. So if we were to decompose this a little more, what does it mean? What does the NetSEC mean, network security? Well, it's basically firewalling, which we've talked about. It's secure web gateways, which we didn't really go into detail, but think about those as application-specific firewalls and things of that",
      "sort. And DLP, data loss prevention, which is something I'll talk about in the data security domain when we get to that topic. But all of these things and more, delivered on the edge, so that's the network security component of all this. The WAN, specifically, is a software-defined WAN. Which is a way of creating a dynamic network where you can change where the boundaries of the network are and provision these in real-time, effectively. So it gives you a lot more agility and flexibility. So we're",
      "adding that capability, this SD-WAN marrying it, merging it, with the network security components and then delivering the thing from the cloud, because the cloud gives us the ability to do scalability. We can scale up and scale down elasticity and agility. Again, lots and lots of flexibility. That's what people are looking for in this case. If I take all of those things and then maybe even add in another thing within the security space, identity management, specifically, authentication and",
      "authorization. So some access controls, then this is what SASE is about. It's combining all of these functions into a single logical component and delivering that from the cloud, at the edge of the network. Another way to look at it is this way. So here we've got our users, here we've got the external network, and then I've got this SASE capability that's here in the middle. And what this delivers is on one end, the networking capabilities I mentioned previously, on the other end, also the",
      "security capabilities-- firewall and DLP and so forth. So all of this is is a way of combining these functions and delivering them. This is a more modern way of delivering all of these capabilities as opposed to what would have in the past each one of these would have been a separate appliance, a separate component, a separate administrative capability, a separate person to administer it and all of those kinds of things. So it's bringing these functions together and having them operate in a more",
      "holistic way. All right, now, we finished the networking topic. A couple of things that I didn't cover and I put here in the etc. just because of the interest of time, I didn't really get into very much in the physical networking side, things like 5G and Wi-Fi and the network security capabilities of those. If there's interest in that, put that down in the comments and maybe we'll revisit that in a future video. Until then, now we've completed the networking portion of our domains. The next one",
      "and, we want you to stay tuned and look for that one. The next one will be in the area of application security as we move along the various domains. Please remember to like, subscribe and hit notify so that you'll be aware when future videos in the series are available."
    ],
    "chunk_count": 56,
    "content_id": "0dca8fd2-97b1-4eba-81b9-8a6afffb8b5b",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555051"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=nthEXs12nFE": {
    "title": "Cybersecurity Architecture: Application Security",
    "url": "https://www.youtube.com/watch?v=nthEXs12nFE",
    "description": "IBM Security QRadar EDR : https://ibm.biz/Bdymjj\n\nIBM Security X-Force Threat Intelligence Index 2023: https://ibm.biz/BdymjZ\n\nSoftware bugs, they are a fact of life. But the longer they remain undetected, the higher the cost of fixing them... and the higher risk of security vulnerabilities that malicious actors can exploit. In this installment of the Cybersecurity Architecture series, IBM Distinguished Engineer and Adjunct Professor Jeff Crume explains the software bug lifecycle and how it can be better addressed through proactive processes that replace the traditional development/operations \"over the wall\" approach.\n\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n#AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume #applicationsecurity",
    "duration": 996,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en Welcome back to the Cyber Security Architecture Series. In previous videos, we covered security principles, some of the foundational concepts, and then we started talking about different cybersecurity domains like identity and access management and endpoint security and network security. Today, we're going to talk about application security. So let's get started. What are some of the things that we need to consider in this? Why do we have to care, maybe, is the best question. Well, it turns out that essentially all software has bugs. Nobody writes a software of any real complexity that's error free. And it turns out that some percentage of those bugs will be security vulnerabilities. So therefore, if you follow the logical conclusion, that means essentially all software is going to have security vulnerabilities in it. So what can we do to try to reduce those and why do we need to do it? Obviously, we don't want buggy software and we don't want security issues, but this will help drive the point home a little bit, I think. If we look at the various stages of application development and look at where security or vulnerabilities in general are introduced. Think about the injection phase. It turns out that most of the vulnerabilities and bugs are introduced in the coding phase, which is not surprising. And then as we move to unit test, functional test, system test and release, then we find fewer and fewer bugs. So that's the green curve, but that's when they're introduced. When are they found? Well, they're found as we kind of move along this process. During the coding phase is when we're introducing them, but not finding them; in the testing phase, we're finding them. And then hopefully when we get to the real world, we don't find quite so many. Now the interesting thing about all of this is the cost. How does the cost go? Well, it turns out cost goes about like this, where we go from 1x as the whatever you want to base the cost to fix a bug down here in the coding phase to -- in some cases 640x. It is vastly more expensive to fix a vulnerablity once it's in the field than it is to catch it early. So there's a huge incentive for us to get this right and get it early. Now, what can we do then in order to get this done more quickly? Well, let's take a look. We're going to take a look, by the way, at this software development lifecycle. That's the SDLC. And traditionally, this is how it's been done. We have some design phase here where we're going to figure out in general what we're going to do in this particular application. And then we're going to move to a coding phase. We're going to write the application itself. Then we're going to go to some sort of testing phase. And then ultimately we release it to the world and put it in production. Now, what is traditionally happened is there's a big line separating these two--where this is the dev part of the process and this is the ops part of the process. This is where we're developing. This is where we're releasing and then just operating it and running it in a continuous state. The problem with this traditional approach is that it's very linear. You can see how this process goes. It's also very siloed. This line right here could be very thick in some organizations. And it kind of leads to this sort of “over the wall” mentality. I'm going to write the code and then I'm going to throw it over there and make them operate it. And so there's not a lot of communication happening here. It can be a slow process and it's fairly inflexible. And by the way, I'm going to suggest to you, we didn't really introduce security very early in this process. Very often security gets introduced out here, and that's a problem. Now, a more modern approach to this is this thing called DevOps. So here we take the two processes, dev, where we're building the code, then we're releasing it, we're deploying it, we're operating it, and we're feeding back. With a DevOps process, now what we've got is a cyclical type of situation. This thing doubles back on itself. There's a feedback loop of continuous improvement. There's no over the wall. There's no us versus them. It's an integrated process. It's much more rapid, and it's designed for agility. So this is a much more flexible kind of capability moving away from this traditional linear approach. But again, we haven't really addressed the security here. And so what more modern approaches have done is introduce this notion of devSecOps, where now we're going to basically bathe or encompass the whole thing in a security layer. We're going to put security at every one of these phases. We don't want this to be something that we wait to the end. Security can't be a bolt-on. That is not going to be effective. We need security to be built in and looked at at every one of these phases. So, for instance, with a devSecOps approach, we're going to do what is referred to as shift left thinking. That if you were thinking of this in a linear phase or thinking of it this way, then we're going to put security not just here, we're going to introduce security at each one of these phases. We're going to do security by design. We're going to design the system so that it stands up and it's resilient to attack from the first. Not again as a bolt-on. Then we're going to create collaboration among what have been traditionally three different groups that maybe didn't always talk together. But with devSecOps, we have it all working together with a lot of collaboration, a lot of feedback. And then ultimately try to leverage a lot of automation. I'll talk about that in terms of tooling a little bit later in the video. Okay. Now, we've covered the software development lifecycle. Next, we're going to take a look at secure coding practices and things like that. What do we need in order to write secure code? If we're going to shift security left all the way to the coding, maybe even design phase, what we're going to focus on the coding part right here. What are my needs? Well one thing I need is a list of secure coding practices. This is a prescriptive way of saying this is how we should go about writing certain code. There are certain things that we need to do. For instance, validate inputs. We need to make sure that if a buffer has been allocated for this size, for input, that when someone sends me something, it's not this big because then in that case, it could be overwriting memory. We call this a buffer overflow. So that's a type of condition that's been around for a long time. If we don't do all the input checking, we could have problems. We need to specify how we would do authentication. What are some of the other ways that we would use cryptography? What kinds of error handling routines? So it's basically a way of saying, have we checked off all of these things? Have we done them all correctly? Well, there's a great resource here. It's called owasp.org. And at this website, what you'll find is they have a list of secure coding practices. They have a very good document that is kind of an industry standard that a lot of people use in this. So I would reference that you take a look there. Trusted libraries. Everyone doesn't write all their code from scratch anymore because there's just too much that needs to be done. So as a result, we end up relying on code that's been written by other people. It might be open source, it might be proprietary, but we're going to use this in our code as well, or make calls to it and leverage it. There's a lot of sources that are out there, but not all of them are trustworthy. And even the trustworthy ones need to be looked at with a certain amount of skepticism and a certain amount of inspection. So trusted libraries are an important thing here as well. And if you want to see what can happen, even with a trusted library, go look up something called Log4J. This is a very well known vulnerability that infected tons and tons of systems because everyone was using this routine, which was a common trusted routine from trusted sources, and yet it had a vulnerability in it that an attacker could exploit. And those vulnerabilities were discovered. Guess what? Over here in this phase, right when it was the most expensive, after it had been released. So, again, we want to try to eliminate that or at least find it much earlier in the process, if we can. And using trusted sources, at least, helps. It doesn't eliminate the risk, but it helps. Standard architectures. I want to have spelled out that when we're going to do a certain approach, this is how the system should look. So we want to spell out in advance what those would look like. There's a lot of different sources for that. IBM has an Application Security Architecture Reference website that you can take a look at. We'll reference here. Also there are other sources, but you want to be able to spell this out for your whole organization so that they can see that. Mistakes to avoid. Again, we're going to go back to our friends at OWASP. They have a thing called the OWASP Top Ten list. And what's really interesting in this is they're showing what are the top ten vulnerabilities that we keep seeing. And it's very interesting to look at this over time. Look, back in 2017, what were the top ten? And look now and the more recent one, 2021? And what you'll find is there have been a little bit of shifting here, but unfortunately, it's sort of the same list, which means we're not getting better. We're not making these defects extinct. We're just kind of shuffling the order a little bit. But things like buffer overflows and other types of input validation errors have been around for a long time. But this is a great learning resource. So again, I refer you to OWASP. And then finally, this notion is really starting to build some momentum. The idea of a software bill of materials. That is, I want to know where everything came from, kind of a supply chain, if you think of it this way. I want to know all the components that I'm using. I want to know where they came from, their origins, the libraries that they came from, and where those are sourced. I want to know what dependencies exist amongst these different components and have those really spelled out. I don't want any surprises in that. I want to know the versions of all of these that I've used across all of my different systems. And if I'm building applications, I need to know which versions are used because those will change over time, need to be updated, and I need to be able to know what vulnerabilities might exist in those. Again, refer to Log4J. Go and do a web search on that if you want to find out more about that example. That's a good example where if we have a software bill of materials and we get hit by one of those vulnerabilities, then we have a much better idea of all the different places that we need to make changes and we can recover much more quickly and hopefully eliminate a certain amount of this cost. Okay. Now, we've covered the software development lifecycle, secure coding, and we're now going to take a look at vulnerability testing. We'd like to test for security throughout the process. Remember, one of the things I mentioned in the DevSecOps process is to use more automation, that is, use more tooling. And two major classes of those tools are static application security testing, SAST. And DAST: dynamic application security testing. You figured if a one was going to be static, the other was going to be dynamic, right? Okay, what's the difference in those two? Well, the static is often referred to as “white box” testing. It's called that because we can look inside and feed it the source code. So it's like it's looking inside our code. So the box is open in that sense. The black box dynamic, instead of giving it source code, we're actually obscuring what the source code is. It's going to look at an executable system. Think about it differently. In this case, we're going to do maybe after the coding phase, I can run one of these tools, I can feed my source code into it, and it will look for vulnerabilities and tell me about those, the ones that it can identify. And the great thing about that is we're moving this much earlier. We're doing a shift left in the dev process and finding those bugs at the earliest stage that they were introduced, therefore, reducing cost. Even if I can't do it in that phase, I’d at least like to catch it during one of these test phases. So it's a little bit later in the development cycle, but it's still before we hit release when everything gets super expensive. So what I'd like to do is run both of these. In other words, it's not an either/or. It's really both/and. Because there's advantages to both of these, the source code scanner will find certain types of vulnerabilities. The dynamic scanner will find other types. So you really need to use them both together and you use them throughout this process. And again, think about that cyclical devSecOps process and you can see how this would fit in and how the tools now become the enabler to do better security. One other sort of tool that's being used in this space a lot for application development. It wasn't initially thought of in that context by a lot of people, but it's really becoming popular and that is chatbots. These large language models with generative capabilities can also generate code. So a chatbot is good at generating code very quickly. You could say “write me a routine” in, say, Python that will find all the ways to make change for a dollar and it'll spit it right out just like that. So that's a nice feature that you can have the chatbot write code for you. It can also debug code. So if I've got stuck at some point during the development of some routine, I could send it into a chatbot and maybe it will find what my vulnerability is or what the bug is that I'm trying to deal with. So that's all good stuff. However, there's a downside. The downside is it could also potentially inject vulnerabilities. So that is, when we get source code back, we could inspect that. But probably you went to the chatbot because you had a lot of code you wanted it to write for you, not just 12 lines of code, because that you could have done on your own. So are you going to inspect all of that and make sure that the chatbot either didn't make a mistake that introduces a vulnerability, or maybe it's been engineered in a way or someone has hacked it and it's intentionally putting a backdoor in so that all of your data is shared out with someone else, or that malware is introduced without your knowledge. So that's a big risk. It's not necessarily a trusted source, like an open source library where we've had potentially thousands of eyes looking at it and inspecting it. In this case, it just came right straight from the source, and now you're putting it right into your code. So there's a big risk there. And another thing is, if I'm using it to do debugging, then that means I'm feeding my source code into this system. And this system might expose intellectual property. If this code I have is considered confidential, if it's a trade secret, if it's something that we might copyright. Now I've just taken that and I've given it directly to what is essentially to the Internet, potentially, because it can now take that information and learn on it. It might use it might re-use your code in some other case. There was one major company that, in fact, had that happen to them where their developers were using a chatbot to do debugging. And it turned out they were releasing their proprietary source code into the system. And the company decided that's not really what we want to be doing. So we need to stop that. OK, so that's a quick look at application security. What we've tried to cover are ways to introduce process, do a more continuous process, and use tooling in order to improve the security of our systems and leverage the best of the learning from sources like OWASP so that we don't keep making the same mistakes over and over again. It feels a lot if you've been doing security like the movie Groundhog Day, we just keep reliving the same problems over and over and over again. So let's learn from those and use those good resources to not keep doing that. So now we've covered application security. In our next video, we're going to move over to cover data security, which is the crown jewels. That's the thing that we ultimately need to protect in most of these cases. And so that you don't miss that, make sure that you hit it like, subscribe, and notify. And that way you'll catch the next video when it comes out.",
    "chunks": [
      "Kind: captions Language: en Welcome back to the Cyber Security Architecture Series. In previous videos, we covered security principles, some of the foundational concepts, and then we started talking about different cybersecurity domains like identity and access management and endpoint security and network security. Today, we're going to talk about application security. So let's get started. What are some of the things that we need to consider in this? Why do we have to care, maybe, is the best",
      "question. Well, it turns out that essentially all software has bugs. Nobody writes a software of any real complexity that's error free. And it turns out that some percentage of those bugs will be security vulnerabilities. So therefore, if you follow the logical conclusion, that means essentially all software is going to have security vulnerabilities in it. So what can we do to try to reduce those and why do we need to do it? Obviously, we don't want buggy software and we don't want security",
      "issues, but this will help drive the point home a little bit, I think. If we look at the various stages of application development and look at where security or vulnerabilities in general are introduced. Think about the injection phase. It turns out that most of the vulnerabilities and bugs are introduced in the coding phase, which is not surprising. And then as we move to unit test, functional test, system test and release, then we find fewer and fewer bugs. So that's the green curve, but that's",
      "when they're introduced. When are they found? Well, they're found as we kind of move along this process. During the coding phase is when we're introducing them, but not finding them; in the testing phase, we're finding them. And then hopefully when we get to the real world, we don't find quite so many. Now the interesting thing about all of this is the cost. How does the cost go? Well, it turns out cost goes about like this, where we go from 1x as the whatever you want to base the cost to fix a",
      "bug down here in the coding phase to -- in some cases 640x. It is vastly more expensive to fix a vulnerablity once it's in the field than it is to catch it early. So there's a huge incentive for us to get this right and get it early. Now, what can we do then in order to get this done more quickly? Well, let's take a look. We're going to take a look, by the way, at this software development lifecycle. That's the SDLC. And traditionally, this is how it's been done. We have some design phase here",
      "where we're going to figure out in general what we're going to do in this particular application. And then we're going to move to a coding phase. We're going to write the application itself. Then we're going to go to some sort of testing phase. And then ultimately we release it to the world and put it in production. Now, what is traditionally happened is there's a big line separating these two--where this is the dev part of the process and this is the ops part of the process. This is where we're",
      "developing. This is where we're releasing and then just operating it and running it in a continuous state. The problem with this traditional approach is that it's very linear. You can see how this process goes. It's also very siloed. This line right here could be very thick in some organizations. And it kind of leads to this sort of “over the wall” mentality. I'm going to write the code and then I'm going to throw it over there and make them operate it. And so there's not a lot of communication",
      "happening here. It can be a slow process and it's fairly inflexible. And by the way, I'm going to suggest to you, we didn't really introduce security very early in this process. Very often security gets introduced out here, and that's a problem. Now, a more modern approach to this is this thing called DevOps. So here we take the two processes, dev, where we're building the code, then we're releasing it, we're deploying it, we're operating it, and we're feeding back. With a DevOps process, now",
      "what we've got is a cyclical type of situation. This thing doubles back on itself. There's a feedback loop of continuous improvement. There's no over the wall. There's no us versus them. It's an integrated process. It's much more rapid, and it's designed for agility. So this is a much more flexible kind of capability moving away from this traditional linear approach. But again, we haven't really addressed the security here. And so what more modern approaches have done is introduce this notion of",
      "devSecOps, where now we're going to basically bathe or encompass the whole thing in a security layer. We're going to put security at every one of these phases. We don't want this to be something that we wait to the end. Security can't be a bolt-on. That is not going to be effective. We need security to be built in and looked at at every one of these phases. So, for instance, with a devSecOps approach, we're going to do what is referred to as shift left thinking. That if you were thinking of this",
      "in a linear phase or thinking of it this way, then we're going to put security not just here, we're going to introduce security at each one of these phases. We're going to do security by design. We're going to design the system so that it stands up and it's resilient to attack from the first. Not again as a bolt-on. Then we're going to create collaboration among what have been traditionally three different groups that maybe didn't always talk together. But with devSecOps, we have it all working",
      "together with a lot of collaboration, a lot of feedback. And then ultimately try to leverage a lot of automation. I'll talk about that in terms of tooling a little bit later in the video. Okay. Now, we've covered the software development lifecycle. Next, we're going to take a look at secure coding practices and things like that. What do we need in order to write secure code? If we're going to shift security left all the way to the coding, maybe even design phase, what we're going to focus on the",
      "coding part right here. What are my needs? Well one thing I need is a list of secure coding practices. This is a prescriptive way of saying this is how we should go about writing certain code. There are certain things that we need to do. For instance, validate inputs. We need to make sure that if a buffer has been allocated for this size, for input, that when someone sends me something, it's not this big because then in that case, it could be overwriting memory. We call this a buffer overflow. So",
      "that's a type of condition that's been around for a long time. If we don't do all the input checking, we could have problems. We need to specify how we would do authentication. What are some of the other ways that we would use cryptography? What kinds of error handling routines? So it's basically a way of saying, have we checked off all of these things? Have we done them all correctly? Well, there's a great resource here. It's called owasp.org. And at this website, what you'll find is they have a",
      "list of secure coding practices. They have a very good document that is kind of an industry standard that a lot of people use in this. So I would reference that you take a look there. Trusted libraries. Everyone doesn't write all their code from scratch anymore because there's just too much that needs to be done. So as a result, we end up relying on code that's been written by other people. It might be open source, it might be proprietary, but we're going to use this in our code as well, or make",
      "calls to it and leverage it. There's a lot of sources that are out there, but not all of them are trustworthy. And even the trustworthy ones need to be looked at with a certain amount of skepticism and a certain amount of inspection. So trusted libraries are an important thing here as well. And if you want to see what can happen, even with a trusted library, go look up something called Log4J. This is a very well known vulnerability that infected tons and tons of systems because everyone was using",
      "this routine, which was a common trusted routine from trusted sources, and yet it had a vulnerability in it that an attacker could exploit. And those vulnerabilities were discovered. Guess what? Over here in this phase, right when it was the most expensive, after it had been released. So, again, we want to try to eliminate that or at least find it much earlier in the process, if we can. And using trusted sources, at least, helps. It doesn't eliminate the risk, but it helps. Standard",
      "architectures. I want to have spelled out that when we're going to do a certain approach, this is how the system should look. So we want to spell out in advance what those would look like. There's a lot of different sources for that. IBM has an Application Security Architecture Reference website that you can take a look at. We'll reference here. Also there are other sources, but you want to be able to spell this out for your whole organization so that they can see that. Mistakes to avoid. Again,",
      "we're going to go back to our friends at OWASP. They have a thing called the OWASP Top Ten list. And what's really interesting in this is they're showing what are the top ten vulnerabilities that we keep seeing. And it's very interesting to look at this over time. Look, back in 2017, what were the top ten? And look now and the more recent one, 2021? And what you'll find is there have been a little bit of shifting here, but unfortunately, it's sort of the same list, which means we're not getting",
      "better. We're not making these defects extinct. We're just kind of shuffling the order a little bit. But things like buffer overflows and other types of input validation errors have been around for a long time. But this is a great learning resource. So again, I refer you to OWASP. And then finally, this notion is really starting to build some momentum. The idea of a software bill of materials. That is, I want to know where everything came from, kind of a supply chain, if you think of it this way.",
      "I want to know all the components that I'm using. I want to know where they came from, their origins, the libraries that they came from, and where those are sourced. I want to know what dependencies exist amongst these different components and have those really spelled out. I don't want any surprises in that. I want to know the versions of all of these that I've used across all of my different systems. And if I'm building applications, I need to know which versions are used because those will",
      "change over time, need to be updated, and I need to be able to know what vulnerabilities might exist in those. Again, refer to Log4J. Go and do a web search on that if you want to find out more about that example. That's a good example where if we have a software bill of materials and we get hit by one of those vulnerabilities, then we have a much better idea of all the different places that we need to make changes and we can recover much more quickly and hopefully eliminate a certain amount of",
      "this cost. Okay. Now, we've covered the software development lifecycle, secure coding, and we're now going to take a look at vulnerability testing. We'd like to test for security throughout the process. Remember, one of the things I mentioned in the DevSecOps process is to use more automation, that is, use more tooling. And two major classes of those tools are static application security testing, SAST. And DAST: dynamic application security testing. You figured if a one was going to be static,",
      "the other was going to be dynamic, right? Okay, what's the difference in those two? Well, the static is often referred to as “white box” testing. It's called that because we can look inside and feed it the source code. So it's like it's looking inside our code. So the box is open in that sense. The black box dynamic, instead of giving it source code, we're actually obscuring what the source code is. It's going to look at an executable system. Think about it differently. In this case, we're going",
      "to do maybe after the coding phase, I can run one of these tools, I can feed my source code into it, and it will look for vulnerabilities and tell me about those, the ones that it can identify. And the great thing about that is we're moving this much earlier. We're doing a shift left in the dev process and finding those bugs at the earliest stage that they were introduced, therefore, reducing cost. Even if I can't do it in that phase, I’d at least like to catch it during one of these test phases.",
      "So it's a little bit later in the development cycle, but it's still before we hit release when everything gets super expensive. So what I'd like to do is run both of these. In other words, it's not an either/or. It's really both/and. Because there's advantages to both of these, the source code scanner will find certain types of vulnerabilities. The dynamic scanner will find other types. So you really need to use them both together and you use them throughout this process. And again, think about",
      "that cyclical devSecOps process and you can see how this would fit in and how the tools now become the enabler to do better security. One other sort of tool that's being used in this space a lot for application development. It wasn't initially thought of in that context by a lot of people, but it's really becoming popular and that is chatbots. These large language models with generative capabilities can also generate code. So a chatbot is good at generating code very quickly. You could say “write",
      "me a routine” in, say, Python that will find all the ways to make change for a dollar and it'll spit it right out just like that. So that's a nice feature that you can have the chatbot write code for you. It can also debug code. So if I've got stuck at some point during the development of some routine, I could send it into a chatbot and maybe it will find what my vulnerability is or what the bug is that I'm trying to deal with. So that's all good stuff. However, there's a downside. The downside",
      "is it could also potentially inject vulnerabilities. So that is, when we get source code back, we could inspect that. But probably you went to the chatbot because you had a lot of code you wanted it to write for you, not just 12 lines of code, because that you could have done on your own. So are you going to inspect all of that and make sure that the chatbot either didn't make a mistake that introduces a vulnerability, or maybe it's been engineered in a way or someone has hacked it and it's",
      "intentionally putting a backdoor in so that all of your data is shared out with someone else, or that malware is introduced without your knowledge. So that's a big risk. It's not necessarily a trusted source, like an open source library where we've had potentially thousands of eyes looking at it and inspecting it. In this case, it just came right straight from the source, and now you're putting it right into your code. So there's a big risk there. And another thing is, if I'm using it to do",
      "debugging, then that means I'm feeding my source code into this system. And this system might expose intellectual property. If this code I have is considered confidential, if it's a trade secret, if it's something that we might copyright. Now I've just taken that and I've given it directly to what is essentially to the Internet, potentially, because it can now take that information and learn on it. It might use it might re-use your code in some other case. There was one major company that, in",
      "fact, had that happen to them where their developers were using a chatbot to do debugging. And it turned out they were releasing their proprietary source code into the system. And the company decided that's not really what we want to be doing. So we need to stop that. OK, so that's a quick look at application security. What we've tried to cover are ways to introduce process, do a more continuous process, and use tooling in order to improve the security of our systems and leverage the best of the",
      "learning from sources like OWASP so that we don't keep making the same mistakes over and over again. It feels a lot if you've been doing security like the movie Groundhog Day, we just keep reliving the same problems over and over and over again. So let's learn from those and use those good resources to not keep doing that. So now we've covered application security. In our next video, we're going to move over to cover data security, which is the crown jewels. That's the thing that we ultimately",
      "need to protect in most of these cases. And so that you don't miss that, make sure that you hit it like, subscribe, and notify. And that way you'll catch the next video when it comes out."
    ],
    "chunk_count": 34,
    "content_id": "dbc32016-51d8-4c77-9305-5b2d76e85b24",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555054"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=x2GRE-RzmD8": {
    "title": "Diffusion Models for AI Image Generation",
    "url": "https://www.youtube.com/watch?v=x2GRE-RzmD8",
    "description": "Want to learn more about Generative AI + Machine Learning? Read the ebook → https://ibm.biz/BdGvdC\nLearn more about Diffusion Models here → https://ibm.biz/BdGvdQ\n\nReverse the diffusion process, and unlock the secrets of AI-generated images. Isaac Ke explores how to harness the power of diffusion models to create stunning, high-quality images from text prompts. From text-to-image generation to image-to-image editing, learn how diffusion models are being applied in various fields, from marketing to medicine.\n\n0:00 - Overview\n0:56 - Forward Diffusion\n5:21 - Reverse Diffusion\n8:02 - Conditional Diffusion\n11:07 - Applications\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdGvd3",
    "duration": 724,
    "uploader": "IBM Technology",
    "transcript": "Kind: captions Language: en If I drop red dye into this beaker of water, the laws of physics say that the particles will diffuse throughout the beaker until the system reaches equilibrium. Now, what if I wanted to somehow reverse this process to get back to the clear water? Keep this idea in mind because this concept of physical diffusion is what motivates the approach for text to image generation with diffusion models. Diffusion models power popular image tools like DALL-E-3 and sample diffusion where you can go from a prompt like a turtle wearing sunglasses playing basketball, to a hyper realistic image of just that. At a high level, diffusion models are a type of deep neural network that learn to add noise to a picture and then learn how to reverse that process to reconstruct a clear image. I know this might sound abstract, so to unpack this more, I'm going to walk through three important concepts that each build off each other. Starting first with Forward Diffusion. Going back to the beaker, think of how the drop of dye diffused and spread out throughout the glass until the water was no longer clear. Similarly with Forward diffusion, we're going to add noise to a training image over a series of time steps until the model starts to lose its features and become unrecognizable. Now this noise is added by what's called a Markov chain, which basically means that the current state of the image only depends on the most recent state. So as an example, let's start with an image of a person. My beautiful stick figure here and labeled this image X at time T equals to zero. For simplicity, imagine that this image is made of just three RGB pixels and we can represent the color of these pixels on our x, y, z plane here. Where the coordinates of each of our pixels correspond to their R, G, and B values. So as we move to the next timestep, T equals to one... now add random Gaussian noise to our image. Think of Gaussian noise as looking a bit like those specks of TV static you get on your TV when you flip to a channel that has a weak connection. Now, mathematically adding Gaussian noise involves randomly sampling from a Gaussian distribution, a.k.a. a normal distribution or bell curve, in order to obtain numbers that will be added to each of the values of our RGB pixels. So to make this more concrete, let's look at this pixel in particular. The color coordinates of this pixel in the original image at time zero, start off at 255, 0, 0, corresponding to the color red. Pure red. Now as we add noise to the image going to timestep one, this involves randomly sampling values from our Gaussian distribution. And say we obtain a random values of -2, 2, and 0. Adding these together, what we get is a new pixel with color values 253, 2, and we can represent this new color on our plane here. And show the change in this color with an arrow. So what just happened basically is that this pixel that was pure red in the original image at time zero has now become slightly less red in the direction of green at time t goes to one. So if we continue this process, so on and so forth, say we go two times, step two.. Adding more and more random Gaussian noise to our image. Again by randomly sampling values from our Gaussian distribution and using it to randomly adjust the color values of each of our pixels, gradually destroying any order or form or structure that can be found in the image. If we repeat this process many times, say over a thousand times steps, what happens is that shapes and edges in the image start to become more and more blurred, and over time, our person completely disappears. And what we end up with is completely white noise or a full screen and just TV static. So how quickly we go from a clear picture to an image of random noise is largely dictated by what's called the noise scheduler or the variance scheduler. This scheduling parameter controls the variance of our Gaussian distribution. Where a higher variance corresponds to larger probabilities of selecting a noise value that is higher in magnitude, thus resulting in more drastic jumps and changes at..for each color of each pixel. So after forward diffusion comes the opposite - reverse diffusion. This is similar to the process of if I took the beaker of red water and I somehow removed the red dye to get back to the clear water. Similarly for reverse diffusion, we're going to start with our image of random noise. And we're going to somehow remove the noise that was added to it in very structured and controlled manners in order to reconstruct a clear image. So to help me explain this more, there's this quote by the famous sculptor named Michelangelo, who once said, \"Every block of stone has a statue inside it and it's the job of the sculptor to discover it.\". In the same way, think of reverse diffusion as every image of random noise has a clear picture in it. And it's the job of the diffusion model to reveal it. So this can be done by training a type of convolutional neural network called a U-Net to learn this reverse diffusion process. So if we start with an image of completely random noise at a random time T, The model learns how to predict the noise that was added to this image at the previous time step. So say that this model predicts that the noise that was added to this image was a lot in the upper left hand corner here. And so the models objective here is to minimize the mean squared error between the predicted noise from the actual noise that was added to it during forward diffusion. We can then take this scale noise prediction and subtract it or remove it from our image at time t in order to obtain a prediction of what the slightly less noisy image looked like at time t minus one. So on our graph here for reverse diffusion, the model essentially learns how to backtrace its steps from each pixel's augmented colors back to its t noise colors. Now, if we repeat this process many times, over time, the model learns how to remove noise and very structured sequences in patterns in order to reveal more features of an image. Say slowly revealing an arm and a leg. It repeats this process until it gets back to one final noise prediction. One final noise removal and then finally, a clear picture. And our person has magically reappeared. So now that we've covered forward and reverse diffusion, it's time to introduce text into the picture by introducing a new concept called conditional fusion or guided diffusion. Up to this point, I've been describing unconditional diffusion because the image generation was done without any influence from outside factors. On the other hand, with conditional diffusion, the process will be guided by or conditioned on some text prompt. So the first step is we have to represent our text within embedding. Think of an embedding as a numeric representation or a numeric vector as able to capture the semantic meaning of natural language input. So as an example, an embedding model is able to understand that the word KING. Is more closely related to the word MAN than it is to the word WOMAN. So during training, these embeddings of these text descriptions are paired with their respective images that they describe in order to form a corpus of image and text pairs that are used to train this model to learn this conditional reverse diffusion process. In other words, learning how much noise to remove in which patterns at a given the current image, and now taking into account the different features of the embedded text. One method for incorporating these embeddings is what's called self attention guidance, which basically forces the model to pay attention to how specific portions of the prompt influenced the generation of certain regions or areas of the image. Another method is called the classifier free guidance. Think of this method as helping to amplify the effect that certain words in the prompt have on how the image is generated. So putting this all together, this means that the model is able to learn the relationship between the meaning of words and how they correlate with certain de-noising sequences that gradually reveal different features and shapes and edges in the picture. So once this process is learned, the model can be used to generate a completely new image. So first, the users text description has to be embedded. Then the model starts with an image of completely random noise. And it uses this text embedding along with the conditional reverse diffusion process it learned during training, to remove noise in the image and structure and patterns, you know, kind of like removing fog from the image until a new image has been generated. So the sophisticated architecture of these diffusion models allows them to pick up on complex patterns and also to create images that it's never seen before. In fact, the application of diffusion models spanned beyond just text to image use cases. Some other use cases involve image to image models, in painting missing components into an image, and even creating other forms of media like audio or video. In fact, diffusion models have been applied in different fields, everything from the marketing field to the medical field to even molecular modeling. Speaking of molecules, let's check on our beaker. If only I could. Well, would you look at that reverse diffusion! Anyways, thank you for watching. I hope you enjoyed this video and I will see you all next time. Peace.",
    "chunks": [
      "Kind: captions Language: en If I drop red dye into this beaker of water, the laws of physics say that the particles will diffuse throughout the beaker until the system reaches equilibrium. Now, what if I wanted to somehow reverse this process to get back to the clear water? Keep this idea in mind because this concept of physical diffusion is what motivates the approach for text to image generation with diffusion models. Diffusion models power popular image tools like DALL-E-3 and sample",
      "diffusion where you can go from a prompt like a turtle wearing sunglasses playing basketball, to a hyper realistic image of just that. At a high level, diffusion models are a type of deep neural network that learn to add noise to a picture and then learn how to reverse that process to reconstruct a clear image. I know this might sound abstract, so to unpack this more, I'm going to walk through three important concepts that each build off each other. Starting first with Forward Diffusion. Going",
      "back to the beaker, think of how the drop of dye diffused and spread out throughout the glass until the water was no longer clear. Similarly with Forward diffusion, we're going to add noise to a training image over a series of time steps until the model starts to lose its features and become unrecognizable. Now this noise is added by what's called a Markov chain, which basically means that the current state of the image only depends on the most recent state. So as an example, let's start with an",
      "image of a person. My beautiful stick figure here and labeled this image X at time T equals to zero. For simplicity, imagine that this image is made of just three RGB pixels and we can represent the color of these pixels on our x, y, z plane here. Where the coordinates of each of our pixels correspond to their R, G, and B values. So as we move to the next timestep, T equals to one... now add random Gaussian noise to our image. Think of Gaussian noise as looking a bit like those specks of TV",
      "static you get on your TV when you flip to a channel that has a weak connection. Now, mathematically adding Gaussian noise involves randomly sampling from a Gaussian distribution, a.k.a. a normal distribution or bell curve, in order to obtain numbers that will be added to each of the values of our RGB pixels. So to make this more concrete, let's look at this pixel in particular. The color coordinates of this pixel in the original image at time zero, start off at 255, 0, 0, corresponding to the",
      "color red. Pure red. Now as we add noise to the image going to timestep one, this involves randomly sampling values from our Gaussian distribution. And say we obtain a random values of -2, 2, and 0. Adding these together, what we get is a new pixel with color values 253, 2, and we can represent this new color on our plane here. And show the change in this color with an arrow. So what just happened basically is that this pixel that was pure red in the original image at time zero has now become",
      "slightly less red in the direction of green at time t goes to one. So if we continue this process, so on and so forth, say we go two times, step two.. Adding more and more random Gaussian noise to our image. Again by randomly sampling values from our Gaussian distribution and using it to randomly adjust the color values of each of our pixels, gradually destroying any order or form or structure that can be found in the image. If we repeat this process many times, say over a thousand times steps,",
      "what happens is that shapes and edges in the image start to become more and more blurred, and over time, our person completely disappears. And what we end up with is completely white noise or a full screen and just TV static. So how quickly we go from a clear picture to an image of random noise is largely dictated by what's called the noise scheduler or the variance scheduler. This scheduling parameter controls the variance of our Gaussian distribution. Where a higher variance corresponds to",
      "larger probabilities of selecting a noise value that is higher in magnitude, thus resulting in more drastic jumps and changes at..for each color of each pixel. So after forward diffusion comes the opposite - reverse diffusion. This is similar to the process of if I took the beaker of red water and I somehow removed the red dye to get back to the clear water. Similarly for reverse diffusion, we're going to start with our image of random noise. And we're going to somehow remove the noise that was",
      "added to it in very structured and controlled manners in order to reconstruct a clear image. So to help me explain this more, there's this quote by the famous sculptor named Michelangelo, who once said, \"Every block of stone has a statue inside it and it's the job of the sculptor to discover it.\". In the same way, think of reverse diffusion as every image of random noise has a clear picture in it. And it's the job of the diffusion model to reveal it. So this can be done by training a type of",
      "convolutional neural network called a U-Net to learn this reverse diffusion process. So if we start with an image of completely random noise at a random time T, The model learns how to predict the noise that was added to this image at the previous time step. So say that this model predicts that the noise that was added to this image was a lot in the upper left hand corner here. And so the models objective here is to minimize the mean squared error between the predicted noise from the actual noise",
      "that was added to it during forward diffusion. We can then take this scale noise prediction and subtract it or remove it from our image at time t in order to obtain a prediction of what the slightly less noisy image looked like at time t minus one. So on our graph here for reverse diffusion, the model essentially learns how to backtrace its steps from each pixel's augmented colors back to its t noise colors. Now, if we repeat this process many times, over time, the model learns how to remove",
      "noise and very structured sequences in patterns in order to reveal more features of an image. Say slowly revealing an arm and a leg. It repeats this process until it gets back to one final noise prediction. One final noise removal and then finally, a clear picture. And our person has magically reappeared. So now that we've covered forward and reverse diffusion, it's time to introduce text into the picture by introducing a new concept called conditional fusion or guided diffusion. Up to this",
      "point, I've been describing unconditional diffusion because the image generation was done without any influence from outside factors. On the other hand, with conditional diffusion, the process will be guided by or conditioned on some text prompt. So the first step is we have to represent our text within embedding. Think of an embedding as a numeric representation or a numeric vector as able to capture the semantic meaning of natural language input. So as an example, an embedding model is able to",
      "understand that the word KING. Is more closely related to the word MAN than it is to the word WOMAN. So during training, these embeddings of these text descriptions are paired with their respective images that they describe in order to form a corpus of image and text pairs that are used to train this model to learn this conditional reverse diffusion process. In other words, learning how much noise to remove in which patterns at a given the current image, and now taking into account the different",
      "features of the embedded text. One method for incorporating these embeddings is what's called self attention guidance, which basically forces the model to pay attention to how specific portions of the prompt influenced the generation of certain regions or areas of the image. Another method is called the classifier free guidance. Think of this method as helping to amplify the effect that certain words in the prompt have on how the image is generated. So putting this all together, this means that",
      "the model is able to learn the relationship between the meaning of words and how they correlate with certain de-noising sequences that gradually reveal different features and shapes and edges in the picture. So once this process is learned, the model can be used to generate a completely new image. So first, the users text description has to be embedded. Then the model starts with an image of completely random noise. And it uses this text embedding along with the conditional reverse diffusion",
      "process it learned during training, to remove noise in the image and structure and patterns, you know, kind of like removing fog from the image until a new image has been generated. So the sophisticated architecture of these diffusion models allows them to pick up on complex patterns and also to create images that it's never seen before. In fact, the application of diffusion models spanned beyond just text to image use cases. Some other use cases involve image to image models, in painting missing",
      "components into an image, and even creating other forms of media like audio or video. In fact, diffusion models have been applied in different fields, everything from the marketing field to the medical field to even molecular modeling. Speaking of molecules, let's check on our beaker. If only I could. Well, would you look at that reverse diffusion! Anyways, thank you for watching. I hope you enjoyed this video and I will see you all next time. Peace."
    ],
    "chunk_count": 19,
    "content_id": "e471c364-43bf-4e27-80fb-d8877b1f5572",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555057"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=5sLYAQS9sWQ": {
    "title": "How Large Language Models Work",
    "url": "https://www.youtube.com/watch?v=5sLYAQS9sWQ",
    "description": "Learn in-demand Machine Learning skills now → https://ibm.biz/BdK65D\nLearn about watsonx → https://ibm.biz/BdvxRj\n\nLarge language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdK65X\n\n#llm #gpt #gpt3 #largelanguagemodel #watsonx #GenerativeAI #Foundationmodels",
    "duration": 333,
    "uploader": "IBM Technology",
    "transcript": "Learn in-demand Machine Learning skills now → https://ibm.biz/BdK65D\nLearn about watsonx → https://ibm.biz/BdvxRj\n\nLarge language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdK65X\n\n#llm #gpt #gpt3 #largelanguagemodel #watsonx #GenerativeAI #Foundationmodels",
    "chunks": [
      "Learn in-demand Machine Learning skills now → https://ibm.biz/BdK65D Learn about watsonx → https://ibm.biz/BdvxRj Large language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address",
      "various business problems. AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdK65X #llm #gpt #gpt3 #largelanguagemodel #watsonx #GenerativeAI #Foundationmodels"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "baea430e-a3ff-4d27-b49e-548adaf80342",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555060"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=ZZ2QUCePgYw": {
    "title": "Intro to AI agents",
    "url": "https://www.youtube.com/watch?v=ZZ2QUCePgYw",
    "description": "Vertex AI Agent Builder quickstart → https://goo.gle/3UPJ7dN\nGenAI powered App with Genkit → https://goo.gle/4fCSTrK\n\nDemystifying AI agents, Googlers Aja Hammerly and Jason Davenport provide a comprehensive overview of their capabilities, applications, and construction. Join us as we unravel the diverse definitions, explore compelling use cases, and delve into the different architectural approaches for building intelligent agents.\n\nChapters:\n0:00 - Intro\n0:18 - What is an AI agent?\n1:54 - Agentic systems examples\n3:10 - Agentic architectures\n4:57 - Get started building agents \n\nMore resources:\n Oscar, Open source contributor agent  →https://goo.gle/3Z2HqMm\nCompass Travel Planning Sample App  → https://goo.gle/4hOczun\n\nWatch more Real Terms for AI → https://goo.gle/AIwordsExplained\nSubscribe to Google Cloud Tech → https://goo.gle/GoogleCloudTech\n\n#GoogleCloud #GenerativeAI\n\nSpeakers: Aja Hammerly, Jason Davenport\nProducts Mentioned: Cloud - AI and Machine Learning - Agents, Vertex AI, Gemini",
    "duration": 370,
    "uploader": "Google Cloud Tech",
    "transcript": "Vertex AI Agent Builder quickstart → https://goo.gle/3UPJ7dN\nGenAI powered App with Genkit → https://goo.gle/4fCSTrK\n\nDemystifying AI agents, Googlers Aja Hammerly and Jason Davenport provide a comprehensive overview of their capabilities, applications, and construction. Join us as we unravel the diverse definitions, explore compelling use cases, and delve into the different architectural approaches for building intelligent agents.\n\nChapters:\n0:00 - Intro\n0:18 - What is an AI agent?\n1:54 - Agentic systems examples\n3:10 - Agentic architectures\n4:57 - Get started building agents \n\nMore resources:\n Oscar, Open source contributor agent  →https://goo.gle/3Z2HqMm\nCompass Travel Planning Sample App  → https://goo.gle/4hOczun\n\nWatch more Real Terms for AI → https://goo.gle/AIwordsExplained\nSubscribe to Google Cloud Tech → https://goo.gle/GoogleCloudTech\n\n#GoogleCloud #GenerativeAI\n\nSpeakers: Aja Hammerly, Jason Davenport\nProducts Mentioned: Cloud - AI and Machine Learning - Agents, Vertex AI, Gemini",
    "chunks": [
      "Vertex AI Agent Builder quickstart → https://goo.gle/3UPJ7dN GenAI powered App with Genkit → https://goo.gle/4fCSTrK Demystifying AI agents, Googlers Aja Hammerly and Jason Davenport provide a comprehensive overview of their capabilities, applications, and construction. Join us as we unravel the diverse definitions, explore compelling use cases, and delve into the different architectural approaches for building intelligent agents. Chapters: 0:00 - Intro 0:18 - What is an AI agent? 1:54 - Agentic",
      "systems examples 3:10 - Agentic architectures 4:57 - Get started building agents More resources: Oscar, Open source contributor agent →https://goo.gle/3Z2HqMm Compass Travel Planning Sample App → https://goo.gle/4hOczun Watch more Real Terms for AI → https://goo.gle/AIwordsExplained Subscribe to Google Cloud Tech → https://goo.gle/GoogleCloudTech #GoogleCloud #GenerativeAI Speakers: Aja Hammerly, Jason Davenport Products Mentioned: Cloud - AI and Machine Learning - Agents, Vertex AI, Gemini"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "05e1a256-8b65-48bc-8743-52c6279936b7",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555063"
    },
    "quality_score": 0.8,
    "tags": []
  },
  "https://www.youtube.com/watch?v=_d0duu3dED4": {
    "title": "Why Everyone’s Talking About MCP?",
    "url": "https://www.youtube.com/watch?v=_d0duu3dED4",
    "description": "Fixed 2:44–3:10 in the video uploaded to X (we’re unable to edit videos that are already uploaded to YouTube):  https://x.com/bytebytego/status/1907838355657863385\n\nGet a Free System Design PDF with 158 pages by subscribing to our weekly newsletter: https://bit.ly/bbg-social\n\nAnimation tools: Adobe Illustrator and After Effects.\n\nCheckout our bestselling System Design Interview books: \nVolume 1: https://amzn.to/3Ou7gkd\nVolume 2: https://amzn.to/3HqGozy\n\nThe digital version of System Design Interview books: https://bit.ly/3mlDSk9\n\nABOUT US: \nCovering topics and trends in large-scale system design, from the authors of the best-selling System Design Interview series.",
    "duration": 303,
    "uploader": "ByteByteGo",
    "transcript": "Fixed 2:44–3:10 in the video uploaded to X (we’re unable to edit videos that are already uploaded to YouTube):  https://x.com/bytebytego/status/1907838355657863385\n\nGet a Free System Design PDF with 158 pages by subscribing to our weekly newsletter: https://bit.ly/bbg-social\n\nAnimation tools: Adobe Illustrator and After Effects.\n\nCheckout our bestselling System Design Interview books: \nVolume 1: https://amzn.to/3Ou7gkd\nVolume 2: https://amzn.to/3HqGozy\n\nThe digital version of System Design Interview books: https://bit.ly/3mlDSk9\n\nABOUT US: \nCovering topics and trends in large-scale system design, from the authors of the best-selling System Design Interview series.",
    "chunks": [
      "Fixed 2:44–3:10 in the video uploaded to X (we’re unable to edit videos that are already uploaded to YouTube): https://x.com/bytebytego/status/1907838355657863385 Get a Free System Design PDF with 158 pages by subscribing to our weekly newsletter: https://bit.ly/bbg-social Animation tools: Adobe Illustrator and After Effects. Checkout our bestselling System Design Interview books: Volume 1: https://amzn.to/3Ou7gkd Volume 2: https://amzn.to/3HqGozy The digital version of System Design Interview",
      "books: https://bit.ly/3mlDSk9 ABOUT US: Covering topics and trends in large-scale system design, from the authors of the best-selling System Design Interview series."
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "8ace2968-bfb9-4ce8-a1f3-da21b374913f",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555067"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=fXizBc03D7E": {
    "title": "5 Types of AI Agents: Autonomous Functions & Real-World Applications",
    "url": "https://www.youtube.com/watch?v=fXizBc03D7E",
    "description": "Ready to become a certified watsonx Generative AI Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnZTF\n\nLearn more about Types of AI agents here → https://ibm.biz/BdnZTE\n\nCan a drone deliver packages safely and efficiently? 🤖 Martin Keen breaks down the 5 types of AI agents—from reflex to learning models—and their role in robotics, decision-making, and automation. Learn how goal-driven and utility-based AI adapt to workflows and complex environments.\n\nIntro - 0:00\nSimple Reflex Agent - 0:50\nModel-Based Reflex Agent - 2:49\nGoal-Based AI Agent - 4:20\nUtility Based AI Agent- 5:43\nLearning AI Agent - 6:55\nUse Cases - 8:22\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnZTX\n\n#aiagents #machinelearning #ai",
    "duration": 621,
    "uploader": "IBM Technology",
    "transcript": "Ready to become a certified watsonx Generative AI Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnZTF\n\nLearn more about Types of AI agents here → https://ibm.biz/BdnZTE\n\nCan a drone deliver packages safely and efficiently? 🤖 Martin Keen breaks down the 5 types of AI agents—from reflex to learning models—and their role in robotics, decision-making, and automation. Learn how goal-driven and utility-based AI adapt to workflows and complex environments.\n\nIntro - 0:00\nSimple Reflex Agent - 0:50\nModel-Based Reflex Agent - 2:49\nGoal-Based AI Agent - 4:20\nUtility Based AI Agent- 5:43\nLearning AI Agent - 6:55\nUse Cases - 8:22\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnZTX\n\n#aiagents #machinelearning #ai",
    "chunks": [
      "Ready to become a certified watsonx Generative AI Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnZTF Learn more about Types of AI agents here → https://ibm.biz/BdnZTE Can a drone deliver packages safely and efficiently? 🤖 Martin Keen breaks down the 5 types of AI agents—from reflex to learning models—and their role in robotics, decision-making, and automation. Learn how goal-driven and utility-based AI adapt to workflows and complex environments.",
      "Intro - 0:00 Simple Reflex Agent - 0:50 Model-Based Reflex Agent - 2:49 Goal-Based AI Agent - 4:20 Utility Based AI Agent- 5:43 Learning AI Agent - 6:55 Use Cases - 8:22 AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnZTX #aiagents #machinelearning #ai"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "c78e62b8-72b9-4422-ad79-dc8c60a06489",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555070"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=_pEEJu-2KKM": {
    "title": "LLMs and AI Agents: Transforming Unstructured Data",
    "url": "https://www.youtube.com/watch?v=_pEEJu-2KKM",
    "description": "Read more about Terzo here → https://ibm.biz/Bdnmpr\n\nLearn more about Intelligent Data Extraction here → https://ibm.biz/BdnqSX\n\nAre AI agents the new assembly line for data? 🤖 Join Eric Pritchett from Terzo as he explores how LLMs, GPT models, and AI agents turn unstructured data into actionable insights. Discover how OCR, NLP, and agentic workflows reshape document intelligence and solve real-world challenges! ✨\n\nDiscover more about Terzo → https://ibm.biz/Bdnmps\n\n#ai #llm #unstructureddata #aiagents",
    "duration": 1220,
    "uploader": "IBM Technology",
    "transcript": "Read more about Terzo here → https://ibm.biz/Bdnmpr\n\nLearn more about Intelligent Data Extraction here → https://ibm.biz/BdnqSX\n\nAre AI agents the new assembly line for data? 🤖 Join Eric Pritchett from Terzo as he explores how LLMs, GPT models, and AI agents turn unstructured data into actionable insights. Discover how OCR, NLP, and agentic workflows reshape document intelligence and solve real-world challenges! ✨\n\nDiscover more about Terzo → https://ibm.biz/Bdnmps\n\n#ai #llm #unstructureddata #aiagents",
    "chunks": [
      "Read more about Terzo here → https://ibm.biz/Bdnmpr Learn more about Intelligent Data Extraction here → https://ibm.biz/BdnqSX Are AI agents the new assembly line for data? 🤖 Join Eric Pritchett from Terzo as he explores how LLMs, GPT models, and AI agents turn unstructured data into actionable insights. Discover how OCR, NLP, and agentic workflows reshape document intelligence and solve real-world challenges! ✨ Discover more about Terzo → https://ibm.biz/Bdnmps #ai #llm #unstructureddata",
      "#aiagents"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "7b91b8fe-7855-49dc-9803-7164076abcb5",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555073"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=F8NKVhkZZWI": {
    "title": "What are AI Agents?",
    "url": "https://www.youtube.com/watch?v=F8NKVhkZZWI",
    "description": "Want to see Maya Murad explain more about AI Agents? Click here to register for a Virtual Agents Webinar → https://ibm.biz/BdaAVa\nWant to play with the technology yourself? Explore our interactive demo → https://ibm.biz/BdKsEf\n\nIn this video, Maya Murad explores the evolution of AI agents and their pivotal role in revolutionizing AI systems. From monolithic models to compound AI systems, discover how AI agents integrate with databases and external tools to enhance problem-solving capabilities and adaptability.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://www.ibm.com/account/reg/us-en/signup?formid=news-urx-52120",
    "duration": 748,
    "uploader": "IBM Technology",
    "transcript": "Want to see Maya Murad explain more about AI Agents? Click here to register for a Virtual Agents Webinar → https://ibm.biz/BdaAVa\nWant to play with the technology yourself? Explore our interactive demo → https://ibm.biz/BdKsEf\n\nIn this video, Maya Murad explores the evolution of AI agents and their pivotal role in revolutionizing AI systems. From monolithic models to compound AI systems, discover how AI agents integrate with databases and external tools to enhance problem-solving capabilities and adaptability.\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://www.ibm.com/account/reg/us-en/signup?formid=news-urx-52120",
    "chunks": [
      "Want to see Maya Murad explain more about AI Agents? Click here to register for a Virtual Agents Webinar → https://ibm.biz/BdaAVa Want to play with the technology yourself? Explore our interactive demo → https://ibm.biz/BdKsEf In this video, Maya Murad explores the evolution of AI agents and their pivotal role in revolutionizing AI systems. From monolithic models to compound AI systems, discover how AI agents integrate with databases and external tools to enhance problem-solving capabilities and",
      "adaptability. AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://www.ibm.com/account/reg/us-en/signup?formid=news-urx-52120"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "d220add7-87ea-4e17-a2ea-2627853a3095",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555076"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=gl1r1XV0SLw": {
    "title": "What is a Vector Database? Powering Semantic Search & AI Applications",
    "url": "https://www.youtube.com/watch?v=gl1r1XV0SLw",
    "description": "Ready to become a certified Qiskit Developer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdG5uz\n\nLearn more about Vector Databases here → https://ibm.biz/BdG5uq\n\nHow can you find images with similar color palettes or landscapes? 🖼️ Martin Keen explains how vector databases use vector embeddings to bridge the semantic gap, enabling advanced semantic search. 🌐 Explore AI innovations like retrieval-augmented generation (RAG) and efficient similarity searches. 🚀\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdG5uf\n\n#vectordatabase #semanticsearch #aiapplications #machinelearning",
    "duration": 588,
    "uploader": "IBM Technology",
    "transcript": "Ready to become a certified Qiskit Developer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdG5uz\n\nLearn more about Vector Databases here → https://ibm.biz/BdG5uq\n\nHow can you find images with similar color palettes or landscapes? 🖼️ Martin Keen explains how vector databases use vector embeddings to bridge the semantic gap, enabling advanced semantic search. 🌐 Explore AI innovations like retrieval-augmented generation (RAG) and efficient similarity searches. 🚀\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdG5uf\n\n#vectordatabase #semanticsearch #aiapplications #machinelearning",
    "chunks": [
      "Ready to become a certified Qiskit Developer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdG5uz Learn more about Vector Databases here → https://ibm.biz/BdG5uq How can you find images with similar color palettes or landscapes? 🖼️ Martin Keen explains how vector databases use vector embeddings to bridge the semantic gap, enabling advanced semantic search. 🌐 Explore AI innovations like retrieval-augmented generation (RAG) and efficient similarity searches. 🚀",
      "AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdG5uf #vectordatabase #semanticsearch #aiapplications #machinelearning"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "b2622347-b4f7-46c7-a747-1b724f3c1a75",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555079"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=McLdlg5Gc9s": {
    "title": "What is vLLM? Efficient AI Inference for Large Language Models",
    "url": "https://www.youtube.com/watch?v=McLdlg5Gc9s",
    "description": "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnaVD\n\nLearn more about Large Language Models (LLMs) here → https://ibm.biz/BdnaVR\n\n💰 Struggling with a slow and expensive AI infrastructure? Cedric Clyburn explains how VLLM tackles memory fragmentation and latency in serving large language models. Learn how innovations like paged attention optimize GPU resources and accelerate inference for scalable AI solutions! 🚀\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnaVF\n\n#ai #llm #gpu #aidevelopment",
    "duration": 297,
    "uploader": "IBM Technology",
    "transcript": "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnaVD\n\nLearn more about Large Language Models (LLMs) here → https://ibm.biz/BdnaVR\n\n💰 Struggling with a slow and expensive AI infrastructure? Cedric Clyburn explains how VLLM tackles memory fragmentation and latency in serving large language models. Learn how innovations like paged attention optimize GPU resources and accelerate inference for scalable AI solutions! 🚀\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnaVF\n\n#ai #llm #gpu #aidevelopment",
    "chunks": [
      "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnaVD Learn more about Large Language Models (LLMs) here → https://ibm.biz/BdnaVR 💰 Struggling with a slow and expensive AI infrastructure? Cedric Clyburn explains how VLLM tackles memory fragmentation and latency in serving large language models. Learn how innovations like paged attention optimize GPU resources and accelerate inference for scalable AI",
      "solutions! 🚀 AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnaVF #ai #llm #gpu #aidevelopment"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "0223e155-b011-4ebd-90be-f7a35ebfca63",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555083"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=uxE8FFiu_UQ": {
    "title": "Run AI Models Locally with Ollama: Fast & Simple Deployment",
    "url": "https://www.youtube.com/watch?v=uxE8FFiu_UQ",
    "description": "Curious about running AI models locally with Ollama? Check out the code here → https://ibm.biz/BdndQU\n\nReady to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdndQ8\n\nLearn more about Large Language Models here → https://ibm.biz/BdndQg\n\nSee how to build AI-powered tools that run locally with Ollama. 🚀 Cedric Clyburn demonstrates how to maintain data privacy, integrate Langchain, and simplify development using local AI deployment. 💡 Discover how to prototype smarter and optimize tools for enterprise tasks with ease. ✨\"\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdndQh\n\n#ollama #llm #aiintegration",
    "duration": 359,
    "uploader": "IBM Technology",
    "transcript": "Curious about running AI models locally with Ollama? Check out the code here → https://ibm.biz/BdndQU\n\nReady to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdndQ8\n\nLearn more about Large Language Models here → https://ibm.biz/BdndQg\n\nSee how to build AI-powered tools that run locally with Ollama. 🚀 Cedric Clyburn demonstrates how to maintain data privacy, integrate Langchain, and simplify development using local AI deployment. 💡 Discover how to prototype smarter and optimize tools for enterprise tasks with ease. ✨\"\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdndQh\n\n#ollama #llm #aiintegration",
    "chunks": [
      "Curious about running AI models locally with Ollama? Check out the code here → https://ibm.biz/BdndQU Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdndQ8 Learn more about Large Language Models here → https://ibm.biz/BdndQg See how to build AI-powered tools that run locally with Ollama. 🚀 Cedric Clyburn demonstrates how to maintain data privacy, integrate Langchain, and simplify development using local",
      "AI deployment. 💡 Discover how to prototype smarter and optimize tools for enterprise tasks with ease. ✨\" AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdndQh #ollama #llm #aiintegration"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "a857481a-012b-451e-8294-02109c414d6d",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555086"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=2A94Mxn3jAc": {
    "title": "Securing AI Systems: Protecting Data, Models, & Usage",
    "url": "https://www.youtube.com/watch?v=2A94Mxn3jAc",
    "description": "Ready to become a certified Professional Architect? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnnHZ\n\nLearn more about AI Security here → https://ibm.biz/BdnnHY\n\n🔐 How do you secure AI systems? Jeff Crume explains that protecting data, models, and usage is critical to defending against threats like Shadow AI and prompt injection attacks. Discover how to assess risks and leverage frameworks like OWASP and MITER to strengthen AI security and governance. 🚀\n\nRead the Cost of a Data Breach report  → https://ibm.biz/BdnnH2\n\n#aisecurity #cybersecurity #aithreats",
    "duration": 560,
    "uploader": "IBM Technology",
    "transcript": "Ready to become a certified Professional Architect? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnnHZ\n\nLearn more about AI Security here → https://ibm.biz/BdnnHY\n\n🔐 How do you secure AI systems? Jeff Crume explains that protecting data, models, and usage is critical to defending against threats like Shadow AI and prompt injection attacks. Discover how to assess risks and leverage frameworks like OWASP and MITER to strengthen AI security and governance. 🚀\n\nRead the Cost of a Data Breach report  → https://ibm.biz/BdnnH2\n\n#aisecurity #cybersecurity #aithreats",
    "chunks": [
      "Ready to become a certified Professional Architect? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnnHZ Learn more about AI Security here → https://ibm.biz/BdnnHY 🔐 How do you secure AI systems? Jeff Crume explains that protecting data, models, and usage is critical to defending against threats like Shadow AI and prompt injection attacks. Discover how to assess risks and leverage frameworks like OWASP and MITER to strengthen AI security and governance. 🚀 Read",
      "the Cost of a Data Breach report → https://ibm.biz/BdnnH2 #aisecurity #cybersecurity #aithreats"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "f3b253e7-4082-45c1-8b00-0d2168aa76b1",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555089"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=a1M_thDTqmU": {
    "title": "Virtual Machine (VM) vs Docker",
    "url": "https://www.youtube.com/watch?v=a1M_thDTqmU",
    "description": "Learn more about Docker → https://ibm.biz/BdPg33\nLearn more about Virtual Machines → https://ibm.biz/BdPg3T\n\nIs Docker just a lightweight virtual machine? It's true that both have one thing in common, namely virtualization, but there are significant differences that you will need to understand in order to pick the right one for your requirements. In this video, Martin Keen explains the ways that Docker and virtual machines are similar as well as their main differences. He also covers their relative strengths and ends by offering recommendations on criteria that will help you choose which is best for your project.\n\nGet started for free on IBM Cloud → https://ibm.biz/sign-up-now\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now",
    "duration": 531,
    "uploader": "IBM Technology",
    "transcript": "Learn more about Docker → https://ibm.biz/BdPg33\nLearn more about Virtual Machines → https://ibm.biz/BdPg3T\n\nIs Docker just a lightweight virtual machine? It's true that both have one thing in common, namely virtualization, but there are significant differences that you will need to understand in order to pick the right one for your requirements. In this video, Martin Keen explains the ways that Docker and virtual machines are similar as well as their main differences. He also covers their relative strengths and ends by offering recommendations on criteria that will help you choose which is best for your project.\n\nGet started for free on IBM Cloud → https://ibm.biz/sign-up-now\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now",
    "chunks": [
      "Learn more about Docker → https://ibm.biz/BdPg33 Learn more about Virtual Machines → https://ibm.biz/BdPg3T Is Docker just a lightweight virtual machine? It's true that both have one thing in common, namely virtualization, but there are significant differences that you will need to understand in order to pick the right one for your requirements. In this video, Martin Keen explains the ways that Docker and virtual machines are similar as well as their main differences. He also covers their",
      "relative strengths and ends by offering recommendations on criteria that will help you choose which is best for your project. Get started for free on IBM Cloud → https://ibm.biz/sign-up-now Subscribe to see more videos like this in the future → http://ibm.biz/subscribe-now"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "b11c09ea-cb43-4687-9b34-337f167da752",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555092"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=7j1t3UZA1TY": {
    "title": "MCP vs API: Simplifying AI Agent Integration with External Data",
    "url": "https://www.youtube.com/watch?v=7j1t3UZA1TY",
    "description": "Ready to become a certified Solution Implementer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnPFQ\n\nLearn more about API here → https://ibm.biz/BdnPFT\n\nMCP or API: Which transforms AI integration? Martin Keen explains how the Model Context Protocol (MCP) revolutionizes AI agents by enabling dynamic discovery, tool execution, and seamless external data retrieval. Discover how MCP simplifies LLM workflows and outpaces traditional APIs. 🚀\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnPFw\n\n#modelcontextprotocol #api #aiintegration",
    "duration": 790,
    "uploader": "IBM Technology",
    "transcript": "Ready to become a certified Solution Implementer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnPFQ\n\nLearn more about API here → https://ibm.biz/BdnPFT\n\nMCP or API: Which transforms AI integration? Martin Keen explains how the Model Context Protocol (MCP) revolutionizes AI agents by enabling dynamic discovery, tool execution, and seamless external data retrieval. Discover how MCP simplifies LLM workflows and outpaces traditional APIs. 🚀\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnPFw\n\n#modelcontextprotocol #api #aiintegration",
    "chunks": [
      "Ready to become a certified Solution Implementer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnPFQ Learn more about API here → https://ibm.biz/BdnPFT MCP or API: Which transforms AI integration? Martin Keen explains how the Model Context Protocol (MCP) revolutionizes AI agents by enabling dynamic discovery, tool execution, and seamless external data retrieval. Discover how MCP simplifies LLM workflows and outpaces traditional APIs. 🚀 AI news moves fast.",
      "Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnPFw #modelcontextprotocol #api #aiintegration"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "3bdaf15e-fb32-4b36-a25e-5edce5ec3629",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555095"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=eur8dUO9mvE": {
    "title": "What is MCP? Integrate AI Agents with Databases & APIs",
    "url": "https://www.youtube.com/watch?v=eur8dUO9mvE",
    "description": "Ready to become a certified Architect on Cloud Pak? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdGhLq\n\nLearn more about AI Agents here → https://ibm.biz/BdGhLf\n\nUnlock the secrets of MCP! 🚀 Dive into the world of Model Context Protocol and learn how to seamlessly connect AI agents to databases, APIs, and more. Roy Derks breaks down its components, from hosts to servers, and showcases real-world applications. Gain the knowledge to revolutionize your AI projects! 🧠\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdGhLP\n\n#aiagents #dataintegration #api",
    "duration": 226,
    "uploader": "IBM Technology",
    "transcript": "Ready to become a certified Architect on Cloud Pak? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdGhLq\n\nLearn more about AI Agents here → https://ibm.biz/BdGhLf\n\nUnlock the secrets of MCP! 🚀 Dive into the world of Model Context Protocol and learn how to seamlessly connect AI agents to databases, APIs, and more. Roy Derks breaks down its components, from hosts to servers, and showcases real-world applications. Gain the knowledge to revolutionize your AI projects! 🧠\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdGhLP\n\n#aiagents #dataintegration #api",
    "chunks": [
      "Ready to become a certified Architect on Cloud Pak? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdGhLq Learn more about AI Agents here → https://ibm.biz/BdGhLf Unlock the secrets of MCP! 🚀 Dive into the world of Model Context Protocol and learn how to seamlessly connect AI agents to databases, APIs, and more. Roy Derks breaks down its components, from hosts to servers, and showcases real-world applications. Gain the knowledge to revolutionize your AI",
      "projects! 🧠 AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdGhLP #aiagents #dataintegration #api"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "95149c07-9af8-4f25-9759-adb7b7f189c9",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555098"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=1bUy-1hGZpI": {
    "title": "What is LangChain?",
    "url": "https://www.youtube.com/watch?v=1bUy-1hGZpI",
    "description": "Learn about IBM watsonx→ https://ibm.biz/BdvkK8\n\nLangChain became immensely popular when it was launched in 2022, but how can it impact your development and application of AI models, Large Language Models (LLM) in particular. In this video Martin Keen shares an overview of the features and uses of LangChain.\n\nGet started for free on IBM Cloud → https://ibm.biz/sign-up-now\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now",
    "duration": 487,
    "uploader": "IBM Technology",
    "transcript": "Learn about IBM watsonx→ https://ibm.biz/BdvkK8\n\nLangChain became immensely popular when it was launched in 2022, but how can it impact your development and application of AI models, Large Language Models (LLM) in particular. In this video Martin Keen shares an overview of the features and uses of LangChain.\n\nGet started for free on IBM Cloud → https://ibm.biz/sign-up-now\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now",
    "chunks": [
      "Learn about IBM watsonx→ https://ibm.biz/BdvkK8\n\nLangChain became immensely popular when it was launched in 2022, but how can it impact your development and application of AI models, Large Language Models (LLM) in particular. In this video Martin Keen shares an overview of the features and uses of LangChain.\n\nGet started for free on IBM Cloud → https://ibm.biz/sign-up-now\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now"
    ],
    "chunk_count": 1,
    "processing_method": "alternative_extraction",
    "content_id": "4f414d51-c286-4527-bb31-13c2c9e0b9f5",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555101"
    },
    "quality_score": 0.4,
    "tags": []
  },
  "https://www.youtube.com/watch?v=eeOANluSqAE": {
    "title": "Your API is not an MCP | DEMFP786",
    "url": "https://www.youtube.com/watch?v=eeOANluSqAE",
    "description": "This is one of many innovative Featured Partner sessions from Microsoft Build 2025.  View more information from this partner at https://aka.ms/Build25_Neon\n\nA lot of companies have jumped on the MCP bandwagon by quickly auto-generating their MCP servers from their APIs. This isn’t right as MCP servers are made to be consumed by LLMs and not by humans. Join us to better understand why MCPs have to be thought of carefully. Experience examples of tools that make sense for agents but not for humans, and why you should be careful designing your MCP set of tools and resources.\n\nTo learn more, please check out these resources:\n * https://neon.tech/msbuild\n\n\n𝗦𝗽𝗲𝗮𝗸𝗲𝗿𝘀:\n * David Gomes\n\n\n𝗦𝗲𝘀𝘀𝗶𝗼𝗻 𝗜𝗻𝗳𝗼𝗿𝗺𝗮𝘁𝗶𝗼𝗻:\nThis is one of many sessions from the Microsoft Build 2025 event. View even more sessions on-demand and learn about Microsoft Build at https://build.microsoft.com\n\nDEMFP786 | English (US) | AI, Copilot & Agents\n\n\n#MSBuild, #AI",
    "duration": 883,
    "uploader": "Microsoft Developer",
    "transcript": "This is one of many innovative Featured Partner sessions from Microsoft Build 2025.  View more information from this partner at https://aka.ms/Build25_Neon\n\nA lot of companies have jumped on the MCP bandwagon by quickly auto-generating their MCP servers from their APIs. This isn’t right as MCP servers are made to be consumed by LLMs and not by humans. Join us to better understand why MCPs have to be thought of carefully. Experience examples of tools that make sense for agents but not for humans, and why you should be careful designing your MCP set of tools and resources.\n\nTo learn more, please check out these resources:\n * https://neon.tech/msbuild\n\n\n𝗦𝗽𝗲𝗮𝗸𝗲𝗿𝘀:\n * David Gomes\n\n\n𝗦𝗲𝘀𝘀𝗶𝗼𝗻 𝗜𝗻𝗳𝗼𝗿𝗺𝗮𝘁𝗶𝗼𝗻:\nThis is one of many sessions from the Microsoft Build 2025 event. View even more sessions on-demand and learn about Microsoft Build at https://build.microsoft.com\n\nDEMFP786 | English (US) | AI, Copilot & Agents\n\n\n#MSBuild, #AI",
    "chunks": [
      "This is one of many innovative Featured Partner sessions from Microsoft Build 2025. View more information from this partner at https://aka.ms/Build25_Neon A lot of companies have jumped on the MCP bandwagon by quickly auto-generating their MCP servers from their APIs. This isn’t right as MCP servers are made to be consumed by LLMs and not by humans. Join us to better understand why MCPs have to be thought of carefully. Experience examples of tools that make sense for agents but not for humans,",
      "and why you should be careful designing your MCP set of tools and resources. To learn more, please check out these resources: * https://neon.tech/msbuild 𝗦𝗽𝗲𝗮𝗸𝗲𝗿𝘀: * David Gomes 𝗦𝗲𝘀𝘀𝗶𝗼𝗻 𝗜𝗻𝗳𝗼𝗿𝗺𝗮𝘁𝗶𝗼𝗻: This is one of many sessions from the Microsoft Build 2025 event. View even more sessions on-demand and learn about Microsoft Build at https://build.microsoft.com DEMFP786 | English (US) | AI, Copilot & Agents #MSBuild, #AI"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "25b70994-530a-40e7-9586-febff324f9fd",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555104"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=EDb37y_MhRw": {
    "title": "Generative vs Agentic AI: Shaping the Future of AI Collaboration",
    "url": "https://www.youtube.com/watch?v=EDb37y_MhRw",
    "description": "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdndTY\n\nLearn more about Generative AI vs. Agentic AI here → https://ibm.biz/BdndT2\n\nWhat’s the difference between generative AI and agentic AI? 🤔 Martin Keen explains how generative AI powers content creation and image generation, while agentic AI uses LLMs and chain of thought reasoning for proactive tasks like personal shopping and conference planning. Discover the future of intelligent AI collaboration! 🚀\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdndTz\n\n#generativeai #agenticai #ai #llm",
    "duration": 438,
    "uploader": "IBM Technology",
    "transcript": "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdndTY\n\nLearn more about Generative AI vs. Agentic AI here → https://ibm.biz/BdndT2\n\nWhat’s the difference between generative AI and agentic AI? 🤔 Martin Keen explains how generative AI powers content creation and image generation, while agentic AI uses LLMs and chain of thought reasoning for proactive tasks like personal shopping and conference planning. Discover the future of intelligent AI collaboration! 🚀\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdndTz\n\n#generativeai #agenticai #ai #llm",
    "chunks": [
      "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdndTY Learn more about Generative AI vs. Agentic AI here → https://ibm.biz/BdndT2 What’s the difference between generative AI and agentic AI? 🤔 Martin Keen explains how generative AI powers content creation and image generation, while agentic AI uses LLMs and chain of thought reasoning for proactive tasks like personal shopping and conference planning.",
      "Discover the future of intelligent AI collaboration! 🚀 AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdndTz #generativeai #agenticai #ai #llm"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "22158d0d-daab-4518-9967-6de9ca658f1a",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555107"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=-ovM0daP6bw": {
    "title": "AI vs Human Thinking: How Large Language Models Really Work",
    "url": "https://www.youtube.com/watch?v=-ovM0daP6bw",
    "description": "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnkLR\n\nLearn more about AI Model here → https://ibm.biz/BdnkLX\n\nAre AI models truly \"thinking,\" or just mimicking humans? 🤔 Master Inventor Martin Keen explores how Large Language Models (LLMs) stack up against the human brain in learning, reasoning, and memory. Uncover the surprising ways humans and AI differ in processing information and making errors! 💡\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnkLH\n\n#ai #llm #humanbrain",
    "duration": 731,
    "uploader": "IBM Technology",
    "transcript": "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnkLR\n\nLearn more about AI Model here → https://ibm.biz/BdnkLX\n\nAre AI models truly \"thinking,\" or just mimicking humans? 🤔 Master Inventor Martin Keen explores how Large Language Models (LLMs) stack up against the human brain in learning, reasoning, and memory. Uncover the surprising ways humans and AI differ in processing information and making errors! 💡\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnkLH\n\n#ai #llm #humanbrain",
    "chunks": [
      "Ready to become a certified watsonx AI Assistant Engineer? Register now and use code IBMTechYT20 for 20% off of your exam → https://ibm.biz/BdnkLR Learn more about AI Model here → https://ibm.biz/BdnkLX Are AI models truly \"thinking,\" or just mimicking humans? 🤔 Master Inventor Martin Keen explores how Large Language Models (LLMs) stack up against the human brain in learning, reasoning, and memory. Uncover the surprising ways humans and AI differ in processing information and making errors! 💡 AI",
      "news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdnkLH #ai #llm #humanbrain"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "38c52487-8c75-4d08-bbcd-9dad37ccac23",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555110"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=XFZ-rQ8eeR8": {
    "title": "The 7 Types of AI - And Why We Talk (Mostly) About 3 of Them",
    "url": "https://www.youtube.com/watch?v=XFZ-rQ8eeR8",
    "description": "Ready to start your career in AI? Begin with this certificate → https://ibm.biz/BdKU73\nLearn about how to build and deploy AI with watsonx→ https://ibm.biz/BdSfxY\n\nResearchers have classified AI into seven categories; you may be disappointed to learn that we've only realized three of them so far! In this video, Master Inventor Martin Keen lays them out, from narrow AI we know and enjoy today to the other extreme, super AI, which may have superior emotional and intellectual intelligence than humans... someday (?).\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdKU7k",
    "duration": 410,
    "uploader": "IBM Technology",
    "transcript": "Ready to start your career in AI? Begin with this certificate → https://ibm.biz/BdKU73\nLearn about how to build and deploy AI with watsonx→ https://ibm.biz/BdSfxY\n\nResearchers have classified AI into seven categories; you may be disappointed to learn that we've only realized three of them so far! In this video, Master Inventor Martin Keen lays them out, from narrow AI we know and enjoy today to the other extreme, super AI, which may have superior emotional and intellectual intelligence than humans... someday (?).\n\nAI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdKU7k",
    "chunks": [
      "Ready to start your career in AI? Begin with this certificate → https://ibm.biz/BdKU73 Learn about how to build and deploy AI with watsonx→ https://ibm.biz/BdSfxY Researchers have classified AI into seven categories; you may be disappointed to learn that we've only realized three of them so far! In this video, Master Inventor Martin Keen lays them out, from narrow AI we know and enjoy today to the other extreme, super AI, which may have superior emotional and intellectual intelligence than",
      "humans... someday (?). AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM → https://ibm.biz/BdKU7k"
    ],
    "chunk_count": 2,
    "processing_method": "alternative_extraction",
    "content_id": "99cdbc2a-b902-47db-a7f3-68afb815d64d",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555113"
    },
    "quality_score": 0.6,
    "tags": []
  },
  "https://www.youtube.com/watch?v=jq_LZ1RFPfU": {
    "title": "Cybersecurity Architecture: Five Principles to Follow (and One to Avoid)",
    "url": "https://www.youtube.com/watch?v=jq_LZ1RFPfU",
    "description": "IBM Security QRadar EDR : https://ibm.biz/Bdyd7k\n\nIBM Security X-Force Threat Intelligence Index 2023: https://ibm.biz/Bdyd76\n\nThis ten part video series is based on a 400 level class on Enterprise Cybersecurity Architecture  taught by Jeff \"the Security Guy\" Crume at a local university. He'll begin by explaining the foundational principles of cybersecurity - five that are best practices and one that's just the opposite. The subsequent episodes will look at the various domains that apply to a well-designed security architecture. Be sure to subscribe so you'll get notified of the next video!\n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-up\n\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume #ibm \n\nChapters\n00:00 Principles Introduction\n01:05 Defense in Depth\n04:20 Least Privilege\n07:55 Separation of Duties\n09:50 Secure by Design\n12:15 Keep It Simple, Stupid (KISS)\n14:43 Security by Obscurity",
    "duration": 1054,
    "uploader": "IBM Technology",
    "transcript": "IBM Security QRadar EDR : https://ibm.biz/Bdyd7k\n\nIBM Security X-Force Threat Intelligence Index 2023: https://ibm.biz/Bdyd76\n\nThis ten part video series is based on a 400 level class on Enterprise Cybersecurity Architecture  taught by Jeff \"the Security Guy\" Crume at a local university. He'll begin by explaining the foundational principles of cybersecurity - five that are best practices and one that's just the opposite. The subsequent episodes will look at the various domains that apply to a well-designed security architecture. Be sure to subscribe so you'll get notified of the next video!\n\nGet started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-up\n\nSubscribe to see more videos like this in the future → http://ibm.biz/subscribe-now\n\n#AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume #ibm \n\nChapters\n00:00 Principles Introduction\n01:05 Defense in Depth\n04:20 Least Privilege\n07:55 Separation of Duties\n09:50 Secure by Design\n12:15 Keep It Simple, Stupid (KISS)\n14:43 Security by Obscurity",
    "chunks": [
      "IBM Security QRadar EDR : https://ibm.biz/Bdyd7k IBM Security X-Force Threat Intelligence Index 2023: https://ibm.biz/Bdyd76 This ten part video series is based on a 400 level class on Enterprise Cybersecurity Architecture taught by Jeff \"the Security Guy\" Crume at a local university. He'll begin by explaining the foundational principles of cybersecurity - five that are best practices and one that's just the opposite. The subsequent episodes will look at the various domains that apply to a",
      "well-designed security architecture. Be sure to subscribe so you'll get notified of the next video! Get started for free on IBM Cloud → https://ibm.biz/ibm-cloud-sign-up Subscribe to see more videos like this in the future → http://ibm.biz/subscribe-now #AI #Software #ITModernization #Cybersecurity #QRadar #JeffCrume #ibm Chapters 00:00 Principles Introduction 01:05 Defense in Depth 04:20 Least Privilege 07:55 Separation of Duties 09:50 Secure by Design 12:15 Keep It Simple, Stupid (KISS) 14:43",
      "Security by Obscurity"
    ],
    "chunk_count": 3,
    "processing_method": "alternative_extraction",
    "content_id": "f175a284-b04a-4d20-942b-dc5d490b76b6",
    "embeddings": null,
    "processing_metadata": {
      "method": "legacy",
      "version": "1.0.0",
      "timestamp": "2025-07-17T00:46:39.555116"
    },
    "quality_score": 0.8,
    "tags": []
  }
}